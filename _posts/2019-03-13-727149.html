---
layout: default
title: Object Detection(目标检测论文、代码资源整合)
---

{{ "%3Cdiv+id%3D%22article_content%22+class%3D%22article_content+clearfix+csdn-tracking-statistics%22+data-pid%3D%22blog%22+data-mod%3D%22popu_307%22+data-dsm%3D%22post%22%3E+%0A+%3Clink+rel%3D%22stylesheet%22+href%3D%22https%3A%2F%2Fcsdnimg.cn%2Frelease%2Fphoenix%2Ftemplate%2Fcss%2Fck_htmledit_views-f57960eb32.css%22%3E+%0A+%3Cdiv+id%3D%22content_views%22+class%3D%22markdown_views+prism-atom-one-dark%22%3E+%0A++%3C%21--+flowchart+%E7%AE%AD%E5%A4%B4%E5%9B%BE%E6%A0%87+%E5%8B%BF%E5%88%A0+--%3E+%0A++%3Csvg+xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22+style%3D%22display%3A+none%3B%22%3E%0A+++%3Cpath+stroke-linecap%3D%22round%22+d%3D%22M5%2C0+0%2C2.5+5%2C5z%22+id%3D%22raphael-marker-block%22+style%3D%22-webkit-tap-highlight-color%3A+rgba%280%2C+0%2C+0%2C+0%29%3B%22%3E%3C%2Fpath%3E%0A++%3C%2Fsvg%3E+%0A++%3Carticle+class%3D%22baidu_pl%22%3E+%0A+++%3Cdiv+id%3D%22article_content%22+class%3D%22article_content+clearfix+csdn-tracking-statistics%22%3E+%0A++++%3Cdiv+id%3D%22content_views%22+class%3D%22markdown_views+prism-atom-one-dark%22%3E+%0A+++++%3C%21--+flowchart+%26%2331661%3B%26%2322836%3B%26%2322270%3B%26%2326631%3B+%26%2321247%3B%26%2321024%3B+--%3E+%0A+++++%3Csvg+xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A++++++%3Cpath+stroke-linecap%3D%22round%22+d%3D%22M5%2C0+0%2C2.5+5%2C5z%22+id%3D%22raphael-marker-block%22%3E%3C%2Fpath%3E%0A+++++%3C%2Fsvg%3E+%0A+++++%3Cdiv+id%3D%22article_content%22+class%3D%22article_content+clearfix+csdn-tracking-statistics%22%3E+%0A++++++%3Cdiv+id%3D%22content_views%22+class%3D%22markdown_views+prism-atom-one-dark%22%3E+%0A+++++++%3C%21--+flowchart+%26amp%3B%2331661%3B%26amp%3B%2322836%3B%26amp%3B%2322270%3B%26amp%3B%2326631%3B+%26amp%3B%2321247%3B%26amp%3B%2321024%3B+--%3E+%0A+++++++%3Cpath+stroke-linecap%3D%22round%22+d%3D%22M5%2C0+0%2C2.5+5%2C5z%22+id%3D%22raphael-marker-block%22%3E%3C%2Fpath%3E+%0A+++++++%3Cp%3E%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%A5%9E%E6%96%87%EF%BC%8C%E9%9D%9E%E5%B8%B8%E5%85%A8%E8%80%8C%E4%B8%94%E6%8C%81%E7%BB%AD%E5%9C%A8%E6%9B%B4%E6%96%B0%E3%80%82%E8%BD%AC%E5%8F%91%E8%87%AA%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Fhandong1587.github.io%2Fdeep_learning%2F2015%2F10%2F09%2Fobject-detection.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fhandong1587.github.io%2Fdeep_learning%2F2015%2F10%2F09%2Fobject-detection.html+%3C%2Fa%3E%EF%BC%8C%E5%A6%82%E6%9C%89%E4%BE%B5%E6%9D%83%E8%81%94%E7%B3%BB%E5%88%A0%E9%99%A4%E3%80%82%3Cbr%3E+%3Cbr%3Ehttps%3A%2F%2Fblog.csdn.net%2Falphonse2017%2Farticle%2Fdetails%2F85103295%EF%BC%8C+%3C%2Fp%3E+%0A+++++++%3Cp%3E%E6%88%91%E4%BC%9A%E8%B7%9F%E8%BF%9B%E5%8E%9F%E4%BD%9C%E8%80%85%E5%8D%9A%E5%AE%A2%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%8C%E5%8A%A0%E5%85%A5%E8%87%AA%E5%B7%B1%E5%AF%B9%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%96%B0%E7%A0%94%E7%A9%B6%E5%8F%8A%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E3%80%82%E5%8D%9A%E5%AE%A2%E6%A0%B9%E6%8D%AE%E9%9C%80%E6%B1%82%E7%9B%B4%E6%8E%A5%E8%BF%9B%E8%A1%8C%E5%85%B3%E9%94%AE%E5%AD%97%E6%90%9C%E7%B4%A2%EF%BC%8C%E4%BE%8B%E5%A6%822018%EF%BC%8C%E5%8F%AF%E6%89%BE%E5%88%B0%E6%9C%80%E6%96%B0%E8%AE%BA%E6%96%87%E3%80%82%3C%2Fp%3E+%0A+++++++%3Cp%3E%3C%2Fp%3E%0A+++++++%3Cdiv+class%3D%22toc%22%3E%0A++++++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22%22%3E%3C%2Fa%3E%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95%3C%2Fh3%3E%0A++++++++%3Cul%3E%0A+++++++++%3Cli%3E%3Ca+href%3D%22%23Papers_39%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPapers%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++%3Cul%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Neural_Networks_for_Object_Detection_45%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Neural+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23OverFeat_Integrated_Recognition_Localization_and_Detection_using_Convolutional_Networks_48%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOverFeat%3A+Integrated+Recognition%2C+Localization+and+Detection+using+Convolutional+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23RCNN_55%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation_56%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERich+feature+hierarchies+for+accurate+object+detection+and+semantic+segmentation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Fast_RCNN_70%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Fast_RCNN_71%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23AFastRCNN_Hard_Positive_Generation_via_Adversary_for_Object_Detection_84%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA-Fast-RCNN%3A+Hard+Positive+Generation+via+Adversary+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Faster_RCNN_91%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaster+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Faster_RCNN_Towards_RealTime_Object_Detection_with_Region_Proposal_Networks_92%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaster+R-CNN%3A+Towards+Real-Time+Object+Detection+with+Region+Proposal+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23RCNN_minus_R_111%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER-CNN+minus+R%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Faster_RCNN_in_MXNet_with_distributed_implementation_and_data_parallelization_116%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaster+R-CNN+in+MXNet+with+distributed+implementation+and+data+parallelization%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Contextual_Priming_and_Feedback_for_Faster_RCNN_120%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContextual+Priming+and+Feedback+for+Faster+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23An_Implementation_of_Faster_RCNN_with_Study_for_Region_Sampling_126%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAn+Implementation+of+Faster+RCNN+with+Study+for+Region+Sampling%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Interpretable_RCNN_132%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EInterpretable+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23LightHead_RCNN_140%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELight-Head+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23LightHead_RCNN_In_Defense_of_TwoStage_Object_Detector_141%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELight-Head+R-CNN%3A+In+Defense+of+Two-Stage+Object+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Cascade_RCNN_Delving_into_High_Quality_Object_Detection_149%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECascade+R-CNN%3A+Delving+into+High+Quality+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23MultiBox_157%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMultiBox%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Scalable_Object_Detection_using_Deep_Neural_Networks_158%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScalable+Object+Detection+using+Deep+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Scalable_HighQuality_Object_Detection_165%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScalable%2C+High-Quality+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23SPPNet_173%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESPP-Net%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Spatial_Pyramid_Pooling_in_Deep_Convolutional_Networks_for_Visual_Recognition_174%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESpatial+Pyramid+Pooling+in+Deep+Convolutional+Networks+for+Visual+Recognition%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DeepIDNet_Deformable_Deep_Convolutional_Neural_Networks_for_Object_Detection_181%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepID-Net%3A+Deformable+Deep+Convolutional+Neural+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detectors_Emerge_in_Deep_Scene_CNNs_188%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detectors+Emerge+in+Deep+Scene+CNNs%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23segDeepM_Exploiting_Segmentation_and_Context_in_Deep_Neural_Networks_for_Object_Detection_196%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EsegDeepM%3A+Exploiting+Segmentation+and+Context+in+Deep+Neural+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_Networks_on_Convolutional_Feature_Maps_203%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+Networks+on+Convolutional+Feature+Maps%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Improving_Object_Detection_with_Deep_Convolutional_Networks_via_Bayesian_Optimization_and_Structured_Prediction_209%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EImproving+Object+Detection+with+Deep+Convolutional+Networks+via+Bayesian+Optimization+and+Structured+Prediction%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DeepBox_Learning_Objectness_with_Convolutional_Networks_215%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepBox%3A+Learning+Objectness+with+Convolutional+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23MRCNN_223%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMR-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_detection_via_a_multiregion__semantic_segmentationaware_CNN_model_224%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+detection+via+a+multi-region+%26amp%3B+semantic+segmentation-aware+CNN+model%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23YOLO_234%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLO%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23You_Only_Look_Once_Unified_RealTime_Object_Detection_235%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYou+Only+Look+Once%3A+Unified%2C+Real-Time+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23darkflow__translate_darknet_to_tensorflow_Load_trained_weights_retrainfinetune_them_using_tensorflow_export_constant_graph_def_to_C_252%22+rel%3D%22nofollow%22+target%3D%22_self%22%3Edarkflow+-+translate+darknet+to+tensorflow.+Load+trained+weights%2C+retrain%2Ffine-tune+them+using+tensorflow%2C+export+constant+graph+def+to+C%2B%2B%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Start_Training_YOLO_with_Our_Own_Data_257%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EStart+Training+YOLO+with+Our+Own+Data%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23YOLO_Core_ML_versus_MPSNNGraph_263%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLO%3A+Core+ML+versus+MPSNNGraph%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23TensorFlow_YOLO_object_detection_on_Android_269%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETensorFlow+YOLO+object+detection+on+Android%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Computer_Vision_in_iOS__Object_Detection_274%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EComputer+Vision+in+iOS+%E2%80%93+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23YOLOv2_280%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLOv2%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23YOLO9000_Better_Faster_Stronger_281%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLO9000%3A+Better%2C+Faster%2C+Stronger%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23darknet_scripts_293%22+rel%3D%22nofollow%22+target%3D%22_self%22%3Edarknet_scripts%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Yolo_mark_GUI_for_marking_bounded_boxes_of_objects_in_images_for_training_Yolo_v2_298%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYolo_mark%3A+GUI+for+marking+bounded+boxes+of+objects+in+images+for+training+Yolo+v2%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23LightNet_Bringing_pjreddies_DarkNet_out_of_the_shadows_302%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELightNet%3A+Bringing+pjreddie%E2%80%99s+DarkNet+out+of+the+shadows%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23YOLO_v2_Bounding_Box_Tool_306%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLO+v2+Bounding+Box+Tool%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23YOLOv3_312%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLOv3%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23YOLOv3_An_Incremental_Improvement_313%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLOv3%3A+An+Incremental+Improvement%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23YOLOLITE_A_RealTime_Object_Detection_Algorithm_Optimized_for_NonGPU_Computers_318%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLO-LITE%3A+A+Real-Time+Object+Detection+Algorithm+Optimized+for+Non-GPU+Computers%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23AttentionNet_Aggregating_Weak_Directions_for_Accurate_Object_Detection_323%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAttentionNet%3A+Aggregating+Weak+Directions+for+Accurate+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23DenseBox_333%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDenseBox%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DenseBox_Unifying_Landmark_Localization_with_End_to_End_Object_Detection_334%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDenseBox%3A+Unifying+Landmark+Localization+with+End+to+End+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23SSD_341%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESSD%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SSD_Single_Shot_MultiBox_Detector_342%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESSD%3A+Single+Shot+MultiBox+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23DSSD_360%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDSSD%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DSSD__Deconvolutional_Single_Shot_Detector_361%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDSSD+%3A+Deconvolutional+Single+Shot+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Enhancement_of_SSD_by_concatenating_feature_maps_for_object_detection_369%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEnhancement+of+SSD+by+concatenating+feature+maps+for+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Contextaware_SingleShot_Detector_374%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContext-aware+Single-Shot+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23FeatureFused_SSD_Fast_Detection_for_Small_Objects_379%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFeature-Fused+SSD%3A+Fast+Detection+for+Small+Objects%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23FSSD_384%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFSSD%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23FSSD_Feature_Fusion_Single_Shot_Multibox_Detector_385%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFSSD%3A+Feature+Fusion+Single+Shot+Multibox+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Weaving_Multiscale_Context_for_Single_Shot_Detector_389%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeaving+Multi-scale+Context+for+Single+Shot+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23ESSD_396%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EESSD%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Extend_the_shallow_part_of_Single_Shot_MultiBox_Detector_via_Convolutional_Neural_Network_397%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EExtend+the+shallow+part+of+Single+Shot+MultiBox+Detector+via+Convolutional+Neural+Network%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Tiny_SSD_A_Tiny_Singleshot_Detection_Deep_Convolutional_Neural_Network_for_Realtime_Embedded_Object_Detection_401%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETiny+SSD%3A+A+Tiny+Single-shot+Detection+Deep+Convolutional+Neural+Network+for+Real-time+Embedded+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23MDSSD_Multiscale_Deconvolutional_Single_Shot_Detector_for_small_objects_405%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMDSSD%3A+Multi-scale+Deconvolutional+Single+Shot+Detector+for+small+objects%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23InsideOutside_Net_ION_411%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EInside-Outside+Net+%28ION%29%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23InsideOutside_Net_Detecting_Objects_in_Context_with_Skip_Pooling_and_Recurrent_Neural_Networks_412%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EInside-Outside+Net%3A+Detecting+Objects+in+Context+with+Skip+Pooling+and+Recurrent+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Adaptive_Object_Detection_Using_Adjacency_and_Zoom_Prediction_419%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdaptive+Object+Detection+Using+Adjacency+and+Zoom+Prediction%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23GCNN_an_Iterative_Grid_Based_Object_Detector_426%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EG-CNN%3A+an+Iterative+Grid+Based+Object+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Factors_in_Finetuning_Deep_Model_for_object_detection_431%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFactors+in+Finetuning+Deep+Model+for+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Factors_in_Finetuning_Deep_Model_for_Object_Detection_with_Longtail_Distribution_432%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFactors+in+Finetuning+Deep+Model+for+Object+Detection+with+Long-tail+Distribution%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23We_dont_need_no_boundingboxes_Training_object_class_detectors_using_only_human_verification_438%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWe+don%E2%80%99t+need+no+bounding-boxes%3A+Training+object+class+detectors+using+only+human+verification%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23HyperNet_Towards_Accurate_Region_Proposal_Generation_and_Joint_Object_Detection_443%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHyperNet%3A+Towards+Accurate+Region+Proposal+Generation+and+Joint+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_MultiPath_Network_for_Object_Detection_448%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+MultiPath+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23CRAFT_455%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECRAFT%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23CRAFT_Objects_from_Images_456%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECRAFT+Objects+from+Images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23OHEM_465%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOHEM%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Training_Regionbased_Object_Detectors_with_Online_Hard_Example_Mining_466%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETraining+Region-based+Object+Detectors+with+Online+Hard+Example+Mining%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SOHEM_Stratified_Online_Hard_Example_Mining_for_Object_Detection_474%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ES-OHEM%3A+Stratified+Online+Hard+Example+Mining+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Exploit_All_the_Layers_Fast_and_Accurate_CNN_Object_Detector_with_Scale_Dependent_Pooling_and_Cascaded_Rejection_Classifiers_478%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EExploit+All+the+Layers%3A+Fast+and+Accurate+CNN+Object+Detector+with+Scale+Dependent+Pooling+and+Cascaded+Rejection+Classifiers%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23RFCN_485%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER-FCN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23RFCN_Object_Detection_via_Regionbased_Fully_Convolutional_Networks_486%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER-FCN%3A+Object+Detection+via+Region-based+Fully+Convolutional+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23RFCN3000_at_30fps_Decoupling_Detection_and_Classification_495%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER-FCN-3000+at+30fps%3A+Decoupling+Detection+and+Classification%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Recycle_deep_features_for_better_object_detection_499%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERecycle+deep+features+for+better+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23MSCNN_504%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMS-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_Unified_Multiscale_Deep_Convolutional_Neural_Network_for_Fast_Object_Detection_505%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Unified+Multi-scale+Deep+Convolutional+Neural+Network+for+Fast+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Multistage_Object_Detection_with_Group_Recursive_Learning_513%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-stage+Object+Detection+with+Group+Recursive+Learning%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Subcategoryaware_Convolutional_Neural_Networks_for_Object_Proposals_and_Detection_518%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESubcategory-aware+Convolutional+Neural+Networks+for+Object+Proposals+and+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23PVANET_525%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPVANET%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23PVANet_Lightweight_Deep_Neural_Networks_for_Realtime_Object_Detection_526%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPVANet%3A+Lightweight+Deep+Neural+Networks+for+Real-time+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23GBDNet_534%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGBD-Net%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Gated_Bidirectional_CNN_for_Object_Detection_535%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGated+Bi-directional+CNN+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Crafting_GBDNet_for_Object_Detection_541%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECrafting+GBD-Net+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23StuffNet_Using_Stuff_to_Improve_Object_Detection_549%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EStuffNet%3A+Using+%E2%80%98Stuff%E2%80%99+to+Improve+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Generalized_Haar_Filter_based_Deep_Networks_for_RealTime_Object_Detection_in_Traffic_Scene_554%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGeneralized+Haar+Filter+based+Deep+Networks+for+Real-Time+Object+Detection+in+Traffic+Scene%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Hierarchical_Object_Detection_with_Deep_Reinforcement_Learning_559%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHierarchical+Object+Detection+with+Deep+Reinforcement+Learning%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_to_detect_and_localize_many_objects_from_few_examples_569%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+to+detect+and+localize+many+objects+from+few+examples%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Speedaccuracy_tradeoffs_for_modern_convolutional_object_detectors_574%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESpeed%2Faccuracy+trade-offs+for+modern+convolutional+object+detectors%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SqueezeDet_Unified_Small_Low_Power_Fully_Convolutional_Neural_Networks_for_RealTime_Object_Detection_for_Autonomous_Driving_580%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESqueezeDet%3A+Unified%2C+Small%2C+Low+Power+Fully+Convolutional+Neural+Networks+for+Real-Time+Object+Detection+for+Autonomous+Driving%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Feature_Pyramid_Network_FPN_587%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFeature+Pyramid+Network+%28FPN%29%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Feature_Pyramid_Networks_for_Object_Detection_588%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFeature+Pyramid+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ActionDriven_Object_Detection_with_TopDown_Visual_Attentions_594%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAction-Driven+Object+Detection+with+Top-Down+Visual+Attentions%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Beyond_Skip_Connections_TopDown_Modulation_for_Object_Detection_598%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBeyond+Skip+Connections%3A+Top-Down+Modulation+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23WideResidualInception_Networks_for_Realtime_Object_Detection_603%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWide-Residual-Inception+Networks+for+Real-time+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Attentional_Network_for_Visual_Object_Detection_608%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAttentional+Network+for+Visual+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Chained_Deep_Features_and_Classifiers_for_Cascade_in_Object_Detection_613%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Chained+Deep+Features+and+Classifiers+for+Cascade+in+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DeNet_Scalable_Realtime_Object_Detection_with_Directed_Sparse_Sampling_620%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeNet%3A+Scalable+Real-time+Object+Detection+with+Directed+Sparse+Sampling%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Discriminative_Bimodal_Networks_for_Visual_Localization_and_Detection_with_Natural_Language_Queries_626%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDiscriminative+Bimodal+Networks+for+Visual+Localization+and+Detection+with+Natural+Language+Queries%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Spatial_Memory_for_Context_Reasoning_in_Object_Detection_632%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESpatial+Memory+for+Context+Reasoning+in+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Accurate_Single_Stage_Detector_Using_Recurrent_Rolling_Convolution_637%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAccurate+Single+Stage+Detector+Using+Recurrent+Rolling+Convolution%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Occlusion_Reasoning_for_MultiCamera_MultiTarget_Detection_645%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Occlusion+Reasoning+for+Multi-Camera+Multi-Target+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23LCDet_LowComplexity_FullyConvolutional_Neural_Networks_for_Object_Detection_in_Embedded_Systems_649%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELCDet%3A+Low-Complexity+Fully-Convolutional+Neural+Networks+for+Object+Detection+in+Embedded+Systems%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Point_Linking_Network_for_Object_Detection_655%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPoint+Linking+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Perceptual_Generative_Adversarial_Networks_for_Small_Object_Detection_661%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPerceptual+Generative+Adversarial+Networks+for+Small+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Fewshot_Object_Detection_665%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFew-shot+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23YesNet_An_effective_Detector_Based_on_Global_Information_669%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYes-Net%3A+An+effective+Detector+Based+on+Global+Information%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SMC_Faster_RCNN_Toward_a_scenespecialized_multiobject_detector_673%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESMC+Faster+R-CNN%3A+Toward+a+scene-specialized+multi-object+detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_lightweight_convolutional_neural_networks_for_object_detection_677%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+lightweight+convolutional+neural+networks+for+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23RON_Reverse_Connection_with_Objectness_Prior_Networks_for_Object_Detection_681%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERON%3A+Reverse+Connection+with+Objectness+Prior+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Mimicking_Very_Efficient_Network_for_Object_Detection_688%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMimicking+Very+Efficient+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Residual_Features_and_Unified_Prediction_Network_for_Single_Stage_Detection_693%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EResidual+Features+and+Unified+Prediction+Network+for+Single+Stage+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deformable_Partbased_Fully_Convolutional_Network_for_Object_Detection_697%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeformable+Part-based+Fully+Convolutional+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Adaptive_Feeding_Achieving_Fast_and_Accurate_Detections_by_Adaptively_Combining_Object_Detectors_702%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdaptive+Feeding%3A+Achieving+Fast+and+Accurate+Detections+by+Adaptively+Combining+Object+Detectors%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Recurrent_Scale_Approximation_for_Object_Detection_in_CNN_707%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERecurrent+Scale+Approximation+for+Object+Detection+in+CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23DSOD_715%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDSOD%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DSOD_Learning_Deeply_Supervised_Object_Detectors_from_Scratch_716%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDSOD%3A+Learning+Deeply+Supervised+Object+Detectors+from+Scratch%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_from_Scratch_with_Deep_Supervision_722%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+from+Scratch+with+Deep+Supervision%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Focal_Loss_for_Dense_Object_Detection_727%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFocal+Loss+for+Dense+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Focal_Loss_Dense_Detector_for_Vehicle_Surveillance_733%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFocal+Loss+Dense+Detector+for+Vehicle+Surveillance%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23CoupleNet_Coupling_Global_Structure_with_Local_Parts_for_Object_Detection_737%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECoupleNet%3A+Coupling+Global+Structure+with+Local+Parts+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Incremental_Learning_of_Object_Detectors_without_Catastrophic_Forgetting_742%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIncremental+Learning+of+Object+Detectors+without+Catastrophic+Forgetting%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Zoom_OutandIn_Network_with_Map_Attention_Decision_for_Region_Proposal_and_Object_Detection_747%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZoom+Out-and-In+Network+with+Map+Attention+Decision+for+Region+Proposal+and+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23StairNet_TopDown_Semantic_Aggregation_for_Accurate_One_Shot_Detection_751%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EStairNet%3A+Top-Down+Semantic+Aggregation+for+Accurate+One+Shot+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Dynamic_Zoomin_Network_for_Fast_Object_Detection_in_Large_Images_755%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDynamic+Zoom-in+Network+for+Fast+Object+Detection+in+Large+Images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ZeroAnnotation_Object_Detection_with_Web_Knowledge_Transfer_759%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Annotation+Object+Detection+with+Web+Knowledge+Transfer%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23MegDet_767%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMegDet%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23MegDet_A_Large_MiniBatch_Object_Detector_768%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMegDet%3A+A+Large+Mini-Batch+Object+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SingleShot_Refinement_Neural_Network_for_Object_Detection_773%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESingle-Shot+Refinement+Neural+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Receptive_Field_Block_Net_for_Accurate_and_Fast_Object_Detection_779%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EReceptive+Field+Block+Net+for+Accurate+and+Fast+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23An_Analysis_of_Scale_Invariance_in_Object_Detection__SNIP_785%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAn+Analysis+of+Scale+Invariance+in+Object+Detection+-+SNIP%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Feature_Selective_Networks_for_Object_Detection_791%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFeature+Selective+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_a_Rotation_Invariant_Detector_with_Rotatable_Bounding_Box_795%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+a+Rotation+Invariant+Detector+with+Rotatable+Bounding+Box%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Scalable_Object_Detection_for_Stylized_Objects_800%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScalable+Object+Detection+for+Stylized+Objects%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Object_Detectors_from_Scratch_with_Gated_Recurrent_Feature_Pyramids_805%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Object+Detectors+from+Scratch+with+Gated+Recurrent+Feature+Pyramids%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Regionlets_for_Object_Detection_810%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Regionlets+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Training_and_Testing_Object_Detectors_with_Virtual_Images_815%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETraining+and+Testing+Object+Detectors+with+Virtual+Images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23LargeScale_Object_Discovery_and_Detector_Adaptation_from_Unlabeled_Video_821%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELarge-Scale+Object+Discovery+and+Detector+Adaptation+from+Unlabeled+Video%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Spot_the_Difference_by_Object_Detection_827%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESpot+the+Difference+by+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23LocalizationAware_Active_Learning_for_Object_Detection_832%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocalization-Aware+Active+Learning+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_with_Maskbased_Feature_Encoding_837%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+with+Mask-based+Feature+Encoding%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23LSTD_A_LowShot_Transfer_Detector_for_Object_Detection_841%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELSTD%3A+A+Low-Shot+Transfer+Detector+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Domain_Adaptive_Faster_RCNN_for_Object_Detection_in_the_Wild_846%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDomain+Adaptive+Faster+R-CNN+for+Object+Detection+in+the+Wild%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Pseudo_Mask_Augmented_Object_Detection_852%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPseudo+Mask+Augmented+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Revisiting_RCNN_On_Awakening_the_Classification_Power_of_Faster_RCNN_856%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERevisiting+RCNN%3A+On+Awakening+the+Classification+Power+of+Faster+RCNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Decoupled_Classification_Refinement_Hard_False_Positive_Suppression_for_Object_Detection_862%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDecoupled+Classification+Refinement%3A+Hard+False+Positive+Suppression+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Region_Features_for_Object_Detection_867%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Region+Features+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SingleShot_Bidirectional_Pyramid_Networks_for_HighQuality_Object_Detection_872%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESingle-Shot+Bidirectional+Pyramid+Networks+for+High-Quality+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_for_Comics_using_Manga109_Annotations_877%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+for+Comics+using+Manga109+Annotations%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23TaskDriven_Super_Resolution_Object_Detection_in_Lowresolution_Images_882%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETask-Driven+Super+Resolution%3A+Object+Detection+in+Low-resolution+Images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Transferring_CommonSense_Knowledge_for_Object_Detection_886%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETransferring+Common-Sense+Knowledge+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Multiscale_Locationaware_Kernel_Representation_for_Object_Detection_890%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-scale+Location-aware+Kernel+Representation+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Loss_Rank_Mining_A_General_Hard_Example_Mining_Method_for_Realtime_Detectors_896%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELoss+Rank+Mining%3A+A+General+Hard+Example+Mining+Method+for+Real-time+Detectors%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DetNet_A_Backbone_network_for_Object_Detection_901%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetNet%3A+A+Backbone+network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Robust_Physical_Adversarial_Attack_on_Faster_RCNN_Object_Detector_906%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERobust+Physical+Adversarial+Attack+on+Faster+R-CNN+Object+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23AdvDetPatch_Attacking_Object_Detectors_with_Adversarial_Patches_910%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdvDetPatch%3A+Attacking+Object+Detectors+with+Adversarial+Patches%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Attacking_Object_Detectors_via_Imperceptible_Patches_on_Background_914%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAttacking+Object+Detectors+via+Imperceptible+Patches+on+Background%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Physical_Adversarial_Examples_for_Object_Detectors_918%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPhysical+Adversarial+Examples+for+Object+Detectors%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Quantization_Mimic_Towards_Very_Tiny_CNN_for_Object_Detection_923%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EQuantization+Mimic%3A+Towards+Very+Tiny+CNN+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_detection_at_200_Frames_Per_Second_927%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+detection+at+200+Frames+Per+Second%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_using_Domain_Randomization_and_Generative_Adversarial_Refinement_of_Synthetic_Images_932%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+using+Domain+Randomization+and+Generative+Adversarial+Refinement+of+Synthetic+Images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SNIPER_Efficient_MultiScale_Training_937%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESNIPER%3A+Efficient+Multi-Scale+Training%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Soft_Sampling_for_Robust_Object_Detection_942%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESoft+Sampling+for+Robust+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23MetaAnchor_Learning_to_Detect_Objects_with_Customized_Anchors_946%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMetaAnchor%3A+Learning+to+Detect+Objects+with+Customized+Anchors%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Localization_Recall_Precision_LRP_A_New_Performance_Metric_for_Object_Detection_951%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocalization+Recall+Precision+%28LRP%29%3A+A+New+Performance+Metric+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23AutoContext_RCNN_957%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAuto-Context+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Pooling_Pyramid_Network_for_Object_Detection_962%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPooling+Pyramid+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Modeling_Visual_Context_is_Key_to_Augmenting_Object_Detection_Datasets_967%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EModeling+Visual+Context+is+Key+to+Augmenting+Object+Detection+Datasets%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Dual_Refinement_Network_for_SingleShot_Object_Detection_972%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDual+Refinement+Network+for+Single-Shot+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Acquisition_of_Localization_Confidence_for_Accurate_Object_Detection_976%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAcquisition+of+Localization+Confidence+for+Accurate+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23CornerNet_Detecting_Objects_as_Paired_Keypoints_982%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECornerNet%3A+Detecting+Objects+as+Paired+Keypoints%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Unsupervised_Hard_Example_Mining_from_Videos_for_Improved_Object_Detection_989%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnsupervised+Hard+Example+Mining+from+Videos+for+Improved+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SAN_Learning_Relationship_between_Convolutional_Features_for_MultiScale_Object_Detection_995%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESAN%3A+Learning+Relationship+between+Convolutional+Features+for+Multi-Scale+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_Survey_of_Modern_Object_Detection_Literature_using_Deep_Learning_999%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Survey+of+Modern+Object+Detection+Literature+using+Deep+Learning%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23TinyDSOD_Lightweight_Object_Detection_for_ResourceRestricted_Usages_1003%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETiny-DSOD%3A+Lightweight+Object+Detection+for+Resource-Restricted+Usages%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Feature_Pyramid_Reconfiguration_for_Object_Detection_1008%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Feature+Pyramid+Reconfiguration+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23MDCN_MultiScale_Deep_Inception_Convolutional_Neural_Networks_for_Efficient_Object_Detection_1012%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMDCN%3A+Multi-Scale%2C+Deep+Inception+Convolutional+Neural+Networks+for+Efficient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Recent_Advances_in_Object_Detection_in_the_Age_of_Deep_Convolutional_Neural_Networks_1016%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERecent+Advances+in+Object+Detection+in+the+Age+of+Deep+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_for_Generic_Object_Detection_A_Survey_1019%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+for+Generic+Object+Detection%3A+A+Survey%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Training_ConfidenceCalibrated_Classifier_for_Detecting_OutofDistribution_Samples_1022%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETraining+Confidence-Calibrated+Classifier+for+Detecting+Out-of-Distribution+Samples%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ScratchDetExploring_to_Train_SingleShot_Object_Detectors_from_Scratch_1026%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScratchDet%3AExploring+to+Train+Single-Shot+Object+Detectors+from+Scratch%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Fast_and_accurate_object_detection_in_high_resolution_4K_and_8K_video_using_GPUs_1030%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+and+accurate+object+detection+in+high+resolution+4K+and+8K+video+using+GPUs%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Hybrid_Knowledge_Routed_Modules_for_Largescale_Object_Detection_1035%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHybrid+Knowledge+Routed+Modules+for+Large-scale+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Gradient_Harmonized_Singlestage_Detector_1040%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGradient+Harmonized+Single-stage+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23M2Det_A_SingleShot_Object_Detector_based_on_MultiLevel_Feature_Pyramid_Network_1044%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EM2Det%3A+A+Single-Shot+Object+Detector+based+on+Multi-Level+Feature+Pyramid+Network%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23BAN_Focusing_on_Boundary_Context_for_Object_Detection_1049%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBAN%3A+Focusing+on+Boundary+Context+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Multilayer_Pruning_Framework_for_Compressing_Single_Shot_MultiBox_Detector_1052%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-layer+Pruning+Framework+for+Compressing+Single+Shot+MultiBox+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23R2CNN_MultiDimensional_Attention_Based_Rotation_Invariant_Detector_with_Robust_Anchor_Strategy_1056%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER2CNN%2B%2B%3A+Multi-Dimensional+Attention+Based+Rotation+Invariant+Detector+with+Robust+Anchor+Strategy%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DeRPN_Taking_a_further_step_toward_more_general_object_detection_1060%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeRPN%3A+Taking+a+further+step+toward+more+general+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Fast_Efficient_Object_Detection_Using_Selective_Attention_1066%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+Efficient+Object+Detection+Using+Selective+Attention%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Sampling_Techniques_for_LargeScale_Object_Detection_from_Sparsely_Annotated_Objects_1070%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESampling+Techniques+for+Large-Scale+Object+Detection+from+Sparsely+Annotated+Objects%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23NonMaximum_Suppression_NMS_1074%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ENon-Maximum+Suppression+%28NMS%29%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23EndtoEnd_Integration_of_a_Convolutional_Network_Deformable_Parts_Model_and_NonMaximum_Suppression_1075%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEnd-to-End+Integration+of+a+Convolutional+Network%2C+Deformable+Parts+Model+and+Non-Maximum+Suppression%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_convnet_for_nonmaximum_suppression_1081%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+convnet+for+non-maximum+suppression%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SoftNMS__Improving_Object_Detection_With_One_Line_of_Code_1086%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESoft-NMS+%E2%80%93+Improving+Object+Detection+With+One+Line+of+Code%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_nonmaximum_suppression_1093%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+non-maximum+suppression%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Relation_Networks_for_Object_Detection_1101%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERelation+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Adversarial_Examples_1108%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdversarial+Examples%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Adversarial_Examples_that_Fool_Detectors_1109%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdversarial+Examples+that+Fool+Detectors%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Adversarial_Examples_Are_Not_Easily_Detected_Bypassing_Ten_Detection_Methods_1114%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdversarial+Examples+Are+Not+Easily+Detected%3A+Bypassing+Ten+Detection+Methods%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Weakly_Supervised_Object_Detection_1121%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeakly+Supervised+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Track_and_Transfer_Watching_Videos_to_Simulate_Strong_Human_Supervision_for_WeaklySupervised_Object_Detection_1122%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETrack+and+Transfer%3A+Watching+Videos+to+Simulate+Strong+Human+Supervision+for+Weakly-Supervised+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Weakly_supervised_object_detection_using_pseudostrong_labels_1127%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeakly+supervised+object+detection+using+pseudo-strong+labels%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Saliency_Guided_EndtoEnd_Learning_for_Weakly_Supervised_Object_Detection_1131%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESaliency+Guided+End-to-End+Learning+for+Weakly+Supervised+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Visual_and_Semantic_Knowledge_Transfer_for_Large_Scale_Semisupervised_Object_Detection_1136%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+and+Semantic+Knowledge+Transfer+for+Large+Scale+Semi-supervised+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Video_Object_Detection_1142%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVideo+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Object_Class_Detectors_from_Weakly_Annotated_Video_1143%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Object+Class+Detectors+from+Weakly+Annotated+Video%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Analysing_domain_shift_factors_between_videos_and_images_for_object_detection_1148%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAnalysing+domain+shift+factors+between+videos+and+images+for+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Video_Object_Recognition_1152%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVideo+Object+Recognition%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_for_Saliency_Prediction_in_Natural_Video_1156%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+for+Saliency+Prediction+in+Natural+Video%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23TCNN_Tubelets_with_Convolutional_Neural_Networks_for_Object_Detection_from_Videos_1162%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ET-CNN%3A+Tubelets+with+Convolutional+Neural+Networks+for+Object+Detection+from+Videos%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_from_Video_Tubelets_with_Convolutional_Neural_Networks_1168%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+from+Video+Tubelets+with+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_Videos_with_Tubelets_and_Multicontext_Cues_1175%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+Videos+with+Tubelets+and+Multi-context+Cues%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Context_Matters_Refining_Object_Detection_in_Video_with_Recurrent_Neural_Networks_1181%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContext+Matters%3A+Refining+Object+Detection+in+Video+with+Recurrent+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23CNN_Based_Object_Detection_in_Large_Video_Images_1188%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECNN+Based+Object+Detection+in+Large+Video+Images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_Videos_with_Tubelet_Proposal_Networks_1194%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+Videos+with+Tubelet+Proposal+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23FlowGuided_Feature_Aggregation_for_Video_Object_Detection_1198%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFlow-Guided+Feature+Aggregation+for+Video+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Video_Object_Detection_using_Faster_RCNN_1203%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVideo+Object+Detection+using+Faster+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Improving_Context_Modeling_for_Video_Object_Detection_and_Tracking_1208%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EImproving+Context+Modeling+for+Video+Object+Detection+and+Tracking%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Temporal_Dynamic_Graph_LSTM_for_Actiondriven_Video_Object_Detection_1212%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETemporal+Dynamic+Graph+LSTM+for+Action-driven+Video+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Mobile_Video_Object_Detection_with_TemporallyAware_Feature_Maps_1217%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMobile+Video+Object+Detection+with+Temporally-Aware+Feature+Maps%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_High_Performance_Video_Object_Detection_1221%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+High+Performance+Video+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Impression_Network_for_Video_Object_Detection_1225%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EImpression+Network+for+Video+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SpatialTemporal_Memory_Networks_for_Video_Object_Detection_1229%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESpatial-Temporal+Memory+Networks+for+Video+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%233DDETNet_a_Single_Stage_VideoBased_Vehicle_Detector_1233%22+rel%3D%22nofollow%22+target%3D%22_self%22%3E3D-DETNet%3A+a+Single+Stage+Video-Based+Vehicle+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_Videos_by_Short_and_Long_Range_Object_Linking_1237%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+Videos+by+Short+and+Long+Range+Object+Linking%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_Video_with_Spatiotemporal_Sampling_Networks_1241%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+Video+with+Spatiotemporal+Sampling+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_High_Performance_Video_Object_Detection_for_Mobiles_1246%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+High+Performance+Video+Object+Detection+for+Mobiles%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Optimizing_Video_Object_Detection_via_a_ScaleTime_Lattice_1251%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOptimizing+Video+Object+Detection+via+a+Scale-Time+Lattice%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Pack_and_Detect_Fast_Object_Detection_in_Videos_Using_RegionofInterest_Packing_1258%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPack+and+Detect%3A+Fast+Object+Detection+in+Videos+Using+Region-of-Interest+Packing%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Fast_Object_Detection_in_Compressed_Video_1261%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+Object+Detection+in+Compressed+Video%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_on_Mobile_Devices_1266%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+on+Mobile+Devices%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Pelee_A_RealTime_Object_Detection_System_on_Mobile_Devices_1267%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPelee%3A+A+Real-Time+Object+Detection+System+on+Mobile+Devices%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_3D_1276%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+3D%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Vote3Deep_Fast_Object_Detection_in_3D_Point_Clouds_Using_Efficient_Convolutional_Neural_Networks_1277%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVote3Deep%3A+Fast+Object+Detection+in+3D+Point+Clouds+Using+Efficient+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ComplexYOLO_Realtime_3D_Object_Detection_on_Point_Clouds_1281%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EComplex-YOLO%3A+Real-time+3D+Object+Detection+on+Point+Clouds%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Focal_Loss_in_3D_Object_Detection_1286%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFocal+Loss+in+3D+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_on_RGBD_1291%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+on+RGB-D%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Rich_Features_from_RGBD_Images_for_Object_Detection_and_Segmentation_1292%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Rich+Features+from+RGB-D+Images+for+Object+Detection+and+Segmentation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Differential_Geometry_Boosts_Convolutional_Neural_Networks_for_Object_Detection_1296%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDifferential+Geometry+Boosts+Convolutional+Neural+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_Selfsupervised_Learning_System_for_Object_Detection_using_Physics_Simulation_and_Multiview_Pose_Estimation_1301%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Self-supervised+Learning+System+for+Object+Detection+using+Physics+Simulation+and+Multi-view+Pose+Estimation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23ZeroShot_Object_Detection_1307%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Shot+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ZeroShot_Detection_1308%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Shot+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ZeroShot_Object_Detection_1314%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Shot+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ZeroShot_Object_Detection_Learning_to_Simultaneously_Recognize_and_Localize_Novel_Concepts_1318%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Shot+Object+Detection%3A+Learning+to+Simultaneously+Recognize+and+Localize+Novel+Concepts%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ZeroShot_Object_Detection_by_Hybrid_Region_Embedding_1323%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Shot+Object+Detection+by+Hybrid+Region+Embedding%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Salient_Object_Detection_1330%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESalient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Best_Deep_Saliency_Detection_Models_CVPR_2016__2015_1333%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBest+Deep+Saliency+Detection+Models+%28CVPR+2016+%26amp%3B+2015%29%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Largescale_optimization_of_hierarchical_features_for_saliency_prediction_in_natural_images_1337%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELarge-scale+optimization+of+hierarchical+features+for+saliency+prediction+in+natural+images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Predicting_Eye_Fixations_using_Convolutional_Neural_Networks_1341%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPredicting+Eye+Fixations+using+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Saliency_Detection_by_MultiContext_Deep_Learning_1345%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESaliency+Detection+by+Multi-Context+Deep+Learning%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DeepSaliency_MultiTask_Deep_Neural_Network_Model_for_Salient_Object_Detection_1349%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepSaliency%3A+Multi-Task+Deep+Neural+Network+Model+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SuperCNN_A_Superpixelwise_Convolutional_Neural_Network_for_Salient_Object_Detection_1353%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESuperCNN%3A+A+Superpixelwise+Convolutional+Neural+Network+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Shallow_and_Deep_Convolutional_Networks_for_Saliency_Prediction_1357%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EShallow+and+Deep+Convolutional+Networks+for+Saliency+Prediction%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Recurrent_Attentional_Networks_for_Saliency_Detection_1363%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERecurrent+Attentional+Networks+for+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23TwoStream_Convolutional_Networks_for_Dynamic_Saliency_Prediction_1368%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETwo-Stream+Convolutional+Networks+for+Dynamic+Saliency+Prediction%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Unconstrained_Salient_Object_Detection_1374%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnconstrained+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Unconstrained_Salient_Object_Detection_via_Proposal_Subset_Optimization_1375%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnconstrained+Salient+Object+Detection+via+Proposal+Subset+Optimization%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DHSNet_Deep_Hierarchical_Saliency_Network_for_Salient_Object_Detection_1383%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDHSNet%3A+Deep+Hierarchical+Saliency+Network+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Salient_Object_Subitizing_1387%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESalient+Object+Subitizing%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DeeplySupervised_Recurrent_Convolutional_Neural_Network_for_Saliency_Detection_1396%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeeply-Supervised+Recurrent+Convolutional+Neural+Network+for+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Saliency_Detection_via_Combining_RegionLevel_and_PixelLevel_Predictions_with_CNNs_1401%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESaliency+Detection+via+Combining+Region-Level+and+Pixel-Level+Predictions+with+CNNs%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Edge_Preserving_and_MultiScale_Contextual_Neural_Network_for_Salient_Object_Detection_1406%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEdge+Preserving+and+Multi-Scale+Contextual+Neural+Network+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_Deep_MultiLevel_Network_for_Saliency_Prediction_1410%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Deep+Multi-Level+Network+for+Saliency+Prediction%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Visual_Saliency_Detection_Based_on_Multiscale_Deep_CNN_Features_1414%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+Saliency+Detection+Based+on+Multiscale+Deep+CNN+Features%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_Deep_Spatial_Contextual_Longterm_Recurrent_Convolutional_Network_for_Saliency_Detection_1419%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Deep+Spatial+Contextual+Long-term+Recurrent+Convolutional+Network+for+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deeply_supervised_salient_object_detection_with_short_connections_1424%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeeply+supervised+salient+object+detection+with+short+connections%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Weakly_Supervised_Topdown_Salient_Object_Detection_1431%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeakly+Supervised+Top-down+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SalGAN_Visual_Saliency_Prediction_with_Generative_Adversarial_Networks_1436%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESalGAN%3A+Visual+Saliency+Prediction+with+Generative+Adversarial+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Visual_Saliency_Prediction_Using_a_Mixture_of_Deep_Neural_Networks_1441%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+Saliency+Prediction+Using+a+Mixture+of+Deep+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_Fast_and_Compact_Salient_Score_Regression_Network_Based_on_Fully_Convolutional_Network_1445%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Fast+and+Compact+Salient+Score+Regression+Network+Based+on+Fully+Convolutional+Network%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Saliency_Detection_by_Forward_and_Backward_Cues_in_DeepCNNs_1449%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESaliency+Detection+by+Forward+and+Backward+Cues+in+Deep-CNNs%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Supervised_Adversarial_Networks_for_Image_Saliency_Detection_1453%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESupervised+Adversarial+Networks+for+Image+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Groupwise_Deep_Cosaliency_Detection_1457%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGroup-wise+Deep+Co-saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_the_Success_Rate_of_One_Realtime_Unconstrained_Salient_Object_Detection_1461%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+the+Success+Rate+of+One%3A+Real-time+Unconstrained+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Amulet_Aggregating_Multilevel_Convolutional_Features_for_Salient_Object_Detection_1466%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAmulet%3A+Aggregating+Multi-level+Convolutional+Features+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Uncertain_Convolutional_Features_for_Accurate_Saliency_Detection_1471%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Uncertain+Convolutional+Features+for+Accurate+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_EdgeAware_Saliency_Detection_1476%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Edge-Aware+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Selfexplanatory_Deep_Salient_Object_Detection_1480%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESelf-explanatory+Deep+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23PiCANet_Learning_Pixelwise_Contextual_Attention_in_ConvNets_and_Its_Application_in_Saliency_Detection_1485%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPiCANet%3A+Learning+Pixel-wise+Contextual+Attention+in+ConvNets+and+Its+Application+in+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DeepFeat_A_Bottom_Up_and_Top_Down_Saliency_Model_Based_on_Deep_Features_of_Convolutional_Neural_Nets_1489%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepFeat%3A+A+Bottom+Up+and+Top+Down+Saliency+Model+Based+on+Deep+Features+of+Convolutional+Neural+Nets%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Recurrently_Aggregating_Deep_Features_for_Salient_Object_Detection_1493%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERecurrently+Aggregating+Deep+Features+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_saliency_What_is_learnt_by_a_deep_network_about_saliency_1498%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+saliency%3A+What+is+learnt+by+a+deep+network+about+saliency%3F%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ContrastOriented_Deep_Neural_Networks_for_Salient_Object_Detection_1503%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContrast-Oriented+Deep+Neural+Networks+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Salient_Object_Detection_by_Lossless_Feature_Reflection_1508%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESalient+Object+Detection+by+Lossless+Feature+Reflection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23HyperFusionNet_Densely_Reflective_Fusion_for_Salient_Object_Detection_1513%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHyperFusion-Net%3A+Densely+Reflective+Fusion+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Video_Saliency_Detection_1520%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVideo+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_For_Video_Saliency_Detection_1521%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+For+Video+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Video_Salient_Object_Detection_Using_Spatiotemporal_Deep_Features_1525%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVideo+Salient+Object+Detection+Using+Spatiotemporal+Deep+Features%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Predicting_Video_Saliency_with_ObjecttoMotion_CNN_and_Twolayer_Convolutional_LSTM_1529%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPredicting+Video+Saliency+with+Object-to-Motion+CNN+and+Two-layer+Convolutional+LSTM%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Visual_Relationship_Detection_1536%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+Relationship+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Visual_Relationship_Detection_with_Language_Priors_1537%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+Relationship+Detection+with+Language+Priors%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ViPCNN_A_Visual_Phrase_Reasoning_Convolutional_Neural_Network_for_Visual_Relationship_Detection_1543%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EViP-CNN%3A+A+Visual+Phrase+Reasoning+Convolutional+Neural+Network+for+Visual+Relationship+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Visual_Translation_Embedding_Network_for_Visual_Relation_Detection_1548%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+Translation+Embedding+Network+for+Visual+Relation+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Variationstructured_Reinforcement_Learning_for_Visual_Relationship_and_Attribute_Detection_1552%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Variation-structured+Reinforcement+Learning+for+Visual+Relationship+and+Attribute+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Visual_Relationships_with_Deep_Relational_Networks_1557%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Visual+Relationships+with+Deep+Relational+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Identifying_Spatial_Relations_in_Images_using_Convolutional_Neural_Networks_1562%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIdentifying+Spatial+Relations+in+Images+using+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23PPRFCN_Weakly_Supervised_Visual_Relation_Detection_via_Parallel_Pairwise_RFCN_1566%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPPR-FCN%3A+Weakly+Supervised+Visual+Relation+Detection+via+Parallel+Pairwise+R-FCN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Natural_Language_Guided_Visual_Relationship_Detection_1571%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ENatural+Language+Guided+Visual+Relationship+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Visual_Relationships_Using_Box_Attention_1575%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Visual+Relationships+Using+Box+Attention%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Google_AI_Open_Images__Visual_Relationship_Track_1580%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGoogle+AI+Open+Images+-+Visual+Relationship+Track%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ContextDependent_Diffusion_Network_for_Visual_Relationship_Detection_1585%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContext-Dependent+Diffusion+Network+for+Visual+Relationship+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_Problem_Reduction_Approach_for_Visual_Relationships_Detection_1589%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Problem+Reduction+Approach+for+Visual+Relationships+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Face_Deteciton_1595%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Deteciton%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Multiview_Face_Detection_Using_Deep_Convolutional_Neural_Networks_1596%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-view+Face+Detection+Using+Deep+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23From_Facial_Parts_Responses_to_Face_Detection_A_Deep_Learning_Approach_1602%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFrom+Facial+Parts+Responses+to+Face+Detection%3A+A+Deep+Learning+Approach%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Compact_Convolutional_Neural_Network_Cascade_for_Face_Detection_1611%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECompact+Convolutional+Neural+Network+Cascade+for+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Face_Detection_with_EndtoEnd_Integration_of_a_ConvNet_and_a_3D_Model_1617%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Detection+with+End-to-End+Integration+of+a+ConvNet+and+a+3D+Model%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23CMSRCNN_Contextual_MultiScale_Regionbased_CNN_for_Unconstrained_Face_Detection_1623%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECMS-RCNN%3A+Contextual+Multi-Scale+Region-based+CNN+for+Unconstrained+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_a_Deep_Learning_Framework_for_Unconstrained_Face_Detection_1628%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+a+Deep+Learning+Framework+for+Unconstrained+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Supervised_Transformer_Network_for_Efficient_Face_Detection_1633%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESupervised+Transformer+Network+for+Efficient+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23UnitBox_An_Advanced_Object_Detection_Network_1637%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnitBox%3A+An+Advanced+Object+Detection+Network%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Bootstrapping_Face_Detection_with_Hard_Negative_Examples_1643%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBootstrapping+Face+Detection+with+Hard+Negative+Examples%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Grid_Loss_Detecting_Occluded_Faces_1649%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGrid+Loss%3A+Detecting+Occluded+Faces%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_MultiScale_Cascade_Fully_Convolutional_Network_Face_Detector_1656%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Multi-Scale+Cascade+Fully+Convolutional+Network+Face+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23MTCNN_1664%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMTCNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Joint_Face_Detection_and_Alignment_using_Multitask_Cascaded_Convolutional_Neural_Networks_1665%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EJoint+Face+Detection+and+Alignment+using+Multi-task+Cascaded+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Face_Detection_using_Deep_Learning_An_Improved_Faster_RCNN_Approach_1679%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Detection+using+Deep+Learning%3A+An+Improved+Faster+RCNN+Approach%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses_1684%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaceness-Net%3A+Face+Detection+through+Deep+Facial+Part+Responses%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23MultiPath_RegionBased_Convolutional_Neural_Network_for_Accurate_Detection_of_Unconstrained_Hard_Faces_1689%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-Path+Region-Based+Convolutional+Neural+Network+for+Accurate+Detection+of+Unconstrained+%E2%80%9CHard+Faces%E2%80%9D%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23EndToEnd_Face_Detection_and_Recognition_1694%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEnd-To-End+Face+Detection+and+Recognition%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Face_RCNN_1698%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Face_Detection_through_ScaleFriendly_Deep_Convolutional_Networks_1702%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Detection+through+Scale-Friendly+Deep+Convolutional+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ScaleAware_Face_Detection_1706%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScale-Aware+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Faces_Using_Inside_Cascaded_Contextual_CNN_1711%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Faces+Using+Inside+Cascaded+Contextual+CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23MultiBranch_Fully_Convolutional_Network_for_Face_Detection_1716%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-Branch+Fully+Convolutional+Network+for+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SSH_Single_Stage_Headless_Face_Detector_1720%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESSH%3A+Single+Stage+Headless+Face+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Dockerface_an_easy_to_install_and_use_Faster_RCNN_face_detector_in_a_Docker_container_1726%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDockerface%3A+an+easy+to+install+and+use+Faster+R-CNN+face+detector+in+a+Docker+container%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23FaceBoxes_A_CPU_Realtime_Face_Detector_with_High_Accuracy_1730%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaceBoxes%3A+A+CPU+Real-time+Face+Detector+with+High+Accuracy%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23S3FD_Single_Shot_Scaleinvariant_Face_Detector_1738%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ES3FD%3A+Single+Shot+Scale-invariant+Face+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Faces_Using_Regionbased_Fully_Convolutional_Networks_1746%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Faces+Using+Region-based+Fully+Convolutional+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23AffordanceNet_An_EndtoEnd_Deep_Learning_Approach_for_Object_Affordance_Detection_1750%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAffordanceNet%3A+An+End-to-End+Deep+Learning+Approach+for+Object+Affordance+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Face_Attention_Network_An_effective_Face_Detector_for_the_Occluded_Faces_1754%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Attention+Network%3A+An+effective+Face+Detector+for+the+Occluded+Faces%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Feature_Agglomeration_Networks_for_Single_Stage_Face_Detection_1758%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFeature+Agglomeration+Networks+for+Single+Stage+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Face_Detection_Using_Improved_Faster_RCNN_1762%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Detection+Using+Improved+Faster+RCNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23PyramidBox_A_Contextassisted_Single_Shot_Face_Detector_1767%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPyramidBox%3A+A+Context-assisted+Single+Shot+Face+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_Fast_Face_Detection_Method_via_Convolutional_Neural_Network_1772%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Fast+Face+Detection+Method+via+Convolutional+Neural+Network%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Beyond_Tradeoff_Accelerate_FCNbased_Face_Detector_with_Higher_Accuracy_1777%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBeyond+Trade-off%3A+Accelerate+FCN-based+Face+Detector+with+Higher+Accuracy%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23RealTime_RotationInvariant_Face_Detection_with_Progressive_Calibration_Networks_1782%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EReal-Time+Rotation-Invariant+Face+Detection+with+Progressive+Calibration+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SFace_An_Efficient_Network_for_Face_Detection_in_Large_Scale_Variations_1788%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESFace%3A+An+Efficient+Network+for+Face+Detection+in+Large+Scale+Variations%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Survey_of_Face_Detection_on_Lowquality_Images_1793%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESurvey+of+Face+Detection+on+Low-quality+Images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Anchor_Cascade_for_Efficient_Face_Detection_1797%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAnchor+Cascade+for+Efficient+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Adversarial_Attacks_on_Face_Detectors_using_Neural_Net_based_Constrained_Optimization_1802%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdversarial+Attacks+on+Face+Detectors+using+Neural+Net+based+Constrained+Optimization%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Selective_Refinement_Network_for_High_Performance_Face_Detection_1807%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESelective+Refinement+Network+for+High+Performance+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DSFD_Dual_Shot_Face_Detector_1810%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDSFD%3A+Dual+Shot+Face+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Better_Features_for_Face_Detection_with_Feature_Fusion_and_Segmentation_Supervision_1813%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Better+Features+for+Face+Detection+with+Feature+Fusion+and+Segmentation+Supervision%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Detect_Small_Faces_1817%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetect+Small+Faces%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Finding_Tiny_Faces_1818%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFinding+Tiny+Faces%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_and_counting_tiny_faces_1827%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+and+counting+tiny+faces%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Seeing_Small_Faces_from_Robust_Anchors_Perspective_1834%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESeeing+Small+Faces+from+Robust+Anchor%E2%80%99s+Perspective%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23FaceMagNet_Magnifying_Feature_Maps_to_Detect_Small_Faces_1839%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace-MagNet%3A+Magnifying+Feature+Maps+to+Detect+Small+Faces%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Person_Head_Detection_1849%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPerson+Head+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Contextaware_CNNs_for_person_head_detection_1850%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContext-aware+CNNs+for+person+head+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Heads_using_Feature_Refine_Net_and_Cascaded_Multiscale_Architecture_1857%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Heads+using+Feature+Refine+Net+and+Cascaded+Multi-scale+Architecture%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_Comparison_of_CNNbased_Face_and_Head_Detectors_for_RealTime_Video_Surveillance_Applications_1861%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Comparison+of+CNN-based+Face+and+Head+Detectors+for+Real-Time+Video+Surveillance+Applications%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23FCHD_A_fast_and_accurate_head_detector_1864%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFCHD%3A+A+fast+and+accurate+head+detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Pedestrian_Detection__People_Detection_1870%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPedestrian+Detection+%2F+People+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Pedestrian_Detection_aided_by_Deep_Learning_Semantic_Tasks_1871%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPedestrian+Detection+aided+by+Deep+Learning+Semantic+Tasks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_Strong_Parts_for_Pedestrian_Detection_1877%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+Strong+Parts+for+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Taking_a_Deeper_Look_at_Pedestrians_1883%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETaking+a+Deeper+Look+at+Pedestrians%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Convolutional_Channel_Features_1888%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EConvolutional+Channel+Features%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Endtoend_people_detection_in_crowded_scenes_1894%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEnd-to-end+people+detection+in+crowded+scenes%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_ComplexityAware_Cascades_for_Deep_Pedestrian_Detection_1901%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Complexity-Aware+Cascades+for+Deep+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_convolutional_neural_networks_for_pedestrian_detection_1906%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+convolutional+neural+networks+for+pedestrian+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Scaleaware_Fast_RCNN_for_Pedestrian_Detection_1911%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScale-aware+Fast+R-CNN+for+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23New_algorithm_improves_speed_and_accuracy_of_pedestrian_detection_1915%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ENew+algorithm+improves+speed+and+accuracy+of+pedestrian+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Pushing_the_Limits_of_Deep_CNNs_for_Pedestrian_Detection_1919%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPushing+the+Limits+of+Deep+CNNs+for+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_RealTime_Deep_Learning_Pedestrian_Detector_for_Robot_Navigation_1924%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Real-Time+Deep+Learning+Pedestrian+Detector+for+Robot+Navigation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_RealTime_Pedestrian_Detector_using_Deep_Learning_for_HumanAware_Navigation_1928%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Real-Time+Pedestrian+Detector+using+Deep+Learning+for+Human-Aware+Navigation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Is_Faster_RCNN_Doing_Well_for_Pedestrian_Detection_1932%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIs+Faster+R-CNN+Doing+Well+for+Pedestrian+Detection%3F%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Unsupervised_Deep_Domain_Adaptation_for_Pedestrian_Detection_1938%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnsupervised+Deep+Domain+Adaptation+for+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Reduced_Memory_Region_Based_Deep_Convolutional_Neural_Network_Detection_1943%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EReduced+Memory+Region+Based+Deep+Convolutional+Neural+Network+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Fused_DNN_A_deep_neural_network_fusion_approach_to_fast_and_robust_pedestrian_detection_1948%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFused+DNN%3A+A+deep+neural+network+fusion+approach+to+fast+and+robust+pedestrian+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_People_in_Artwork_with_CNNs_1952%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+People+in+Artwork+with+CNNs%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Multispectral_Deep_Neural_Networks_for_Pedestrian_Detection_1957%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMultispectral+Deep+Neural+Networks+for+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Multicamera_People_Detection_1962%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Multi-camera+People+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Expecting_the_Unexpected_Training_Detectors_for_Unusual_Pedestrians_with_Adversarial_Imposters_1966%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EExpecting+the+Unexpected%3A+Training+Detectors+for+Unusual+Pedestrians+with+Adversarial+Imposters%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23What_Can_Help_Pedestrian_Detection_1973%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWhat+Can+Help+Pedestrian+Detection%3F%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Illuminating_Pedestrians_via_Simultaneous_Detection__Segmentation_1980%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIlluminating+Pedestrians+via+Simultaneous+Detection+%26amp%3B+Segmentation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Rotational_Rectification_Network_for_Robust_Pedestrian_Detection_1984%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERotational+Rectification+Network+for+Robust+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23STDPD_Generating_Synthetic_Training_Data_for_Pedestrian_Detection_in_Unannotated_Videos_1989%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESTD-PD%3A+Generating+Synthetic+Training+Data+for+Pedestrian+Detection+in+Unannotated+Videos%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Too_Far_to_See_Not_Really__Pedestrian_Detection_with_Scaleaware_Localization_Policy_1994%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EToo+Far+to+See%3F+Not+Really%21+%E2%80%94+Pedestrian+Detection+with+Scale-aware+Localization+Policy%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Repulsion_Loss_Detecting_Pedestrians_in_a_Crowd_1998%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERepulsion+Loss%3A+Detecting+Pedestrians+in+a+Crowd%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Aggregated_Channels_Network_for_RealTime_Pedestrian_Detection_2002%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAggregated+Channels+Network+for+Real-Time+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Illuminationaware_Faster_RCNN_for_Robust_Multispectral_Pedestrian_Detection_2006%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIllumination-aware+Faster+R-CNN+for+Robust+Multispectral+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Exploring_MultiBranch_and_HighLevel_Semantic_Networks_for_Improving_Pedestrian_Detection_2011%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EExploring+Multi-Branch+and+High-Level+Semantic+Networks+for+Improving+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23PedestrianSynthesisGAN_Generating_Pedestrian_Data_in_Real_Scene_and_Beyond_2015%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPedestrian-Synthesis-GAN%3A+Generating+Pedestrian+Data+in+Real+Scene+and+Beyond%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23PCN_Part_and_Context_Information_for_Pedestrian_Detection_with_CNNs_2019%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPCN%3A+Part+and+Context+Information+for+Pedestrian+Detection+with+CNNs%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Smallscale_Pedestrian_Detection_Based_on_Somatic_Topology_Localization_and_Temporal_Feature_Aggregation_2024%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESmall-scale+Pedestrian+Detection+Based+on+Somatic+Topology+Localization+and+Temporal+Feature+Aggregation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Occlusionaware_RCNN_Detecting_Pedestrians_in_a_Crowd_2029%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOcclusion-aware+R-CNN%3A+Detecting+Pedestrians+in+a+Crowd%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Multispectral_Pedestrian_Detection_via_Simultaneous_Detection_and_Segmentation_2034%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMultispectral+Pedestrian+Detection+via+Simultaneous+Detection+and+Segmentation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Vehicle_Detection_2041%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVehicle+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DAVE_A_Unified_Framework_for_Fast_Vehicle_Detection_and_Annotation_2042%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDAVE%3A+A+Unified+Framework+for+Fast+Vehicle+Detection+and+Annotation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Evolving_Boxes_for_fast_Vehicle_Detection_2047%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEvolving+Boxes+for+fast+Vehicle+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23FineGrained_Car_Detection_for_Visual_Census_Estimation_2051%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFine-Grained+Car+Detection+for+Visual+Census+Estimation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SINet_A_Scaleinsensitive_Convolutional_Neural_Network_for_Fast_Vehicle_Detection_2056%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESINet%3A+A+Scale-insensitive+Convolutional+Neural+Network+for+Fast+Vehicle+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Label_and_Sample_Efficient_Training_of_Vehicle_Object_Detector_from_Sparsely_Labeled_Data_2061%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELabel+and+Sample%3A+Efficient+Training+of+Vehicle+Object+Detector+from+Sparsely+Labeled+Data%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Domain_Randomization_for_SceneSpecific_Car_Detection_and_Pose_Estimation_2065%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDomain+Randomization+for+Scene-Specific+Car+Detection+and+Pose+Estimation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ShuffleDet_RealTime_Vehicle_Detection_Network_in_Onboard_Embedded_UAV_Imagery_2068%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EShuffleDet%3A+Real-Time+Vehicle+Detection+Network+in+On-board+Embedded+UAV+Imagery%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23TrafficSign_Detection_2075%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETraffic-Sign+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23TrafficSign_Detection_and_Classification_in_the_Wild_2076%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETraffic-Sign+Detection+and+Classification+in+the+Wild%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Evaluating_Stateoftheart_Object_Detector_on_Challenging_Traffic_Light_Data_2083%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEvaluating+State-of-the-art+Object+Detector+on+Challenging+Traffic+Light+Data%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Small_Signs_from_Large_Images_2088%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Small+Signs+from+Large+Images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Localized_Traffic_Sign_Detection_with_Multiscale_Deconvolution_Networks_2093%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocalized+Traffic+Sign+Detection+with+Multi-scale+Deconvolution+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Traffic_Lights_by_Single_Shot_Detection_2097%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Traffic+Lights+by+Single+Shot+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23A_Hierarchical_Deep_Architecture_and_MiniBatch_Selection_Method_For_Joint_Traffic_Sign_and_Light_Detection_2102%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Hierarchical+Deep+Architecture+and+Mini-Batch+Selection+Method+For+Joint+Traffic+Sign+and+Light+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Skeleton_Detection_2110%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESkeleton+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Skeleton_Extraction_in_Natural_Images_by_Fusing_Scaleassociated_Deep_Side_Outputs_2111%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Skeleton+Extraction+in+Natural+Images+by+Fusing+Scale-associated+Deep+Side+Outputs%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DeepSkeleton_Learning_Multitask_Scaleassociated_Deep_Side_Outputs_for_Object_Skeleton_Extraction_in_Natural_Images_2116%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepSkeleton%3A+Learning+Multi-task+Scale-associated+Deep+Side+Outputs+for+Object+Skeleton+Extraction+in+Natural+Images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23SRN_Sideoutput_Residual_Network_for_Object_Symmetry_Detection_in_the_Wild_2120%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESRN%3A+Side-output+Residual+Network+for+Object+Symmetry+Detection+in+the+Wild%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23HiFi_Hierarchical_Feature_Integration_for_Skeleton_Detection_2126%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHi-Fi%3A+Hierarchical+Feature+Integration+for+Skeleton+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Fruit_Detection_2132%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFruit+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Fruit_Detection_in_Orchards_2133%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Fruit+Detection+in+Orchards%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Image_Segmentation_for_Fruit_Detection_and_Yield_Estimation_in_Apple_Orchards_2137%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EImage+Segmentation+for+Fruit+Detection+and+Yield+Estimation+in+Apple+Orchards%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Shadow_Detection_2144%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EShadow+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Fast_Shadow_Detection_from_a_Single_Image_Using_a_Patched_Convolutional_Neural_Network_2145%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+Shadow+Detection+from+a+Single+Image+Using+a+Patched+Convolutional+Neural+Network%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ADNet_Shadow_Detection_with_Adversarial_Shadow_Attenuation_2149%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA%2BD-Net%3A+Shadow+Detection+with+Adversarial+Shadow+Attenuation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Stacked_Conditional_Generative_Adversarial_Networks_for_Jointly_Learning_Shadow_Detection_and_Shadow_Removal_2153%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EStacked+Conditional+Generative+Adversarial+Networks+for+Jointly+Learning+Shadow+Detection+and+Shadow+Removal%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Directionaware_Spatial_Context_Features_for_Shadow_Detection_2157%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDirection-aware+Spatial+Context+Features+for+Shadow+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Directionaware_Spatial_Context_Features_for_Shadow_Detection_and_Removal_2162%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDirection-aware+Spatial+Context+Features+for+Shadow+Detection+and+Removal%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Others_Detection_2169%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOthers+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Deformation_Network_for_Object_Landmark_Localization_2170%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Deformation+Network+for+Object+Landmark+Localization%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Fashion_Landmark_Detection_in_the_Wild_2174%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFashion+Landmark+Detection+in+the+Wild%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_for_Fast_and_Accurate_Fashion_Item_Detection_2181%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+for+Fast+and+Accurate+Fashion+Item+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23OSMDeepOD__OSM_and_Deep_Learning_based_Object_Detection_from_Aerial_Imagery_formerly_known_as_OSMCrosswalkDetection_2187%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOSMDeepOD+-+OSM+and+Deep+Learning+based+Object+Detection+from+Aerial+Imagery+%28formerly+known+as+%E2%80%9COSM-Crosswalk-Detection%E2%80%9D%29%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Selfie_Detection_by_SynergyConstraint_Based_Convolutional_Neural_Network_2191%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESelfie+Detection+by+Synergy-Constraint+Based+Convolutional+Neural+Network%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Associative_EmbeddingEndtoEnd_Learning_for_Joint_Detection_and_Grouping_2196%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAssociative+Embedding%3AEnd-to-End+Learning+for+Joint+Detection+and+Grouping%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Cuboid_Detection_Beyond_2D_Bounding_Boxes_2200%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Cuboid+Detection%3A+Beyond+2D+Bounding+Boxes%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Automatic_Model_Based_Dataset_Generation_for_Fast_and_Accurate_Crop_and_Weeds_Detection_2205%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAutomatic+Model+Based+Dataset+Generation+for+Fast+and+Accurate+Crop+and+Weeds+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_Logo_Detection_with_Data_Expansion_by_Synthesising_Context_2209%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+Logo+Detection+with+Data+Expansion+by+Synthesising+Context%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Scalable_Deep_Learning_Logo_Detection_2213%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScalable+Deep+Learning+Logo+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Pixelwise_Ear_Detection_with_Convolutional_EncoderDecoder_Networks_2217%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPixel-wise+Ear+Detection+with+Convolutional+Encoder-Decoder+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Automatic_Handgun_Detection_Alarm_in_Videos_Using_Deep_Learning_2221%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAutomatic+Handgun+Detection+Alarm+in+Videos+Using+Deep+Learning%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Objects_as_context_for_part_detection_2226%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObjects+as+context+for+part+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Using_Deep_Networks_for_Drone_Detection_2230%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUsing+Deep+Networks+for+Drone+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Cut_Paste_and_Learn_Surprisingly_Easy_Synthesis_for_Instance_Detection_2235%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECut%2C+Paste+and+Learn%3A+Surprisingly+Easy+Synthesis+for+Instance+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Target_Driven_Instance_Detection_2240%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETarget+Driven+Instance+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DeepVoting_An_Explainable_Framework_for_Semantic_Part_Detection_under_Partial_Occlusion_2244%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepVoting%3A+An+Explainable+Framework+for+Semantic+Part+Detection+under+Partial+Occlusion%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23VPGNet_Vanishing_Point_Guided_Network_for_Lane_and_Road_Marking_Detection_and_Recognition_2248%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVPGNet%3A+Vanishing+Point+Guided+Network+for+Lane+and+Road+Marking+Detection+and+Recognition%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Grab_Pay_and_Eat_Semantic_Food_Detection_for_Smart_Restaurants_2254%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGrab%2C+Pay+and+Eat%3A+Semantic+Food+Detection+for+Smart+Restaurants%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ReMotENet_Efficient_Relevant_Motion_Event_Detection_for_Largescale_Home_Surveillance_Videos_2258%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EReMotENet%3A+Efficient+Relevant+Motion+Event+Detection+for+Large-scale+Home+Surveillance+Videos%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_Object_Detection_Methods_for_Ecological_Camera_Trap_Data_2263%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+Object+Detection+Methods+for+Ecological+Camera+Trap+Data%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ELGAN_Embedding_Loss_Driven_Generative_Adversarial_Networks_for_Lane_Detection_2268%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEL-GAN%3A+Embedding+Loss+Driven+Generative+Adversarial+Networks+for+Lane+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_EndtoEnd_Lane_Detection_an_Instance_Segmentation_Approach_2272%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+End-to-End+Lane+Detection%3A+an+Instance+Segmentation+Approach%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23iCAN_InstanceCentric_Attention_Network_for_HumanObject_Interaction_Detection_2276%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EiCAN%3A+Instance-Centric+Attention+Network+for+Human-Object+Interaction+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Densely_Supervised_Grasp_Detector_DSGD_2283%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDensely+Supervised+Grasp+Detector+%28DSGD%29%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Proposal_2287%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Proposal%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23DeepProposal_Hunting_Objects_by_Cascading_Deep_Convolutional_Layers_2288%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepProposal%3A+Hunting+Objects+by+Cascading+Deep+Convolutional+Layers%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Scaleaware_Pixelwise_Object_Proposal_Networks_2293%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScale-aware+Pixel-wise+Object+Proposal+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Attend_Refine_Repeat_Active_Box_Proposal_Generation_via_InOut_Localization_2298%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAttend+Refine+Repeat%3A+Active+Box+Proposal+Generation+via+In-Out+Localization%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_to_Segment_Object_Proposals_via_Recursive_Neural_Networks_2304%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+to+Segment+Object+Proposals+via+Recursive+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Detection_with_Diverse_Proposals_2308%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Detection+with+Diverse+Proposals%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ScaleNet_Guiding_Object_Proposal_Generation_in_Supermarkets_and_Beyond_2314%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScaleNet%3A+Guiding+Object+Proposal+Generation+in+Supermarkets+and+Beyond%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Improving_Small_Object_Proposals_for_Company_Logo_Detection_2319%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EImproving+Small+Object+Proposals+for+Company+Logo+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Open_Logo_Detection_Challenge_2324%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOpen+Logo+Detection+Challenge%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23AttentionMask_Attentive_Efficient_Object_Proposal_Generation_Focusing_on_Small_Objects_2332%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAttentionMask%3A+Attentive%2C+Efficient+Object+Proposal+Generation+Focusing+on+Small+Objects%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Localization_2338%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocalization%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Beyond_Bounding_Boxes_Precise_Localization_of_Objects_in_Images_2339%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBeyond+Bounding+Boxes%3A+Precise+Localization+of+Objects+in+Images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Weakly_Supervised_Object_Localization_with_Multifold_Multiple_Instance_Learning_2346%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeakly+Supervised+Object+Localization+with+Multi-fold+Multiple+Instance+Learning%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Weakly_Supervised_Object_Localization_Using_Size_Estimates_2350%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeakly+Supervised+Object+Localization+Using+Size+Estimates%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Active_Object_Localization_with_Deep_Reinforcement_Learning_2354%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EActive+Object+Localization+with+Deep+Reinforcement+Learning%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Localizing_objects_using_referring_expressions_2360%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocalizing+objects+using+referring+expressions%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23LocNet_Improving_Localization_Accuracy_for_Object_Detection_2367%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocNet%3A+Improving+Localization+Accuracy+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Deep_Features_for_Discriminative_Localization_2373%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Deep+Features+for+Discriminative+Localization%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23ContextLocNet_ContextAware_Deep_Network_Models_for_Weakly_Supervised_Localization_2381%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContextLocNet%3A+Context-Aware+Deep+Network+Models+for+Weakly+Supervised+Localization%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Ensemble_of_Part_Detectors_for_Simultaneous_Classification_and_Localization_2388%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEnsemble+of+Part+Detectors+for+Simultaneous+Classification+and+Localization%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23STNet_Selective_Tuning_of_Convolutional_Networks_for_Object_Localization_2392%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESTNet%3A+Selective+Tuning+of+Convolutional+Networks+for+Object+Localization%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Soft_Proposal_Networks_for_Weakly_Supervised_Object_Localization_2396%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESoft+Proposal+Networks+for+Weakly+Supervised+Object+Localization%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Finegrained_Discriminative_Localization_via_Saliencyguided_Faster_RCNN_2401%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFine-grained+Discriminative+Localization+via+Saliency-guided+Faster+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Tutorials__Talks_2408%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETutorials+%2F+Talks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Convolutional_Feature_Maps_Elements_of_efficient_and_accurate_CNNbased_object_detection_2409%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EConvolutional+Feature+Maps%3A+Elements+of+efficient+%28and+accurate%29+CNN-based+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_Good_Practices_for_Recognition__Detection_2413%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+Good+Practices+for+Recognition+%26amp%3B+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Work_in_progress_Improving_object_detection_and_instance_segmentation_for_small_objects_2418%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWork+in+progress%3A+Improving+object+detection+and+instance+segmentation+for+small+objects%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_with_Deep_Learning_A_Review_2422%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+with+Deep+Learning%3A+A+Review%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Projects_2428%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EProjects%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Detectron_2429%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetectron%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23TensorBox_a_simple_framework_for_training_neural_networks_to_detect_objects_in_images_2434%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETensorBox%3A+a+simple+framework+for+training+neural+networks+to+detect+objects+in+images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_detection_in_torch_Implementation_of_some_object_detection_frameworks_in_torch_2439%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+detection+in+torch%3A+Implementation+of+some+object+detection+frameworks+in+torch%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Using_DIGITS_to_train_an_Object_Detection_network_2443%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUsing+DIGITS+to+train+an+Object+Detection+network%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23FCNMultiBox_Detector_2447%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFCN-MultiBox+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23KittiBox_A_car_detection_model_implemented_in_Tensorflow_2452%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EKittiBox%3A+A+car+detection+model+implemented+in+Tensorflow.%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deformable_Convolutional_Networks__MST__SoftNMS_2458%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeformable+Convolutional+Networks+%2B+MST+%2B+Soft-NMS%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23How_to_Build_a_Realtime_HandDetector_using_Neural_Networks_SSD_on_Tensorflow_2462%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHow+to+Build+a+Real-time+Hand-Detector+using+Neural+Networks+%28SSD%29+on+Tensorflow%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Metrics_for_object_detection_2467%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMetrics+for+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23MobileNetv2SSDLite_2473%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMobileNetv2-SSDLite%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Leaderboard_2478%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELeaderboard%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Detection_Results_VOC2012_2479%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetection+Results%3A+VOC2012%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Tools_2486%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETools%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23BeaverDam_Video_annotation_tool_for_deep_learning_training_labels_2487%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBeaverDam%3A+Video+annotation+tool+for+deep+learning+training+labels%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A++++++++++%3Cli%3E%3Ca+href%3D%22%23Blogs_2493%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBlogs%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3Cul%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Convolutional_Neural_Networks_for_Object_Detection_2494%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EConvolutional+Neural+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Introducing_automatic_object_detection_to_visual_search_Pinterest_2498%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIntroducing+automatic+object+detection+to+visual+search+%28Pinterest%29%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_for_Object_Detection_with_DIGITS_2505%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+for+Object+Detection+with+DIGITS%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Analyzing_The_Papers_Behind_Facebooks_Computer_Vision_Approach_2509%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAnalyzing+The+Papers+Behind+Facebook%E2%80%99s+Computer+Vision+Approach%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Easily_Create_High_Quality_Object_Detectors_with_Deep_Learning_2514%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEasily+Create+High+Quality+Object+Detectors+with+Deep+Learning%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23How_to_Train_a_DeepLearned_Object_Detection_Model_in_the_Microsoft_Cognitive_Toolkit_2519%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHow+to+Train+a+Deep-Learned+Object+Detection+Model+in+the+Microsoft+Cognitive+Toolkit%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_Satellite_Imagery_a_Low_Overhead_Approach_2524%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+Satellite+Imagery%2C+a+Low+Overhead+Approach%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23You_Only_Look_Twice%E2%80%8A%E2%80%8AMultiScale_Object_Detection_in_Satellite_Imagery_With_Convolutional_Neural_Networks_2529%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYou+Only+Look+Twice%E2%80%8A%E2%80%94%E2%80%8AMulti-Scale+Object+Detection+in+Satellite+Imagery+With+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Faster_RCNN_Pedestrian_and_Car_Detection_2534%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaster+R-CNN+Pedestrian+and+Car+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Small_UNet_for_vehicle_detection_2540%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESmall+U-Net+for+vehicle+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Region_of_interest_pooling_explained_2544%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERegion+of+interest+pooling+explained%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Supercharge_your_Computer_Vision_models_with_the_TensorFlow_Object_Detection_API_2549%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESupercharge+your+Computer+Vision+models+with+the+TensorFlow+Object+Detection+API%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Understanding_SSD_MultiBox%E2%80%8A%E2%80%8ARealTime_Object_Detection_In_Deep_Learning_2554%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnderstanding+SSD+MultiBox%E2%80%8A%E2%80%94%E2%80%8AReal-Time+Object+Detection+In+Deep+Learning%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23Oneshot_object_detection_2558%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOne-shot+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++++++%3Cli%3E%3Ca+href%3D%22%23An_overview_of_object_detection_onestage_methods_2562%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAn+overview+of+object+detection%3A+one-stage+methods%3C%2Fa%3E%3C%2Fli%3E%0A++++++++++%3C%2Ful%3E%0A+++++++++%3C%2Ful%3E%0A++++++++%3C%2Ful%3E%0A+++++++%3C%2Fdiv%3E%0A+++++++%3Cp%3E%3C%2Fp%3E+%0A++++++%3C%2Fdiv%3E%0A+++++%3C%2Fdiv%3E%0A+++++%3Cdiv+class%3D%22table-box%22%3E%0A++++++%3Cdiv+class%3D%22table-box%22%3E%0A+++++++%3Ctable%3E+%0A++++++++%3Cthead%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Cth%3EMethod%3C%2Fth%3E+%0A++++++++++%3Cth+align%3D%22center%22%3Ebackbone%3C%2Fth%3E+%0A++++++++++%3Cth+align%3D%22center%22%3Etest+size%3C%2Fth%3E+%0A++++++++++%3Cth+align%3D%22center%22%3EVOC2007%3C%2Fth%3E+%0A++++++++++%3Cth+align%3D%22center%22%3EVOC2010%3C%2Fth%3E+%0A++++++++++%3Cth+align%3D%22center%22%3EVOC2012%3C%2Fth%3E+%0A++++++++++%3Cth+align%3D%22center%22%3EILSVRC+2013%3C%2Fth%3E+%0A++++++++++%3Cth+align%3D%22center%22%3EMSCOCO+2015%3C%2Fth%3E+%0A++++++++++%3Cth+align%3D%22center%22%3ESpeed%3C%2Fth%3E+%0A+++++++++%3C%2Ftr%3E+%0A++++++++%3C%2Fthead%3E+%0A++++++++%3Ctbody%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EOverFeat%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E24.3%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ER-CNN%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EAlexNet%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E58.5%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E53.7%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E53.3%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E31.4%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ER-CNN%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EVGG17%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E66.0%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ESPP_net%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EZF-5%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E54.2%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E31.84%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EDeepID-Net%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E64.1%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E50.3%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ENoC%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E73.3%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E68.8%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EFast-RCNN%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EVGG16%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E70.0%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E68.8%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E68.4%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E19.7%25%28%40%5B0.5-0.95%5D%29%2C+35.9%25%28%400.5%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EMR-CNN%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E78.2%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E73.9%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EFaster-RCNN%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EVGG16%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E78.8%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E75.9%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E21.9%25%28%40%5B0.5-0.95%5D%29%2C+42.7%25%28%400.5%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E198ms%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EFaster-RCNN%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E85.6%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E83.8%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E37.4%25%28%40%5B0.5-0.95%5D%29%2C+59.0%25%28%400.5%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EYOLO%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E63.4%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E57.9%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E45+fps%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EYOLO%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EVGG-16%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E66.4%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E21+fps%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EYOLOv2%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E448x448%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E78.6%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E73.4%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E21.6%25%28%40%5B0.5-0.95%5D%29%2C+44.0%25%28%400.5%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E40+fps%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ESSD%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EVGG16%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E300x300%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E77.2%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E75.8%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E25.1%25%28%40%5B0.5-0.95%5D%29%2C+43.1%25%28%400.5%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E46+fps%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ESSD%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EVGG16%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E512x512%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E79.8%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E78.5%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E28.8%25%28%40%5B0.5-0.95%5D%29%2C+48.5%25%28%400.5%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E19+fps%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ESSD%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E300x300%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E28.0%25%28%40%5B0.5-0.95%5D%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E16+fps%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ESSD%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E512x512%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E31.2%25%28%40%5B0.5-0.95%5D%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E8+fps%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EDSSD%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E300x300%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E28.0%25%28%40%5B0.5-0.95%5D%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E8+fps%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EDSSD%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E500x500%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E33.2%25%28%40%5B0.5-0.95%5D%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E6+fps%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EION%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E79.2%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E76.4%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ECRAFT%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E75.7%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E71.3%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E48.5%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EOHEM%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E78.9%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E76.3%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E25.5%25%28%40%5B0.5-0.95%5D%29%2C+45.9%25%28%400.5%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ER-FCN%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EResNet50%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E77.4%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E0.12sec%28K40%29%2C+0.09sec%28TitianX%29%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ER-FCN%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E79.5%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E0.17sec%28K40%29%2C+0.12sec%28TitianX%29%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ER-FCN%28ms+train%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E83.6%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E82.0%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E31.5%25%28%40%5B0.5-0.95%5D%29%2C+53.2%25%28%400.5%29%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3EPVANet+9.0%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E84.9%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E84.2%25%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E750ms%28CPU%29%2C+46ms%28TitianX%29%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ERetinaNet%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EResNet101-FPN%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ELight-Head+R-CNN%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EXception*%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E800%2F1200%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E31.5%25%40%5B0.5%3A0.95%5D%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E95+fps%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A+++++++++%3Ctr%3E+%0A++++++++++%3Ctd%3ELight-Head+R-CNN%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3EXception*%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E700%2F1100%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E30.7%25%40%5B0.5%3A0.95%5D%3C%2Ftd%3E+%0A++++++++++%3Ctd+align%3D%22center%22%3E102+fps%3C%2Ftd%3E+%0A+++++++++%3C%2Ftr%3E+%0A++++++++%3C%2Ftbody%3E+%0A+++++++%3C%2Ftable%3E%0A++++++%3C%2Fdiv%3E%0A+++++%3C%2Fdiv%3E%0A+++++%3Ch1%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Papers_39%22+target%3D%22_blank%22%3E%3C%2Fa%3EPapers%3C%2Fh1%3E+%0A+++++%3Chr%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Neural_Networks_for_Object_Detection_45%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Neural+Networks+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5207-deep-neural-networks-for-object-detection.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5207-deep-neural-networks-for-object-detection.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22OverFeat_Integrated_Recognition_Localization_and_Detection_using_Convolutional_Networks_48%22+target%3D%22_blank%22%3E%3C%2Fa%3EOverFeat%3A+Integrated+Recognition%2C+Localization+and+Detection+using+Convolutional+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1312.6229%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1312.6229%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fsermanet%2FOverFeat%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fsermanet%2FOverFeat%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Ecode%3A+%3Ca+href%3D%22http%3A%2F%2Fcilvr.nyu.edu%2Fdoku.php%3Fid%3Dsoftware%3Aoverfeat%3Astart%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcilvr.nyu.edu%2Fdoku.php%3Fid%3Dsoftware%3Aoverfeat%3Astart%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RCNN_55%22+target%3D%22_blank%22%3E%3C%2Fa%3ER-CNN%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation_56%22+target%3D%22_blank%22%3E%3C%2Fa%3ERich+feature+hierarchies+for+accurate+object+detection+and+semantic+segmentation%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+R-CNN%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1311.2524%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1311.2524%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Esupp%3A+%3Ca+href%3D%22http%3A%2F%2Fpeople.eecs.berkeley.edu%2F%7Erbg%2Fpapers%2Fr-cnn-cvpr-supp.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpeople.eecs.berkeley.edu%2F%7Erbg%2Fpapers%2Fr-cnn-cvpr-supp.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.image-net.org%2Fchallenges%2FLSVRC%2F2013%2Fslides%2Fr-cnn-ilsvrc2013-workshop.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.image-net.org%2Fchallenges%2FLSVRC%2F2013%2Fslides%2Fr-cnn-ilsvrc2013-workshop.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cs.berkeley.edu%2F%7Erbg%2Fslides%2Frcnn-cvpr14-slides.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cs.berkeley.edu%2F%7Erbg%2Fslides%2Frcnn-cvpr14-slides.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frbgirshick%2Frcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frbgirshick%2Frcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fzhangliliang.com%2F2014%2F07%2F23%2Fpaper-note-rcnn%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fzhangliliang.com%2F2014%2F07%2F23%2Fpaper-note-rcnn%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Ecaffe-pr%28%E2%80%9CMake+R-CNN+the+Caffe+detection+example%E2%80%9D%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fpull%2F482%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fpull%2F482%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fast_RCNN_70%22+target%3D%22_blank%22%3E%3C%2Fa%3EFast+R-CNN%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fast_RCNN_71%22+target%3D%22_blank%22%3E%3C%2Fa%3EFast+R-CNN%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1504.08083%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1504.08083%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Ftutorial.caffe.berkeleyvision.org%2Fcaffe-cvpr15-detection.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Ftutorial.caffe.berkeleyvision.org%2Fcaffe-cvpr15-detection.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28COCO-branch%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%2Ftree%2Fcoco%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%2Ftree%2Fcoco%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Ewebcam+demo%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%2Fpull%2F29%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%2Fpull%2F29%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fzhangliliang.com%2F2015%2F05%2F17%2Fpaper-note-fast-rcnn%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fzhangliliang.com%2F2015%2F05%2F17%2Fpaper-note-fast-rcnn%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fblog.csdn.net%2Flinj_m%2Farticle%2Fdetails%2F48930179%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fblog.csdn.net%2Flinj_m%2Farticle%2Fdetails%2F48930179%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28%E2%80%9CFast+R-CNN+in+MXNet%E2%80%9D%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fprecedenceguo%2Fmx-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fprecedenceguo%2Fmx-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmahyarnajibi%2Ffast-rcnn-torch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmahyarnajibi%2Ffast-rcnn-torch%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fapple2373%2Fchainer-simple-fast-rnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fapple2373%2Fchainer-simple-fast-rnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzplizzi%2Ftensorflow-fast-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzplizzi%2Ftensorflow-fast-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22AFastRCNN_Hard_Positive_Generation_via_Adversary_for_Object_Detection_84%22+target%3D%22_blank%22%3E%3C%2Fa%3EA-Fast-RCNN%3A+Hard+Positive+Generation+via+Adversary+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2017%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.03414%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.03414%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fabhinavsh.info%2Fpapers%2Fpdfs%2Fadversarial_object_detection.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fabhinavsh.info%2Fpapers%2Fpdfs%2Fadversarial_object_detection.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fxiaolonw%2Fadversarial-frcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fxiaolonw%2Fadversarial-frcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Faster_RCNN_91%22+target%3D%22_blank%22%3E%3C%2Fa%3EFaster+R-CNN%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Faster_RCNN_Towards_RealTime_Object_Detection_with_Region_Proposal_Networks_92%22+target%3D%22_blank%22%3E%3C%2Fa%3EFaster+R-CNN%3A+Towards+Real-Time+Object+Detection+with+Region+Proposal+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+NIPS+2015%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1506.01497%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1506.01497%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egitxiv%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.gitxiv.com%2Fposts%2F8pfpcvefDYn2gSgXk%2Ffaster-r-cnn-towards-real-time-object-detection-with-region%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.gitxiv.com%2Fposts%2F8pfpcvefDYn2gSgXk%2Ffaster-r-cnn-towards-real-time-object-detection-with-region%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fweb.cs.hacettepe.edu.tr%2F%7Eaykut%2Fclasses%2Fspring2016%2Fbil722%2Fslides%2Fw05-FasterR-CNN.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fweb.cs.hacettepe.edu.tr%2F%7Eaykut%2Fclasses%2Fspring2016%2Fbil722%2Fslides%2Fw05-FasterR-CNN.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28official%2C+Matlab%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FShaoqingRen%2Ffaster_rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FShaoqingRen%2Ffaster_rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frbgirshick%2Fpy-faster-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frbgirshick%2Fpy-faster-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmsracver%2FDeformable-ConvNets%2Ftree%2Fmaster%2Ffaster_rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmsracver%2FDeformable-ConvNets%2Ftree%2Fmaster%2Ffaster_rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2F%2Fjwyang%2Ffaster-rcnn.pytorch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2F%2Fjwyang%2Ffaster-rcnn.pytorch%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmitmul%2Fchainer-faster-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmitmul%2Fchainer-faster-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fandreaskoepf%2Ffaster-rcnn.torch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fandreaskoepf%2Ffaster-rcnn.torch%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fruotianluo%2FFaster-RCNN-Densecap-torch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fruotianluo%2FFaster-RCNN-Densecap-torch%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fsmallcorgi%2FFaster-RCNN_TF%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fsmallcorgi%2FFaster-RCNN_TF%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FCharlesShang%2FTFFRCNN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FCharlesShang%2FTFFRCNN%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28C%2B%2B+demo%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FYihangLou%2FFasterRCNN-Encapsulation-Cplusplus%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FYihangLou%2FFasterRCNN-Encapsulation-Cplusplus%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fyhenon%2Fkeras-frcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fyhenon%2Fkeras-frcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FEniac-Xie%2Ffaster-rcnn-resnet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FEniac-Xie%2Ffaster-rcnn-resnet%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28C%2B%2B%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FD-X-Y%2Fcaffe-faster-rcnn%2Ftree%2Fdev%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FD-X-Y%2Fcaffe-faster-rcnn%2Ftree%2Fdev%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RCNN_minus_R_111%22+target%3D%22_blank%22%3E%3C%2Fa%3ER-CNN+minus+R%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+BMVC+2015%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1506.06981%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1506.06981%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Faster_RCNN_in_MXNet_with_distributed_implementation_and_data_parallelization_116%22+target%3D%22_blank%22%3E%3C%2Fa%3EFaster+R-CNN+in+MXNet+with+distributed+implementation+and+data+parallelization%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fdmlc%2Fmxnet%2Ftree%2Fmaster%2Fexample%2Frcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fdmlc%2Fmxnet%2Ftree%2Fmaster%2Fexample%2Frcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Contextual_Priming_and_Feedback_for_Faster_RCNN_120%22+target%3D%22_blank%22%3E%3C%2Fa%3EContextual+Priming+and+Feedback+for+Faster+R-CNN%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2016.+Carnegie+Mellon+University%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fabhinavsh.info%2Fcontext_priming_feedback.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fabhinavsh.info%2Fcontext_priming_feedback.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eposter%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-1A-20.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-1A-20.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22An_Implementation_of_Faster_RCNN_with_Study_for_Region_Sampling_126%22+target%3D%22_blank%22%3E%3C%2Fa%3EAn+Implementation+of+Faster+RCNN+with+Study+for+Region+Sampling%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Technical+Report%2C+3+pages.+CMU%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.02138%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.02138%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fendernewton%2Ftf-faster-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fendernewton%2Ftf-faster-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Interpretable_RCNN_132%22+target%3D%22_blank%22%3E%3C%2Fa%3EInterpretable+R-CNN%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+North+Carolina+State+University+%26amp%3B+Alibaba%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+AND-OR+Graph+%28AOG%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.05226%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.05226%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LightHead_RCNN_140%22+target%3D%22_blank%22%3E%3C%2Fa%3ELight-Head+R-CNN%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LightHead_RCNN_In_Defense_of_TwoStage_Object_Detector_141%22+target%3D%22_blank%22%3E%3C%2Fa%3ELight-Head+R-CNN%3A+In+Defense+of+Two-Stage+Object+Detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Tsinghua+University+%26amp%3B+Megvii+Inc%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.07264%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.07264%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28official%2C+Tensorflow%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzengarden%2Flight_head_rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzengarden%2Flight_head_rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fterrychenism%2FDeformable-ConvNets%2Fblob%2Fmaster%2Frfcn%2Fsymbols%2Fresnet_v1_101_rfcn_light.py%23L784%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fterrychenism%2FDeformable-ConvNets%2Fblob%2Fmaster%2Frfcn%2Fsymbols%2Fresnet_v1_101_rfcn_light.py%23L784%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Cp%3E%23%23Cascade+R-CNN%3C%2Fp%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Cascade_RCNN_Delving_into_High_Quality_Object_Detection_149%22+target%3D%22_blank%22%3E%3C%2Fa%3ECascade+R-CNN%3A+Delving+into+High+Quality+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2018.+UC+San+Diego%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.00726%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.00726%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Caffe%2C+official%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzhaoweicai%2Fcascade-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzhaoweicai%2Fcascade-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MultiBox_157%22+target%3D%22_blank%22%3E%3C%2Fa%3EMultiBox%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Scalable_Object_Detection_using_Deep_Neural_Networks_158%22+target%3D%22_blank%22%3E%3C%2Fa%3EScalable+Object+Detection+using+Deep+Neural+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+first+MultiBox.+Train+a+CNN+to+predict+Region+of+Interest.%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1312.2249%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1312.2249%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fgoogle%2Fmultibox%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fgoogle%2Fmultibox%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eblog%3A+%3Ca+href%3D%22https%3A%2F%2Fresearch.googleblog.com%2F2014%2F12%2Fhigh-quality-object-detection-at-scale.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fresearch.googleblog.com%2F2014%2F12%2Fhigh-quality-object-detection-at-scale.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Scalable_HighQuality_Object_Detection_165%22+target%3D%22_blank%22%3E%3C%2Fa%3EScalable%2C+High-Quality+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+second+MultiBox%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1412.1441%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1412.1441%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fgoogle%2Fmultibox%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fgoogle%2Fmultibox%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SPPNet_173%22+target%3D%22_blank%22%3E%3C%2Fa%3ESPP-Net%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Spatial_Pyramid_Pooling_in_Deep_Convolutional_Networks_for_Visual_Recognition_174%22+target%3D%22_blank%22%3E%3C%2Fa%3ESpatial+Pyramid+Pooling+in+Deep+Convolutional+Networks+for+Visual+Recognition%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2014+%2F+TPAMI+2015%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1406.4729%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1406.4729%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FShaoqingRen%2FSPP_net%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FShaoqingRen%2FSPP_net%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fzhangliliang.com%2F2014%2F09%2F13%2Fpaper-note-sppnet%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fzhangliliang.com%2F2014%2F09%2F13%2Fpaper-note-sppnet%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeepIDNet_Deformable_Deep_Convolutional_Neural_Networks_for_Object_Detection_181%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeepID-Net%3A+Deformable+Deep+Convolutional+Neural+Networks+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+PAMI+2016%3C%2Fli%3E+%0A++++++%3Cli%3Eintro%3A+an+extension+of+R-CNN.+box+pre-training%2C+cascade+on+region+proposals%2C+deformation+layers+and+context+representations%3C%2Fli%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%25CB%259Cwlouyang%2Fprojects%2FimagenetDeepId%2Findex.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%CB%9Cwlouyang%2Fprojects%2FimagenetDeepId%2Findex.html%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1412.5661%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1412.5661%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detectors_Emerge_in_Deep_Scene_CNNs_188%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detectors+Emerge+in+Deep+Scene+CNNs%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICLR+2015%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1412.6856%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1412.6856%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.robots.ox.ac.uk%2F%7Evgg%2Frg%2Fpapers%2Fzhou_iclr15.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.robots.ox.ac.uk%2F%7Evgg%2Frg%2Fpapers%2Fzhou_iclr15.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fpeople.csail.mit.edu%2Fkhosla%2Fpapers%2Ficlr2015_zhou.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fpeople.csail.mit.edu%2Fkhosla%2Fpapers%2Ficlr2015_zhou.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fplaces.csail.mit.edu%2Fslide_iclr2015.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fplaces.csail.mit.edu%2Fslide_iclr2015.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22segDeepM_Exploiting_Segmentation_and_Context_in_Deep_Neural_Networks_for_Object_Detection_196%22+target%3D%22_blank%22%3E%3C%2Fa%3EsegDeepM%3A+Exploiting+Segmentation+and+Context+in+Deep+Neural+Networks+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2015%3C%2Fli%3E+%0A++++++%3Cli%3Eproject%28code%2Bdata%29%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.cs.toronto.edu%2F%7Eyukun%2Fsegdeepm.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.cs.toronto.edu%2F%7Eyukun%2Fsegdeepm.html%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1502.04275%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1502.04275%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FYknZhu%2FsegDeepM%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FYknZhu%2FsegDeepM%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_Networks_on_Convolutional_Feature_Maps_203%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+Networks+on+Convolutional+Feature+Maps%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+TPAMI+2015%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+NoC%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1504.06066%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1504.06066%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca+id%3D%22Improving_Object_Detection_with_Deep_Convolutional_Networks_via_Bayesian_Optimization_and_Structured_Prediction_209%22+target%3D%22_blank%22%3E%3C%2Fa%3EImproving+Object+Detection+with+Deep+Convolutional+Networks+via+Bayesian+Optimization+and+Structured+Prediction%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1504.03293%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1504.03293%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.ytzhang.net%2Ffiles%2Fpublications%2F2015-cvpr-det-slides.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.ytzhang.net%2Ffiles%2Fpublications%2F2015-cvpr-det-slides.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FYutingZhang%2Ffgs-obj%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FYutingZhang%2Ffgs-obj%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeepBox_Learning_Objectness_with_Convolutional_Networks_215%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeepBox%3A+Learning+Objectness+with+Convolutional+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Ekeywords%3A+DeepBox%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1505.02146%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1505.02146%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fweichengkuo%2FDeepBox%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fweichengkuo%2FDeepBox%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MRCNN_223%22+target%3D%22_blank%22%3E%3C%2Fa%3EMR-CNN%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_detection_via_a_multiregion__semantic_segmentationaware_CNN_model_224%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+detection+via+a+multi-region+%26amp%3B+semantic+segmentation-aware+CNN+model%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2015.+MR-CNN%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1505.01749%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1505.01749%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fgidariss%2Fmrcnn-object-detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fgidariss%2Fmrcnn-object-detection%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fzhangliliang.com%2F2015%2F05%2F17%2Fpaper-note-ms-cnn%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fzhangliliang.com%2F2015%2F05%2F17%2Fpaper-note-ms-cnn%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fblog.cvmarcher.com%2Fposts%2F2015%2F05%2F17%2Fmulti-region-semantic-segmentation-aware-cnn%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fblog.cvmarcher.com%2Fposts%2F2015%2F05%2F17%2Fmulti-region-semantic-segmentation-aware-cnn%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLO_234%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLO%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22You_Only_Look_Once_Unified_RealTime_Object_Detection_235%22+target%3D%22_blank%22%3E%3C%2Fa%3EYou+Only+Look+Once%3A+Unified%2C+Real-Time+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F201808211442120%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1506.02640%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1506.02640%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Ecode%3A+%3Ca+href%3D%22http%3A%2F%2Fpjreddie.com%2Fdarknet%2Fyolo%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpjreddie.com%2Fdarknet%2Fyolo%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fpjreddie%2Fdarknet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fpjreddie%2Fdarknet%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eblog%3A+%3Ca+href%3D%22https%3A%2F%2Fpjreddie.com%2Fpublications%2Fyolo%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fpjreddie.com%2Fpublications%2Fyolo%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22https%3A%2F%2Fdocs.google.com%2Fpresentation%2Fd%2F1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA%2Fpub%3Fstart%3Dfalse%26amp%3Bloop%3Dfalse%26amp%3Bdelayms%3D3000%26amp%3Bslide%3Did.p%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fdocs.google.com%2Fpresentation%2Fd%2F1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA%2Fpub%3Fstart%3Dfalse%26amp%3Bloop%3Dfalse%26amp%3Bdelayms%3D3000%26amp%3Bslide%3Did.p%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Ereddit%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F3a3m0o%2Frealtime_object_detection_with_yolo%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F3a3m0o%2Frealtime_object_detection_with_yolo%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fgliese581gg%2FYOLO_tensorflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fgliese581gg%2FYOLO_tensorflow%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fxingwangsfu%2Fcaffe-yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fxingwangsfu%2Fcaffe-yolo%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ffrankzhangrui%2FDarknet-Yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ffrankzhangrui%2FDarknet-Yolo%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBriSkyHekun%2Fpy-darknet-yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBriSkyHekun%2Fpy-darknet-yolo%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ftommy-qichang%2Fyolo.torch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ftommy-qichang%2Fyolo.torch%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ffrischzenger%2Fyolo-windows%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ffrischzenger%2Fyolo-windows%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FAlexeyAB%2Fyolo-windows%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FAlexeyAB%2Fyolo-windows%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fnilboy%2Ftensorflow-yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fnilboy%2Ftensorflow-yolo%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca+id%3D%22darkflow__translate_darknet_to_tensorflow_Load_trained_weights_retrainfinetune_them_using_tensorflow_export_constant_graph_def_to_C_252%22+target%3D%22_blank%22%3E%3C%2Fa%3Edarkflow+-+translate+darknet+to+tensorflow.+Load+trained+weights%2C+retrain%2Ffine-tune+them+using+tensorflow%2C+export+constant+graph+def+to+C%2B%2B%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eblog%3A+%3Ca+href%3D%22https%3A%2F%2Fthtrieu.github.io%2Fnotes%2Fyolo-tensorflow-graph-buffer-cpp%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fthtrieu.github.io%2Fnotes%2Fyolo-tensorflow-graph-buffer-cpp%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fthtrieu%2Fdarkflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fthtrieu%2Fdarkflow%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Start_Training_YOLO_with_Our_Own_Data_257%22+target%3D%22_blank%22%3E%3C%2Fa%3EStart+Training+YOLO+with+Our+Own+Data%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F20180821144236849%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+train+with+customized+data+and+class+numbers%2Flabels.+Linux+%2F+Windows+version+for+darknet.%3C%2Fli%3E+%0A++++++%3Cli%3Eblog%3A+%3Ca+href%3D%22http%3A%2F%2Fguanghan.info%2Fblog%2Fen%2Fmy-works%2Ftrain-yolo%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fguanghan.info%2Fblog%2Fen%2Fmy-works%2Ftrain-yolo%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FGuanghan%2Fdarknet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FGuanghan%2Fdarknet%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLO_Core_ML_versus_MPSNNGraph_263%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLO%3A+Core+ML+versus+MPSNNGraph%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Tiny+YOLO+for+iOS+implemented+using+CoreML+but+also+using+the+new+MPS+graph+API.%3C%2Fli%3E+%0A++++++%3Cli%3Eblog%3A+%3Ca+href%3D%22http%3A%2F%2Fmachinethink.net%2Fblog%2Fyolo-coreml-versus-mps-graph%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fmachinethink.net%2Fblog%2Fyolo-coreml-versus-mps-graph%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fhollance%2FYOLO-CoreML-MPSNNGraph%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fhollance%2FYOLO-CoreML-MPSNNGraph%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22TensorFlow_YOLO_object_detection_on_Android_269%22+target%3D%22_blank%22%3E%3C%2Fa%3ETensorFlow+YOLO+object+detection+on+Android%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Real-time+object+detection+on+Android+using+the+YOLO+network+with+TensorFlow%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fnatanielruiz%2Fandroid-yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fnatanielruiz%2Fandroid-yolo%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Computer_Vision_in_iOS__Object_Detection_274%22+target%3D%22_blank%22%3E%3C%2Fa%3EComputer+Vision+in+iOS+%E2%80%93+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eblog%3A+%3Ca+href%3D%22https%3A%2F%2Fsriraghu.com%2F2017%2F07%2F12%2Fcomputer-vision-in-ios-object-detection%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fsriraghu.com%2F2017%2F07%2F12%2Fcomputer-vision-in-ios-object-detection%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fr4ghu%2FiOS-CoreML-Yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fr4ghu%2FiOS-CoreML-Yolo%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLOv2_280%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLOv2%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLO9000_Better_Faster_Stronger_281%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLO9000%3A+Better%2C+Faster%2C+Stronger%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.08242%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.08242%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Ecode%3A+%3Ca+href%3D%22http%3A%2F%2Fpjreddie.com%2Fyolo9000%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpjreddie.com%2Fyolo9000%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Chainer%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fleetenki%2FYOLOv2%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fleetenki%2FYOLOv2%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Keras%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fallanzelener%2FYAD2K%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fallanzelener%2FYAD2K%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28PyTorch%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Flongcw%2Fyolo2-pytorch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Flongcw%2Fyolo2-pytorch%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Tensorflow%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fhizhangp%2Fyolo_tensorflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fhizhangp%2Fyolo_tensorflow%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Windows%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FAlexeyAB%2Fdarknet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FAlexeyAB%2Fdarknet%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FchoasUp%2Fcaffe-yolo9000%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FchoasUp%2Fcaffe-yolo9000%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fphilipperemy%2Fyolo-9000%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fphilipperemy%2Fyolo-9000%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22darknet_scripts_293%22+target%3D%22_blank%22%3E%3C%2Fa%3Edarknet_scripts%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Auxilary+scripts+to+work+with+%28YOLO%29+darknet+deep+learning+famework.+AKA+-%26gt%3B+How+to+generate+YOLO+anchors%3F%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FJumabek%2Fdarknet_scripts%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FJumabek%2Fdarknet_scripts%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Yolo_mark_GUI_for_marking_bounded_boxes_of_objects_in_images_for_training_Yolo_v2_298%22+target%3D%22_blank%22%3E%3C%2Fa%3EYolo_mark%3A+GUI+for+marking+bounded+boxes+of+objects+in+images+for+training+Yolo+v2%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FAlexeyAB%2FYolo_mark%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FAlexeyAB%2FYolo_mark%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LightNet_Bringing_pjreddies_DarkNet_out_of_the_shadows_302%22+target%3D%22_blank%22%3E%3C%2Fa%3ELightNet%3A+Bringing+pjreddie%E2%80%99s+DarkNet+out+of+the+shadows%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2F%2Fexplosion%2Flightnet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2F%2Fexplosion%2Flightnet%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLO_v2_Bounding_Box_Tool_306%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLO+v2+Bounding+Box+Tool%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Bounding+box+labeler+tool+to+generate+the+training+data+in+the+format+YOLO+v2+requires.%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FCartucho%2Fyolo-boundingbox-labeler-GUI%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FCartucho%2Fyolo-boundingbox-labeler-GUI%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLOv3_312%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLOv3%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLOv3_An_Incremental_Improvement_313%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLOv3%3A+An+Incremental+Improvement%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22https%3A%2F%2Fpjreddie.com%2Fdarknet%2Fyolo%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fpjreddie.com%2Fdarknet%2Fyolo%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.02767%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.02767%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLOLITE_A_RealTime_Object_Detection_Algorithm_Optimized_for_NonGPU_Computers_318%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLO-LITE%3A+A+Real-Time+Object+Detection+Algorithm+Optimized+for+Non-GPU+Computers%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.05588%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.05588%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22AttentionNet_Aggregating_Weak_Directions_for_Accurate_Object_Detection_323%22+target%3D%22_blank%22%3E%3C%2Fa%3EAttentionNet%3A+Aggregating+Weak+Directions+for+Accurate+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2015%3C%2Fli%3E+%0A++++++%3Cli%3Eintro%3A+state-of-the-art+performance+of+65%25+%28AP%29+on+PASCAL+VOC+2007%2F2012+human+detection+task%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1506.07704%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1506.07704%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.robots.ox.ac.uk%2F%7Evgg%2Frg%2Fslides%2FAttentionNet.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.robots.ox.ac.uk%2F%7Evgg%2Frg%2Fslides%2FAttentionNet.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks%2Flunit-kaist-slide.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks%2Flunit-kaist-slide.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DenseBox_333%22+target%3D%22_blank%22%3E%3C%2Fa%3EDenseBox%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DenseBox_Unifying_Landmark_Localization_with_End_to_End_Object_Detection_334%22+target%3D%22_blank%22%3E%3C%2Fa%3EDenseBox%3A+Unifying+Landmark+Localization+with+End+to+End+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1509.04874%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1509.04874%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Edemo%3A+%3Ca+href%3D%22http%3A%2F%2Fpan.baidu.com%2Fs%2F1mgoWWsS%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpan.baidu.com%2Fs%2F1mgoWWsS%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3EKITTI+result%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cvlibs.net%2Fdatasets%2Fkitti%2Feval_object.php%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cvlibs.net%2Fdatasets%2Fkitti%2Feval_object.php%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SSD_341%22+target%3D%22_blank%22%3E%3C%2Fa%3ESSD%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SSD_Single_Shot_MultiBox_Detector_342%22+target%3D%22_blank%22%3E%3C%2Fa%3ESSD%3A+Single+Shot+MultiBox+Detector%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F20180821144311974%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2016+Oral%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1512.02325%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1512.02325%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cs.unc.edu%2F%7Ewliu%2Fpapers%2Fssd.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cs.unc.edu%2F%7Ewliu%2Fpapers%2Fssd.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cs.unc.edu%2F%257Ewliu%2Fpapers%2Fssd_eccv2016_slide.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cs.unc.edu%2F%7Ewliu%2Fpapers%2Fssd_eccv2016_slide.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Official%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fweiliu89%2Fcaffe%2Ftree%2Fssd%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fweiliu89%2Fcaffe%2Ftree%2Fssd%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Evideo%3A+%3Ca+href%3D%22http%3A%2F%2Fweibo.com%2Fp%2F2304447a2326da963254c963c97fb05dd3a973%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fweibo.com%2Fp%2F2304447a2326da963254c963c97fb05dd3a973%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzhreshold%2Fmxnet-ssd%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzhreshold%2Fmxnet-ssd%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzhreshold%2Fmxnet-ssd.cpp%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzhreshold%2Fmxnet-ssd.cpp%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frykov8%2Fssd_keras%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frykov8%2Fssd_keras%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbalancap%2FSSD-Tensorflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbalancap%2FSSD-Tensorflow%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Famdegroot%2Fssd.pytorch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Famdegroot%2Fssd.pytorch%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fchuanqi305%2FMobileNet-SSD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fchuanqi305%2FMobileNet-SSD%3C%2Fa%3E%3Cbr%3E+What%E2%80%99s+the+diffience+in+performance+between+this+new+code+you+pushed+and+the+previous+code%3F+%23327%3Cbr%3E+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fweiliu89%2Fcaffe%2Fissues%2F327%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fweiliu89%2Fcaffe%2Fissues%2F327%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DSSD_360%22+target%3D%22_blank%22%3E%3C%2Fa%3EDSSD%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DSSD__Deconvolutional_Single_Shot_Detector_361%22+target%3D%22_blank%22%3E%3C%2Fa%3EDSSD+%3A+Deconvolutional+Single+Shot+Detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+UNC+Chapel+Hill+%26amp%3B+Amazon+Inc%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1701.06659%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1701.06659%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fchengyangfu%2Fcaffe%2Ftree%2Fdssd%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fchengyangfu%2Fcaffe%2Ftree%2Fdssd%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FMTCloudVision%2Fmxnet-dssd%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FMTCloudVision%2Fmxnet-dssd%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Edemo%3A+%3Ca+href%3D%22http%3A%2F%2F120.52.72.53%2Fwww.cs.unc.edu%2Fc3pr90ntc0td%2F%7Ecyfu%2Fdssd_lalaland.mp4%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2F120.52.72.53%2Fwww.cs.unc.edu%2Fc3pr90ntc0td%2F%7Ecyfu%2Fdssd_lalaland.mp4%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Enhancement_of_SSD_by_concatenating_feature_maps_for_object_detection_369%22+target%3D%22_blank%22%3E%3C%2Fa%3EEnhancement+of+SSD+by+concatenating+feature+maps+for+object+detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+rainbow+SSD+%28R-SSD%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1705.09587%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1705.09587%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Contextaware_SingleShot_Detector_374%22+target%3D%22_blank%22%3E%3C%2Fa%3EContext-aware+Single-Shot+Detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Ekeywords%3A+CSSD%2C+DiCSSD%2C+DeCSSD%2C+effective+receptive+fields+%28ERFs%29%2C+theoretical+receptive+fields+%28TRFs%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.08682%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.08682%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22FeatureFused_SSD_Fast_Detection_for_Small_Objects_379%22+target%3D%22_blank%22%3E%3C%2Fa%3EFeature-Fused+SSD%3A+Fast+Detection+for+Small+Objects%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1709.05054%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1709.05054%3C%2Fa%3E%3C%2Fp%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22FSSD_384%22+target%3D%22_blank%22%3E%3C%2Fa%3EFSSD%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22FSSD_Feature_Fusion_Single_Shot_Multibox_Detector_385%22+target%3D%22_blank%22%3E%3C%2Fa%3EFSSD%3A+Feature+Fusion+Single+Shot+Multibox+Detector%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.00960%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.00960%3C%2Fa%3E%3C%2Fp%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Weaving_Multiscale_Context_for_Single_Shot_Detector_389%22+target%3D%22_blank%22%3E%3C%2Fa%3EWeaving+Multi-scale+Context+for+Single+Shot+Detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+WeaveNet%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+fuse+multi-scale+information%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.03149%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.03149%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ESSD_396%22+target%3D%22_blank%22%3E%3C%2Fa%3EESSD%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Extend_the_shallow_part_of_Single_Shot_MultiBox_Detector_via_Convolutional_Neural_Network_397%22+target%3D%22_blank%22%3E%3C%2Fa%3EExtend+the+shallow+part+of+Single+Shot+MultiBox+Detector+via+Convolutional+Neural+Network%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.05918%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.05918%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca+id%3D%22Tiny_SSD_A_Tiny_Singleshot_Detection_Deep_Convolutional_Neural_Network_for_Realtime_Embedded_Object_Detection_401%22+target%3D%22_blank%22%3E%3C%2Fa%3ETiny+SSD%3A+A+Tiny+Single-shot+Detection+Deep+Convolutional+Neural+Network+for+Real-time+Embedded+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1802.06488%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1802.06488%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MDSSD_Multiscale_Deconvolutional_Single_Shot_Detector_for_small_objects_405%22+target%3D%22_blank%22%3E%3C%2Fa%3EMDSSD%3A+Multi-scale+Deconvolutional+Single+Shot+Detector+for+small+objects%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Zhengzhou+University%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.07009%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.07009%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22InsideOutside_Net_ION_411%22+target%3D%22_blank%22%3E%3C%2Fa%3EInside-Outside+Net+%28ION%29%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22InsideOutside_Net_Detecting_Objects_in_Context_with_Skip_Pooling_and_Recurrent_Neural_Networks_412%22+target%3D%22_blank%22%3E%3C%2Fa%3EInside-Outside+Net%3A+Detecting+Objects+in+Context+with+Skip+Pooling+and+Recurrent+Neural+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+%E2%80%9C0.8s+per+image+on+a+Titan+X+GPU+%28excluding+proposal+generation%29+without+two-stage+bounding-box+regression+and+1.15s+per+image+with+it%E2%80%9D.%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1512.04143%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1512.04143%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.seanbell.ca%2Ftmp%2Fion-coco-talk-bell2015.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.seanbell.ca%2Ftmp%2Fion-coco-talk-bell2015.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Ecoco-leaderboard%3A+%3Ca+href%3D%22http%3A%2F%2Fmscoco.org%2Fdataset%2F%23detections-leaderboard%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fmscoco.org%2Fdataset%2F%23detections-leaderboard%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Adaptive_Object_Detection_Using_Adjacency_and_Zoom_Prediction_419%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdaptive+Object+Detection+Using+Adjacency+and+Zoom+Prediction%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2016.+AZ-Net%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1512.07711%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1512.07711%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fluyongxi%2Faz-net%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fluyongxi%2Faz-net%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eyoutube%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYmFtuNwxaNM%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYmFtuNwxaNM%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22GCNN_an_Iterative_Grid_Based_Object_Detector_426%22+target%3D%22_blank%22%3E%3C%2Fa%3EG-CNN%3A+an+Iterative+Grid+Based+Object+Detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1512.07729%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1512.07729%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Factors_in_Finetuning_Deep_Model_for_object_detection_431%22+target%3D%22_blank%22%3E%3C%2Fa%3EFactors+in+Finetuning+Deep+Model+for+object+detection%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Factors_in_Finetuning_Deep_Model_for_Object_Detection_with_Longtail_Distribution_432%22+target%3D%22_blank%22%3E%3C%2Fa%3EFactors+in+Finetuning+Deep+Model+for+Object+Detection+with+Long-tail+Distribution%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2016.rank+3rd+for+provided+data+and+2nd+for+external+data+on+ILSVRC+2015+object+detection%3C%2Fli%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Ewlouyang%2Fprojects%2FImageNetFactors%2FCVPR16.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Ewlouyang%2Fprojects%2FImageNetFactors%2FCVPR16.html%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1601.05150%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1601.05150%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22We_dont_need_no_boundingboxes_Training_object_class_detectors_using_only_human_verification_438%22+target%3D%22_blank%22%3E%3C%2Fa%3EWe+don%E2%80%99t+need+no+bounding-boxes%3A+Training+object+class+detectors+using+only+human+verification%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1602.08405%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1602.08405%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22HyperNet_Towards_Accurate_Region_Proposal_Generation_and_Joint_Object_Detection_443%22+target%3D%22_blank%22%3E%3C%2Fa%3EHyperNet%3A+Towards+Accurate+Region+Proposal+Generation+and+Joint+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.00600%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.00600%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_MultiPath_Network_for_Object_Detection_448%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+MultiPath+Network+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+BMVC+2016.+Facebook+AI+Research+%28FAIR%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.02135%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.02135%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fmultipathnet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fmultipathnet%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CRAFT_455%22+target%3D%22_blank%22%3E%3C%2Fa%3ECRAFT%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CRAFT_Objects_from_Images_456%22+target%3D%22_blank%22%3E%3C%2Fa%3ECRAFT+Objects+from+Images%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2016.+Cascade+Region-proposal-network+And+FasT-rcnn.+an+extension+of+Faster+R-CNN%3C%2Fli%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fbyangderek.github.io%2Fprojects%2Fcraft.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fbyangderek.github.io%2Fprojects%2Fcraft.html%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1604.03239%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1604.03239%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FYang_CRAFT_Objects_From_CVPR_2016_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FYang_CRAFT_Objects_From_CVPR_2016_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbyangderek%2FCRAFT%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbyangderek%2FCRAFT%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22OHEM_465%22+target%3D%22_blank%22%3E%3C%2Fa%3EOHEM%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Training_Regionbased_Object_Detectors_with_Online_Hard_Example_Mining_466%22+target%3D%22_blank%22%3E%3C%2Fa%3ETraining+Region-based+Object+Detectors+with+Online+Hard+Example+Mining%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2016+Oral.+Online+hard+example+mining+%28OHEM%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.03540%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.03540%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FShrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FShrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Official%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fabhi2610%2Fohem%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fabhi2610%2Fohem%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eauthor+page%3A+%3Ca+href%3D%22http%3A%2F%2Fabhinav-shrivastava.info%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fabhinav-shrivastava.info%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SOHEM_Stratified_Online_Hard_Example_Mining_for_Object_Detection_474%22+target%3D%22_blank%22%3E%3C%2Fa%3ES-OHEM%3A+Stratified+Online+Hard+Example+Mining+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1705.02233%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1705.02233%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca+id%3D%22Exploit_All_the_Layers_Fast_and_Accurate_CNN_Object_Detector_with_Scale_Dependent_Pooling_and_Cascaded_Rejection_Classifiers_478%22+target%3D%22_blank%22%3E%3C%2Fa%3EExploit+All+the+Layers%3A+Fast+and+Accurate+CNN+Object+Detector+with+Scale+Dependent+Pooling+and+Cascaded+Rejection+Classifiers%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2016%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+scale-dependent+pooling+%28SDP%29%2C+cascaded+rejection+classifiers+%28CRC%29%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww-personal.umich.edu%2F%7Ewgchoi%2FSDP-CRC_camready.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww-personal.umich.edu%2F%7Ewgchoi%2FSDP-CRC_camready.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RFCN_485%22+target%3D%22_blank%22%3E%3C%2Fa%3ER-FCN%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RFCN_Object_Detection_via_Regionbased_Fully_Convolutional_Networks_486%22+target%3D%22_blank%22%3E%3C%2Fa%3ER-FCN%3A+Object+Detection+via+Region-based+Fully+Convolutional+Networks%3C%2Fh3%3E+%0A+++++%3Cp%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1605.06409%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1605.06409%3C%2Fa%3E%3Cbr%3E+github%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fdaijifeng001%2FR-FCN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fdaijifeng001%2FR-FCN%3C%2Fa%3E%3Cbr%3E+github%28MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmsracver%2FDeformable-ConvNets%2Ftree%2Fmaster%2Frfcn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmsracver%2FDeformable-ConvNets%2Ftree%2Fmaster%2Frfcn%3C%2Fa%3E%3Cbr%3E+github%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FOrpine%2Fpy-R-FCN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FOrpine%2Fpy-R-FCN%3C%2Fa%3E%3Cbr%3E+github%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FPureDiors%2Fpytorch_RFCN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FPureDiors%2Fpytorch_RFCN%3C%2Fa%3E%3Cbr%3E+github%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fpy-R-FCN-multiGPU%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fpy-R-FCN-multiGPU%3C%2Fa%3E%3Cbr%3E+github%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fxdever%2FRFCN-tensorflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fxdever%2FRFCN-tensorflow%3C%2Fa%3E%3C%2Fp%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RFCN3000_at_30fps_Decoupling_Detection_and_Classification_495%22+target%3D%22_blank%22%3E%3C%2Fa%3ER-FCN-3000+at+30fps%3A+Decoupling+Detection+and+Classification%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.01802%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.01802%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Recycle_deep_features_for_better_object_detection_499%22+target%3D%22_blank%22%3E%3C%2Fa%3ERecycle+deep+features+for+better+object+detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.05066%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.05066%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MSCNN_504%22+target%3D%22_blank%22%3E%3C%2Fa%3EMS-CNN%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Unified_Multiscale_Deep_Convolutional_Neural_Network_for_Fast_Object_Detection_505%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Unified+Multi-scale+Deep+Convolutional+Neural+Network+for+Fast+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2016%3C%2Fli%3E+%0A++++++%3Cli%3Eintro%3A+640%C3%97480%3A+15+fps%2C+960%C3%97720%3A+8+fps%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.07155%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.07155%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzhaoweicai%2Fmscnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzhaoweicai%2Fmscnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eposter%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-2B-38.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-2B-38.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Multistage_Object_Detection_with_Group_Recursive_Learning_513%22+target%3D%22_blank%22%3E%3C%2Fa%3EMulti-stage+Object+Detection+with+Group+Recursive+Learning%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+VOC2007%3A+78.6%25%2C+VOC2012%3A+74.9%25%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.05159%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.05159%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Subcategoryaware_Convolutional_Neural_Networks_for_Object_Proposals_and_Detection_518%22+target%3D%22_blank%22%3E%3C%2Fa%3ESubcategory-aware+Convolutional+Neural+Networks+for+Object+Proposals+and+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+WACV+2017.+SubCNN%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.04693%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.04693%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ftanshen%2FSubCNN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ftanshen%2FSubCNN%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22PVANET_525%22+target%3D%22_blank%22%3E%3C%2Fa%3EPVANET%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22PVANet_Lightweight_Deep_Neural_Networks_for_Realtime_Object_Detection_526%22+target%3D%22_blank%22%3E%3C%2Fa%3EPVANet%3A+Lightweight+Deep+Neural+Networks+for+Real-time+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Presented+at+NIPS+2016+Workshop+on+Efficient+Methods+for+Deep+Neural+Networks+%28EMDNN%29.+Continuation+of+arXiv%3A1608.08021%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.08588%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.08588%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fsanghoon%2Fpva-faster-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fsanghoon%2Fpva-faster-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eleaderboard%28PVANet+9.0%29%3A+%3Ca+href%3D%22http%3A%2F%2Fhost.robots.ox.ac.uk%3A8080%2Fleaderboard%2Fdisplaylb.php%3Fchallengeid%3D11%26amp%3Bcompid%3D4%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fhost.robots.ox.ac.uk%3A8080%2Fleaderboard%2Fdisplaylb.php%3Fchallengeid%3D11%26amp%3Bcompid%3D4%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22GBDNet_534%22+target%3D%22_blank%22%3E%3C%2Fa%3EGBD-Net%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Gated_Bidirectional_CNN_for_Object_Detection_535%22+target%3D%22_blank%22%3E%3C%2Fa%3EGated+Bi-directional+CNN+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+The+Chinese+University+of+Hong+Kong+%26amp%3B+Sensetime+Group+Limited%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-319-46478-7_22%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-319-46478-7_22%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Emirror%3A+%3Ca+href%3D%22https%3A%2F%2Fpan.baidu.com%2Fs%2F1dFohO7v%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fpan.baidu.com%2Fs%2F1dFohO7v%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Crafting_GBDNet_for_Object_Detection_541%22+target%3D%22_blank%22%3E%3C%2Fa%3ECrafting+GBD-Net+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+winner+of+the+ImageNet+object+detection+challenge+of+2016.+CUImage+and+CUVideo%3C%2Fli%3E+%0A++++++%3Cli%3Eintro%3A+gated+bi-directional+CNN+%28GBD-Net%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1610.02579%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1610.02579%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FcraftGBD%2FcraftGBD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FcraftGBD%2FcraftGBD%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22StuffNet_Using_Stuff_to_Improve_Object_Detection_549%22+target%3D%22_blank%22%3E%3C%2Fa%3EStuffNet%3A+Using+%E2%80%98Stuff%E2%80%99+to+Improve+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1610.05861%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1610.05861%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Generalized_Haar_Filter_based_Deep_Networks_for_RealTime_Object_Detection_in_Traffic_Scene_554%22+target%3D%22_blank%22%3E%3C%2Fa%3EGeneralized+Haar+Filter+based+Deep+Networks+for+Real-Time+Object+Detection+in+Traffic+Scene%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1610.09609%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1610.09609%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Hierarchical_Object_Detection_with_Deep_Reinforcement_Learning_559%22+target%3D%22_blank%22%3E%3C%2Fa%3EHierarchical+Object+Detection+with+Deep+Reinforcement+Learning%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Deep+Reinforcement+Learning+Workshop+%28NIPS+2016%29%3C%2Fli%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22https%3A%2F%2Fimatge-upc.github.io%2Fdetection-2016-nipsws%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fimatge-upc.github.io%2Fdetection-2016-nipsws%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.03718%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.03718%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.slideshare.net%2Fxavigiro%2Fhierarchical-object-detection-with-deep-reinforcement-learning%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.slideshare.net%2Fxavigiro%2Fhierarchical-object-detection-with-deep-reinforcement-learning%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fimatge-upc%2Fdetection-2016-nipsws%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fimatge-upc%2Fdetection-2016-nipsws%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eblog%3A+%3Ca+href%3D%22http%3A%2F%2Fjorditorres.org%2Fnips%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fjorditorres.org%2Fnips%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_to_detect_and_localize_many_objects_from_few_examples_569%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+to+detect+and+localize+many+objects+from+few+examples%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.05664%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.05664%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Speedaccuracy_tradeoffs_for_modern_convolutional_object_detectors_574%22+target%3D%22_blank%22%3E%3C%2Fa%3ESpeed%2Faccuracy+trade-offs+for+modern+convolutional+object+detectors%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2017.+Google+Research%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.10012%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.10012%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca+id%3D%22SqueezeDet_Unified_Small_Low_Power_Fully_Convolutional_Neural_Networks_for_RealTime_Object_Detection_for_Autonomous_Driving_580%22+target%3D%22_blank%22%3E%3C%2Fa%3ESqueezeDet%3A+Unified%2C+Small%2C+Low+Power+Fully+Convolutional+Neural+Networks+for+Real-Time+Object+Detection+for+Autonomous+Driving%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.01051%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.01051%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBichenWuUCB%2FsqueezeDet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBichenWuUCB%2FsqueezeDet%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ffregu856%2F2D_detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ffregu856%2F2D_detection%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Feature_Pyramid_Network_FPN_587%22+target%3D%22_blank%22%3E%3C%2Fa%3EFeature+Pyramid+Network+%28FPN%29%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Feature_Pyramid_Networks_for_Object_Detection_588%22+target%3D%22_blank%22%3E%3C%2Fa%3EFeature+Pyramid+Networks+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Facebook+AI+Research%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.03144%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.03144%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ActionDriven_Object_Detection_with_TopDown_Visual_Attentions_594%22+target%3D%22_blank%22%3E%3C%2Fa%3EAction-Driven+Object+Detection+with+Top-Down+Visual+Attentions%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.06704%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.06704%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Beyond_Skip_Connections_TopDown_Modulation_for_Object_Detection_598%22+target%3D%22_blank%22%3E%3C%2Fa%3EBeyond+Skip+Connections%3A+Top-Down+Modulation+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CMU+%26amp%3B+UC+Berkeley+%26amp%3B+Google+Research%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.06851%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.06851%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22WideResidualInception_Networks_for_Realtime_Object_Detection_603%22+target%3D%22_blank%22%3E%3C%2Fa%3EWide-Residual-Inception+Networks+for+Real-time+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Inha+University%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.01243%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.01243%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Attentional_Network_for_Visual_Object_Detection_608%22+target%3D%22_blank%22%3E%3C%2Fa%3EAttentional+Network+for+Visual+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+University+of+Maryland+%26amp%3B+Mitsubishi+Electric+Research+Laboratories%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.01478%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.01478%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Chained_Deep_Features_and_Classifiers_for_Cascade_in_Object_Detection_613%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Chained+Deep+Features+and+Classifiers+for+Cascade+in+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Ekeykwords%3A+CC-Net%3C%2Fli%3E+%0A++++++%3Cli%3Eintro%3A+chained+cascade+network+%28CC-Net%29.+81.1%25+mAP+on+PASCAL+VOC+2007%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.07054%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.07054%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeNet_Scalable_Realtime_Object_Detection_with_Directed_Sparse_Sampling_620%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeNet%3A+Scalable+Real-time+Object+Detection+with+Directed+Sparse+Sampling%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2017+%28poster%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1703.10295%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1703.10295%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Discriminative_Bimodal_Networks_for_Visual_Localization_and_Detection_with_Natural_Language_Queries_626%22+target%3D%22_blank%22%3E%3C%2Fa%3EDiscriminative+Bimodal+Networks+for+Visual+Localization+and+Detection+with+Natural+Language+Queries%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2017%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.03944%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.03944%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Spatial_Memory_for_Context_Reasoning_in_Object_Detection_632%22+target%3D%22_blank%22%3E%3C%2Fa%3ESpatial+Memory+for+Context+Reasoning+in+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.04224%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.04224%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Accurate_Single_Stage_Detector_Using_Recurrent_Rolling_Convolution_637%22+target%3D%22_blank%22%3E%3C%2Fa%3EAccurate+Single+Stage+Detector+Using+Recurrent+Rolling+Convolution%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2017.+SenseTime%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+Recurrent+Rolling+Convolution+%28RRC%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.05776%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.05776%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FxiaohaoChen%2Frrc_detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FxiaohaoChen%2Frrc_detection%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Occlusion_Reasoning_for_MultiCamera_MultiTarget_Detection_645%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Occlusion+Reasoning+for+Multi-Camera+Multi-Target+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.05775%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.05775%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LCDet_LowComplexity_FullyConvolutional_Neural_Networks_for_Object_Detection_in_Embedded_Systems_649%22+target%3D%22_blank%22%3E%3C%2Fa%3ELCDet%3A+Low-Complexity+Fully-Convolutional+Neural+Networks+for+Object+Detection+in+Embedded+Systems%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Embedded+Vision+Workshop+in+CVPR.+UC+San+Diego+%26amp%3B+Qualcomm+Inc%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1705.05922%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1705.05922%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Point_Linking_Network_for_Object_Detection_655%22+target%3D%22_blank%22%3E%3C%2Fa%3EPoint+Linking+Network+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Point+Linking+Network+%28PLN%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.03646%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.03646%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Perceptual_Generative_Adversarial_Networks_for_Small_Object_Detection_661%22+target%3D%22_blank%22%3E%3C%2Fa%3EPerceptual+Generative+Adversarial+Networks+for+Small+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.05274%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.05274%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fewshot_Object_Detection_665%22+target%3D%22_blank%22%3E%3C%2Fa%3EFew-shot+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.08249%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.08249%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YesNet_An_effective_Detector_Based_on_Global_Information_669%22+target%3D%22_blank%22%3E%3C%2Fa%3EYes-Net%3A+An+effective+Detector+Based+on+Global+Information%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.09180%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.09180%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SMC_Faster_RCNN_Toward_a_scenespecialized_multiobject_detector_673%22+target%3D%22_blank%22%3E%3C%2Fa%3ESMC+Faster+R-CNN%3A+Toward+a+scene-specialized+multi-object+detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.10217%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.10217%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Towards_lightweight_convolutional_neural_networks_for_object_detection_677%22+target%3D%22_blank%22%3E%3C%2Fa%3ETowards+lightweight+convolutional+neural+networks+for+object+detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.01395%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.01395%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RON_Reverse_Connection_with_Objectness_Prior_Networks_for_Object_Detection_681%22+target%3D%22_blank%22%3E%3C%2Fa%3ERON%3A+Reverse+Connection+with+Objectness+Prior+Networks+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2017%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.01691%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.01691%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ftaokong%2FRON%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ftaokong%2FRON%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Mimicking_Very_Efficient_Network_for_Object_Detection_688%22+target%3D%22_blank%22%3E%3C%2Fa%3EMimicking+Very+Efficient+Network+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2017.+SenseTime+%26amp%3B+Beihang+University%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_cvpr_2017%2Fpapers%2FLi_Mimicking_Very_Efficient_CVPR_2017_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_cvpr_2017%2Fpapers%2FLi_Mimicking_Very_Efficient_CVPR_2017_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Residual_Features_and_Unified_Prediction_Network_for_Single_Stage_Detection_693%22+target%3D%22_blank%22%3E%3C%2Fa%3EResidual+Features+and+Unified+Prediction+Network+for+Single+Stage+Detection%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.05031%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.05031%3C%2Fa%3E%3C%2Fp%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deformable_Partbased_Fully_Convolutional_Network_for_Object_Detection_697%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeformable+Part-based+Fully+Convolutional+Network+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+BMVC+2017+%28oral%29.+Sorbonne+Universit%C3%A9s+%26amp%3B+CEDRIC%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.06175%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.06175%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Adaptive_Feeding_Achieving_Fast_and_Accurate_Detections_by_Adaptively_Combining_Object_Detectors_702%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdaptive+Feeding%3A+Achieving+Fast+and+Accurate+Detections+by+Adaptively+Combining+Object+Detectors%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2017%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.06399%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.06399%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Recurrent_Scale_Approximation_for_Object_Detection_in_CNN_707%22+target%3D%22_blank%22%3E%3C%2Fa%3ERecurrent+Scale+Approximation+for+Object+Detection+in+CNN%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2017%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+Recurrent+Scale+Approximation+%28RSA%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.09531%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.09531%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fsciencefans%2FRSA-for-object-detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fsciencefans%2FRSA-for-object-detection%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DSOD_715%22+target%3D%22_blank%22%3E%3C%2Fa%3EDSOD%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DSOD_Learning_Deeply_Supervised_Object_Detectors_from_Scratch_716%22+target%3D%22_blank%22%3E%3C%2Fa%3EDSOD%3A+Learning+Deeply+Supervised+Object+Detectors+from+Scratch%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F201808211443428%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2017.+Fudan+University+%26amp%3B+Tsinghua+University+%26amp%3B+Intel+Labs+China%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.01241%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.01241%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fszq0214%2FDSOD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fszq0214%2FDSOD%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_from_Scratch_with_Deep_Supervision_722%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+from+Scratch+with+Deep+Supervision%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.09294%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.09294%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Cp%3E%23%23RetinaNet%3C%2Fp%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Focal_Loss_for_Dense_Object_Detection_727%22+target%3D%22_blank%22%3E%3C%2Fa%3EFocal+Loss+for+Dense+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2017+Best+student+paper+award.+Facebook+AI+Research%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+RetinaNet%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.02002%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.02002%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Focal_Loss_Dense_Detector_for_Vehicle_Surveillance_733%22+target%3D%22_blank%22%3E%3C%2Fa%3EFocal+Loss+Dense+Detector+for+Vehicle+Surveillance%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.01114%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.01114%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CoupleNet_Coupling_Global_Structure_with_Local_Parts_for_Object_Detection_737%22+target%3D%22_blank%22%3E%3C%2Fa%3ECoupleNet%3A+Coupling+Global+Structure+with+Local+Parts+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2017%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.02863%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.02863%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Incremental_Learning_of_Object_Detectors_without_Catastrophic_Forgetting_742%22+target%3D%22_blank%22%3E%3C%2Fa%3EIncremental+Learning+of+Object+Detectors+without+Catastrophic+Forgetting%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2017.+Inria%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.06977%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.06977%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Zoom_OutandIn_Network_with_Map_Attention_Decision_for_Region_Proposal_and_Object_Detection_747%22+target%3D%22_blank%22%3E%3C%2Fa%3EZoom+Out-and-In+Network+with+Map+Attention+Decision+for+Region+Proposal+and+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1709.04347%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1709.04347%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22StairNet_TopDown_Semantic_Aggregation_for_Accurate_One_Shot_Detection_751%22+target%3D%22_blank%22%3E%3C%2Fa%3EStairNet%3A+Top-Down+Semantic+Aggregation+for+Accurate+One+Shot+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1709.05788%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1709.05788%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Dynamic_Zoomin_Network_for_Fast_Object_Detection_in_Large_Images_755%22+target%3D%22_blank%22%3E%3C%2Fa%3EDynamic+Zoom-in+Network+for+Fast+Object+Detection+in+Large+Images%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.05187%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.05187%3C%2Fa%3E%3C%2Fp%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroAnnotation_Object_Detection_with_Web_Knowledge_Transfer_759%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Annotation+Object+Detection+with+Web+Knowledge+Transfer%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+NTU%2C+Singapore+%26amp%3B+Amazon%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+multi-instance+multi-label+domain+adaption+learning+framework%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.05954%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.05954%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MegDet_767%22+target%3D%22_blank%22%3E%3C%2Fa%3EMegDet%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MegDet_A_Large_MiniBatch_Object_Detector_768%22+target%3D%22_blank%22%3E%3C%2Fa%3EMegDet%3A+A+Large+Mini-Batch+Object+Detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Peking+University+%26amp%3B+Tsinghua+University+%26amp%3B+Megvii+Inc%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.07240%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.07240%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SingleShot_Refinement_Neural_Network_for_Object_Detection_773%22+target%3D%22_blank%22%3E%3C%2Fa%3ESingle-Shot+Refinement+Neural+Network+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.06897%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.06897%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fsfzhang15%2FRefineDet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fsfzhang15%2FRefineDet%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FMTCloudVision%2FRefineDet-Mxnet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FMTCloudVision%2FRefineDet-Mxnet%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Receptive_Field_Block_Net_for_Accurate_and_Fast_Object_Detection_779%22+target%3D%22_blank%22%3E%3C%2Fa%3EReceptive+Field+Block+Net+for+Accurate+and+Fast+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+RFBNet%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.07767%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.07767%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2F%2Fruinmessi%2FRFBNet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2F%2Fruinmessi%2FRFBNet%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22An_Analysis_of_Scale_Invariance_in_Object_Detection__SNIP_785%22+target%3D%22_blank%22%3E%3C%2Fa%3EAn+Analysis+of+Scale+Invariance+in+Object+Detection+-+SNIP%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.08189%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.08189%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fsnip%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fsnip%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Feature_Selective_Networks_for_Object_Detection_791%22+target%3D%22_blank%22%3E%3C%2Fa%3EFeature+Selective+Networks+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.08879%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.08879%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_a_Rotation_Invariant_Detector_with_Rotatable_Bounding_Box_795%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+a+Rotation+Invariant+Detector+with+Rotatable+Bounding+Box%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.09405%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.09405%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28official%2C+Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fliulei01%2FDRBox%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fliulei01%2FDRBox%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Scalable_Object_Detection_for_Stylized_Objects_800%22+target%3D%22_blank%22%3E%3C%2Fa%3EScalable+Object+Detection+for+Stylized+Objects%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Microsoft+AI+%26amp%3B+Research+Munich%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.09822%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.09822%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Object_Detectors_from_Scratch_with_Gated_Recurrent_Feature_Pyramids_805%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Object+Detectors+from+Scratch+with+Gated+Recurrent+Feature+Pyramids%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.00886%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.00886%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fszq0214%2FGRP-DSOD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fszq0214%2FGRP-DSOD%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Regionlets_for_Object_Detection_810%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Regionlets+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Ekeywords%3A+region+selection+network%2C+gating+network%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.02408%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.02408%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Training_and_Testing_Object_Detectors_with_Virtual_Images_815%22+target%3D%22_blank%22%3E%3C%2Fa%3ETraining+and+Testing+Object+Detectors+with+Virtual+Images%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+IEEE%2FCAA+Journal+of+Automatica+Sinica%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.08470%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.08470%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LargeScale_Object_Discovery_and_Detector_Adaptation_from_Unlabeled_Video_821%22+target%3D%22_blank%22%3E%3C%2Fa%3ELarge-Scale+Object+Discovery+and+Detector+Adaptation+from+Unlabeled+Video%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Ekeywords%3A+object+mining%2C+object+tracking%2C+unsupervised+object+discovery+by+appearance-based+clustering%2C+self-supervised+detector+adaptation%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.08832%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.08832%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Spot_the_Difference_by_Object_Detection_827%22+target%3D%22_blank%22%3E%3C%2Fa%3ESpot+the+Difference+by+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Tsinghua+University+%26amp%3B+JD+Group%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.01051%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.01051%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LocalizationAware_Active_Learning_for_Object_Detection_832%22+target%3D%22_blank%22%3E%3C%2Fa%3ELocalization-Aware+Active+Learning+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.05124%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.05124%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_with_Maskbased_Feature_Encoding_837%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+with+Mask-based+Feature+Encoding%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1802.03934%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1802.03934%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LSTD_A_LowShot_Transfer_Detector_for_Object_Detection_841%22+target%3D%22_blank%22%3E%3C%2Fa%3ELSTD%3A+A+Low-Shot+Transfer+Detector+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+AAAI+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.01529%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.01529%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Domain_Adaptive_Faster_RCNN_for_Object_Detection_in_the_Wild_846%22+target%3D%22_blank%22%3E%3C%2Fa%3EDomain+Adaptive+Faster+R-CNN+for+Object+Detection+in+the+Wild%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2018.+ETH+Zurich+%26amp%3B+ESAT%2FPSI%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.03243%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.03243%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28official.+Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fyuhuayc%2Fda-faster-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fyuhuayc%2Fda-faster-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Pseudo_Mask_Augmented_Object_Detection_852%22+target%3D%22_blank%22%3E%3C%2Fa%3EPseudo+Mask+Augmented+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.05858%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.05858%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Revisiting_RCNN_On_Awakening_the_Classification_Power_of_Faster_RCNN_856%22+target%3D%22_blank%22%3E%3C%2Fa%3ERevisiting+RCNN%3A+On+Awakening+the+Classification+Power+of+Faster+RCNN%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+DCR+V1%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.06799%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.06799%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28official%2C+MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbowenc0221%2FDecoupled-Classification-Refinement%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbowenc0221%2FDecoupled-Classification-Refinement%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Decoupled_Classification_Refinement_Hard_False_Positive_Suppression_for_Object_Detection_862%22+target%3D%22_blank%22%3E%3C%2Fa%3EDecoupled+Classification+Refinement%3A+Hard+False+Positive+Suppression+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Ekeywords%3A+DCR+V2%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1810.04002%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1810.04002%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28official%2C+MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbowenc0221%2FDecoupled-Classification-Refinement%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbowenc0221%2FDecoupled-Classification-Refinement%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Region_Features_for_Object_Detection_867%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Region+Features+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Peking+University+%26amp%3B+MSRA%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.07066%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.07066%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SingleShot_Bidirectional_Pyramid_Networks_for_HighQuality_Object_Detection_872%22+target%3D%22_blank%22%3E%3C%2Fa%3ESingle-Shot+Bidirectional+Pyramid+Networks+for+High-Quality+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Singapore+Management+University+%26amp%3B+Zhejiang+University%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.08208%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.08208%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_for_Comics_using_Manga109_Annotations_877%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+for+Comics+using+Manga109+Annotations%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+University+of+Tokyo+%26amp%3B+National+Institute+of+Informatics%2C+Japan%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.08670%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.08670%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22TaskDriven_Super_Resolution_Object_Detection_in_Lowresolution_Images_882%22+target%3D%22_blank%22%3E%3C%2Fa%3ETask-Driven+Super+Resolution%3A+Object+Detection+in+Low-resolution+Images%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.11316%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.11316%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Transferring_CommonSense_Knowledge_for_Object_Detection_886%22+target%3D%22_blank%22%3E%3C%2Fa%3ETransferring+Common-Sense+Knowledge+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.01077%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.01077%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Multiscale_Locationaware_Kernel_Representation_for_Object_Detection_890%22+target%3D%22_blank%22%3E%3C%2Fa%3EMulti-scale+Location-aware+Kernel+Representation+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.00428%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.00428%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FHwang64%2FMLKP%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FHwang64%2FMLKP%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Loss_Rank_Mining_A_General_Hard_Example_Mining_Method_for_Realtime_Detectors_896%22+target%3D%22_blank%22%3E%3C%2Fa%3ELoss+Rank+Mining%3A+A+General+Hard+Example+Mining+Method+for+Real-time+Detectors%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+National+University+of+Defense+Technology%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.04606%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.04606%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DetNet_A_Backbone_network_for_Object_Detection_901%22+target%3D%22_blank%22%3E%3C%2Fa%3EDetNet%3A+A+Backbone+network+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Tsinghua+University+%26amp%3B+Megvii+Inc%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.06215%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.06215%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Robust_Physical_Adversarial_Attack_on_Faster_RCNN_Object_Detector_906%22+target%3D%22_blank%22%3E%3C%2Fa%3ERobust+Physical+Adversarial+Attack+on+Faster+R-CNN+Object+Detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.05810%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.05810%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22AdvDetPatch_Attacking_Object_Detectors_with_Adversarial_Patches_910%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdvDetPatch%3A+Attacking+Object+Detectors+with+Adversarial+Patches%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1806.02299%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1806.02299%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Attacking_Object_Detectors_via_Imperceptible_Patches_on_Background_914%22+target%3D%22_blank%22%3E%3C%2Fa%3EAttacking+Object+Detectors+via+Imperceptible+Patches+on+Background%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.05966%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.05966%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Physical_Adversarial_Examples_for_Object_Detectors_918%22+target%3D%22_blank%22%3E%3C%2Fa%3EPhysical+Adversarial+Examples+for+Object+Detectors%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+WOOT+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.07769%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.07769%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Quantization_Mimic_Towards_Very_Tiny_CNN_for_Object_Detection_923%22+target%3D%22_blank%22%3E%3C%2Fa%3EQuantization+Mimic%3A+Towards+Very+Tiny+CNN+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.02152%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.02152%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_detection_at_200_Frames_Per_Second_927%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+detection+at+200+Frames+Per+Second%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+United+Technologies+Research+Center-Ireland%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.06361%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.06361%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca+id%3D%22Object_Detection_using_Domain_Randomization_and_Generative_Adversarial_Refinement_of_Synthetic_Images_932%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+using+Domain+Randomization+and+Generative+Adversarial+Refinement+of+Synthetic+Images%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2018+Deep+Vision+Workshop%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.11778%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.11778%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SNIPER_Efficient_MultiScale_Training_937%22+target%3D%22_blank%22%3E%3C%2Fa%3ESNIPER%3A+Efficient+Multi-Scale+Training%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.09300%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.09300%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmahyarnajibi%2FSNIPER%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmahyarnajibi%2FSNIPER%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Soft_Sampling_for_Robust_Object_Detection_942%22+target%3D%22_blank%22%3E%3C%2Fa%3ESoft+Sampling+for+Robust+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1806.06986%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1806.06986%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MetaAnchor_Learning_to_Detect_Objects_with_Customized_Anchors_946%22+target%3D%22_blank%22%3E%3C%2Fa%3EMetaAnchor%3A+Learning+to+Detect+Objects+with+Customized+Anchors%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Megvii+Inc+%28Face%2B%2B%29+%26amp%3B+Fudan+University%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.00980%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.00980%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Localization_Recall_Precision_LRP_A_New_Performance_Metric_for_Object_Detection_951%22+target%3D%22_blank%22%3E%3C%2Fa%3ELocalization+Recall+Precision+%28LRP%29%3A+A+New+Performance+Metric+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2018.+Middle+East+Technical+University%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.01696%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.01696%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fcancam%2FLRP%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fcancam%2FLRP%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22AutoContext_RCNN_957%22+target%3D%22_blank%22%3E%3C%2Fa%3EAuto-Context+R-CNN%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Rejected+by+ECCV18%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.02842%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.02842%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Pooling_Pyramid_Network_for_Object_Detection_962%22+target%3D%22_blank%22%3E%3C%2Fa%3EPooling+Pyramid+Network+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Google+AI+Perception%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.03284%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.03284%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Modeling_Visual_Context_is_Key_to_Augmenting_Object_Detection_Datasets_967%22+target%3D%22_blank%22%3E%3C%2Fa%3EModeling+Visual+Context+is+Key+to+Augmenting+Object+Detection+Datasets%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.07428%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.07428%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Dual_Refinement_Network_for_SingleShot_Object_Detection_972%22+target%3D%22_blank%22%3E%3C%2Fa%3EDual+Refinement+Network+for+Single-Shot+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.08638%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.08638%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Acquisition_of_Localization_Confidence_for_Accurate_Object_Detection_976%22+target%3D%22_blank%22%3E%3C%2Fa%3EAcquisition+of+Localization+Confidence+for+Accurate+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.11590%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.11590%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egihtub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fvacancy%2FPreciseRoIPooling%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fvacancy%2FPreciseRoIPooling%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CornerNet_Detecting_Objects_as_Paired_Keypoints_982%22+target%3D%22_blank%22%3E%3C%2Fa%3ECornerNet%3A+Detecting+Objects+as+Paired+Keypoints%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+IoU-Net%2C+PreciseRoIPooling%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1808.01244%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1808.01244%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fumich-vl%2FCornerNet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fumich-vl%2FCornerNet%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Unsupervised_Hard_Example_Mining_from_Videos_for_Improved_Object_Detection_989%22+target%3D%22_blank%22%3E%3C%2Fa%3EUnsupervised+Hard+Example+Mining+from+Videos+for+Improved+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1808.04285%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1808.04285%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SAN_Learning_Relationship_between_Convolutional_Features_for_MultiScale_Object_Detection_995%22+target%3D%22_blank%22%3E%3C%2Fa%3ESAN%3A+Learning+Relationship+between+Convolutional+Features+for+Multi-Scale+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1808.04974%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1808.04974%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Survey_of_Modern_Object_Detection_Literature_using_Deep_Learning_999%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Survey+of+Modern+Object+Detection+Literature+using+Deep+Learning%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1808.07256%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1808.07256%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22TinyDSOD_Lightweight_Object_Detection_for_ResourceRestricted_Usages_1003%22+target%3D%22_blank%22%3E%3C%2Fa%3ETiny-DSOD%3A+Lightweight+Object+Detection+for+Resource-Restricted+Usages%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+BMVC+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.11013%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.11013%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Flyxok1%2FTiny-DSOD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Flyxok1%2FTiny-DSOD%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Feature_Pyramid_Reconfiguration_for_Object_Detection_1008%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Feature+Pyramid+Reconfiguration+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1808.07993%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1808.07993%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MDCN_MultiScale_Deep_Inception_Convolutional_Neural_Networks_for_Efficient_Object_Detection_1012%22+target%3D%22_blank%22%3E%3C%2Fa%3EMDCN%3A+Multi-Scale%2C+Deep+Inception+Convolutional+Neural+Networks+for+Efficient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICPR+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.01791%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.01791%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Recent_Advances_in_Object_Detection_in_the_Age_of_Deep_Convolutional_Neural_Networks_1016%22+target%3D%22_blank%22%3E%3C%2Fa%3ERecent+Advances+in+Object+Detection+in+the+Age+of+Deep+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.03193%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.03193%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Learning_for_Generic_Object_Detection_A_Survey_1019%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Learning+for+Generic+Object+Detection%3A+A+Survey%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.02165%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.02165%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Training_ConfidenceCalibrated_Classifier_for_Detecting_OutofDistribution_Samples_1022%22+target%3D%22_blank%22%3E%3C%2Fa%3ETraining+Confidence-Calibrated+Classifier+for+Detecting+Out-of-Distribution+Samples%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICLR+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Falinlab%2FConfident_classifier%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Falinlab%2FConfident_classifier%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ScratchDetExploring_to_Train_SingleShot_Object_Detectors_from_Scratch_1026%22+target%3D%22_blank%22%3E%3C%2Fa%3EScratchDet%3AExploring+to+Train+Single-Shot+Object+Detectors+from+Scratch%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1810.08425%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1810.08425%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FKimSoybean%2FScratchDet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FKimSoybean%2FScratchDet%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fast_and_accurate_object_detection_in_high_resolution_4K_and_8K_video_using_GPUs_1030%22+target%3D%22_blank%22%3E%3C%2Fa%3EFast+and+accurate+object+detection+in+high+resolution+4K+and+8K+video+using+GPUs%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Best+Paper+Finalist+at+IEEE+High+Performance+Extreme+Computing+Conference+%28HPEC%29+2018%3C%2Fli%3E+%0A++++++%3Cli%3Eintro%3A+Carnegie+Mellon+University%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1810.10551%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1810.10551%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Hybrid_Knowledge_Routed_Modules_for_Largescale_Object_Detection_1035%22+target%3D%22_blank%22%3E%3C%2Fa%3EHybrid+Knowledge+Routed+Modules+for+Large-scale+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+NIPS+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1810.12681%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1810.12681%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28official%2C+PyTorch%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fchanyn%2FHKRM%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fchanyn%2FHKRM%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Gradient_Harmonized_Singlestage_Detector_1040%22+target%3D%22_blank%22%3E%3C%2Fa%3EGradient+Harmonized+Single-stage+Detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+AAAI+2019%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.05181%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.05181%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22M2Det_A_SingleShot_Object_Detector_based_on_MultiLevel_Feature_Pyramid_Network_1044%22+target%3D%22_blank%22%3E%3C%2Fa%3EM2Det%3A+A+Single-Shot+Object+Detector+based+on+Multi-Level+Feature+Pyramid+Network%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+AAAI+2019%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.04533%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.04533%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fqijiezhao%2FM2Det%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fqijiezhao%2FM2Det%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22BAN_Focusing_on_Boundary_Context_for_Object_Detection_1049%22+target%3D%22_blank%22%3E%3C%2Fa%3EBAN%3A+Focusing+on+Boundary+Context+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.05243%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.05243%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Multilayer_Pruning_Framework_for_Compressing_Single_Shot_MultiBox_Detector_1052%22+target%3D%22_blank%22%3E%3C%2Fa%3EMulti-layer+Pruning+Framework+for+Compressing+Single+Shot+MultiBox+Detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+WACV+2019%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.08342%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.08342%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22R2CNN_MultiDimensional_Attention_Based_Rotation_Invariant_Detector_with_Robust_Anchor_Strategy_1056%22+target%3D%22_blank%22%3E%3C%2Fa%3ER2CNN%2B%2B%3A+Multi-Dimensional+Attention+Based+Rotation+Invariant+Detector+with+Robust+Anchor+Strategy%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.07126%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.07126%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FDetectionTeamUCAS%2FR2CNN-Plus-Plus_Tensorflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FDetectionTeamUCAS%2FR2CNN-Plus-Plus_Tensorflow%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeRPN_Taking_a_further_step_toward_more_general_object_detection_1060%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeRPN%3A+Taking+a+further+step+toward+more+general+object+detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+AAAI+2019%3C%2Fli%3E+%0A++++++%3Cli%3Eintro%3A+South+China+University+of+Technology%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.06700%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.06700%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FHCIILAB%2FDeRPN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FHCIILAB%2FDeRPN%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fast_Efficient_Object_Detection_Using_Selective_Attention_1066%22+target%3D%22_blank%22%3E%3C%2Fa%3EFast+Efficient+Object+Detection+Using+Selective+Attention%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.07502%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.07502%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Sampling_Techniques_for_LargeScale_Object_Detection_from_Sparsely_Annotated_Objects_1070%22+target%3D%22_blank%22%3E%3C%2Fa%3ESampling+Techniques+for+Large-Scale+Object+Detection+from+Sparsely+Annotated+Objects%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.10862%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.10862%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22NonMaximum_Suppression_NMS_1074%22+target%3D%22_blank%22%3E%3C%2Fa%3ENon-Maximum+Suppression+%28NMS%29%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca+id%3D%22EndtoEnd_Integration_of_a_Convolutional_Network_Deformable_Parts_Model_and_NonMaximum_Suppression_1075%22+target%3D%22_blank%22%3E%3C%2Fa%3EEnd-to-End+Integration+of+a+Convolutional+Network%2C+Deformable+Parts+Model+and+Non-Maximum+Suppression%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2015%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1411.5309%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1411.5309%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2015%2Fpapers%2FWan_End-to-End_Integration_of_2015_CVPR_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2015%2Fpapers%2FWan_End-to-End_Integration_of_2015_CVPR_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_convnet_for_nonmaximum_suppression_1081%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+convnet+for+non-maximum+suppression%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1511.06437%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1511.06437%3C%2Fa%3E%3Cbr%3E+Improving+Object+Detection+With+One+Line+of+Code%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SoftNMS__Improving_Object_Detection_With_One_Line_of_Code_1086%22+target%3D%22_blank%22%3E%3C%2Fa%3ESoft-NMS+%E2%80%93+Improving+Object+Detection+With+One+Line+of+Code%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2017.+University+of+Maryland%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+Soft-NMS%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.04503%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.04503%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fsoft-nms%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fsoft-nms%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_nonmaximum_suppression_1093%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+non-maximum+suppression%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2017%3C%2Fli%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.mpi-inf.mpg.de%2Fdepartments%2Fcomputer-vision-and-multimodal-computing%2Fresearch%2Fobject-recognition-and-scene-understanding%2Flearning-nms%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.mpi-inf.mpg.de%2Fdepartments%2Fcomputer-vision-and-multimodal-computing%2Fresearch%2Fobject-recognition-and-scene-understanding%2Flearning-nms%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1705.02950%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1705.02950%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fhosang%2Fgossipnet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fhosang%2Fgossipnet%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Relation_Networks_for_Object_Detection_1101%22+target%3D%22_blank%22%3E%3C%2Fa%3ERelation+Networks+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2018+oral%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.11575%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.11575%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28official%2C+MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmsracver%2FRelation-Networks-for-Object-Detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmsracver%2FRelation-Networks-for-Object-Detection%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Adversarial_Examples_1108%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdversarial+Examples%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Adversarial_Examples_that_Fool_Detectors_1109%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdversarial+Examples+that+Fool+Detectors%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+University+of+Illinois%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.02494%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.02494%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Adversarial_Examples_Are_Not_Easily_Detected_Bypassing_Ten_Detection_Methods_1114%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdversarial+Examples+Are+Not+Easily+Detected%3A+Bypassing+Ten+Detection+Methods%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fnicholas.carlini.com%2Fcode%2Fnn_breaking_detection%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fnicholas.carlini.com%2Fcode%2Fnn_breaking_detection%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1705.07263%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1705.07263%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fcarlini%2Fnn_breaking_detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fcarlini%2Fnn_breaking_detection%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Weakly_Supervised_Object_Detection_1121%22+target%3D%22_blank%22%3E%3C%2Fa%3EWeakly+Supervised+Object+Detection%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca+id%3D%22Track_and_Transfer_Watching_Videos_to_Simulate_Strong_Human_Supervision_for_WeaklySupervised_Object_Detection_1122%22+target%3D%22_blank%22%3E%3C%2Fa%3ETrack+and+Transfer%3A+Watching+Videos+to+Simulate+Strong+Human+Supervision+for+Weakly-Supervised+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2016%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.05766%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.05766%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Weakly_supervised_object_detection_using_pseudostrong_labels_1127%22+target%3D%22_blank%22%3E%3C%2Fa%3EWeakly+supervised+object+detection+using+pseudo-strong+labels%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.04731%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.04731%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Saliency_Guided_EndtoEnd_Learning_for_Weakly_Supervised_Object_Detection_1131%22+target%3D%22_blank%22%3E%3C%2Fa%3ESaliency+Guided+End-to-End+Learning+for+Weakly+Supervised+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+IJCAI+2017%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.06768%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.06768%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_and_Semantic_Knowledge_Transfer_for_Large_Scale_Semisupervised_Object_Detection_1136%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+and+Semantic+Knowledge+Transfer+for+Large+Scale+Semi-supervised+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+TPAMI+2017.+National+Institutes+of+Health+%28NIH%29+Clinical+Center%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.03145%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.03145%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Video_Object_Detection_1142%22+target%3D%22_blank%22%3E%3C%2Fa%3EVideo+Object+Detection%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Object_Class_Detectors_from_Weakly_Annotated_Video_1143%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Object+Class+Detectors+from+Weakly+Annotated+Video%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2012%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.vision.ee.ethz.ch%2Fpublications%2Fpapers%2Fproceedings%2Feth_biwi_00905.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.vision.ee.ethz.ch%2Fpublications%2Fpapers%2Fproceedings%2Feth_biwi_00905.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Analysing_domain_shift_factors_between_videos_and_images_for_object_detection_1148%22+target%3D%22_blank%22%3E%3C%2Fa%3EAnalysing+domain+shift+factors+between+videos+and+images+for+object+detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1501.01186%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1501.01186%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Video_Object_Recognition_1152%22+target%3D%22_blank%22%3E%3C%2Fa%3EVideo+Object+Recognition%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fvision.princeton.edu%2Fcourses%2FCOS598%2F2015sp%2Fslides%2FVideoRecog%2FVideo%2520Object%2520Recognition.pptx%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fvision.princeton.edu%2Fcourses%2FCOS598%2F2015sp%2Fslides%2FVideoRecog%2FVideo+Object+Recognition.pptx%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Learning_for_Saliency_Prediction_in_Natural_Video_1156%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Learning+for+Saliency+Prediction+in+Natural+Video%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Submitted+on+12+Jan+2016%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+Deep+learning%2C+saliency+map%2C+optical+flow%2C+convolution+network%2C+contrast+features%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fhal.archives-ouvertes.fr%2Fhal-01251614%2Fdocument%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fhal.archives-ouvertes.fr%2Fhal-01251614%2Fdocument%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22TCNN_Tubelets_with_Convolutional_Neural_Networks_for_Object_Detection_from_Videos_1162%22+target%3D%22_blank%22%3E%3C%2Fa%3ET-CNN%3A+Tubelets+with+Convolutional+Neural+Networks+for+Object+Detection+from+Videos%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Winning+solution+in+ILSVRC2015+Object+Detection+from+Video%28VID%29+Task%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.02532%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.02532%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmyfavouritekk%2FT-CNN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmyfavouritekk%2FT-CNN%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_from_Video_Tubelets_with_Convolutional_Neural_Networks_1168%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+from+Video+Tubelets+with+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2016+Spotlight+paper%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1604.04053%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1604.04053%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Ewlouyang%2FPapers%2FKangVideoDet_CVPR16.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Ewlouyang%2FPapers%2FKangVideoDet_CVPR16.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egihtub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmyfavouritekk%2Fvdetlib%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmyfavouritekk%2Fvdetlib%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_in_Videos_with_Tubelets_and_Multicontext_Cues_1175%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+in+Videos+with+Tubelets+and+Multi-context+Cues%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+SenseTime+Group%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Exgwang%2FCUvideo.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Exgwang%2FCUvideo.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks%2FObject%2520Detection%2520in%2520Videos%2520with%2520Tubelets%2520and%2520Multi-context%2520Cues%2520-%2520Final.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks%2FObject+Detection+in+Videos+with+Tubelets+and+Multi-context+Cues+-+Final.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Context_Matters_Refining_Object_Detection_in_Video_with_Recurrent_Neural_Networks_1181%22+target%3D%22_blank%22%3E%3C%2Fa%3EContext+Matters%3A+Refining+Object+Detection+in+Video+with+Recurrent+Neural+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+BMVC+2016%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+pseudo-labeler%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.04648%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.04648%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fvision.cornell.edu%2Fse3%2Fwp-content%2Fuploads%2F2016%2F07%2Fvideo_object_detection_BMVC.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fvision.cornell.edu%2Fse3%2Fwp-content%2Fuploads%2F2016%2F07%2Fvideo_object_detection_BMVC.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CNN_Based_Object_Detection_in_Large_Video_Images_1188%22+target%3D%22_blank%22%3E%3C%2Fa%3ECNN+Based+Object+Detection+in+Large+Video+Images%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+WangTao+%40+%E7%88%B1%E5%A5%87%E8%89%BA%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+object+retrieval%2C+object+detection%2C+scene+classification%3C%2Fli%3E+%0A++++++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fon-demand.gputechconf.com%2Fgtc%2F2016%2Fpresentation%2Fs6362-wang-tao-cnn-based-object-detection-large-video-images.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fon-demand.gputechconf.com%2Fgtc%2F2016%2Fpresentation%2Fs6362-wang-tao-cnn-based-object-detection-large-video-images.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_in_Videos_with_Tubelet_Proposal_Networks_1194%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+in+Videos+with+Tubelet+Proposal+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.06355%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.06355%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22FlowGuided_Feature_Aggregation_for_Video_Object_Detection_1198%22+target%3D%22_blank%22%3E%3C%2Fa%3EFlow-Guided+Feature+Aggregation+for+Video+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+MSRA%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1703.10025%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1703.10025%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Video_Object_Detection_using_Faster_RCNN_1203%22+target%3D%22_blank%22%3E%3C%2Fa%3EVideo+Object+Detection+using+Faster+R-CNN%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eblog%3A+%3Ca+href%3D%22http%3A%2F%2Fandrewliao11.github.io%2Fobject_detection%2Ffaster_rcnn%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fandrewliao11.github.io%2Fobject_detection%2Ffaster_rcnn%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fandrewliao11%2Fpy-faster-rcnn-imagenet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fandrewliao11%2Fpy-faster-rcnn-imagenet%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Improving_Context_Modeling_for_Video_Object_Detection_and_Tracking_1208%22+target%3D%22_blank%22%3E%3C%2Fa%3EImproving+Context+Modeling+for+Video+Object+Detection+and+Tracking%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Ca+href%3D%22http%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks_2017%2Filsvrc2017_short%28poster%29.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks_2017%2Filsvrc2017_short%28poster%29.pdf%3C%2Fa%3E%3C%2Fp%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Temporal_Dynamic_Graph_LSTM_for_Actiondriven_Video_Object_Detection_1212%22+target%3D%22_blank%22%3E%3C%2Fa%3ETemporal+Dynamic+Graph+LSTM+for+Action-driven+Video+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2017%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.00666%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.00666%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Mobile_Video_Object_Detection_with_TemporallyAware_Feature_Maps_1217%22+target%3D%22_blank%22%3E%3C%2Fa%3EMobile+Video+Object+Detection+with+Temporally-Aware+Feature+Maps%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.06368%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.06368%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Towards_High_Performance_Video_Object_Detection_1221%22+target%3D%22_blank%22%3E%3C%2Fa%3ETowards+High+Performance+Video+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.11577%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.11577%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Impression_Network_for_Video_Object_Detection_1225%22+target%3D%22_blank%22%3E%3C%2Fa%3EImpression+Network+for+Video+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.05896%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.05896%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SpatialTemporal_Memory_Networks_for_Video_Object_Detection_1229%22+target%3D%22_blank%22%3E%3C%2Fa%3ESpatial-Temporal+Memory+Networks+for+Video+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.06317%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.06317%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%223DDETNet_a_Single_Stage_VideoBased_Vehicle_Detector_1233%22+target%3D%22_blank%22%3E%3C%2Fa%3E3D-DETNet%3A+a+Single+Stage+Video-Based+Vehicle+Detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.01769%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.01769%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_in_Videos_by_Short_and_Long_Range_Object_Linking_1237%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+in+Videos+by+Short+and+Long+Range+Object+Linking%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.09823%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.09823%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_in_Video_with_Spatiotemporal_Sampling_Networks_1241%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+in+Video+with+Spatiotemporal+Sampling+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+University+of+Pennsylvania%2C+2Dartmouth+College%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.05549%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.05549%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Towards_High_Performance_Video_Object_Detection_for_Mobiles_1246%22+target%3D%22_blank%22%3E%3C%2Fa%3ETowards+High+Performance+Video+Object+Detection+for+Mobiles%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Microsoft+Research+Asia%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.05830%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.05830%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Optimizing_Video_Object_Detection_via_a_ScaleTime_Lattice_1251%22+target%3D%22_blank%22%3E%3C%2Fa%3EOptimizing+Video+Object+Detection+via+a+Scale-Time+Lattice%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2018%3C%2Fli%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fmmlab.ie.cuhk.edu.hk%2Fprojects%2FST-Lattice%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fmmlab.ie.cuhk.edu.hk%2Fprojects%2FST-Lattice%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.05472%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.05472%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fhellock%2Fscale-time-lattice%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fhellock%2Fscale-time-lattice%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Pack_and_Detect_Fast_Object_Detection_in_Videos_Using_RegionofInterest_Packing_1258%22+target%3D%22_blank%22%3E%3C%2Fa%3EPack+and+Detect%3A+Fast+Object+Detection+in+Videos+Using+Region-of-Interest+Packing%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.01701%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.01701%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fast_Object_Detection_in_Compressed_Video_1261%22+target%3D%22_blank%22%3E%3C%2Fa%3EFast+Object+Detection+in+Compressed+Video%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.11057%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.11057%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_on_Mobile_Devices_1266%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+on+Mobile+Devices%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Pelee_A_RealTime_Object_Detection_System_on_Mobile_Devices_1267%22+target%3D%22_blank%22%3E%3C%2Fa%3EPelee%3A+A+Real-Time+Object+Detection+System+on+Mobile+Devices%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICLR+2018+workshop+track%3C%2Fli%3E+%0A++++++%3Cli%3Eintro%3A+based+on+the+SSD%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.06882%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.06882%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FRobert-JunWang%2FPelee%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FRobert-JunWang%2FPelee%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_in_3D_1276%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+in+3D%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Vote3Deep_Fast_Object_Detection_in_3D_Point_Clouds_Using_Efficient_Convolutional_Neural_Networks_1277%22+target%3D%22_blank%22%3E%3C%2Fa%3EVote3Deep%3A+Fast+Object+Detection+in+3D+Point+Clouds+Using+Efficient+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1609.06666%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1609.06666%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ComplexYOLO_Realtime_3D_Object_Detection_on_Point_Clouds_1281%22+target%3D%22_blank%22%3E%3C%2Fa%3EComplex-YOLO%3A+Real-time+3D+Object+Detection+on+Point+Clouds%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Valeo+Schalter+und+Sensoren+GmbH+%26amp%3B+Ilmenau+University+of+Technology%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.06199%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.06199%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Focal_Loss_in_3D_Object_Detection_1286%22+target%3D%22_blank%22%3E%3C%2Fa%3EFocal+Loss+in+3D+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.06065%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.06065%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fpyun-ram%2FFL3D%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fpyun-ram%2FFL3D%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_on_RGBD_1291%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+on+RGB-D%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Rich_Features_from_RGBD_Images_for_Object_Detection_and_Segmentation_1292%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Rich+Features+from+RGB-D+Images+for+Object+Detection+and+Segmentation%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1407.5736%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1407.5736%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Differential_Geometry_Boosts_Convolutional_Neural_Networks_for_Object_Detection_1296%22+target%3D%22_blank%22%3E%3C%2Fa%3EDifferential+Geometry+Boosts+Convolutional+Neural+Networks+for+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2016%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016_workshops%2Fw23%2Fhtml%2FWang_Differential_Geometry_Boosts_CVPR_2016_paper.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016_workshops%2Fw23%2Fhtml%2FWang_Differential_Geometry_Boosts_CVPR_2016_paper.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca+id%3D%22A_Selfsupervised_Learning_System_for_Object_Detection_using_Physics_Simulation_and_Multiview_Pose_Estimation_1301%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Self-supervised+Learning+System+for+Object+Detection+using+Physics+Simulation+and+Multi-view+Pose+Estimation%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1703.03347%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1703.03347%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroShot_Object_Detection_1307%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Shot+Object+Detection%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroShot_Detection_1308%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Shot+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Australian+National+University%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+YOLO%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.07113%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.07113%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroShot_Object_Detection_1314%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Shot+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.04340%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.04340%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroShot_Object_Detection_Learning_to_Simultaneously_Recognize_and_Localize_Novel_Concepts_1318%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Shot+Object+Detection%3A+Learning+to+Simultaneously+Recognize+and+Localize+Novel+Concepts%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Australian+National+University%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.06049%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.06049%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroShot_Object_Detection_by_Hybrid_Region_Embedding_1323%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Shot+Object+Detection+by+Hybrid+Region+Embedding%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Middle+East+Technical+University+%26amp%3B+Hacettepe+University%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.06157%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.06157%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Salient_Object_Detection_1330%22+target%3D%22_blank%22%3E%3C%2Fa%3ESalient+Object+Detection%3C%2Fh2%3E+%0A+++++%3Cp%3EThis+task+involves+predicting+the+salient+regions+of+an+image+given+by+human+eye+fixations.%3C%2Fp%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Best_Deep_Saliency_Detection_Models_CVPR_2016__2015_1333%22+target%3D%22_blank%22%3E%3C%2Fa%3EBest+Deep+Saliency+Detection+Models+%28CVPR+2016+%26amp%3B+2015%29%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Epage%3A+%3Ca+href%3D%22http%3A%2F%2Fi.cs.hku.hk%2F%7Eyzyu%2Fvision.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fi.cs.hku.hk%2F%7Eyzyu%2Fvision.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Largescale_optimization_of_hierarchical_features_for_saliency_prediction_in_natural_images_1337%22+target%3D%22_blank%22%3E%3C%2Fa%3ELarge-scale+optimization+of+hierarchical+features+for+saliency+prediction+in+natural+images%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fcoxlab.org%2Fpdfs%2Fcvpr2014_vig_saliency.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcoxlab.org%2Fpdfs%2Fcvpr2014_vig_saliency.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Predicting_Eye_Fixations_using_Convolutional_Neural_Networks_1341%22+target%3D%22_blank%22%3E%3C%2Fa%3EPredicting+Eye+Fixations+using+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.escience.cn%2Fsystem%2Ffile%3FfileId%3D72648%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.escience.cn%2Fsystem%2Ffile%3FfileId%3D72648%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Saliency_Detection_by_MultiContext_Deep_Learning_1345%22+target%3D%22_blank%22%3E%3C%2Fa%3ESaliency+Detection+by+Multi-Context+Deep+Learning%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2015%2Fpapers%2FZhao_Saliency_Detection_by_2015_CVPR_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2015%2Fpapers%2FZhao_Saliency_Detection_by_2015_CVPR_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeepSaliency_MultiTask_Deep_Neural_Network_Model_for_Salient_Object_Detection_1349%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeepSaliency%3A+Multi-Task+Deep+Neural+Network+Model+for+Salient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1510.05484%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1510.05484%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SuperCNN_A_Superpixelwise_Convolutional_Neural_Network_for_Salient_Object_Detection_1353%22+target%3D%22_blank%22%3E%3C%2Fa%3ESuperCNN%3A+A+Superpixelwise+Convolutional+Neural+Network+for+Salient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.shengfenghe.com%2Fsupercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ewww.shengfenghe.com%2Fsupercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Shallow_and_Deep_Convolutional_Networks_for_Saliency_Prediction_1357%22+target%3D%22_blank%22%3E%3C%2Fa%3EShallow+and+Deep+Convolutional+Networks+for+Saliency+Prediction%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2016%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1603.00845%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1603.00845%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fimatge-upc%2Fsaliency-2016-cvpr%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fimatge-upc%2Fsaliency-2016-cvpr%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Recurrent_Attentional_Networks_for_Saliency_Detection_1363%22+target%3D%22_blank%22%3E%3C%2Fa%3ERecurrent+Attentional+Networks+for+Saliency+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2016.+recurrent+attentional+convolutional-deconvolution+network+%28RACDNN%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.03227%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.03227%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22TwoStream_Convolutional_Networks_for_Dynamic_Saliency_Prediction_1368%22+target%3D%22_blank%22%3E%3C%2Fa%3ETwo-Stream+Convolutional+Networks+for+Dynamic+Saliency+Prediction%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.04730%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.04730%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Unconstrained_Salient_Object_Detection_1374%22+target%3D%22_blank%22%3E%3C%2Fa%3EUnconstrained+Salient+Object+Detection%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Unconstrained_Salient_Object_Detection_via_Proposal_Subset_Optimization_1375%22+target%3D%22_blank%22%3E%3C%2Fa%3EUnconstrained+Salient+Object+Detection+via+Proposal+Subset+Optimization%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F20180821144418842%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2016%3C%2Fli%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2Fsod.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2Fsod.html%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2FSOD%2FCVPR16SOD_camera_ready.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2FSOD%2FCVPR16SOD_camera_ready.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fjimmie33%2FSOD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fjimmie33%2FSOD%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Ecaffe+model+zoo%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fwiki%2FModel-Zoo%23cnn-object-proposal-models-for-salient-object-detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fwiki%2FModel-Zoo%23cnn-object-proposal-models-for-salient-object-detection%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DHSNet_Deep_Hierarchical_Saliency_Network_for_Salient_Object_Detection_1383%22+target%3D%22_blank%22%3E%3C%2Fa%3EDHSNet%3A+Deep+Hierarchical+Saliency+Network+for+Salient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FLiu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FLiu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Salient_Object_Subitizing_1387%22+target%3D%22_blank%22%3E%3C%2Fa%3ESalient+Object+Subitizing%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F20180821144443814%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2015%3C%2Fli%3E+%0A++++++%3Cli%3Eintro%3A+predicting+the+existence+and+the+number+of+salient+objects+in+an+image+using+holistic+cues%3C%2Fli%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2Fsos.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2Fsos.html%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.07525%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.07525%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2FSOS%2FSOS_preprint.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2FSOS%2FSOS_preprint.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Ecaffe+model+zoo%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fwiki%2FModel-Zoo%23cnn-models-for-salient-object-subitizing%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fwiki%2FModel-Zoo%23cnn-models-for-salient-object-subitizing%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeeplySupervised_Recurrent_Convolutional_Neural_Network_for_Saliency_Detection_1396%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeeply-Supervised+Recurrent+Convolutional+Neural+Network+for+Saliency+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ACMMM+2016.+deeply-supervised+recurrent+convolutional+neural+network+%28DSRCNN%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.05177%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.05177%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Saliency_Detection_via_Combining_RegionLevel_and_PixelLevel_Predictions_with_CNNs_1401%22+target%3D%22_blank%22%3E%3C%2Fa%3ESaliency+Detection+via+Combining+Region-Level+and+Pixel-Level+Predictions+with+CNNs%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2016%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.05186%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.05186%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Edge_Preserving_and_MultiScale_Contextual_Neural_Network_for_Salient_Object_Detection_1406%22+target%3D%22_blank%22%3E%3C%2Fa%3EEdge+Preserving+and+Multi-Scale+Contextual+Neural+Network+for+Salient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.08029%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.08029%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Deep_MultiLevel_Network_for_Saliency_Prediction_1410%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Deep+Multi-Level+Network+for+Saliency+Prediction%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1609.01064%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1609.01064%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_Saliency_Detection_Based_on_Multiscale_Deep_CNN_Features_1414%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+Saliency+Detection+Based+on+Multiscale+Deep+CNN+Features%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+IEEE+Transactions+on+Image+Processing%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1609.02077%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1609.02077%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Deep_Spatial_Contextual_Longterm_Recurrent_Convolutional_Network_for_Saliency_Detection_1419%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Deep+Spatial+Contextual+Long-term+Recurrent+Convolutional+Network+for+Saliency+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+DSCLRCN%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1610.01708%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1610.01708%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deeply_supervised_salient_object_detection_with_short_connections_1424%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeeply+supervised+salient+object+detection+with+short+connections%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+IEEE+TPAMI+2018+%28IEEE+CVPR+2017%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.04849%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.04849%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28official%2C+Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FAndrew-Qibin%2FDSS%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FAndrew-Qibin%2FDSS%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Tensorflow%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FJoker316701882%2FSalient-Object-Detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FJoker316701882%2FSalient-Object-Detection%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Weakly_Supervised_Topdown_Salient_Object_Detection_1431%22+target%3D%22_blank%22%3E%3C%2Fa%3EWeakly+Supervised+Top-down+Salient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Nanyang+Technological+University%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.05345%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.05345%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SalGAN_Visual_Saliency_Prediction_with_Generative_Adversarial_Networks_1436%22+target%3D%22_blank%22%3E%3C%2Fa%3ESalGAN%3A+Visual+Saliency+Prediction+with+Generative+Adversarial+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22https%3A%2F%2Fimatge-upc.github.io%2Fsaliency-salgan-2017%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fimatge-upc.github.io%2Fsaliency-salgan-2017%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1701.01081%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1701.01081%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_Saliency_Prediction_Using_a_Mixture_of_Deep_Neural_Networks_1441%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+Saliency+Prediction+Using+a+Mixture+of+Deep+Neural+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.00372%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.00372%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Fast_and_Compact_Salient_Score_Regression_Network_Based_on_Fully_Convolutional_Network_1445%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Fast+and+Compact+Salient+Score+Regression+Network+Based+on+Fully+Convolutional+Network%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.00615%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.00615%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Saliency_Detection_by_Forward_and_Backward_Cues_in_DeepCNNs_1449%22+target%3D%22_blank%22%3E%3C%2Fa%3ESaliency+Detection+by+Forward+and+Backward+Cues+in+Deep-CNNs%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1703.00152%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1703.00152%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Supervised_Adversarial_Networks_for_Image_Saliency_Detection_1453%22+target%3D%22_blank%22%3E%3C%2Fa%3ESupervised+Adversarial+Networks+for+Image+Saliency+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.07242%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.07242%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Groupwise_Deep_Cosaliency_Detection_1457%22+target%3D%22_blank%22%3E%3C%2Fa%3EGroup-wise+Deep+Co-saliency+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.07381%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.07381%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Towards_the_Success_Rate_of_One_Realtime_Unconstrained_Salient_Object_Detection_1461%22+target%3D%22_blank%22%3E%3C%2Fa%3ETowards+the+Success+Rate+of+One%3A+Real-time+Unconstrained+Salient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+University+of+Maryland+College+Park+%26amp%3B+eBay+Inc%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.00079%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.00079%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Amulet_Aggregating_Multilevel_Convolutional_Features_for_Salient_Object_Detection_1466%22+target%3D%22_blank%22%3E%3C%2Fa%3EAmulet%3A+Aggregating+Multi-level+Convolutional+Features+for+Salient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2017%3C%2Fli%3E+%0A++++++%3Cli%3Earixv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.02001%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.02001%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Uncertain_Convolutional_Features_for_Accurate_Saliency_Detection_1471%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Uncertain+Convolutional+Features+for+Accurate+Saliency+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Accepted+as+a+poster+in+ICCV+2017%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.02031%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.02031%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_EdgeAware_Saliency_Detection_1476%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Edge-Aware+Saliency+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.04366%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.04366%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Selfexplanatory_Deep_Salient_Object_Detection_1480%22+target%3D%22_blank%22%3E%3C%2Fa%3ESelf-explanatory+Deep+Salient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+National+University+of+Defense+Technology%2C+China+%26amp%3B+National+University+of+Singapore%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.05595%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.05595%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca+id%3D%22PiCANet_Learning_Pixelwise_Contextual_Attention_in_ConvNets_and_Its_Application_in_Saliency_Detection_1485%22+target%3D%22_blank%22%3E%3C%2Fa%3EPiCANet%3A+Learning+Pixel-wise+Contextual+Attention+in+ConvNets+and+Its+Application+in+Saliency+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.06433%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.06433%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca+id%3D%22DeepFeat_A_Bottom_Up_and_Top_Down_Saliency_Model_Based_on_Deep_Features_of_Convolutional_Neural_Nets_1489%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeepFeat%3A+A+Bottom+Up+and+Top+Down+Saliency+Model+Based+on+Deep+Features+of+Convolutional+Neural+Nets%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1709.02495%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1709.02495%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Recurrently_Aggregating_Deep_Features_for_Salient_Object_Detection_1493%22+target%3D%22_blank%22%3E%3C%2Fa%3ERecurrently+Aggregating+Deep+Features+for+Salient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+AAAI+2018%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FAAAI%2FAAAI18%2Fpaper%2Fview%2F16775%2F16281%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FAAAI%2FAAAI18%2Fpaper%2Fview%2F16775%2F16281%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_saliency_What_is_learnt_by_a_deep_network_about_saliency_1498%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+saliency%3A+What+is+learnt+by+a+deep+network+about+saliency%3F%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+2nd+Workshop+on+Visualisation+for+Deep+Learning+in+the+34th+International+Conference+On+Machine+Learning%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.04261%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.04261%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ContrastOriented_Deep_Neural_Networks_for_Salient_Object_Detection_1503%22+target%3D%22_blank%22%3E%3C%2Fa%3EContrast-Oriented+Deep+Neural+Networks+for+Salient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+TNNLS%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.11395%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.11395%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Salient_Object_Detection_by_Lossless_Feature_Reflection_1508%22+target%3D%22_blank%22%3E%3C%2Fa%3ESalient+Object+Detection+by+Lossless+Feature+Reflection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+IJCAI+2018%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1802.06527%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1802.06527%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22HyperFusionNet_Densely_Reflective_Fusion_for_Salient_Object_Detection_1513%22+target%3D%22_blank%22%3E%3C%2Fa%3EHyperFusion-Net%3A+Densely+Reflective+Fusion+for+Salient+Object+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.05142%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.05142%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Video_Saliency_Detection_1520%22+target%3D%22_blank%22%3E%3C%2Fa%3EVideo+Saliency+Detection%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Learning_For_Video_Saliency_Detection_1521%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Learning+For+Video+Saliency+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.00871%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.00871%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Video_Salient_Object_Detection_Using_Spatiotemporal_Deep_Features_1525%22+target%3D%22_blank%22%3E%3C%2Fa%3EVideo+Salient+Object+Detection+Using+Spatiotemporal+Deep+Features%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.01447%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.01447%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Predicting_Video_Saliency_with_ObjecttoMotion_CNN_and_Twolayer_Convolutional_LSTM_1529%22+target%3D%22_blank%22%3E%3C%2Fa%3EPredicting+Video+Saliency+with+Object-to-Motion+CNN+and+Two-layer+Convolutional+LSTM%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1709.06316%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1709.06316%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_Relationship_Detection_1536%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+Relationship+Detection%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_Relationship_Detection_with_Language_Priors_1537%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+Relationship+Detection+with+Language+Priors%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2016+oral%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fcs.stanford.edu%2Fpeople%2Franjaykrishna%2Fvrd%2Fvrd.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fcs.stanford.edu%2Fpeople%2Franjaykrishna%2Fvrd%2Fvrd.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FProf-Lu-Cewu%2FVisual-Relationship-Detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FProf-Lu-Cewu%2FVisual-Relationship-Detection%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ViPCNN_A_Visual_Phrase_Reasoning_Convolutional_Neural_Network_for_Visual_Relationship_Detection_1543%22+target%3D%22_blank%22%3E%3C%2Fa%3EViP-CNN%3A+A+Visual+Phrase+Reasoning+Convolutional+Neural+Network+for+Visual+Relationship+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Visual+Phrase+reasoning+Convolutional+Neural+Network+%28ViP-CNN%29%2C+Visual+Phrase+Reasoning+Structure+%28VPRS%29%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.07191%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.07191%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_Translation_Embedding_Network_for_Visual_Relation_Detection_1548%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+Translation+Embedding+Network+for+Visual+Relation+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.arxiv.org%2Fabs%2F1702.08319%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.arxiv.org%2Fabs%2F1702.08319%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Variationstructured_Reinforcement_Learning_for_Visual_Relationship_and_Attribute_Detection_1552%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Variation-structured+Reinforcement+Learning+for+Visual+Relationship+and+Attribute+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2017+spotlight+paper%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1703.03054%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1703.03054%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Detecting_Visual_Relationships_with_Deep_Relational_Networks_1557%22+target%3D%22_blank%22%3E%3C%2Fa%3EDetecting+Visual+Relationships+with+Deep+Relational+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CVPR+2017+oral.+The+Chinese+University+of+Hong+Kong%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.03114%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.03114%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Identifying_Spatial_Relations_in_Images_using_Convolutional_Neural_Networks_1562%22+target%3D%22_blank%22%3E%3C%2Fa%3EIdentifying+Spatial+Relations+in+Images+using+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.04215%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.04215%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22PPRFCN_Weakly_Supervised_Visual_Relation_Detection_via_Parallel_Pairwise_RFCN_1566%22+target%3D%22_blank%22%3E%3C%2Fa%3EPPR-FCN%3A+Weakly+Supervised+Visual+Relation+Detection+via+Parallel+Pairwise+R-FCN%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.01956%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.01956%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Natural_Language_Guided_Visual_Relationship_Detection_1571%22+target%3D%22_blank%22%3E%3C%2Fa%3ENatural+Language+Guided+Visual+Relationship+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.06032%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.06032%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Detecting_Visual_Relationships_Using_Box_Attention_1575%22+target%3D%22_blank%22%3E%3C%2Fa%3EDetecting+Visual+Relationships+Using+Box+Attention%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Google+AI+%26amp%3B+IST+Austria%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.02136%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.02136%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Google_AI_Open_Images__Visual_Relationship_Track_1580%22+target%3D%22_blank%22%3E%3C%2Fa%3EGoogle+AI+Open+Images+-+Visual+Relationship+Track%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Detect+pairs+of+objects+in+particular+relationships%3C%2Fli%3E+%0A++++++%3Cli%3Ekaggle%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.kaggle.com%2Fc%2Fgoogle-ai-open-images-visual-relationship-track%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.kaggle.com%2Fc%2Fgoogle-ai-open-images-visual-relationship-track%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ContextDependent_Diffusion_Network_for_Visual_Relationship_Detection_1585%22+target%3D%22_blank%22%3E%3C%2Fa%3EContext-Dependent+Diffusion+Network+for+Visual+Relationship+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+2018+ACM+Multimedia+Conference%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.06213%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.06213%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Problem_Reduction_Approach_for_Visual_Relationships_Detection_1589%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Problem+Reduction+Approach+for+Visual+Relationships+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2018+Workshop%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.09828%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.09828%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Face_Deteciton_1595%22+target%3D%22_blank%22%3E%3C%2Fa%3EFace+Deteciton%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Multiview_Face_Detection_Using_Deep_Convolutional_Neural_Networks_1596%22+target%3D%22_blank%22%3E%3C%2Fa%3EMulti-view+Face+Detection+Using+Deep+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+Yahoo%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1502.02766%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1502.02766%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fguoyilin%2FFaceDetection_CNN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fguoyilin%2FFaceDetection_CNN%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22From_Facial_Parts_Responses_to_Face_Detection_A_Deep_Learning_Approach_1602%22+target%3D%22_blank%22%3E%3C%2Fa%3EFrom+Facial+Parts+Responses+to+Face+Detection%3A+A+Deep+Learning+Approach%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICCV+2015.+CUHK%3C%2Fli%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fpersonal.ie.cuhk.edu.hk%2F%7Eys014%2Fprojects%2FFaceness%2FFaceness.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpersonal.ie.cuhk.edu.hk%2F%7Eys014%2Fprojects%2FFaceness%2FFaceness.html%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1509.06451%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1509.06451%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_iccv_2015%2Fpapers%2FYang_From_Facial_Parts_ICCV_2015_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_iccv_2015%2Fpapers%2FYang_From_Facial_Parts_ICCV_2015_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Compact_Convolutional_Neural_Network_Cascade_for_Face_Detection_1611%22+target%3D%22_blank%22%3E%3C%2Fa%3ECompact+Convolutional+Neural+Network+Cascade+for+Face+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1508.01292%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1508.01292%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBkmz21%2FFD-Evaluation%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBkmz21%2FFD-Evaluation%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBkmz21%2FCompactCNNCascade%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBkmz21%2FCompactCNNCascade%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Face_Detection_with_EndtoEnd_Integration_of_a_ConvNet_and_a_3D_Model_1617%22+target%3D%22_blank%22%3E%3C%2Fa%3EFace+Detection+with+End-to-End+Integration+of+a+ConvNet+and+a+3D+Model%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2016%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1606.00850%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1606.00850%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ftfwu%2FFaceDetection-ConvNet-3D%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ftfwu%2FFaceDetection-ConvNet-3D%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CMSRCNN_Contextual_MultiScale_Regionbased_CNN_for_Unconstrained_Face_Detection_1623%22+target%3D%22_blank%22%3E%3C%2Fa%3ECMS-RCNN%3A+Contextual+Multi-Scale+Region-based+CNN+for+Unconstrained+Face+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+CMU%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1606.05413%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1606.05413%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Towards_a_Deep_Learning_Framework_for_Unconstrained_Face_Detection_1628%22+target%3D%22_blank%22%3E%3C%2Fa%3ETowards+a+Deep+Learning+Framework+for+Unconstrained+Face+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+overlap+with+CMS-RCNN%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.05322%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.05322%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Supervised_Transformer_Network_for_Efficient_Face_Detection_1633%22+target%3D%22_blank%22%3E%3C%2Fa%3ESupervised+Transformer+Network+for+Efficient+Face+Detection%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.05477%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.05477%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22UnitBox_An_Advanced_Object_Detection_Network_1637%22+target%3D%22_blank%22%3E%3C%2Fa%3EUnitBox%3A+An+Advanced+Object+Detection+Network%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ACM+MM+2016%3C%2Fli%3E+%0A++++++%3Cli%3Ekeywords%3A+IOULoss%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.01471%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.01471%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Bootstrapping_Face_Detection_with_Hard_Negative_Examples_1643%22+target%3D%22_blank%22%3E%3C%2Fa%3EBootstrapping+Face+Detection+with+Hard+Negative+Examples%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eauthor%3A+%E4%B8%87%E9%9F%B6%E5%8D%8E+%40+%E5%B0%8F%E7%B1%B3.%3C%2Fli%3E+%0A++++++%3Cli%3Eintro%3A+Faster+R-CNN%2C+hard+negative+mining.+state-of-the-art+on+the+FDDB+dataset%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.02236%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.02236%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Grid_Loss_Detecting_Occluded_Faces_1649%22+target%3D%22_blank%22%3E%3C%2Fa%3EGrid+Loss%3A+Detecting+Occluded+Faces%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ECCV+2016%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1609.00129%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1609.00129%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Flrs.icg.tugraz.at%2Fpubs%2Fopitz_eccv_16.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Flrs.icg.tugraz.at%2Fpubs%2Fopitz_eccv_16.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Eposter%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-2A-34.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-2A-34.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_MultiScale_Cascade_Fully_Convolutional_Network_Face_Detector_1656%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Multi-Scale+Cascade+Fully+Convolutional+Network+Face+Detector%3C%2Fh3%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eintro%3A+ICPR+2016%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1609.03536%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1609.03536%3C%2Fa%3E%3C%2Fli%3E+%0A+++++%3C%2Ful%3E+%0A+++++%3Chr%3E+%0A+++++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MTCNN_1664%22+target%3D%22_blank%22%3E%3C%2Fa%3EMTCNN%3C%2Fh2%3E+%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Joint_Face_Detection_and_Alignment_using_Multitask_Cascaded_Convolutional_Neural_Networks_1665%22+target%3D%22_blank%22%3E%3C%2Fa%3EJoint+Face+Detection+and+Alignment+using+Multi-task+Cascaded+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A+++++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F20180821144506912%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A+++++%3Cul%3E+%0A++++++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22https%3A%2F%2Fkpzhang93.github.io%2FMTCNN_face_detection_alignment%2Findex.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fkpzhang93.github.io%2FMTCNN_face_detection_alignment%2Findex.html%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1604.02878%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1604.02878%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28official%2C+Matlab%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fkpzhang93%2FMTCNN_face_detection_alignment%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fkpzhang93%2FMTCNN_face_detection_alignment%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fpangyupo%2Fmxnet_mtcnn_face_detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fpangyupo%2Fmxnet_mtcnn_face_detection%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FDaFuCoding%2FMTCNN_Caffe%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FDaFuCoding%2FMTCNN_Caffe%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FSeanlinx%2Fmtcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FSeanlinx%2Fmtcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FPi-DeepLearning%2FRaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FPi-DeepLearning%2FRaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FforeverYoungGitHub%2FMTCNN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FforeverYoungGitHub%2FMTCNN%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FCongWeilin%2Fmtcnn-caffe%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FCongWeilin%2Fmtcnn-caffe%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28OpenCV%2BOpenBlas%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FAlphaQi%2FMTCNN-light%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FAlphaQi%2FMTCNN-light%3C%2Fa%3E%3C%2Fli%3E+%0A++++++%3Cli%3Egithub%28Tensorflow%2Bgolang%29%3A+%3C%2Fli%3E%0A+++++%3C%2Ful%3E%0A++++%3C%2Fdiv%3E%0A+++%3C%2Fdiv%3E%0A++%3C%2Farticle%3E+%0A+%3C%2Fdiv%3E+%0A+%3Clink+href%3D%22https%3A%2F%2Fcsdnimg.cn%2Frelease%2Fphoenix%2Fmdeditor%2Fmarkdown_views-258a4616f7.css%22+rel%3D%22stylesheet%22%3E+%0A%3C%2Fdiv%3E%0A%3Cdiv+id%3D%22article_content%22+class%3D%22article_content+clearfix+csdn-tracking-statistics%22%3E+%0A+%3Cdiv+id%3D%22content_views%22+class%3D%22markdown_views+prism-atom-one-dark%22%3E+%0A++%3C%21--+flowchart+%26%2331661%3B%26%2322836%3B%26%2322270%3B%26%2326631%3B+%26%2321247%3B%26%2321024%3B+--%3E+%0A++%3Csvg+xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A+++%3Cpath+stroke-linecap%3D%22round%22+d%3D%22M5%2C0+0%2C2.5+5%2C5z%22+id%3D%22raphael-marker-block%22%3E%3C%2Fpath%3E%0A++%3C%2Fsvg%3E+%0A++%3Cdiv+id%3D%22article_content%22+class%3D%22article_content+clearfix+csdn-tracking-statistics%22%3E+%0A+++%3Cdiv+id%3D%22content_views%22+class%3D%22markdown_views+prism-atom-one-dark%22%3E+%0A++++%3C%21--+flowchart+%26amp%3B%2331661%3B%26amp%3B%2322836%3B%26amp%3B%2322270%3B%26amp%3B%2326631%3B+%26amp%3B%2321247%3B%26amp%3B%2321024%3B+--%3E+%0A++++%3Cpath+stroke-linecap%3D%22round%22+d%3D%22M5%2C0+0%2C2.5+5%2C5z%22+id%3D%22raphael-marker-block%22%3E%3C%2Fpath%3E+%0A++++%3Cp%3E%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%A5%9E%E6%96%87%EF%BC%8C%E9%9D%9E%E5%B8%B8%E5%85%A8%E8%80%8C%E4%B8%94%E6%8C%81%E7%BB%AD%E5%9C%A8%E6%9B%B4%E6%96%B0%E3%80%82%E8%BD%AC%E5%8F%91%E8%87%AA%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Fhandong1587.github.io%2Fdeep_learning%2F2015%2F10%2F09%2Fobject-detection.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fhandong1587.github.io%2Fdeep_learning%2F2015%2F10%2F09%2Fobject-detection.html+%3C%2Fa%3E%EF%BC%8C%E5%A6%82%E6%9C%89%E4%BE%B5%E6%9D%83%E8%81%94%E7%B3%BB%E5%88%A0%E9%99%A4%E3%80%82%3Cbr%3E+%3Cbr%3Ehttps%3A%2F%2Fblog.csdn.net%2Falphonse2017%2Farticle%2Fdetails%2F85103295%EF%BC%8C+%3C%2Fp%3E+%0A++++%3Cp%3E%E6%88%91%E4%BC%9A%E8%B7%9F%E8%BF%9B%E5%8E%9F%E4%BD%9C%E8%80%85%E5%8D%9A%E5%AE%A2%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%8C%E5%8A%A0%E5%85%A5%E8%87%AA%E5%B7%B1%E5%AF%B9%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%96%B0%E7%A0%94%E7%A9%B6%E5%8F%8A%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E3%80%82%E5%8D%9A%E5%AE%A2%E6%A0%B9%E6%8D%AE%E9%9C%80%E6%B1%82%E7%9B%B4%E6%8E%A5%E8%BF%9B%E8%A1%8C%E5%85%B3%E9%94%AE%E5%AD%97%E6%90%9C%E7%B4%A2%EF%BC%8C%E4%BE%8B%E5%A6%822018%EF%BC%8C%E5%8F%AF%E6%89%BE%E5%88%B0%E6%9C%80%E6%96%B0%E8%AE%BA%E6%96%87%E3%80%82%3C%2Fp%3E+%0A++++%3Cp%3E%3C%2Fp%3E%0A++++%3Cdiv+class%3D%22toc%22%3E%0A+++++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22%22%3E%3C%2Fa%3E%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95%3C%2Fh3%3E%0A+++++%3Cul%3E%0A++++++%3Cli%3E%3Ca+href%3D%22%23Papers_39%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPapers%3C%2Fa%3E%3C%2Fli%3E%0A++++++%3Cul%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Neural_Networks_for_Object_Detection_45%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Neural+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23OverFeat_Integrated_Recognition_Localization_and_Detection_using_Convolutional_Networks_48%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOverFeat%3A+Integrated+Recognition%2C+Localization+and+Detection+using+Convolutional+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23RCNN_55%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation_56%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERich+feature+hierarchies+for+accurate+object+detection+and+semantic+segmentation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Fast_RCNN_70%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Fast_RCNN_71%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23AFastRCNN_Hard_Positive_Generation_via_Adversary_for_Object_Detection_84%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA-Fast-RCNN%3A+Hard+Positive+Generation+via+Adversary+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Faster_RCNN_91%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaster+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Faster_RCNN_Towards_RealTime_Object_Detection_with_Region_Proposal_Networks_92%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaster+R-CNN%3A+Towards+Real-Time+Object+Detection+with+Region+Proposal+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23RCNN_minus_R_111%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER-CNN+minus+R%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Faster_RCNN_in_MXNet_with_distributed_implementation_and_data_parallelization_116%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaster+R-CNN+in+MXNet+with+distributed+implementation+and+data+parallelization%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Contextual_Priming_and_Feedback_for_Faster_RCNN_120%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContextual+Priming+and+Feedback+for+Faster+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23An_Implementation_of_Faster_RCNN_with_Study_for_Region_Sampling_126%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAn+Implementation+of+Faster+RCNN+with+Study+for+Region+Sampling%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Interpretable_RCNN_132%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EInterpretable+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23LightHead_RCNN_140%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELight-Head+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23LightHead_RCNN_In_Defense_of_TwoStage_Object_Detector_141%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELight-Head+R-CNN%3A+In+Defense+of+Two-Stage+Object+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Cascade_RCNN_Delving_into_High_Quality_Object_Detection_149%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECascade+R-CNN%3A+Delving+into+High+Quality+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23MultiBox_157%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMultiBox%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Scalable_Object_Detection_using_Deep_Neural_Networks_158%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScalable+Object+Detection+using+Deep+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Scalable_HighQuality_Object_Detection_165%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScalable%2C+High-Quality+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23SPPNet_173%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESPP-Net%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Spatial_Pyramid_Pooling_in_Deep_Convolutional_Networks_for_Visual_Recognition_174%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESpatial+Pyramid+Pooling+in+Deep+Convolutional+Networks+for+Visual+Recognition%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DeepIDNet_Deformable_Deep_Convolutional_Neural_Networks_for_Object_Detection_181%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepID-Net%3A+Deformable+Deep+Convolutional+Neural+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detectors_Emerge_in_Deep_Scene_CNNs_188%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detectors+Emerge+in+Deep+Scene+CNNs%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23segDeepM_Exploiting_Segmentation_and_Context_in_Deep_Neural_Networks_for_Object_Detection_196%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EsegDeepM%3A+Exploiting+Segmentation+and+Context+in+Deep+Neural+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_Networks_on_Convolutional_Feature_Maps_203%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+Networks+on+Convolutional+Feature+Maps%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Improving_Object_Detection_with_Deep_Convolutional_Networks_via_Bayesian_Optimization_and_Structured_Prediction_209%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EImproving+Object+Detection+with+Deep+Convolutional+Networks+via+Bayesian+Optimization+and+Structured+Prediction%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DeepBox_Learning_Objectness_with_Convolutional_Networks_215%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepBox%3A+Learning+Objectness+with+Convolutional+Networks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23MRCNN_223%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMR-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_detection_via_a_multiregion__semantic_segmentationaware_CNN_model_224%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+detection+via+a+multi-region+%26amp%3B+semantic+segmentation-aware+CNN+model%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23YOLO_234%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLO%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23You_Only_Look_Once_Unified_RealTime_Object_Detection_235%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYou+Only+Look+Once%3A+Unified%2C+Real-Time+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23darkflow__translate_darknet_to_tensorflow_Load_trained_weights_retrainfinetune_them_using_tensorflow_export_constant_graph_def_to_C_252%22+rel%3D%22nofollow%22+target%3D%22_self%22%3Edarkflow+-+translate+darknet+to+tensorflow.+Load+trained+weights%2C+retrain%2Ffine-tune+them+using+tensorflow%2C+export+constant+graph+def+to+C%2B%2B%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Start_Training_YOLO_with_Our_Own_Data_257%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EStart+Training+YOLO+with+Our+Own+Data%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23YOLO_Core_ML_versus_MPSNNGraph_263%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLO%3A+Core+ML+versus+MPSNNGraph%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23TensorFlow_YOLO_object_detection_on_Android_269%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETensorFlow+YOLO+object+detection+on+Android%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Computer_Vision_in_iOS__Object_Detection_274%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EComputer+Vision+in+iOS+%E2%80%93+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23YOLOv2_280%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLOv2%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23YOLO9000_Better_Faster_Stronger_281%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLO9000%3A+Better%2C+Faster%2C+Stronger%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23darknet_scripts_293%22+rel%3D%22nofollow%22+target%3D%22_self%22%3Edarknet_scripts%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Yolo_mark_GUI_for_marking_bounded_boxes_of_objects_in_images_for_training_Yolo_v2_298%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYolo_mark%3A+GUI+for+marking+bounded+boxes+of+objects+in+images+for+training+Yolo+v2%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23LightNet_Bringing_pjreddies_DarkNet_out_of_the_shadows_302%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELightNet%3A+Bringing+pjreddie%E2%80%99s+DarkNet+out+of+the+shadows%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23YOLO_v2_Bounding_Box_Tool_306%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLO+v2+Bounding+Box+Tool%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23YOLOv3_312%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLOv3%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23YOLOv3_An_Incremental_Improvement_313%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLOv3%3A+An+Incremental+Improvement%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23YOLOLITE_A_RealTime_Object_Detection_Algorithm_Optimized_for_NonGPU_Computers_318%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYOLO-LITE%3A+A+Real-Time+Object+Detection+Algorithm+Optimized+for+Non-GPU+Computers%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23AttentionNet_Aggregating_Weak_Directions_for_Accurate_Object_Detection_323%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAttentionNet%3A+Aggregating+Weak+Directions+for+Accurate+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23DenseBox_333%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDenseBox%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DenseBox_Unifying_Landmark_Localization_with_End_to_End_Object_Detection_334%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDenseBox%3A+Unifying+Landmark+Localization+with+End+to+End+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23SSD_341%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESSD%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SSD_Single_Shot_MultiBox_Detector_342%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESSD%3A+Single+Shot+MultiBox+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23DSSD_360%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDSSD%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DSSD__Deconvolutional_Single_Shot_Detector_361%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDSSD+%3A+Deconvolutional+Single+Shot+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Enhancement_of_SSD_by_concatenating_feature_maps_for_object_detection_369%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEnhancement+of+SSD+by+concatenating+feature+maps+for+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Contextaware_SingleShot_Detector_374%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContext-aware+Single-Shot+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23FeatureFused_SSD_Fast_Detection_for_Small_Objects_379%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFeature-Fused+SSD%3A+Fast+Detection+for+Small+Objects%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23FSSD_384%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFSSD%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23FSSD_Feature_Fusion_Single_Shot_Multibox_Detector_385%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFSSD%3A+Feature+Fusion+Single+Shot+Multibox+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Weaving_Multiscale_Context_for_Single_Shot_Detector_389%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeaving+Multi-scale+Context+for+Single+Shot+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23ESSD_396%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EESSD%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Extend_the_shallow_part_of_Single_Shot_MultiBox_Detector_via_Convolutional_Neural_Network_397%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EExtend+the+shallow+part+of+Single+Shot+MultiBox+Detector+via+Convolutional+Neural+Network%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Tiny_SSD_A_Tiny_Singleshot_Detection_Deep_Convolutional_Neural_Network_for_Realtime_Embedded_Object_Detection_401%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETiny+SSD%3A+A+Tiny+Single-shot+Detection+Deep+Convolutional+Neural+Network+for+Real-time+Embedded+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23MDSSD_Multiscale_Deconvolutional_Single_Shot_Detector_for_small_objects_405%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMDSSD%3A+Multi-scale+Deconvolutional+Single+Shot+Detector+for+small+objects%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23InsideOutside_Net_ION_411%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EInside-Outside+Net+%28ION%29%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23InsideOutside_Net_Detecting_Objects_in_Context_with_Skip_Pooling_and_Recurrent_Neural_Networks_412%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EInside-Outside+Net%3A+Detecting+Objects+in+Context+with+Skip+Pooling+and+Recurrent+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Adaptive_Object_Detection_Using_Adjacency_and_Zoom_Prediction_419%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdaptive+Object+Detection+Using+Adjacency+and+Zoom+Prediction%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23GCNN_an_Iterative_Grid_Based_Object_Detector_426%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EG-CNN%3A+an+Iterative+Grid+Based+Object+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Factors_in_Finetuning_Deep_Model_for_object_detection_431%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFactors+in+Finetuning+Deep+Model+for+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Factors_in_Finetuning_Deep_Model_for_Object_Detection_with_Longtail_Distribution_432%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFactors+in+Finetuning+Deep+Model+for+Object+Detection+with+Long-tail+Distribution%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23We_dont_need_no_boundingboxes_Training_object_class_detectors_using_only_human_verification_438%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWe+don%E2%80%99t+need+no+bounding-boxes%3A+Training+object+class+detectors+using+only+human+verification%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23HyperNet_Towards_Accurate_Region_Proposal_Generation_and_Joint_Object_Detection_443%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHyperNet%3A+Towards+Accurate+Region+Proposal+Generation+and+Joint+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_MultiPath_Network_for_Object_Detection_448%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+MultiPath+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23CRAFT_455%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECRAFT%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23CRAFT_Objects_from_Images_456%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECRAFT+Objects+from+Images%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23OHEM_465%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOHEM%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Training_Regionbased_Object_Detectors_with_Online_Hard_Example_Mining_466%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETraining+Region-based+Object+Detectors+with+Online+Hard+Example+Mining%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SOHEM_Stratified_Online_Hard_Example_Mining_for_Object_Detection_474%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ES-OHEM%3A+Stratified+Online+Hard+Example+Mining+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Exploit_All_the_Layers_Fast_and_Accurate_CNN_Object_Detector_with_Scale_Dependent_Pooling_and_Cascaded_Rejection_Classifiers_478%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EExploit+All+the+Layers%3A+Fast+and+Accurate+CNN+Object+Detector+with+Scale+Dependent+Pooling+and+Cascaded+Rejection+Classifiers%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23RFCN_485%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER-FCN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23RFCN_Object_Detection_via_Regionbased_Fully_Convolutional_Networks_486%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER-FCN%3A+Object+Detection+via+Region-based+Fully+Convolutional+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23RFCN3000_at_30fps_Decoupling_Detection_and_Classification_495%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER-FCN-3000+at+30fps%3A+Decoupling+Detection+and+Classification%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Recycle_deep_features_for_better_object_detection_499%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERecycle+deep+features+for+better+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23MSCNN_504%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMS-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_Unified_Multiscale_Deep_Convolutional_Neural_Network_for_Fast_Object_Detection_505%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Unified+Multi-scale+Deep+Convolutional+Neural+Network+for+Fast+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Multistage_Object_Detection_with_Group_Recursive_Learning_513%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-stage+Object+Detection+with+Group+Recursive+Learning%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Subcategoryaware_Convolutional_Neural_Networks_for_Object_Proposals_and_Detection_518%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESubcategory-aware+Convolutional+Neural+Networks+for+Object+Proposals+and+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23PVANET_525%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPVANET%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23PVANet_Lightweight_Deep_Neural_Networks_for_Realtime_Object_Detection_526%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPVANet%3A+Lightweight+Deep+Neural+Networks+for+Real-time+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23GBDNet_534%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGBD-Net%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Gated_Bidirectional_CNN_for_Object_Detection_535%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGated+Bi-directional+CNN+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Crafting_GBDNet_for_Object_Detection_541%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECrafting+GBD-Net+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23StuffNet_Using_Stuff_to_Improve_Object_Detection_549%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EStuffNet%3A+Using+%E2%80%98Stuff%E2%80%99+to+Improve+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Generalized_Haar_Filter_based_Deep_Networks_for_RealTime_Object_Detection_in_Traffic_Scene_554%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGeneralized+Haar+Filter+based+Deep+Networks+for+Real-Time+Object+Detection+in+Traffic+Scene%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Hierarchical_Object_Detection_with_Deep_Reinforcement_Learning_559%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHierarchical+Object+Detection+with+Deep+Reinforcement+Learning%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_to_detect_and_localize_many_objects_from_few_examples_569%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+to+detect+and+localize+many+objects+from+few+examples%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Speedaccuracy_tradeoffs_for_modern_convolutional_object_detectors_574%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESpeed%2Faccuracy+trade-offs+for+modern+convolutional+object+detectors%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SqueezeDet_Unified_Small_Low_Power_Fully_Convolutional_Neural_Networks_for_RealTime_Object_Detection_for_Autonomous_Driving_580%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESqueezeDet%3A+Unified%2C+Small%2C+Low+Power+Fully+Convolutional+Neural+Networks+for+Real-Time+Object+Detection+for+Autonomous+Driving%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Feature_Pyramid_Network_FPN_587%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFeature+Pyramid+Network+%28FPN%29%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Feature_Pyramid_Networks_for_Object_Detection_588%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFeature+Pyramid+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ActionDriven_Object_Detection_with_TopDown_Visual_Attentions_594%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAction-Driven+Object+Detection+with+Top-Down+Visual+Attentions%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Beyond_Skip_Connections_TopDown_Modulation_for_Object_Detection_598%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBeyond+Skip+Connections%3A+Top-Down+Modulation+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23WideResidualInception_Networks_for_Realtime_Object_Detection_603%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWide-Residual-Inception+Networks+for+Real-time+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Attentional_Network_for_Visual_Object_Detection_608%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAttentional+Network+for+Visual+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Chained_Deep_Features_and_Classifiers_for_Cascade_in_Object_Detection_613%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Chained+Deep+Features+and+Classifiers+for+Cascade+in+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DeNet_Scalable_Realtime_Object_Detection_with_Directed_Sparse_Sampling_620%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeNet%3A+Scalable+Real-time+Object+Detection+with+Directed+Sparse+Sampling%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Discriminative_Bimodal_Networks_for_Visual_Localization_and_Detection_with_Natural_Language_Queries_626%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDiscriminative+Bimodal+Networks+for+Visual+Localization+and+Detection+with+Natural+Language+Queries%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Spatial_Memory_for_Context_Reasoning_in_Object_Detection_632%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESpatial+Memory+for+Context+Reasoning+in+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Accurate_Single_Stage_Detector_Using_Recurrent_Rolling_Convolution_637%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAccurate+Single+Stage+Detector+Using+Recurrent+Rolling+Convolution%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Occlusion_Reasoning_for_MultiCamera_MultiTarget_Detection_645%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Occlusion+Reasoning+for+Multi-Camera+Multi-Target+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23LCDet_LowComplexity_FullyConvolutional_Neural_Networks_for_Object_Detection_in_Embedded_Systems_649%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELCDet%3A+Low-Complexity+Fully-Convolutional+Neural+Networks+for+Object+Detection+in+Embedded+Systems%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Point_Linking_Network_for_Object_Detection_655%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPoint+Linking+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Perceptual_Generative_Adversarial_Networks_for_Small_Object_Detection_661%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPerceptual+Generative+Adversarial+Networks+for+Small+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Fewshot_Object_Detection_665%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFew-shot+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23YesNet_An_effective_Detector_Based_on_Global_Information_669%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYes-Net%3A+An+effective+Detector+Based+on+Global+Information%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SMC_Faster_RCNN_Toward_a_scenespecialized_multiobject_detector_673%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESMC+Faster+R-CNN%3A+Toward+a+scene-specialized+multi-object+detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_lightweight_convolutional_neural_networks_for_object_detection_677%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+lightweight+convolutional+neural+networks+for+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23RON_Reverse_Connection_with_Objectness_Prior_Networks_for_Object_Detection_681%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERON%3A+Reverse+Connection+with+Objectness+Prior+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Mimicking_Very_Efficient_Network_for_Object_Detection_688%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMimicking+Very+Efficient+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Residual_Features_and_Unified_Prediction_Network_for_Single_Stage_Detection_693%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EResidual+Features+and+Unified+Prediction+Network+for+Single+Stage+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deformable_Partbased_Fully_Convolutional_Network_for_Object_Detection_697%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeformable+Part-based+Fully+Convolutional+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Adaptive_Feeding_Achieving_Fast_and_Accurate_Detections_by_Adaptively_Combining_Object_Detectors_702%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdaptive+Feeding%3A+Achieving+Fast+and+Accurate+Detections+by+Adaptively+Combining+Object+Detectors%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Recurrent_Scale_Approximation_for_Object_Detection_in_CNN_707%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERecurrent+Scale+Approximation+for+Object+Detection+in+CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23DSOD_715%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDSOD%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DSOD_Learning_Deeply_Supervised_Object_Detectors_from_Scratch_716%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDSOD%3A+Learning+Deeply+Supervised+Object+Detectors+from+Scratch%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_from_Scratch_with_Deep_Supervision_722%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+from+Scratch+with+Deep+Supervision%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Focal_Loss_for_Dense_Object_Detection_727%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFocal+Loss+for+Dense+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Focal_Loss_Dense_Detector_for_Vehicle_Surveillance_733%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFocal+Loss+Dense+Detector+for+Vehicle+Surveillance%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23CoupleNet_Coupling_Global_Structure_with_Local_Parts_for_Object_Detection_737%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECoupleNet%3A+Coupling+Global+Structure+with+Local+Parts+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Incremental_Learning_of_Object_Detectors_without_Catastrophic_Forgetting_742%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIncremental+Learning+of+Object+Detectors+without+Catastrophic+Forgetting%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Zoom_OutandIn_Network_with_Map_Attention_Decision_for_Region_Proposal_and_Object_Detection_747%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZoom+Out-and-In+Network+with+Map+Attention+Decision+for+Region+Proposal+and+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23StairNet_TopDown_Semantic_Aggregation_for_Accurate_One_Shot_Detection_751%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EStairNet%3A+Top-Down+Semantic+Aggregation+for+Accurate+One+Shot+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Dynamic_Zoomin_Network_for_Fast_Object_Detection_in_Large_Images_755%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDynamic+Zoom-in+Network+for+Fast+Object+Detection+in+Large+Images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ZeroAnnotation_Object_Detection_with_Web_Knowledge_Transfer_759%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Annotation+Object+Detection+with+Web+Knowledge+Transfer%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23MegDet_767%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMegDet%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23MegDet_A_Large_MiniBatch_Object_Detector_768%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMegDet%3A+A+Large+Mini-Batch+Object+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SingleShot_Refinement_Neural_Network_for_Object_Detection_773%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESingle-Shot+Refinement+Neural+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Receptive_Field_Block_Net_for_Accurate_and_Fast_Object_Detection_779%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EReceptive+Field+Block+Net+for+Accurate+and+Fast+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23An_Analysis_of_Scale_Invariance_in_Object_Detection__SNIP_785%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAn+Analysis+of+Scale+Invariance+in+Object+Detection+-+SNIP%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Feature_Selective_Networks_for_Object_Detection_791%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFeature+Selective+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_a_Rotation_Invariant_Detector_with_Rotatable_Bounding_Box_795%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+a+Rotation+Invariant+Detector+with+Rotatable+Bounding+Box%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Scalable_Object_Detection_for_Stylized_Objects_800%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScalable+Object+Detection+for+Stylized+Objects%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Object_Detectors_from_Scratch_with_Gated_Recurrent_Feature_Pyramids_805%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Object+Detectors+from+Scratch+with+Gated+Recurrent+Feature+Pyramids%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Regionlets_for_Object_Detection_810%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Regionlets+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Training_and_Testing_Object_Detectors_with_Virtual_Images_815%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETraining+and+Testing+Object+Detectors+with+Virtual+Images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23LargeScale_Object_Discovery_and_Detector_Adaptation_from_Unlabeled_Video_821%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELarge-Scale+Object+Discovery+and+Detector+Adaptation+from+Unlabeled+Video%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Spot_the_Difference_by_Object_Detection_827%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESpot+the+Difference+by+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23LocalizationAware_Active_Learning_for_Object_Detection_832%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocalization-Aware+Active+Learning+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_with_Maskbased_Feature_Encoding_837%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+with+Mask-based+Feature+Encoding%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23LSTD_A_LowShot_Transfer_Detector_for_Object_Detection_841%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELSTD%3A+A+Low-Shot+Transfer+Detector+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Domain_Adaptive_Faster_RCNN_for_Object_Detection_in_the_Wild_846%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDomain+Adaptive+Faster+R-CNN+for+Object+Detection+in+the+Wild%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Pseudo_Mask_Augmented_Object_Detection_852%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPseudo+Mask+Augmented+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Revisiting_RCNN_On_Awakening_the_Classification_Power_of_Faster_RCNN_856%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERevisiting+RCNN%3A+On+Awakening+the+Classification+Power+of+Faster+RCNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Decoupled_Classification_Refinement_Hard_False_Positive_Suppression_for_Object_Detection_862%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDecoupled+Classification+Refinement%3A+Hard+False+Positive+Suppression+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Region_Features_for_Object_Detection_867%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Region+Features+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SingleShot_Bidirectional_Pyramid_Networks_for_HighQuality_Object_Detection_872%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESingle-Shot+Bidirectional+Pyramid+Networks+for+High-Quality+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_for_Comics_using_Manga109_Annotations_877%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+for+Comics+using+Manga109+Annotations%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23TaskDriven_Super_Resolution_Object_Detection_in_Lowresolution_Images_882%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETask-Driven+Super+Resolution%3A+Object+Detection+in+Low-resolution+Images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Transferring_CommonSense_Knowledge_for_Object_Detection_886%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETransferring+Common-Sense+Knowledge+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Multiscale_Locationaware_Kernel_Representation_for_Object_Detection_890%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-scale+Location-aware+Kernel+Representation+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Loss_Rank_Mining_A_General_Hard_Example_Mining_Method_for_Realtime_Detectors_896%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELoss+Rank+Mining%3A+A+General+Hard+Example+Mining+Method+for+Real-time+Detectors%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DetNet_A_Backbone_network_for_Object_Detection_901%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetNet%3A+A+Backbone+network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Robust_Physical_Adversarial_Attack_on_Faster_RCNN_Object_Detector_906%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERobust+Physical+Adversarial+Attack+on+Faster+R-CNN+Object+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23AdvDetPatch_Attacking_Object_Detectors_with_Adversarial_Patches_910%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdvDetPatch%3A+Attacking+Object+Detectors+with+Adversarial+Patches%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Attacking_Object_Detectors_via_Imperceptible_Patches_on_Background_914%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAttacking+Object+Detectors+via+Imperceptible+Patches+on+Background%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Physical_Adversarial_Examples_for_Object_Detectors_918%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPhysical+Adversarial+Examples+for+Object+Detectors%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Quantization_Mimic_Towards_Very_Tiny_CNN_for_Object_Detection_923%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EQuantization+Mimic%3A+Towards+Very+Tiny+CNN+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_detection_at_200_Frames_Per_Second_927%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+detection+at+200+Frames+Per+Second%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_using_Domain_Randomization_and_Generative_Adversarial_Refinement_of_Synthetic_Images_932%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+using+Domain+Randomization+and+Generative+Adversarial+Refinement+of+Synthetic+Images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SNIPER_Efficient_MultiScale_Training_937%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESNIPER%3A+Efficient+Multi-Scale+Training%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Soft_Sampling_for_Robust_Object_Detection_942%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESoft+Sampling+for+Robust+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23MetaAnchor_Learning_to_Detect_Objects_with_Customized_Anchors_946%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMetaAnchor%3A+Learning+to+Detect+Objects+with+Customized+Anchors%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Localization_Recall_Precision_LRP_A_New_Performance_Metric_for_Object_Detection_951%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocalization+Recall+Precision+%28LRP%29%3A+A+New+Performance+Metric+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23AutoContext_RCNN_957%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAuto-Context+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Pooling_Pyramid_Network_for_Object_Detection_962%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPooling+Pyramid+Network+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Modeling_Visual_Context_is_Key_to_Augmenting_Object_Detection_Datasets_967%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EModeling+Visual+Context+is+Key+to+Augmenting+Object+Detection+Datasets%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Dual_Refinement_Network_for_SingleShot_Object_Detection_972%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDual+Refinement+Network+for+Single-Shot+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Acquisition_of_Localization_Confidence_for_Accurate_Object_Detection_976%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAcquisition+of+Localization+Confidence+for+Accurate+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23CornerNet_Detecting_Objects_as_Paired_Keypoints_982%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECornerNet%3A+Detecting+Objects+as+Paired+Keypoints%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Unsupervised_Hard_Example_Mining_from_Videos_for_Improved_Object_Detection_989%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnsupervised+Hard+Example+Mining+from+Videos+for+Improved+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SAN_Learning_Relationship_between_Convolutional_Features_for_MultiScale_Object_Detection_995%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESAN%3A+Learning+Relationship+between+Convolutional+Features+for+Multi-Scale+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_Survey_of_Modern_Object_Detection_Literature_using_Deep_Learning_999%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Survey+of+Modern+Object+Detection+Literature+using+Deep+Learning%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23TinyDSOD_Lightweight_Object_Detection_for_ResourceRestricted_Usages_1003%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETiny-DSOD%3A+Lightweight+Object+Detection+for+Resource-Restricted+Usages%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Feature_Pyramid_Reconfiguration_for_Object_Detection_1008%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Feature+Pyramid+Reconfiguration+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23MDCN_MultiScale_Deep_Inception_Convolutional_Neural_Networks_for_Efficient_Object_Detection_1012%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMDCN%3A+Multi-Scale%2C+Deep+Inception+Convolutional+Neural+Networks+for+Efficient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Recent_Advances_in_Object_Detection_in_the_Age_of_Deep_Convolutional_Neural_Networks_1016%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERecent+Advances+in+Object+Detection+in+the+Age+of+Deep+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_for_Generic_Object_Detection_A_Survey_1019%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+for+Generic+Object+Detection%3A+A+Survey%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Training_ConfidenceCalibrated_Classifier_for_Detecting_OutofDistribution_Samples_1022%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETraining+Confidence-Calibrated+Classifier+for+Detecting+Out-of-Distribution+Samples%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ScratchDetExploring_to_Train_SingleShot_Object_Detectors_from_Scratch_1026%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScratchDet%3AExploring+to+Train+Single-Shot+Object+Detectors+from+Scratch%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Fast_and_accurate_object_detection_in_high_resolution_4K_and_8K_video_using_GPUs_1030%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+and+accurate+object+detection+in+high+resolution+4K+and+8K+video+using+GPUs%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Hybrid_Knowledge_Routed_Modules_for_Largescale_Object_Detection_1035%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHybrid+Knowledge+Routed+Modules+for+Large-scale+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Gradient_Harmonized_Singlestage_Detector_1040%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGradient+Harmonized+Single-stage+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23M2Det_A_SingleShot_Object_Detector_based_on_MultiLevel_Feature_Pyramid_Network_1044%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EM2Det%3A+A+Single-Shot+Object+Detector+based+on+Multi-Level+Feature+Pyramid+Network%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23BAN_Focusing_on_Boundary_Context_for_Object_Detection_1049%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBAN%3A+Focusing+on+Boundary+Context+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Multilayer_Pruning_Framework_for_Compressing_Single_Shot_MultiBox_Detector_1052%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-layer+Pruning+Framework+for+Compressing+Single+Shot+MultiBox+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23R2CNN_MultiDimensional_Attention_Based_Rotation_Invariant_Detector_with_Robust_Anchor_Strategy_1056%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ER2CNN%2B%2B%3A+Multi-Dimensional+Attention+Based+Rotation+Invariant+Detector+with+Robust+Anchor+Strategy%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DeRPN_Taking_a_further_step_toward_more_general_object_detection_1060%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeRPN%3A+Taking+a+further+step+toward+more+general+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Fast_Efficient_Object_Detection_Using_Selective_Attention_1066%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+Efficient+Object+Detection+Using+Selective+Attention%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Sampling_Techniques_for_LargeScale_Object_Detection_from_Sparsely_Annotated_Objects_1070%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESampling+Techniques+for+Large-Scale+Object+Detection+from+Sparsely+Annotated+Objects%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23NonMaximum_Suppression_NMS_1074%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ENon-Maximum+Suppression+%28NMS%29%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23EndtoEnd_Integration_of_a_Convolutional_Network_Deformable_Parts_Model_and_NonMaximum_Suppression_1075%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEnd-to-End+Integration+of+a+Convolutional+Network%2C+Deformable+Parts+Model+and+Non-Maximum+Suppression%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_convnet_for_nonmaximum_suppression_1081%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+convnet+for+non-maximum+suppression%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SoftNMS__Improving_Object_Detection_With_One_Line_of_Code_1086%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESoft-NMS+%E2%80%93+Improving+Object+Detection+With+One+Line+of+Code%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_nonmaximum_suppression_1093%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+non-maximum+suppression%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Relation_Networks_for_Object_Detection_1101%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERelation+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Adversarial_Examples_1108%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdversarial+Examples%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Adversarial_Examples_that_Fool_Detectors_1109%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdversarial+Examples+that+Fool+Detectors%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Adversarial_Examples_Are_Not_Easily_Detected_Bypassing_Ten_Detection_Methods_1114%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdversarial+Examples+Are+Not+Easily+Detected%3A+Bypassing+Ten+Detection+Methods%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Weakly_Supervised_Object_Detection_1121%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeakly+Supervised+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Track_and_Transfer_Watching_Videos_to_Simulate_Strong_Human_Supervision_for_WeaklySupervised_Object_Detection_1122%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETrack+and+Transfer%3A+Watching+Videos+to+Simulate+Strong+Human+Supervision+for+Weakly-Supervised+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Weakly_supervised_object_detection_using_pseudostrong_labels_1127%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeakly+supervised+object+detection+using+pseudo-strong+labels%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Saliency_Guided_EndtoEnd_Learning_for_Weakly_Supervised_Object_Detection_1131%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESaliency+Guided+End-to-End+Learning+for+Weakly+Supervised+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Visual_and_Semantic_Knowledge_Transfer_for_Large_Scale_Semisupervised_Object_Detection_1136%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+and+Semantic+Knowledge+Transfer+for+Large+Scale+Semi-supervised+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Video_Object_Detection_1142%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVideo+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Object_Class_Detectors_from_Weakly_Annotated_Video_1143%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Object+Class+Detectors+from+Weakly+Annotated+Video%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Analysing_domain_shift_factors_between_videos_and_images_for_object_detection_1148%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAnalysing+domain+shift+factors+between+videos+and+images+for+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Video_Object_Recognition_1152%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVideo+Object+Recognition%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_for_Saliency_Prediction_in_Natural_Video_1156%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+for+Saliency+Prediction+in+Natural+Video%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23TCNN_Tubelets_with_Convolutional_Neural_Networks_for_Object_Detection_from_Videos_1162%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ET-CNN%3A+Tubelets+with+Convolutional+Neural+Networks+for+Object+Detection+from+Videos%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_from_Video_Tubelets_with_Convolutional_Neural_Networks_1168%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+from+Video+Tubelets+with+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_Videos_with_Tubelets_and_Multicontext_Cues_1175%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+Videos+with+Tubelets+and+Multi-context+Cues%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Context_Matters_Refining_Object_Detection_in_Video_with_Recurrent_Neural_Networks_1181%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContext+Matters%3A+Refining+Object+Detection+in+Video+with+Recurrent+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23CNN_Based_Object_Detection_in_Large_Video_Images_1188%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECNN+Based+Object+Detection+in+Large+Video+Images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_Videos_with_Tubelet_Proposal_Networks_1194%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+Videos+with+Tubelet+Proposal+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23FlowGuided_Feature_Aggregation_for_Video_Object_Detection_1198%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFlow-Guided+Feature+Aggregation+for+Video+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Video_Object_Detection_using_Faster_RCNN_1203%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVideo+Object+Detection+using+Faster+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Improving_Context_Modeling_for_Video_Object_Detection_and_Tracking_1208%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EImproving+Context+Modeling+for+Video+Object+Detection+and+Tracking%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Temporal_Dynamic_Graph_LSTM_for_Actiondriven_Video_Object_Detection_1212%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETemporal+Dynamic+Graph+LSTM+for+Action-driven+Video+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Mobile_Video_Object_Detection_with_TemporallyAware_Feature_Maps_1217%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMobile+Video+Object+Detection+with+Temporally-Aware+Feature+Maps%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_High_Performance_Video_Object_Detection_1221%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+High+Performance+Video+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Impression_Network_for_Video_Object_Detection_1225%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EImpression+Network+for+Video+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SpatialTemporal_Memory_Networks_for_Video_Object_Detection_1229%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESpatial-Temporal+Memory+Networks+for+Video+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%233DDETNet_a_Single_Stage_VideoBased_Vehicle_Detector_1233%22+rel%3D%22nofollow%22+target%3D%22_self%22%3E3D-DETNet%3A+a+Single+Stage+Video-Based+Vehicle+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_Videos_by_Short_and_Long_Range_Object_Linking_1237%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+Videos+by+Short+and+Long+Range+Object+Linking%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_Video_with_Spatiotemporal_Sampling_Networks_1241%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+Video+with+Spatiotemporal+Sampling+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_High_Performance_Video_Object_Detection_for_Mobiles_1246%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+High+Performance+Video+Object+Detection+for+Mobiles%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Optimizing_Video_Object_Detection_via_a_ScaleTime_Lattice_1251%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOptimizing+Video+Object+Detection+via+a+Scale-Time+Lattice%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Pack_and_Detect_Fast_Object_Detection_in_Videos_Using_RegionofInterest_Packing_1258%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPack+and+Detect%3A+Fast+Object+Detection+in+Videos+Using+Region-of-Interest+Packing%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Fast_Object_Detection_in_Compressed_Video_1261%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+Object+Detection+in+Compressed+Video%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_on_Mobile_Devices_1266%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+on+Mobile+Devices%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Pelee_A_RealTime_Object_Detection_System_on_Mobile_Devices_1267%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPelee%3A+A+Real-Time+Object+Detection+System+on+Mobile+Devices%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_3D_1276%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+3D%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Vote3Deep_Fast_Object_Detection_in_3D_Point_Clouds_Using_Efficient_Convolutional_Neural_Networks_1277%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVote3Deep%3A+Fast+Object+Detection+in+3D+Point+Clouds+Using+Efficient+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ComplexYOLO_Realtime_3D_Object_Detection_on_Point_Clouds_1281%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EComplex-YOLO%3A+Real-time+3D+Object+Detection+on+Point+Clouds%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Focal_Loss_in_3D_Object_Detection_1286%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFocal+Loss+in+3D+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_on_RGBD_1291%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+on+RGB-D%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Rich_Features_from_RGBD_Images_for_Object_Detection_and_Segmentation_1292%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Rich+Features+from+RGB-D+Images+for+Object+Detection+and+Segmentation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Differential_Geometry_Boosts_Convolutional_Neural_Networks_for_Object_Detection_1296%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDifferential+Geometry+Boosts+Convolutional+Neural+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_Selfsupervised_Learning_System_for_Object_Detection_using_Physics_Simulation_and_Multiview_Pose_Estimation_1301%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Self-supervised+Learning+System+for+Object+Detection+using+Physics+Simulation+and+Multi-view+Pose+Estimation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23ZeroShot_Object_Detection_1307%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Shot+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ZeroShot_Detection_1308%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Shot+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ZeroShot_Object_Detection_1314%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Shot+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ZeroShot_Object_Detection_Learning_to_Simultaneously_Recognize_and_Localize_Novel_Concepts_1318%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Shot+Object+Detection%3A+Learning+to+Simultaneously+Recognize+and+Localize+Novel+Concepts%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ZeroShot_Object_Detection_by_Hybrid_Region_Embedding_1323%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EZero-Shot+Object+Detection+by+Hybrid+Region+Embedding%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Salient_Object_Detection_1330%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESalient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Best_Deep_Saliency_Detection_Models_CVPR_2016__2015_1333%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBest+Deep+Saliency+Detection+Models+%28CVPR+2016+%26amp%3B+2015%29%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Largescale_optimization_of_hierarchical_features_for_saliency_prediction_in_natural_images_1337%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELarge-scale+optimization+of+hierarchical+features+for+saliency+prediction+in+natural+images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Predicting_Eye_Fixations_using_Convolutional_Neural_Networks_1341%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPredicting+Eye+Fixations+using+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Saliency_Detection_by_MultiContext_Deep_Learning_1345%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESaliency+Detection+by+Multi-Context+Deep+Learning%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DeepSaliency_MultiTask_Deep_Neural_Network_Model_for_Salient_Object_Detection_1349%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepSaliency%3A+Multi-Task+Deep+Neural+Network+Model+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SuperCNN_A_Superpixelwise_Convolutional_Neural_Network_for_Salient_Object_Detection_1353%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESuperCNN%3A+A+Superpixelwise+Convolutional+Neural+Network+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Shallow_and_Deep_Convolutional_Networks_for_Saliency_Prediction_1357%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EShallow+and+Deep+Convolutional+Networks+for+Saliency+Prediction%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Recurrent_Attentional_Networks_for_Saliency_Detection_1363%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERecurrent+Attentional+Networks+for+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23TwoStream_Convolutional_Networks_for_Dynamic_Saliency_Prediction_1368%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETwo-Stream+Convolutional+Networks+for+Dynamic+Saliency+Prediction%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Unconstrained_Salient_Object_Detection_1374%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnconstrained+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Unconstrained_Salient_Object_Detection_via_Proposal_Subset_Optimization_1375%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnconstrained+Salient+Object+Detection+via+Proposal+Subset+Optimization%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DHSNet_Deep_Hierarchical_Saliency_Network_for_Salient_Object_Detection_1383%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDHSNet%3A+Deep+Hierarchical+Saliency+Network+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Salient_Object_Subitizing_1387%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESalient+Object+Subitizing%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DeeplySupervised_Recurrent_Convolutional_Neural_Network_for_Saliency_Detection_1396%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeeply-Supervised+Recurrent+Convolutional+Neural+Network+for+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Saliency_Detection_via_Combining_RegionLevel_and_PixelLevel_Predictions_with_CNNs_1401%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESaliency+Detection+via+Combining+Region-Level+and+Pixel-Level+Predictions+with+CNNs%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Edge_Preserving_and_MultiScale_Contextual_Neural_Network_for_Salient_Object_Detection_1406%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEdge+Preserving+and+Multi-Scale+Contextual+Neural+Network+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_Deep_MultiLevel_Network_for_Saliency_Prediction_1410%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Deep+Multi-Level+Network+for+Saliency+Prediction%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Visual_Saliency_Detection_Based_on_Multiscale_Deep_CNN_Features_1414%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+Saliency+Detection+Based+on+Multiscale+Deep+CNN+Features%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_Deep_Spatial_Contextual_Longterm_Recurrent_Convolutional_Network_for_Saliency_Detection_1419%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Deep+Spatial+Contextual+Long-term+Recurrent+Convolutional+Network+for+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deeply_supervised_salient_object_detection_with_short_connections_1424%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeeply+supervised+salient+object+detection+with+short+connections%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Weakly_Supervised_Topdown_Salient_Object_Detection_1431%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeakly+Supervised+Top-down+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SalGAN_Visual_Saliency_Prediction_with_Generative_Adversarial_Networks_1436%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESalGAN%3A+Visual+Saliency+Prediction+with+Generative+Adversarial+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Visual_Saliency_Prediction_Using_a_Mixture_of_Deep_Neural_Networks_1441%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+Saliency+Prediction+Using+a+Mixture+of+Deep+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_Fast_and_Compact_Salient_Score_Regression_Network_Based_on_Fully_Convolutional_Network_1445%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Fast+and+Compact+Salient+Score+Regression+Network+Based+on+Fully+Convolutional+Network%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Saliency_Detection_by_Forward_and_Backward_Cues_in_DeepCNNs_1449%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESaliency+Detection+by+Forward+and+Backward+Cues+in+Deep-CNNs%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Supervised_Adversarial_Networks_for_Image_Saliency_Detection_1453%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESupervised+Adversarial+Networks+for+Image+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Groupwise_Deep_Cosaliency_Detection_1457%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGroup-wise+Deep+Co-saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_the_Success_Rate_of_One_Realtime_Unconstrained_Salient_Object_Detection_1461%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+the+Success+Rate+of+One%3A+Real-time+Unconstrained+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Amulet_Aggregating_Multilevel_Convolutional_Features_for_Salient_Object_Detection_1466%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAmulet%3A+Aggregating+Multi-level+Convolutional+Features+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Uncertain_Convolutional_Features_for_Accurate_Saliency_Detection_1471%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Uncertain+Convolutional+Features+for+Accurate+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_EdgeAware_Saliency_Detection_1476%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Edge-Aware+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Selfexplanatory_Deep_Salient_Object_Detection_1480%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESelf-explanatory+Deep+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23PiCANet_Learning_Pixelwise_Contextual_Attention_in_ConvNets_and_Its_Application_in_Saliency_Detection_1485%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPiCANet%3A+Learning+Pixel-wise+Contextual+Attention+in+ConvNets+and+Its+Application+in+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DeepFeat_A_Bottom_Up_and_Top_Down_Saliency_Model_Based_on_Deep_Features_of_Convolutional_Neural_Nets_1489%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepFeat%3A+A+Bottom+Up+and+Top+Down+Saliency+Model+Based+on+Deep+Features+of+Convolutional+Neural+Nets%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Recurrently_Aggregating_Deep_Features_for_Salient_Object_Detection_1493%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERecurrently+Aggregating+Deep+Features+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_saliency_What_is_learnt_by_a_deep_network_about_saliency_1498%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+saliency%3A+What+is+learnt+by+a+deep+network+about+saliency%3F%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ContrastOriented_Deep_Neural_Networks_for_Salient_Object_Detection_1503%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContrast-Oriented+Deep+Neural+Networks+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Salient_Object_Detection_by_Lossless_Feature_Reflection_1508%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESalient+Object+Detection+by+Lossless+Feature+Reflection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23HyperFusionNet_Densely_Reflective_Fusion_for_Salient_Object_Detection_1513%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHyperFusion-Net%3A+Densely+Reflective+Fusion+for+Salient+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Video_Saliency_Detection_1520%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVideo+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_For_Video_Saliency_Detection_1521%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+For+Video+Saliency+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Video_Salient_Object_Detection_Using_Spatiotemporal_Deep_Features_1525%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVideo+Salient+Object+Detection+Using+Spatiotemporal+Deep+Features%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Predicting_Video_Saliency_with_ObjecttoMotion_CNN_and_Twolayer_Convolutional_LSTM_1529%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPredicting+Video+Saliency+with+Object-to-Motion+CNN+and+Two-layer+Convolutional+LSTM%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Visual_Relationship_Detection_1536%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+Relationship+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Visual_Relationship_Detection_with_Language_Priors_1537%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+Relationship+Detection+with+Language+Priors%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ViPCNN_A_Visual_Phrase_Reasoning_Convolutional_Neural_Network_for_Visual_Relationship_Detection_1543%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EViP-CNN%3A+A+Visual+Phrase+Reasoning+Convolutional+Neural+Network+for+Visual+Relationship+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Visual_Translation_Embedding_Network_for_Visual_Relation_Detection_1548%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVisual+Translation+Embedding+Network+for+Visual+Relation+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Variationstructured_Reinforcement_Learning_for_Visual_Relationship_and_Attribute_Detection_1552%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Variation-structured+Reinforcement+Learning+for+Visual+Relationship+and+Attribute+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Visual_Relationships_with_Deep_Relational_Networks_1557%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Visual+Relationships+with+Deep+Relational+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Identifying_Spatial_Relations_in_Images_using_Convolutional_Neural_Networks_1562%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIdentifying+Spatial+Relations+in+Images+using+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23PPRFCN_Weakly_Supervised_Visual_Relation_Detection_via_Parallel_Pairwise_RFCN_1566%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPPR-FCN%3A+Weakly+Supervised+Visual+Relation+Detection+via+Parallel+Pairwise+R-FCN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Natural_Language_Guided_Visual_Relationship_Detection_1571%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ENatural+Language+Guided+Visual+Relationship+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Visual_Relationships_Using_Box_Attention_1575%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Visual+Relationships+Using+Box+Attention%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Google_AI_Open_Images__Visual_Relationship_Track_1580%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGoogle+AI+Open+Images+-+Visual+Relationship+Track%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ContextDependent_Diffusion_Network_for_Visual_Relationship_Detection_1585%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContext-Dependent+Diffusion+Network+for+Visual+Relationship+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_Problem_Reduction_Approach_for_Visual_Relationships_Detection_1589%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Problem+Reduction+Approach+for+Visual+Relationships+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Face_Deteciton_1595%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Deteciton%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Multiview_Face_Detection_Using_Deep_Convolutional_Neural_Networks_1596%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-view+Face+Detection+Using+Deep+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23From_Facial_Parts_Responses_to_Face_Detection_A_Deep_Learning_Approach_1602%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFrom+Facial+Parts+Responses+to+Face+Detection%3A+A+Deep+Learning+Approach%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Compact_Convolutional_Neural_Network_Cascade_for_Face_Detection_1611%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECompact+Convolutional+Neural+Network+Cascade+for+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Face_Detection_with_EndtoEnd_Integration_of_a_ConvNet_and_a_3D_Model_1617%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Detection+with+End-to-End+Integration+of+a+ConvNet+and+a+3D+Model%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23CMSRCNN_Contextual_MultiScale_Regionbased_CNN_for_Unconstrained_Face_Detection_1623%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECMS-RCNN%3A+Contextual+Multi-Scale+Region-based+CNN+for+Unconstrained+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_a_Deep_Learning_Framework_for_Unconstrained_Face_Detection_1628%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+a+Deep+Learning+Framework+for+Unconstrained+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Supervised_Transformer_Network_for_Efficient_Face_Detection_1633%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESupervised+Transformer+Network+for+Efficient+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23UnitBox_An_Advanced_Object_Detection_Network_1637%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnitBox%3A+An+Advanced+Object+Detection+Network%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Bootstrapping_Face_Detection_with_Hard_Negative_Examples_1643%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBootstrapping+Face+Detection+with+Hard+Negative+Examples%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Grid_Loss_Detecting_Occluded_Faces_1649%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGrid+Loss%3A+Detecting+Occluded+Faces%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_MultiScale_Cascade_Fully_Convolutional_Network_Face_Detector_1656%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Multi-Scale+Cascade+Fully+Convolutional+Network+Face+Detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23MTCNN_1664%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMTCNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Joint_Face_Detection_and_Alignment_using_Multitask_Cascaded_Convolutional_Neural_Networks_1665%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EJoint+Face+Detection+and+Alignment+using+Multi-task+Cascaded+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Face_Detection_using_Deep_Learning_An_Improved_Faster_RCNN_Approach_1679%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Detection+using+Deep+Learning%3A+An+Improved+Faster+RCNN+Approach%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses_1684%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaceness-Net%3A+Face+Detection+through+Deep+Facial+Part+Responses%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23MultiPath_RegionBased_Convolutional_Neural_Network_for_Accurate_Detection_of_Unconstrained_Hard_Faces_1689%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-Path+Region-Based+Convolutional+Neural+Network+for+Accurate+Detection+of+Unconstrained+%E2%80%9CHard+Faces%E2%80%9D%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23EndToEnd_Face_Detection_and_Recognition_1694%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEnd-To-End+Face+Detection+and+Recognition%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Face_RCNN_1698%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Face_Detection_through_ScaleFriendly_Deep_Convolutional_Networks_1702%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Detection+through+Scale-Friendly+Deep+Convolutional+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ScaleAware_Face_Detection_1706%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScale-Aware+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Faces_Using_Inside_Cascaded_Contextual_CNN_1711%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Faces+Using+Inside+Cascaded+Contextual+CNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23MultiBranch_Fully_Convolutional_Network_for_Face_Detection_1716%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMulti-Branch+Fully+Convolutional+Network+for+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SSH_Single_Stage_Headless_Face_Detector_1720%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESSH%3A+Single+Stage+Headless+Face+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Dockerface_an_easy_to_install_and_use_Faster_RCNN_face_detector_in_a_Docker_container_1726%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDockerface%3A+an+easy+to+install+and+use+Faster+R-CNN+face+detector+in+a+Docker+container%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23FaceBoxes_A_CPU_Realtime_Face_Detector_with_High_Accuracy_1730%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaceBoxes%3A+A+CPU+Real-time+Face+Detector+with+High+Accuracy%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23S3FD_Single_Shot_Scaleinvariant_Face_Detector_1738%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ES3FD%3A+Single+Shot+Scale-invariant+Face+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Faces_Using_Regionbased_Fully_Convolutional_Networks_1746%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Faces+Using+Region-based+Fully+Convolutional+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23AffordanceNet_An_EndtoEnd_Deep_Learning_Approach_for_Object_Affordance_Detection_1750%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAffordanceNet%3A+An+End-to-End+Deep+Learning+Approach+for+Object+Affordance+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Face_Attention_Network_An_effective_Face_Detector_for_the_Occluded_Faces_1754%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Attention+Network%3A+An+effective+Face+Detector+for+the+Occluded+Faces%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Feature_Agglomeration_Networks_for_Single_Stage_Face_Detection_1758%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFeature+Agglomeration+Networks+for+Single+Stage+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Face_Detection_Using_Improved_Faster_RCNN_1762%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace+Detection+Using+Improved+Faster+RCNN%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23PyramidBox_A_Contextassisted_Single_Shot_Face_Detector_1767%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPyramidBox%3A+A+Context-assisted+Single+Shot+Face+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_Fast_Face_Detection_Method_via_Convolutional_Neural_Network_1772%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Fast+Face+Detection+Method+via+Convolutional+Neural+Network%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Beyond_Tradeoff_Accelerate_FCNbased_Face_Detector_with_Higher_Accuracy_1777%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBeyond+Trade-off%3A+Accelerate+FCN-based+Face+Detector+with+Higher+Accuracy%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23RealTime_RotationInvariant_Face_Detection_with_Progressive_Calibration_Networks_1782%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EReal-Time+Rotation-Invariant+Face+Detection+with+Progressive+Calibration+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SFace_An_Efficient_Network_for_Face_Detection_in_Large_Scale_Variations_1788%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESFace%3A+An+Efficient+Network+for+Face+Detection+in+Large+Scale+Variations%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Survey_of_Face_Detection_on_Lowquality_Images_1793%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESurvey+of+Face+Detection+on+Low-quality+Images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Anchor_Cascade_for_Efficient_Face_Detection_1797%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAnchor+Cascade+for+Efficient+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Adversarial_Attacks_on_Face_Detectors_using_Neural_Net_based_Constrained_Optimization_1802%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAdversarial+Attacks+on+Face+Detectors+using+Neural+Net+based+Constrained+Optimization%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Selective_Refinement_Network_for_High_Performance_Face_Detection_1807%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESelective+Refinement+Network+for+High+Performance+Face+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DSFD_Dual_Shot_Face_Detector_1810%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDSFD%3A+Dual+Shot+Face+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Better_Features_for_Face_Detection_with_Feature_Fusion_and_Segmentation_Supervision_1813%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Better+Features+for+Face+Detection+with+Feature+Fusion+and+Segmentation+Supervision%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Detect_Small_Faces_1817%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetect+Small+Faces%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Finding_Tiny_Faces_1818%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFinding+Tiny+Faces%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_and_counting_tiny_faces_1827%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+and+counting+tiny+faces%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Seeing_Small_Faces_from_Robust_Anchors_Perspective_1834%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESeeing+Small+Faces+from+Robust+Anchor%E2%80%99s+Perspective%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23FaceMagNet_Magnifying_Feature_Maps_to_Detect_Small_Faces_1839%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFace-MagNet%3A+Magnifying+Feature+Maps+to+Detect+Small+Faces%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Person_Head_Detection_1849%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPerson+Head+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Contextaware_CNNs_for_person_head_detection_1850%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContext-aware+CNNs+for+person+head+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Heads_using_Feature_Refine_Net_and_Cascaded_Multiscale_Architecture_1857%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Heads+using+Feature+Refine+Net+and+Cascaded+Multi-scale+Architecture%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_Comparison_of_CNNbased_Face_and_Head_Detectors_for_RealTime_Video_Surveillance_Applications_1861%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Comparison+of+CNN-based+Face+and+Head+Detectors+for+Real-Time+Video+Surveillance+Applications%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23FCHD_A_fast_and_accurate_head_detector_1864%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFCHD%3A+A+fast+and+accurate+head+detector%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Pedestrian_Detection__People_Detection_1870%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPedestrian+Detection+%2F+People+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Pedestrian_Detection_aided_by_Deep_Learning_Semantic_Tasks_1871%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPedestrian+Detection+aided+by+Deep+Learning+Semantic+Tasks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_Strong_Parts_for_Pedestrian_Detection_1877%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+Strong+Parts+for+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Taking_a_Deeper_Look_at_Pedestrians_1883%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETaking+a+Deeper+Look+at+Pedestrians%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Convolutional_Channel_Features_1888%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EConvolutional+Channel+Features%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Endtoend_people_detection_in_crowded_scenes_1894%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEnd-to-end+people+detection+in+crowded+scenes%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_ComplexityAware_Cascades_for_Deep_Pedestrian_Detection_1901%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Complexity-Aware+Cascades+for+Deep+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_convolutional_neural_networks_for_pedestrian_detection_1906%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+convolutional+neural+networks+for+pedestrian+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Scaleaware_Fast_RCNN_for_Pedestrian_Detection_1911%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScale-aware+Fast+R-CNN+for+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23New_algorithm_improves_speed_and_accuracy_of_pedestrian_detection_1915%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ENew+algorithm+improves+speed+and+accuracy+of+pedestrian+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Pushing_the_Limits_of_Deep_CNNs_for_Pedestrian_Detection_1919%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPushing+the+Limits+of+Deep+CNNs+for+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_RealTime_Deep_Learning_Pedestrian_Detector_for_Robot_Navigation_1924%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Real-Time+Deep+Learning+Pedestrian+Detector+for+Robot+Navigation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_RealTime_Pedestrian_Detector_using_Deep_Learning_for_HumanAware_Navigation_1928%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Real-Time+Pedestrian+Detector+using+Deep+Learning+for+Human-Aware+Navigation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Is_Faster_RCNN_Doing_Well_for_Pedestrian_Detection_1932%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIs+Faster+R-CNN+Doing+Well+for+Pedestrian+Detection%3F%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Unsupervised_Deep_Domain_Adaptation_for_Pedestrian_Detection_1938%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnsupervised+Deep+Domain+Adaptation+for+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Reduced_Memory_Region_Based_Deep_Convolutional_Neural_Network_Detection_1943%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EReduced+Memory+Region+Based+Deep+Convolutional+Neural+Network+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Fused_DNN_A_deep_neural_network_fusion_approach_to_fast_and_robust_pedestrian_detection_1948%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFused+DNN%3A+A+deep+neural+network+fusion+approach+to+fast+and+robust+pedestrian+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_People_in_Artwork_with_CNNs_1952%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+People+in+Artwork+with+CNNs%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Multispectral_Deep_Neural_Networks_for_Pedestrian_Detection_1957%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMultispectral+Deep+Neural+Networks+for+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Multicamera_People_Detection_1962%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Multi-camera+People+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Expecting_the_Unexpected_Training_Detectors_for_Unusual_Pedestrians_with_Adversarial_Imposters_1966%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EExpecting+the+Unexpected%3A+Training+Detectors+for+Unusual+Pedestrians+with+Adversarial+Imposters%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23What_Can_Help_Pedestrian_Detection_1973%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWhat+Can+Help+Pedestrian+Detection%3F%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Illuminating_Pedestrians_via_Simultaneous_Detection__Segmentation_1980%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIlluminating+Pedestrians+via+Simultaneous+Detection+%26amp%3B+Segmentation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Rotational_Rectification_Network_for_Robust_Pedestrian_Detection_1984%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERotational+Rectification+Network+for+Robust+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23STDPD_Generating_Synthetic_Training_Data_for_Pedestrian_Detection_in_Unannotated_Videos_1989%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESTD-PD%3A+Generating+Synthetic+Training+Data+for+Pedestrian+Detection+in+Unannotated+Videos%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Too_Far_to_See_Not_Really__Pedestrian_Detection_with_Scaleaware_Localization_Policy_1994%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EToo+Far+to+See%3F+Not+Really%21+%E2%80%94+Pedestrian+Detection+with+Scale-aware+Localization+Policy%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Repulsion_Loss_Detecting_Pedestrians_in_a_Crowd_1998%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERepulsion+Loss%3A+Detecting+Pedestrians+in+a+Crowd%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Aggregated_Channels_Network_for_RealTime_Pedestrian_Detection_2002%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAggregated+Channels+Network+for+Real-Time+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Illuminationaware_Faster_RCNN_for_Robust_Multispectral_Pedestrian_Detection_2006%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIllumination-aware+Faster+R-CNN+for+Robust+Multispectral+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Exploring_MultiBranch_and_HighLevel_Semantic_Networks_for_Improving_Pedestrian_Detection_2011%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EExploring+Multi-Branch+and+High-Level+Semantic+Networks+for+Improving+Pedestrian+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23PedestrianSynthesisGAN_Generating_Pedestrian_Data_in_Real_Scene_and_Beyond_2015%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPedestrian-Synthesis-GAN%3A+Generating+Pedestrian+Data+in+Real+Scene+and+Beyond%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23PCN_Part_and_Context_Information_for_Pedestrian_Detection_with_CNNs_2019%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPCN%3A+Part+and+Context+Information+for+Pedestrian+Detection+with+CNNs%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Smallscale_Pedestrian_Detection_Based_on_Somatic_Topology_Localization_and_Temporal_Feature_Aggregation_2024%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESmall-scale+Pedestrian+Detection+Based+on+Somatic+Topology+Localization+and+Temporal+Feature+Aggregation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Occlusionaware_RCNN_Detecting_Pedestrians_in_a_Crowd_2029%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOcclusion-aware+R-CNN%3A+Detecting+Pedestrians+in+a+Crowd%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Multispectral_Pedestrian_Detection_via_Simultaneous_Detection_and_Segmentation_2034%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMultispectral+Pedestrian+Detection+via+Simultaneous+Detection+and+Segmentation%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Vehicle_Detection_2041%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVehicle+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DAVE_A_Unified_Framework_for_Fast_Vehicle_Detection_and_Annotation_2042%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDAVE%3A+A+Unified+Framework+for+Fast+Vehicle+Detection+and+Annotation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Evolving_Boxes_for_fast_Vehicle_Detection_2047%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEvolving+Boxes+for+fast+Vehicle+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23FineGrained_Car_Detection_for_Visual_Census_Estimation_2051%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFine-Grained+Car+Detection+for+Visual+Census+Estimation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SINet_A_Scaleinsensitive_Convolutional_Neural_Network_for_Fast_Vehicle_Detection_2056%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESINet%3A+A+Scale-insensitive+Convolutional+Neural+Network+for+Fast+Vehicle+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Label_and_Sample_Efficient_Training_of_Vehicle_Object_Detector_from_Sparsely_Labeled_Data_2061%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELabel+and+Sample%3A+Efficient+Training+of+Vehicle+Object+Detector+from+Sparsely+Labeled+Data%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Domain_Randomization_for_SceneSpecific_Car_Detection_and_Pose_Estimation_2065%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDomain+Randomization+for+Scene-Specific+Car+Detection+and+Pose+Estimation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ShuffleDet_RealTime_Vehicle_Detection_Network_in_Onboard_Embedded_UAV_Imagery_2068%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EShuffleDet%3A+Real-Time+Vehicle+Detection+Network+in+On-board+Embedded+UAV+Imagery%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23TrafficSign_Detection_2075%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETraffic-Sign+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23TrafficSign_Detection_and_Classification_in_the_Wild_2076%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETraffic-Sign+Detection+and+Classification+in+the+Wild%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Evaluating_Stateoftheart_Object_Detector_on_Challenging_Traffic_Light_Data_2083%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEvaluating+State-of-the-art+Object+Detector+on+Challenging+Traffic+Light+Data%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Small_Signs_from_Large_Images_2088%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Small+Signs+from+Large+Images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Localized_Traffic_Sign_Detection_with_Multiscale_Deconvolution_Networks_2093%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocalized+Traffic+Sign+Detection+with+Multi-scale+Deconvolution+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Detecting_Traffic_Lights_by_Single_Shot_Detection_2097%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetecting+Traffic+Lights+by+Single+Shot+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23A_Hierarchical_Deep_Architecture_and_MiniBatch_Selection_Method_For_Joint_Traffic_Sign_and_Light_Detection_2102%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA+Hierarchical+Deep+Architecture+and+Mini-Batch+Selection+Method+For+Joint+Traffic+Sign+and+Light+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Skeleton_Detection_2110%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESkeleton+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Skeleton_Extraction_in_Natural_Images_by_Fusing_Scaleassociated_Deep_Side_Outputs_2111%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Skeleton+Extraction+in+Natural+Images+by+Fusing+Scale-associated+Deep+Side+Outputs%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DeepSkeleton_Learning_Multitask_Scaleassociated_Deep_Side_Outputs_for_Object_Skeleton_Extraction_in_Natural_Images_2116%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepSkeleton%3A+Learning+Multi-task+Scale-associated+Deep+Side+Outputs+for+Object+Skeleton+Extraction+in+Natural+Images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23SRN_Sideoutput_Residual_Network_for_Object_Symmetry_Detection_in_the_Wild_2120%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESRN%3A+Side-output+Residual+Network+for+Object+Symmetry+Detection+in+the+Wild%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23HiFi_Hierarchical_Feature_Integration_for_Skeleton_Detection_2126%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHi-Fi%3A+Hierarchical+Feature+Integration+for+Skeleton+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Fruit_Detection_2132%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFruit+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Fruit_Detection_in_Orchards_2133%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Fruit+Detection+in+Orchards%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Image_Segmentation_for_Fruit_Detection_and_Yield_Estimation_in_Apple_Orchards_2137%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EImage+Segmentation+for+Fruit+Detection+and+Yield+Estimation+in+Apple+Orchards%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Shadow_Detection_2144%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EShadow+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Fast_Shadow_Detection_from_a_Single_Image_Using_a_Patched_Convolutional_Neural_Network_2145%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFast+Shadow+Detection+from+a+Single+Image+Using+a+Patched+Convolutional+Neural+Network%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ADNet_Shadow_Detection_with_Adversarial_Shadow_Attenuation_2149%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EA%2BD-Net%3A+Shadow+Detection+with+Adversarial+Shadow+Attenuation%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Stacked_Conditional_Generative_Adversarial_Networks_for_Jointly_Learning_Shadow_Detection_and_Shadow_Removal_2153%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EStacked+Conditional+Generative+Adversarial+Networks+for+Jointly+Learning+Shadow+Detection+and+Shadow+Removal%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Directionaware_Spatial_Context_Features_for_Shadow_Detection_2157%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDirection-aware+Spatial+Context+Features+for+Shadow+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Directionaware_Spatial_Context_Features_for_Shadow_Detection_and_Removal_2162%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDirection-aware+Spatial+Context+Features+for+Shadow+Detection+and+Removal%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Others_Detection_2169%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOthers+Detection%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Deformation_Network_for_Object_Landmark_Localization_2170%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Deformation+Network+for+Object+Landmark+Localization%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Fashion_Landmark_Detection_in_the_Wild_2174%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFashion+Landmark+Detection+in+the+Wild%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_for_Fast_and_Accurate_Fashion_Item_Detection_2181%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+for+Fast+and+Accurate+Fashion+Item+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23OSMDeepOD__OSM_and_Deep_Learning_based_Object_Detection_from_Aerial_Imagery_formerly_known_as_OSMCrosswalkDetection_2187%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOSMDeepOD+-+OSM+and+Deep+Learning+based+Object+Detection+from+Aerial+Imagery+%28formerly+known+as+%E2%80%9COSM-Crosswalk-Detection%E2%80%9D%29%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Selfie_Detection_by_SynergyConstraint_Based_Convolutional_Neural_Network_2191%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESelfie+Detection+by+Synergy-Constraint+Based+Convolutional+Neural+Network%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Associative_EmbeddingEndtoEnd_Learning_for_Joint_Detection_and_Grouping_2196%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAssociative+Embedding%3AEnd-to-End+Learning+for+Joint+Detection+and+Grouping%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Cuboid_Detection_Beyond_2D_Bounding_Boxes_2200%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Cuboid+Detection%3A+Beyond+2D+Bounding+Boxes%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Automatic_Model_Based_Dataset_Generation_for_Fast_and_Accurate_Crop_and_Weeds_Detection_2205%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAutomatic+Model+Based+Dataset+Generation+for+Fast+and+Accurate+Crop+and+Weeds+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_Logo_Detection_with_Data_Expansion_by_Synthesising_Context_2209%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+Logo+Detection+with+Data+Expansion+by+Synthesising+Context%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Scalable_Deep_Learning_Logo_Detection_2213%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScalable+Deep+Learning+Logo+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Pixelwise_Ear_Detection_with_Convolutional_EncoderDecoder_Networks_2217%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EPixel-wise+Ear+Detection+with+Convolutional+Encoder-Decoder+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Automatic_Handgun_Detection_Alarm_in_Videos_Using_Deep_Learning_2221%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAutomatic+Handgun+Detection+Alarm+in+Videos+Using+Deep+Learning%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Objects_as_context_for_part_detection_2226%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObjects+as+context+for+part+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Using_Deep_Networks_for_Drone_Detection_2230%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUsing+Deep+Networks+for+Drone+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Cut_Paste_and_Learn_Surprisingly_Easy_Synthesis_for_Instance_Detection_2235%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ECut%2C+Paste+and+Learn%3A+Surprisingly+Easy+Synthesis+for+Instance+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Target_Driven_Instance_Detection_2240%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETarget+Driven+Instance+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DeepVoting_An_Explainable_Framework_for_Semantic_Part_Detection_under_Partial_Occlusion_2244%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepVoting%3A+An+Explainable+Framework+for+Semantic+Part+Detection+under+Partial+Occlusion%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23VPGNet_Vanishing_Point_Guided_Network_for_Lane_and_Road_Marking_Detection_and_Recognition_2248%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EVPGNet%3A+Vanishing+Point+Guided+Network+for+Lane+and+Road+Marking+Detection+and+Recognition%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Grab_Pay_and_Eat_Semantic_Food_Detection_for_Smart_Restaurants_2254%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EGrab%2C+Pay+and+Eat%3A+Semantic+Food+Detection+for+Smart+Restaurants%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ReMotENet_Efficient_Relevant_Motion_Event_Detection_for_Largescale_Home_Surveillance_Videos_2258%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EReMotENet%3A+Efficient+Relevant+Motion+Event+Detection+for+Large-scale+Home+Surveillance+Videos%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_Object_Detection_Methods_for_Ecological_Camera_Trap_Data_2263%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+Object+Detection+Methods+for+Ecological+Camera+Trap+Data%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ELGAN_Embedding_Loss_Driven_Generative_Adversarial_Networks_for_Lane_Detection_2268%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEL-GAN%3A+Embedding+Loss+Driven+Generative+Adversarial+Networks+for+Lane+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_EndtoEnd_Lane_Detection_an_Instance_Segmentation_Approach_2272%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+End-to-End+Lane+Detection%3A+an+Instance+Segmentation+Approach%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23iCAN_InstanceCentric_Attention_Network_for_HumanObject_Interaction_Detection_2276%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EiCAN%3A+Instance-Centric+Attention+Network+for+Human-Object+Interaction+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Densely_Supervised_Grasp_Detector_DSGD_2283%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDensely+Supervised+Grasp+Detector+%28DSGD%29%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Object_Proposal_2287%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Proposal%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23DeepProposal_Hunting_Objects_by_Cascading_Deep_Convolutional_Layers_2288%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeepProposal%3A+Hunting+Objects+by+Cascading+Deep+Convolutional+Layers%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Scaleaware_Pixelwise_Object_Proposal_Networks_2293%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScale-aware+Pixel-wise+Object+Proposal+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Attend_Refine_Repeat_Active_Box_Proposal_Generation_via_InOut_Localization_2298%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAttend+Refine+Repeat%3A+Active+Box+Proposal+Generation+via+In-Out+Localization%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_to_Segment_Object_Proposals_via_Recursive_Neural_Networks_2304%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+to+Segment+Object+Proposals+via+Recursive+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Detection_with_Diverse_Proposals_2308%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Detection+with+Diverse+Proposals%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ScaleNet_Guiding_Object_Proposal_Generation_in_Supermarkets_and_Beyond_2314%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EScaleNet%3A+Guiding+Object+Proposal+Generation+in+Supermarkets+and+Beyond%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Improving_Small_Object_Proposals_for_Company_Logo_Detection_2319%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EImproving+Small+Object+Proposals+for+Company+Logo+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Open_Logo_Detection_Challenge_2324%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOpen+Logo+Detection+Challenge%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23AttentionMask_Attentive_Efficient_Object_Proposal_Generation_Focusing_on_Small_Objects_2332%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAttentionMask%3A+Attentive%2C+Efficient+Object+Proposal+Generation+Focusing+on+Small+Objects%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Localization_2338%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocalization%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Beyond_Bounding_Boxes_Precise_Localization_of_Objects_in_Images_2339%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBeyond+Bounding+Boxes%3A+Precise+Localization+of+Objects+in+Images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Weakly_Supervised_Object_Localization_with_Multifold_Multiple_Instance_Learning_2346%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeakly+Supervised+Object+Localization+with+Multi-fold+Multiple+Instance+Learning%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Weakly_Supervised_Object_Localization_Using_Size_Estimates_2350%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWeakly+Supervised+Object+Localization+Using+Size+Estimates%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Active_Object_Localization_with_Deep_Reinforcement_Learning_2354%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EActive+Object+Localization+with+Deep+Reinforcement+Learning%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Localizing_objects_using_referring_expressions_2360%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocalizing+objects+using+referring+expressions%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23LocNet_Improving_Localization_Accuracy_for_Object_Detection_2367%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELocNet%3A+Improving+Localization+Accuracy+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Learning_Deep_Features_for_Discriminative_Localization_2373%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELearning+Deep+Features+for+Discriminative+Localization%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23ContextLocNet_ContextAware_Deep_Network_Models_for_Weakly_Supervised_Localization_2381%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EContextLocNet%3A+Context-Aware+Deep+Network+Models+for+Weakly+Supervised+Localization%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Ensemble_of_Part_Detectors_for_Simultaneous_Classification_and_Localization_2388%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEnsemble+of+Part+Detectors+for+Simultaneous+Classification+and+Localization%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23STNet_Selective_Tuning_of_Convolutional_Networks_for_Object_Localization_2392%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESTNet%3A+Selective+Tuning+of+Convolutional+Networks+for+Object+Localization%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Soft_Proposal_Networks_for_Weakly_Supervised_Object_Localization_2396%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESoft+Proposal+Networks+for+Weakly+Supervised+Object+Localization%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Finegrained_Discriminative_Localization_via_Saliencyguided_Faster_RCNN_2401%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFine-grained+Discriminative+Localization+via+Saliency-guided+Faster+R-CNN%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Tutorials__Talks_2408%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETutorials+%2F+Talks%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Convolutional_Feature_Maps_Elements_of_efficient_and_accurate_CNNbased_object_detection_2409%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EConvolutional+Feature+Maps%3A+Elements+of+efficient+%28and+accurate%29+CNN-based+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Towards_Good_Practices_for_Recognition__Detection_2413%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETowards+Good+Practices+for+Recognition+%26amp%3B+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Work_in_progress_Improving_object_detection_and_instance_segmentation_for_small_objects_2418%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EWork+in+progress%3A+Improving+object+detection+and+instance+segmentation+for+small+objects%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_with_Deep_Learning_A_Review_2422%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+with+Deep+Learning%3A+A+Review%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Projects_2428%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EProjects%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Detectron_2429%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetectron%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23TensorBox_a_simple_framework_for_training_neural_networks_to_detect_objects_in_images_2434%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETensorBox%3A+a+simple+framework+for+training+neural+networks+to+detect+objects+in+images%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_detection_in_torch_Implementation_of_some_object_detection_frameworks_in_torch_2439%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+detection+in+torch%3A+Implementation+of+some+object+detection+frameworks+in+torch%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Using_DIGITS_to_train_an_Object_Detection_network_2443%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUsing+DIGITS+to+train+an+Object+Detection+network%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23FCNMultiBox_Detector_2447%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFCN-MultiBox+Detector%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23KittiBox_A_car_detection_model_implemented_in_Tensorflow_2452%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EKittiBox%3A+A+car+detection+model+implemented+in+Tensorflow.%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deformable_Convolutional_Networks__MST__SoftNMS_2458%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeformable+Convolutional+Networks+%2B+MST+%2B+Soft-NMS%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23How_to_Build_a_Realtime_HandDetector_using_Neural_Networks_SSD_on_Tensorflow_2462%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHow+to+Build+a+Real-time+Hand-Detector+using+Neural+Networks+%28SSD%29+on+Tensorflow%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Metrics_for_object_detection_2467%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMetrics+for+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23MobileNetv2SSDLite_2473%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EMobileNetv2-SSDLite%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Leaderboard_2478%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ELeaderboard%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Detection_Results_VOC2012_2479%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDetection+Results%3A+VOC2012%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Tools_2486%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ETools%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23BeaverDam_Video_annotation_tool_for_deep_learning_training_labels_2487%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBeaverDam%3A+Video+annotation+tool+for+deep+learning+training+labels%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A+++++++%3Cli%3E%3Ca+href%3D%22%23Blogs_2493%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EBlogs%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3Cul%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Convolutional_Neural_Networks_for_Object_Detection_2494%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EConvolutional+Neural+Networks+for+Object+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Introducing_automatic_object_detection_to_visual_search_Pinterest_2498%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EIntroducing+automatic+object+detection+to+visual+search+%28Pinterest%29%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Deep_Learning_for_Object_Detection_with_DIGITS_2505%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EDeep+Learning+for+Object+Detection+with+DIGITS%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Analyzing_The_Papers_Behind_Facebooks_Computer_Vision_Approach_2509%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAnalyzing+The+Papers+Behind+Facebook%E2%80%99s+Computer+Vision+Approach%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Easily_Create_High_Quality_Object_Detectors_with_Deep_Learning_2514%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EEasily+Create+High+Quality+Object+Detectors+with+Deep+Learning%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23How_to_Train_a_DeepLearned_Object_Detection_Model_in_the_Microsoft_Cognitive_Toolkit_2519%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EHow+to+Train+a+Deep-Learned+Object+Detection+Model+in+the+Microsoft+Cognitive+Toolkit%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Object_Detection_in_Satellite_Imagery_a_Low_Overhead_Approach_2524%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EObject+Detection+in+Satellite+Imagery%2C+a+Low+Overhead+Approach%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23You_Only_Look_Twice%E2%80%8A%E2%80%8AMultiScale_Object_Detection_in_Satellite_Imagery_With_Convolutional_Neural_Networks_2529%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EYou+Only+Look+Twice%E2%80%8A%E2%80%94%E2%80%8AMulti-Scale+Object+Detection+in+Satellite+Imagery+With+Convolutional+Neural+Networks%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Faster_RCNN_Pedestrian_and_Car_Detection_2534%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EFaster+R-CNN+Pedestrian+and+Car+Detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Small_UNet_for_vehicle_detection_2540%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESmall+U-Net+for+vehicle+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Region_of_interest_pooling_explained_2544%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ERegion+of+interest+pooling+explained%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Supercharge_your_Computer_Vision_models_with_the_TensorFlow_Object_Detection_API_2549%22+rel%3D%22nofollow%22+target%3D%22_self%22%3ESupercharge+your+Computer+Vision+models+with+the+TensorFlow+Object+Detection+API%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Understanding_SSD_MultiBox%E2%80%8A%E2%80%8ARealTime_Object_Detection_In_Deep_Learning_2554%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EUnderstanding+SSD+MultiBox%E2%80%8A%E2%80%94%E2%80%8AReal-Time+Object+Detection+In+Deep+Learning%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23Oneshot_object_detection_2558%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EOne-shot+object+detection%3C%2Fa%3E%3C%2Fli%3E%0A++++++++%3Cli%3E%3Ca+href%3D%22%23An_overview_of_object_detection_onestage_methods_2562%22+rel%3D%22nofollow%22+target%3D%22_self%22%3EAn+overview+of+object+detection%3A+one-stage+methods%3C%2Fa%3E%3C%2Fli%3E%0A+++++++%3C%2Ful%3E%0A++++++%3C%2Ful%3E%0A+++++%3C%2Ful%3E%0A++++%3C%2Fdiv%3E%0A++++%3Cp%3E%3C%2Fp%3E+%0A+++%3C%2Fdiv%3E%0A++%3C%2Fdiv%3E%0A++%3Cdiv+class%3D%22table-box%22%3E%0A+++%3Cdiv+class%3D%22table-box%22%3E%0A++++%3Ctable%3E+%0A+++++%3Cthead%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Cth%3EMethod%3C%2Fth%3E+%0A+++++++%3Cth+align%3D%22center%22%3Ebackbone%3C%2Fth%3E+%0A+++++++%3Cth+align%3D%22center%22%3Etest+size%3C%2Fth%3E+%0A+++++++%3Cth+align%3D%22center%22%3EVOC2007%3C%2Fth%3E+%0A+++++++%3Cth+align%3D%22center%22%3EVOC2010%3C%2Fth%3E+%0A+++++++%3Cth+align%3D%22center%22%3EVOC2012%3C%2Fth%3E+%0A+++++++%3Cth+align%3D%22center%22%3EILSVRC+2013%3C%2Fth%3E+%0A+++++++%3Cth+align%3D%22center%22%3EMSCOCO+2015%3C%2Fth%3E+%0A+++++++%3Cth+align%3D%22center%22%3ESpeed%3C%2Fth%3E+%0A++++++%3C%2Ftr%3E+%0A+++++%3C%2Fthead%3E+%0A+++++%3Ctbody%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EOverFeat%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E24.3%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ER-CNN%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EAlexNet%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E58.5%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E53.7%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E53.3%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E31.4%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ER-CNN%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EVGG17%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E66.0%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ESPP_net%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EZF-5%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E54.2%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E31.84%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EDeepID-Net%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E64.1%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E50.3%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ENoC%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E73.3%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E68.8%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EFast-RCNN%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EVGG16%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E70.0%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E68.8%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E68.4%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E19.7%25%28%40%5B0.5-0.95%5D%29%2C+35.9%25%28%400.5%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EMR-CNN%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E78.2%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E73.9%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EFaster-RCNN%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EVGG16%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E78.8%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E75.9%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E21.9%25%28%40%5B0.5-0.95%5D%29%2C+42.7%25%28%400.5%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E198ms%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EFaster-RCNN%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E85.6%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E83.8%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E37.4%25%28%40%5B0.5-0.95%5D%29%2C+59.0%25%28%400.5%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EYOLO%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E63.4%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E57.9%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E45+fps%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EYOLO%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EVGG-16%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E66.4%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E21+fps%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EYOLOv2%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E448x448%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E78.6%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E73.4%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E21.6%25%28%40%5B0.5-0.95%5D%29%2C+44.0%25%28%400.5%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E40+fps%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ESSD%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EVGG16%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E300x300%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E77.2%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E75.8%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E25.1%25%28%40%5B0.5-0.95%5D%29%2C+43.1%25%28%400.5%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E46+fps%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ESSD%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EVGG16%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E512x512%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E79.8%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E78.5%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E28.8%25%28%40%5B0.5-0.95%5D%29%2C+48.5%25%28%400.5%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E19+fps%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ESSD%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E300x300%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E28.0%25%28%40%5B0.5-0.95%5D%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E16+fps%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ESSD%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E512x512%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E31.2%25%28%40%5B0.5-0.95%5D%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E8+fps%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EDSSD%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E300x300%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E28.0%25%28%40%5B0.5-0.95%5D%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E8+fps%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EDSSD%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E500x500%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E33.2%25%28%40%5B0.5-0.95%5D%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E6+fps%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EION%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E79.2%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E76.4%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ECRAFT%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E75.7%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E71.3%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E48.5%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EOHEM%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E78.9%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E76.3%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E25.5%25%28%40%5B0.5-0.95%5D%29%2C+45.9%25%28%400.5%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ER-FCN%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EResNet50%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E77.4%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E0.12sec%28K40%29%2C+0.09sec%28TitianX%29%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ER-FCN%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E79.5%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E0.17sec%28K40%29%2C+0.12sec%28TitianX%29%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ER-FCN%28ms+train%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EResNet101%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E83.6%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E82.0%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E31.5%25%28%40%5B0.5-0.95%5D%29%2C+53.2%25%28%400.5%29%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3EPVANet+9.0%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E84.9%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E84.2%25%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E750ms%28CPU%29%2C+46ms%28TitianX%29%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ERetinaNet%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EResNet101-FPN%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ELight-Head+R-CNN%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EXception*%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E800%2F1200%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E31.5%25%40%5B0.5%3A0.95%5D%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E95+fps%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A++++++%3Ctr%3E+%0A+++++++%3Ctd%3ELight-Head+R-CNN%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3EXception*%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E700%2F1100%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E30.7%25%40%5B0.5%3A0.95%5D%3C%2Ftd%3E+%0A+++++++%3Ctd+align%3D%22center%22%3E102+fps%3C%2Ftd%3E+%0A++++++%3C%2Ftr%3E+%0A+++++%3C%2Ftbody%3E+%0A++++%3C%2Ftable%3E%0A+++%3C%2Fdiv%3E%0A++%3C%2Fdiv%3E%0A++%3Ch1%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Papers_39%22+target%3D%22_blank%22%3E%3C%2Fa%3EPapers%3C%2Fh1%3E+%0A++%3Chr%3E+%0A++%3Chr%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Neural_Networks_for_Object_Detection_45%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Neural+Networks+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5207-deep-neural-networks-for-object-detection.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5207-deep-neural-networks-for-object-detection.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22OverFeat_Integrated_Recognition_Localization_and_Detection_using_Convolutional_Networks_48%22+target%3D%22_blank%22%3E%3C%2Fa%3EOverFeat%3A+Integrated+Recognition%2C+Localization+and+Detection+using+Convolutional+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1312.6229%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1312.6229%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fsermanet%2FOverFeat%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fsermanet%2FOverFeat%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Ecode%3A+%3Ca+href%3D%22http%3A%2F%2Fcilvr.nyu.edu%2Fdoku.php%3Fid%3Dsoftware%3Aoverfeat%3Astart%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcilvr.nyu.edu%2Fdoku.php%3Fid%3Dsoftware%3Aoverfeat%3Astart%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RCNN_55%22+target%3D%22_blank%22%3E%3C%2Fa%3ER-CNN%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation_56%22+target%3D%22_blank%22%3E%3C%2Fa%3ERich+feature+hierarchies+for+accurate+object+detection+and+semantic+segmentation%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+R-CNN%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1311.2524%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1311.2524%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Esupp%3A+%3Ca+href%3D%22http%3A%2F%2Fpeople.eecs.berkeley.edu%2F%7Erbg%2Fpapers%2Fr-cnn-cvpr-supp.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpeople.eecs.berkeley.edu%2F%7Erbg%2Fpapers%2Fr-cnn-cvpr-supp.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.image-net.org%2Fchallenges%2FLSVRC%2F2013%2Fslides%2Fr-cnn-ilsvrc2013-workshop.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.image-net.org%2Fchallenges%2FLSVRC%2F2013%2Fslides%2Fr-cnn-ilsvrc2013-workshop.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cs.berkeley.edu%2F%7Erbg%2Fslides%2Frcnn-cvpr14-slides.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cs.berkeley.edu%2F%7Erbg%2Fslides%2Frcnn-cvpr14-slides.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frbgirshick%2Frcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frbgirshick%2Frcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fzhangliliang.com%2F2014%2F07%2F23%2Fpaper-note-rcnn%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fzhangliliang.com%2F2014%2F07%2F23%2Fpaper-note-rcnn%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Ecaffe-pr%28%E2%80%9CMake+R-CNN+the+Caffe+detection+example%E2%80%9D%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fpull%2F482%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fpull%2F482%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fast_RCNN_70%22+target%3D%22_blank%22%3E%3C%2Fa%3EFast+R-CNN%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fast_RCNN_71%22+target%3D%22_blank%22%3E%3C%2Fa%3EFast+R-CNN%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1504.08083%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1504.08083%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Ftutorial.caffe.berkeleyvision.org%2Fcaffe-cvpr15-detection.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Ftutorial.caffe.berkeleyvision.org%2Fcaffe-cvpr15-detection.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28COCO-branch%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%2Ftree%2Fcoco%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%2Ftree%2Fcoco%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Ewebcam+demo%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%2Fpull%2F29%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frbgirshick%2Ffast-rcnn%2Fpull%2F29%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fzhangliliang.com%2F2015%2F05%2F17%2Fpaper-note-fast-rcnn%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fzhangliliang.com%2F2015%2F05%2F17%2Fpaper-note-fast-rcnn%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fblog.csdn.net%2Flinj_m%2Farticle%2Fdetails%2F48930179%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fblog.csdn.net%2Flinj_m%2Farticle%2Fdetails%2F48930179%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28%E2%80%9CFast+R-CNN+in+MXNet%E2%80%9D%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fprecedenceguo%2Fmx-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fprecedenceguo%2Fmx-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmahyarnajibi%2Ffast-rcnn-torch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmahyarnajibi%2Ffast-rcnn-torch%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fapple2373%2Fchainer-simple-fast-rnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fapple2373%2Fchainer-simple-fast-rnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzplizzi%2Ftensorflow-fast-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzplizzi%2Ftensorflow-fast-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22AFastRCNN_Hard_Positive_Generation_via_Adversary_for_Object_Detection_84%22+target%3D%22_blank%22%3E%3C%2Fa%3EA-Fast-RCNN%3A+Hard+Positive+Generation+via+Adversary+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2017%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.03414%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.03414%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fabhinavsh.info%2Fpapers%2Fpdfs%2Fadversarial_object_detection.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fabhinavsh.info%2Fpapers%2Fpdfs%2Fadversarial_object_detection.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fxiaolonw%2Fadversarial-frcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fxiaolonw%2Fadversarial-frcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Faster_RCNN_91%22+target%3D%22_blank%22%3E%3C%2Fa%3EFaster+R-CNN%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Faster_RCNN_Towards_RealTime_Object_Detection_with_Region_Proposal_Networks_92%22+target%3D%22_blank%22%3E%3C%2Fa%3EFaster+R-CNN%3A+Towards+Real-Time+Object+Detection+with+Region+Proposal+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+NIPS+2015%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1506.01497%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1506.01497%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egitxiv%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.gitxiv.com%2Fposts%2F8pfpcvefDYn2gSgXk%2Ffaster-r-cnn-towards-real-time-object-detection-with-region%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.gitxiv.com%2Fposts%2F8pfpcvefDYn2gSgXk%2Ffaster-r-cnn-towards-real-time-object-detection-with-region%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fweb.cs.hacettepe.edu.tr%2F%7Eaykut%2Fclasses%2Fspring2016%2Fbil722%2Fslides%2Fw05-FasterR-CNN.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fweb.cs.hacettepe.edu.tr%2F%7Eaykut%2Fclasses%2Fspring2016%2Fbil722%2Fslides%2Fw05-FasterR-CNN.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28official%2C+Matlab%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FShaoqingRen%2Ffaster_rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FShaoqingRen%2Ffaster_rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frbgirshick%2Fpy-faster-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frbgirshick%2Fpy-faster-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmsracver%2FDeformable-ConvNets%2Ftree%2Fmaster%2Ffaster_rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmsracver%2FDeformable-ConvNets%2Ftree%2Fmaster%2Ffaster_rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2F%2Fjwyang%2Ffaster-rcnn.pytorch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2F%2Fjwyang%2Ffaster-rcnn.pytorch%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmitmul%2Fchainer-faster-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmitmul%2Fchainer-faster-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fandreaskoepf%2Ffaster-rcnn.torch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fandreaskoepf%2Ffaster-rcnn.torch%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fruotianluo%2FFaster-RCNN-Densecap-torch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fruotianluo%2FFaster-RCNN-Densecap-torch%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fsmallcorgi%2FFaster-RCNN_TF%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fsmallcorgi%2FFaster-RCNN_TF%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FCharlesShang%2FTFFRCNN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FCharlesShang%2FTFFRCNN%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28C%2B%2B+demo%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FYihangLou%2FFasterRCNN-Encapsulation-Cplusplus%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FYihangLou%2FFasterRCNN-Encapsulation-Cplusplus%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fyhenon%2Fkeras-frcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fyhenon%2Fkeras-frcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FEniac-Xie%2Ffaster-rcnn-resnet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FEniac-Xie%2Ffaster-rcnn-resnet%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28C%2B%2B%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FD-X-Y%2Fcaffe-faster-rcnn%2Ftree%2Fdev%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FD-X-Y%2Fcaffe-faster-rcnn%2Ftree%2Fdev%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RCNN_minus_R_111%22+target%3D%22_blank%22%3E%3C%2Fa%3ER-CNN+minus+R%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+BMVC+2015%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1506.06981%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1506.06981%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Faster_RCNN_in_MXNet_with_distributed_implementation_and_data_parallelization_116%22+target%3D%22_blank%22%3E%3C%2Fa%3EFaster+R-CNN+in+MXNet+with+distributed+implementation+and+data+parallelization%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fdmlc%2Fmxnet%2Ftree%2Fmaster%2Fexample%2Frcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fdmlc%2Fmxnet%2Ftree%2Fmaster%2Fexample%2Frcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Contextual_Priming_and_Feedback_for_Faster_RCNN_120%22+target%3D%22_blank%22%3E%3C%2Fa%3EContextual+Priming+and+Feedback+for+Faster+R-CNN%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2016.+Carnegie+Mellon+University%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fabhinavsh.info%2Fcontext_priming_feedback.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fabhinavsh.info%2Fcontext_priming_feedback.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eposter%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-1A-20.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-1A-20.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22An_Implementation_of_Faster_RCNN_with_Study_for_Region_Sampling_126%22+target%3D%22_blank%22%3E%3C%2Fa%3EAn+Implementation+of+Faster+RCNN+with+Study+for+Region+Sampling%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Technical+Report%2C+3+pages.+CMU%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.02138%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.02138%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fendernewton%2Ftf-faster-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fendernewton%2Ftf-faster-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Interpretable_RCNN_132%22+target%3D%22_blank%22%3E%3C%2Fa%3EInterpretable+R-CNN%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+North+Carolina+State+University+%26amp%3B+Alibaba%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+AND-OR+Graph+%28AOG%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.05226%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.05226%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LightHead_RCNN_140%22+target%3D%22_blank%22%3E%3C%2Fa%3ELight-Head+R-CNN%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LightHead_RCNN_In_Defense_of_TwoStage_Object_Detector_141%22+target%3D%22_blank%22%3E%3C%2Fa%3ELight-Head+R-CNN%3A+In+Defense+of+Two-Stage+Object+Detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Tsinghua+University+%26amp%3B+Megvii+Inc%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.07264%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.07264%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28official%2C+Tensorflow%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzengarden%2Flight_head_rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzengarden%2Flight_head_rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fterrychenism%2FDeformable-ConvNets%2Fblob%2Fmaster%2Frfcn%2Fsymbols%2Fresnet_v1_101_rfcn_light.py%23L784%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fterrychenism%2FDeformable-ConvNets%2Fblob%2Fmaster%2Frfcn%2Fsymbols%2Fresnet_v1_101_rfcn_light.py%23L784%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Cp%3E%23%23Cascade+R-CNN%3C%2Fp%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Cascade_RCNN_Delving_into_High_Quality_Object_Detection_149%22+target%3D%22_blank%22%3E%3C%2Fa%3ECascade+R-CNN%3A+Delving+into+High+Quality+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2018.+UC+San+Diego%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.00726%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.00726%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Caffe%2C+official%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzhaoweicai%2Fcascade-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzhaoweicai%2Fcascade-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MultiBox_157%22+target%3D%22_blank%22%3E%3C%2Fa%3EMultiBox%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Scalable_Object_Detection_using_Deep_Neural_Networks_158%22+target%3D%22_blank%22%3E%3C%2Fa%3EScalable+Object+Detection+using+Deep+Neural+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+first+MultiBox.+Train+a+CNN+to+predict+Region+of+Interest.%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1312.2249%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1312.2249%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fgoogle%2Fmultibox%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fgoogle%2Fmultibox%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eblog%3A+%3Ca+href%3D%22https%3A%2F%2Fresearch.googleblog.com%2F2014%2F12%2Fhigh-quality-object-detection-at-scale.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fresearch.googleblog.com%2F2014%2F12%2Fhigh-quality-object-detection-at-scale.html%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Scalable_HighQuality_Object_Detection_165%22+target%3D%22_blank%22%3E%3C%2Fa%3EScalable%2C+High-Quality+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+second+MultiBox%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1412.1441%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1412.1441%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fgoogle%2Fmultibox%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fgoogle%2Fmultibox%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SPPNet_173%22+target%3D%22_blank%22%3E%3C%2Fa%3ESPP-Net%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Spatial_Pyramid_Pooling_in_Deep_Convolutional_Networks_for_Visual_Recognition_174%22+target%3D%22_blank%22%3E%3C%2Fa%3ESpatial+Pyramid+Pooling+in+Deep+Convolutional+Networks+for+Visual+Recognition%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2014+%2F+TPAMI+2015%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1406.4729%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1406.4729%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FShaoqingRen%2FSPP_net%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FShaoqingRen%2FSPP_net%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fzhangliliang.com%2F2014%2F09%2F13%2Fpaper-note-sppnet%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fzhangliliang.com%2F2014%2F09%2F13%2Fpaper-note-sppnet%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeepIDNet_Deformable_Deep_Convolutional_Neural_Networks_for_Object_Detection_181%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeepID-Net%3A+Deformable+Deep+Convolutional+Neural+Networks+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+PAMI+2016%3C%2Fli%3E+%0A+++%3Cli%3Eintro%3A+an+extension+of+R-CNN.+box+pre-training%2C+cascade+on+region+proposals%2C+deformation+layers+and+context+representations%3C%2Fli%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%25CB%259Cwlouyang%2Fprojects%2FimagenetDeepId%2Findex.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%CB%9Cwlouyang%2Fprojects%2FimagenetDeepId%2Findex.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1412.5661%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1412.5661%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detectors_Emerge_in_Deep_Scene_CNNs_188%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detectors+Emerge+in+Deep+Scene+CNNs%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICLR+2015%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1412.6856%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1412.6856%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.robots.ox.ac.uk%2F%7Evgg%2Frg%2Fpapers%2Fzhou_iclr15.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.robots.ox.ac.uk%2F%7Evgg%2Frg%2Fpapers%2Fzhou_iclr15.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fpeople.csail.mit.edu%2Fkhosla%2Fpapers%2Ficlr2015_zhou.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fpeople.csail.mit.edu%2Fkhosla%2Fpapers%2Ficlr2015_zhou.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fplaces.csail.mit.edu%2Fslide_iclr2015.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fplaces.csail.mit.edu%2Fslide_iclr2015.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22segDeepM_Exploiting_Segmentation_and_Context_in_Deep_Neural_Networks_for_Object_Detection_196%22+target%3D%22_blank%22%3E%3C%2Fa%3EsegDeepM%3A+Exploiting+Segmentation+and+Context+in+Deep+Neural+Networks+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2015%3C%2Fli%3E+%0A+++%3Cli%3Eproject%28code%2Bdata%29%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.cs.toronto.edu%2F%7Eyukun%2Fsegdeepm.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.cs.toronto.edu%2F%7Eyukun%2Fsegdeepm.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1502.04275%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1502.04275%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FYknZhu%2FsegDeepM%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FYknZhu%2FsegDeepM%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_Networks_on_Convolutional_Feature_Maps_203%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+Networks+on+Convolutional+Feature+Maps%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+TPAMI+2015%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+NoC%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1504.06066%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1504.06066%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca+id%3D%22Improving_Object_Detection_with_Deep_Convolutional_Networks_via_Bayesian_Optimization_and_Structured_Prediction_209%22+target%3D%22_blank%22%3E%3C%2Fa%3EImproving+Object+Detection+with+Deep+Convolutional+Networks+via+Bayesian+Optimization+and+Structured+Prediction%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1504.03293%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1504.03293%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.ytzhang.net%2Ffiles%2Fpublications%2F2015-cvpr-det-slides.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.ytzhang.net%2Ffiles%2Fpublications%2F2015-cvpr-det-slides.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FYutingZhang%2Ffgs-obj%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FYutingZhang%2Ffgs-obj%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeepBox_Learning_Objectness_with_Convolutional_Networks_215%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeepBox%3A+Learning+Objectness+with+Convolutional+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Ekeywords%3A+DeepBox%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1505.02146%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1505.02146%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fweichengkuo%2FDeepBox%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fweichengkuo%2FDeepBox%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MRCNN_223%22+target%3D%22_blank%22%3E%3C%2Fa%3EMR-CNN%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_detection_via_a_multiregion__semantic_segmentationaware_CNN_model_224%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+detection+via+a+multi-region+%26amp%3B+semantic+segmentation-aware+CNN+model%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2015.+MR-CNN%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1505.01749%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1505.01749%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fgidariss%2Fmrcnn-object-detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fgidariss%2Fmrcnn-object-detection%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fzhangliliang.com%2F2015%2F05%2F17%2Fpaper-note-ms-cnn%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fzhangliliang.com%2F2015%2F05%2F17%2Fpaper-note-ms-cnn%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Enotes%3A+%3Ca+href%3D%22http%3A%2F%2Fblog.cvmarcher.com%2Fposts%2F2015%2F05%2F17%2Fmulti-region-semantic-segmentation-aware-cnn%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fblog.cvmarcher.com%2Fposts%2F2015%2F05%2F17%2Fmulti-region-semantic-segmentation-aware-cnn%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLO_234%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLO%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22You_Only_Look_Once_Unified_RealTime_Object_Detection_235%22+target%3D%22_blank%22%3E%3C%2Fa%3EYou+Only+Look+Once%3A+Unified%2C+Real-Time+Object+Detection%3C%2Fh3%3E+%0A++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F201808211442120%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1506.02640%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1506.02640%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Ecode%3A+%3Ca+href%3D%22http%3A%2F%2Fpjreddie.com%2Fdarknet%2Fyolo%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpjreddie.com%2Fdarknet%2Fyolo%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fpjreddie%2Fdarknet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fpjreddie%2Fdarknet%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eblog%3A+%3Ca+href%3D%22https%3A%2F%2Fpjreddie.com%2Fpublications%2Fyolo%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fpjreddie.com%2Fpublications%2Fyolo%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22https%3A%2F%2Fdocs.google.com%2Fpresentation%2Fd%2F1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA%2Fpub%3Fstart%3Dfalse%26amp%3Bloop%3Dfalse%26amp%3Bdelayms%3D3000%26amp%3Bslide%3Did.p%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fdocs.google.com%2Fpresentation%2Fd%2F1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA%2Fpub%3Fstart%3Dfalse%26amp%3Bloop%3Dfalse%26amp%3Bdelayms%3D3000%26amp%3Bslide%3Did.p%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Ereddit%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F3a3m0o%2Frealtime_object_detection_with_yolo%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F3a3m0o%2Frealtime_object_detection_with_yolo%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fgliese581gg%2FYOLO_tensorflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fgliese581gg%2FYOLO_tensorflow%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fxingwangsfu%2Fcaffe-yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fxingwangsfu%2Fcaffe-yolo%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ffrankzhangrui%2FDarknet-Yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ffrankzhangrui%2FDarknet-Yolo%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBriSkyHekun%2Fpy-darknet-yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBriSkyHekun%2Fpy-darknet-yolo%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ftommy-qichang%2Fyolo.torch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ftommy-qichang%2Fyolo.torch%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ffrischzenger%2Fyolo-windows%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ffrischzenger%2Fyolo-windows%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FAlexeyAB%2Fyolo-windows%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FAlexeyAB%2Fyolo-windows%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fnilboy%2Ftensorflow-yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fnilboy%2Ftensorflow-yolo%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca+id%3D%22darkflow__translate_darknet_to_tensorflow_Load_trained_weights_retrainfinetune_them_using_tensorflow_export_constant_graph_def_to_C_252%22+target%3D%22_blank%22%3E%3C%2Fa%3Edarkflow+-+translate+darknet+to+tensorflow.+Load+trained+weights%2C+retrain%2Ffine-tune+them+using+tensorflow%2C+export+constant+graph+def+to+C%2B%2B%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eblog%3A+%3Ca+href%3D%22https%3A%2F%2Fthtrieu.github.io%2Fnotes%2Fyolo-tensorflow-graph-buffer-cpp%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fthtrieu.github.io%2Fnotes%2Fyolo-tensorflow-graph-buffer-cpp%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fthtrieu%2Fdarkflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fthtrieu%2Fdarkflow%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Start_Training_YOLO_with_Our_Own_Data_257%22+target%3D%22_blank%22%3E%3C%2Fa%3EStart+Training+YOLO+with+Our+Own+Data%3C%2Fh3%3E+%0A++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F20180821144236849%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+train+with+customized+data+and+class+numbers%2Flabels.+Linux+%2F+Windows+version+for+darknet.%3C%2Fli%3E+%0A+++%3Cli%3Eblog%3A+%3Ca+href%3D%22http%3A%2F%2Fguanghan.info%2Fblog%2Fen%2Fmy-works%2Ftrain-yolo%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fguanghan.info%2Fblog%2Fen%2Fmy-works%2Ftrain-yolo%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FGuanghan%2Fdarknet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FGuanghan%2Fdarknet%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLO_Core_ML_versus_MPSNNGraph_263%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLO%3A+Core+ML+versus+MPSNNGraph%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Tiny+YOLO+for+iOS+implemented+using+CoreML+but+also+using+the+new+MPS+graph+API.%3C%2Fli%3E+%0A+++%3Cli%3Eblog%3A+%3Ca+href%3D%22http%3A%2F%2Fmachinethink.net%2Fblog%2Fyolo-coreml-versus-mps-graph%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fmachinethink.net%2Fblog%2Fyolo-coreml-versus-mps-graph%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fhollance%2FYOLO-CoreML-MPSNNGraph%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fhollance%2FYOLO-CoreML-MPSNNGraph%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22TensorFlow_YOLO_object_detection_on_Android_269%22+target%3D%22_blank%22%3E%3C%2Fa%3ETensorFlow+YOLO+object+detection+on+Android%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Real-time+object+detection+on+Android+using+the+YOLO+network+with+TensorFlow%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fnatanielruiz%2Fandroid-yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fnatanielruiz%2Fandroid-yolo%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Computer_Vision_in_iOS__Object_Detection_274%22+target%3D%22_blank%22%3E%3C%2Fa%3EComputer+Vision+in+iOS+%E2%80%93+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eblog%3A+%3Ca+href%3D%22https%3A%2F%2Fsriraghu.com%2F2017%2F07%2F12%2Fcomputer-vision-in-ios-object-detection%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fsriraghu.com%2F2017%2F07%2F12%2Fcomputer-vision-in-ios-object-detection%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fr4ghu%2FiOS-CoreML-Yolo%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fr4ghu%2FiOS-CoreML-Yolo%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLOv2_280%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLOv2%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLO9000_Better_Faster_Stronger_281%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLO9000%3A+Better%2C+Faster%2C+Stronger%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.08242%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.08242%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Ecode%3A+%3Ca+href%3D%22http%3A%2F%2Fpjreddie.com%2Fyolo9000%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpjreddie.com%2Fyolo9000%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Chainer%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fleetenki%2FYOLOv2%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fleetenki%2FYOLOv2%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Keras%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fallanzelener%2FYAD2K%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fallanzelener%2FYAD2K%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28PyTorch%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Flongcw%2Fyolo2-pytorch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Flongcw%2Fyolo2-pytorch%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Tensorflow%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fhizhangp%2Fyolo_tensorflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fhizhangp%2Fyolo_tensorflow%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Windows%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FAlexeyAB%2Fdarknet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FAlexeyAB%2Fdarknet%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FchoasUp%2Fcaffe-yolo9000%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FchoasUp%2Fcaffe-yolo9000%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fphilipperemy%2Fyolo-9000%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fphilipperemy%2Fyolo-9000%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22darknet_scripts_293%22+target%3D%22_blank%22%3E%3C%2Fa%3Edarknet_scripts%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Auxilary+scripts+to+work+with+%28YOLO%29+darknet+deep+learning+famework.+AKA+-%26gt%3B+How+to+generate+YOLO+anchors%3F%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FJumabek%2Fdarknet_scripts%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FJumabek%2Fdarknet_scripts%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Yolo_mark_GUI_for_marking_bounded_boxes_of_objects_in_images_for_training_Yolo_v2_298%22+target%3D%22_blank%22%3E%3C%2Fa%3EYolo_mark%3A+GUI+for+marking+bounded+boxes+of+objects+in+images+for+training+Yolo+v2%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FAlexeyAB%2FYolo_mark%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FAlexeyAB%2FYolo_mark%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LightNet_Bringing_pjreddies_DarkNet_out_of_the_shadows_302%22+target%3D%22_blank%22%3E%3C%2Fa%3ELightNet%3A+Bringing+pjreddie%E2%80%99s+DarkNet+out+of+the+shadows%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2F%2Fexplosion%2Flightnet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2F%2Fexplosion%2Flightnet%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLO_v2_Bounding_Box_Tool_306%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLO+v2+Bounding+Box+Tool%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Bounding+box+labeler+tool+to+generate+the+training+data+in+the+format+YOLO+v2+requires.%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FCartucho%2Fyolo-boundingbox-labeler-GUI%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FCartucho%2Fyolo-boundingbox-labeler-GUI%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLOv3_312%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLOv3%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLOv3_An_Incremental_Improvement_313%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLOv3%3A+An+Incremental+Improvement%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22https%3A%2F%2Fpjreddie.com%2Fdarknet%2Fyolo%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fpjreddie.com%2Fdarknet%2Fyolo%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.02767%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.02767%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YOLOLITE_A_RealTime_Object_Detection_Algorithm_Optimized_for_NonGPU_Computers_318%22+target%3D%22_blank%22%3E%3C%2Fa%3EYOLO-LITE%3A+A+Real-Time+Object+Detection+Algorithm+Optimized+for+Non-GPU+Computers%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.05588%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.05588%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22AttentionNet_Aggregating_Weak_Directions_for_Accurate_Object_Detection_323%22+target%3D%22_blank%22%3E%3C%2Fa%3EAttentionNet%3A+Aggregating+Weak+Directions+for+Accurate+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2015%3C%2Fli%3E+%0A+++%3Cli%3Eintro%3A+state-of-the-art+performance+of+65%25+%28AP%29+on+PASCAL+VOC+2007%2F2012+human+detection+task%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1506.07704%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1506.07704%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.robots.ox.ac.uk%2F%7Evgg%2Frg%2Fslides%2FAttentionNet.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.robots.ox.ac.uk%2F%7Evgg%2Frg%2Fslides%2FAttentionNet.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks%2Flunit-kaist-slide.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks%2Flunit-kaist-slide.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DenseBox_333%22+target%3D%22_blank%22%3E%3C%2Fa%3EDenseBox%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DenseBox_Unifying_Landmark_Localization_with_End_to_End_Object_Detection_334%22+target%3D%22_blank%22%3E%3C%2Fa%3EDenseBox%3A+Unifying+Landmark+Localization+with+End+to+End+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1509.04874%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1509.04874%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Edemo%3A+%3Ca+href%3D%22http%3A%2F%2Fpan.baidu.com%2Fs%2F1mgoWWsS%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpan.baidu.com%2Fs%2F1mgoWWsS%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3EKITTI+result%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cvlibs.net%2Fdatasets%2Fkitti%2Feval_object.php%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cvlibs.net%2Fdatasets%2Fkitti%2Feval_object.php%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SSD_341%22+target%3D%22_blank%22%3E%3C%2Fa%3ESSD%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SSD_Single_Shot_MultiBox_Detector_342%22+target%3D%22_blank%22%3E%3C%2Fa%3ESSD%3A+Single+Shot+MultiBox+Detector%3C%2Fh3%3E+%0A++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F20180821144311974%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2016+Oral%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1512.02325%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1512.02325%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cs.unc.edu%2F%7Ewliu%2Fpapers%2Fssd.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cs.unc.edu%2F%7Ewliu%2Fpapers%2Fssd.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cs.unc.edu%2F%257Ewliu%2Fpapers%2Fssd_eccv2016_slide.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cs.unc.edu%2F%7Ewliu%2Fpapers%2Fssd_eccv2016_slide.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Official%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fweiliu89%2Fcaffe%2Ftree%2Fssd%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fweiliu89%2Fcaffe%2Ftree%2Fssd%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Evideo%3A+%3Ca+href%3D%22http%3A%2F%2Fweibo.com%2Fp%2F2304447a2326da963254c963c97fb05dd3a973%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fweibo.com%2Fp%2F2304447a2326da963254c963c97fb05dd3a973%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzhreshold%2Fmxnet-ssd%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzhreshold%2Fmxnet-ssd%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzhreshold%2Fmxnet-ssd.cpp%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzhreshold%2Fmxnet-ssd.cpp%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Frykov8%2Fssd_keras%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Frykov8%2Fssd_keras%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbalancap%2FSSD-Tensorflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbalancap%2FSSD-Tensorflow%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Famdegroot%2Fssd.pytorch%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Famdegroot%2Fssd.pytorch%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fchuanqi305%2FMobileNet-SSD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fchuanqi305%2FMobileNet-SSD%3C%2Fa%3E%3Cbr%3E+What%E2%80%99s+the+diffience+in+performance+between+this+new+code+you+pushed+and+the+previous+code%3F+%23327%3Cbr%3E+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fweiliu89%2Fcaffe%2Fissues%2F327%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fweiliu89%2Fcaffe%2Fissues%2F327%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DSSD_360%22+target%3D%22_blank%22%3E%3C%2Fa%3EDSSD%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DSSD__Deconvolutional_Single_Shot_Detector_361%22+target%3D%22_blank%22%3E%3C%2Fa%3EDSSD+%3A+Deconvolutional+Single+Shot+Detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+UNC+Chapel+Hill+%26amp%3B+Amazon+Inc%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1701.06659%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1701.06659%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fchengyangfu%2Fcaffe%2Ftree%2Fdssd%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fchengyangfu%2Fcaffe%2Ftree%2Fdssd%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FMTCloudVision%2Fmxnet-dssd%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FMTCloudVision%2Fmxnet-dssd%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Edemo%3A+%3Ca+href%3D%22http%3A%2F%2F120.52.72.53%2Fwww.cs.unc.edu%2Fc3pr90ntc0td%2F%7Ecyfu%2Fdssd_lalaland.mp4%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2F120.52.72.53%2Fwww.cs.unc.edu%2Fc3pr90ntc0td%2F%7Ecyfu%2Fdssd_lalaland.mp4%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Enhancement_of_SSD_by_concatenating_feature_maps_for_object_detection_369%22+target%3D%22_blank%22%3E%3C%2Fa%3EEnhancement+of+SSD+by+concatenating+feature+maps+for+object+detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+rainbow+SSD+%28R-SSD%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1705.09587%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1705.09587%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Contextaware_SingleShot_Detector_374%22+target%3D%22_blank%22%3E%3C%2Fa%3EContext-aware+Single-Shot+Detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Ekeywords%3A+CSSD%2C+DiCSSD%2C+DeCSSD%2C+effective+receptive+fields+%28ERFs%29%2C+theoretical+receptive+fields+%28TRFs%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.08682%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.08682%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22FeatureFused_SSD_Fast_Detection_for_Small_Objects_379%22+target%3D%22_blank%22%3E%3C%2Fa%3EFeature-Fused+SSD%3A+Fast+Detection+for+Small+Objects%3C%2Fh3%3E+%0A++%3Cp%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1709.05054%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1709.05054%3C%2Fa%3E%3C%2Fp%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22FSSD_384%22+target%3D%22_blank%22%3E%3C%2Fa%3EFSSD%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22FSSD_Feature_Fusion_Single_Shot_Multibox_Detector_385%22+target%3D%22_blank%22%3E%3C%2Fa%3EFSSD%3A+Feature+Fusion+Single+Shot+Multibox+Detector%3C%2Fh3%3E+%0A++%3Cp%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.00960%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.00960%3C%2Fa%3E%3C%2Fp%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Weaving_Multiscale_Context_for_Single_Shot_Detector_389%22+target%3D%22_blank%22%3E%3C%2Fa%3EWeaving+Multi-scale+Context+for+Single+Shot+Detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+WeaveNet%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+fuse+multi-scale+information%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.03149%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.03149%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ESSD_396%22+target%3D%22_blank%22%3E%3C%2Fa%3EESSD%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Extend_the_shallow_part_of_Single_Shot_MultiBox_Detector_via_Convolutional_Neural_Network_397%22+target%3D%22_blank%22%3E%3C%2Fa%3EExtend+the+shallow+part+of+Single+Shot+MultiBox+Detector+via+Convolutional+Neural+Network%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.05918%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.05918%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca+id%3D%22Tiny_SSD_A_Tiny_Singleshot_Detection_Deep_Convolutional_Neural_Network_for_Realtime_Embedded_Object_Detection_401%22+target%3D%22_blank%22%3E%3C%2Fa%3ETiny+SSD%3A+A+Tiny+Single-shot+Detection+Deep+Convolutional+Neural+Network+for+Real-time+Embedded+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1802.06488%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1802.06488%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MDSSD_Multiscale_Deconvolutional_Single_Shot_Detector_for_small_objects_405%22+target%3D%22_blank%22%3E%3C%2Fa%3EMDSSD%3A+Multi-scale+Deconvolutional+Single+Shot+Detector+for+small+objects%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Zhengzhou+University%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.07009%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.07009%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22InsideOutside_Net_ION_411%22+target%3D%22_blank%22%3E%3C%2Fa%3EInside-Outside+Net+%28ION%29%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22InsideOutside_Net_Detecting_Objects_in_Context_with_Skip_Pooling_and_Recurrent_Neural_Networks_412%22+target%3D%22_blank%22%3E%3C%2Fa%3EInside-Outside+Net%3A+Detecting+Objects+in+Context+with+Skip+Pooling+and+Recurrent+Neural+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+%E2%80%9C0.8s+per+image+on+a+Titan+X+GPU+%28excluding+proposal+generation%29+without+two-stage+bounding-box+regression+and+1.15s+per+image+with+it%E2%80%9D.%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1512.04143%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1512.04143%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.seanbell.ca%2Ftmp%2Fion-coco-talk-bell2015.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.seanbell.ca%2Ftmp%2Fion-coco-talk-bell2015.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Ecoco-leaderboard%3A+%3Ca+href%3D%22http%3A%2F%2Fmscoco.org%2Fdataset%2F%23detections-leaderboard%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fmscoco.org%2Fdataset%2F%23detections-leaderboard%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Adaptive_Object_Detection_Using_Adjacency_and_Zoom_Prediction_419%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdaptive+Object+Detection+Using+Adjacency+and+Zoom+Prediction%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2016.+AZ-Net%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1512.07711%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1512.07711%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fluyongxi%2Faz-net%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fluyongxi%2Faz-net%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eyoutube%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYmFtuNwxaNM%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYmFtuNwxaNM%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22GCNN_an_Iterative_Grid_Based_Object_Detector_426%22+target%3D%22_blank%22%3E%3C%2Fa%3EG-CNN%3A+an+Iterative+Grid+Based+Object+Detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1512.07729%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1512.07729%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Factors_in_Finetuning_Deep_Model_for_object_detection_431%22+target%3D%22_blank%22%3E%3C%2Fa%3EFactors+in+Finetuning+Deep+Model+for+object+detection%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Factors_in_Finetuning_Deep_Model_for_Object_Detection_with_Longtail_Distribution_432%22+target%3D%22_blank%22%3E%3C%2Fa%3EFactors+in+Finetuning+Deep+Model+for+Object+Detection+with+Long-tail+Distribution%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2016.rank+3rd+for+provided+data+and+2nd+for+external+data+on+ILSVRC+2015+object+detection%3C%2Fli%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Ewlouyang%2Fprojects%2FImageNetFactors%2FCVPR16.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Ewlouyang%2Fprojects%2FImageNetFactors%2FCVPR16.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1601.05150%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1601.05150%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22We_dont_need_no_boundingboxes_Training_object_class_detectors_using_only_human_verification_438%22+target%3D%22_blank%22%3E%3C%2Fa%3EWe+don%E2%80%99t+need+no+bounding-boxes%3A+Training+object+class+detectors+using+only+human+verification%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1602.08405%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1602.08405%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22HyperNet_Towards_Accurate_Region_Proposal_Generation_and_Joint_Object_Detection_443%22+target%3D%22_blank%22%3E%3C%2Fa%3EHyperNet%3A+Towards+Accurate+Region+Proposal+Generation+and+Joint+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.00600%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.00600%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_MultiPath_Network_for_Object_Detection_448%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+MultiPath+Network+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+BMVC+2016.+Facebook+AI+Research+%28FAIR%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.02135%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.02135%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fmultipathnet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fmultipathnet%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CRAFT_455%22+target%3D%22_blank%22%3E%3C%2Fa%3ECRAFT%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CRAFT_Objects_from_Images_456%22+target%3D%22_blank%22%3E%3C%2Fa%3ECRAFT+Objects+from+Images%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2016.+Cascade+Region-proposal-network+And+FasT-rcnn.+an+extension+of+Faster+R-CNN%3C%2Fli%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fbyangderek.github.io%2Fprojects%2Fcraft.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fbyangderek.github.io%2Fprojects%2Fcraft.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1604.03239%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1604.03239%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FYang_CRAFT_Objects_From_CVPR_2016_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FYang_CRAFT_Objects_From_CVPR_2016_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbyangderek%2FCRAFT%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbyangderek%2FCRAFT%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22OHEM_465%22+target%3D%22_blank%22%3E%3C%2Fa%3EOHEM%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Training_Regionbased_Object_Detectors_with_Online_Hard_Example_Mining_466%22+target%3D%22_blank%22%3E%3C%2Fa%3ETraining+Region-based+Object+Detectors+with+Online+Hard+Example+Mining%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2016+Oral.+Online+hard+example+mining+%28OHEM%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.03540%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.03540%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FShrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FShrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Official%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fabhi2610%2Fohem%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fabhi2610%2Fohem%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eauthor+page%3A+%3Ca+href%3D%22http%3A%2F%2Fabhinav-shrivastava.info%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fabhinav-shrivastava.info%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SOHEM_Stratified_Online_Hard_Example_Mining_for_Object_Detection_474%22+target%3D%22_blank%22%3E%3C%2Fa%3ES-OHEM%3A+Stratified+Online+Hard+Example+Mining+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1705.02233%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1705.02233%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca+id%3D%22Exploit_All_the_Layers_Fast_and_Accurate_CNN_Object_Detector_with_Scale_Dependent_Pooling_and_Cascaded_Rejection_Classifiers_478%22+target%3D%22_blank%22%3E%3C%2Fa%3EExploit+All+the+Layers%3A+Fast+and+Accurate+CNN+Object+Detector+with+Scale+Dependent+Pooling+and+Cascaded+Rejection+Classifiers%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2016%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+scale-dependent+pooling+%28SDP%29%2C+cascaded+rejection+classifiers+%28CRC%29%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww-personal.umich.edu%2F%7Ewgchoi%2FSDP-CRC_camready.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww-personal.umich.edu%2F%7Ewgchoi%2FSDP-CRC_camready.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RFCN_485%22+target%3D%22_blank%22%3E%3C%2Fa%3ER-FCN%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RFCN_Object_Detection_via_Regionbased_Fully_Convolutional_Networks_486%22+target%3D%22_blank%22%3E%3C%2Fa%3ER-FCN%3A+Object+Detection+via+Region-based+Fully+Convolutional+Networks%3C%2Fh3%3E+%0A++%3Cp%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1605.06409%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1605.06409%3C%2Fa%3E%3Cbr%3E+github%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fdaijifeng001%2FR-FCN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fdaijifeng001%2FR-FCN%3C%2Fa%3E%3Cbr%3E+github%28MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmsracver%2FDeformable-ConvNets%2Ftree%2Fmaster%2Frfcn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmsracver%2FDeformable-ConvNets%2Ftree%2Fmaster%2Frfcn%3C%2Fa%3E%3Cbr%3E+github%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FOrpine%2Fpy-R-FCN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FOrpine%2Fpy-R-FCN%3C%2Fa%3E%3Cbr%3E+github%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FPureDiors%2Fpytorch_RFCN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FPureDiors%2Fpytorch_RFCN%3C%2Fa%3E%3Cbr%3E+github%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fpy-R-FCN-multiGPU%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fpy-R-FCN-multiGPU%3C%2Fa%3E%3Cbr%3E+github%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fxdever%2FRFCN-tensorflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fxdever%2FRFCN-tensorflow%3C%2Fa%3E%3C%2Fp%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RFCN3000_at_30fps_Decoupling_Detection_and_Classification_495%22+target%3D%22_blank%22%3E%3C%2Fa%3ER-FCN-3000+at+30fps%3A+Decoupling+Detection+and+Classification%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.01802%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.01802%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Recycle_deep_features_for_better_object_detection_499%22+target%3D%22_blank%22%3E%3C%2Fa%3ERecycle+deep+features+for+better+object+detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.05066%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.05066%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MSCNN_504%22+target%3D%22_blank%22%3E%3C%2Fa%3EMS-CNN%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Unified_Multiscale_Deep_Convolutional_Neural_Network_for_Fast_Object_Detection_505%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Unified+Multi-scale+Deep+Convolutional+Neural+Network+for+Fast+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2016%3C%2Fli%3E+%0A+++%3Cli%3Eintro%3A+640%C3%97480%3A+15+fps%2C+960%C3%97720%3A+8+fps%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.07155%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.07155%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fzhaoweicai%2Fmscnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fzhaoweicai%2Fmscnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eposter%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-2B-38.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-2B-38.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Multistage_Object_Detection_with_Group_Recursive_Learning_513%22+target%3D%22_blank%22%3E%3C%2Fa%3EMulti-stage+Object+Detection+with+Group+Recursive+Learning%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+VOC2007%3A+78.6%25%2C+VOC2012%3A+74.9%25%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.05159%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.05159%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Subcategoryaware_Convolutional_Neural_Networks_for_Object_Proposals_and_Detection_518%22+target%3D%22_blank%22%3E%3C%2Fa%3ESubcategory-aware+Convolutional+Neural+Networks+for+Object+Proposals+and+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+WACV+2017.+SubCNN%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.04693%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.04693%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ftanshen%2FSubCNN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ftanshen%2FSubCNN%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22PVANET_525%22+target%3D%22_blank%22%3E%3C%2Fa%3EPVANET%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22PVANet_Lightweight_Deep_Neural_Networks_for_Realtime_Object_Detection_526%22+target%3D%22_blank%22%3E%3C%2Fa%3EPVANet%3A+Lightweight+Deep+Neural+Networks+for+Real-time+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Presented+at+NIPS+2016+Workshop+on+Efficient+Methods+for+Deep+Neural+Networks+%28EMDNN%29.+Continuation+of+arXiv%3A1608.08021%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.08588%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.08588%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fsanghoon%2Fpva-faster-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fsanghoon%2Fpva-faster-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eleaderboard%28PVANet+9.0%29%3A+%3Ca+href%3D%22http%3A%2F%2Fhost.robots.ox.ac.uk%3A8080%2Fleaderboard%2Fdisplaylb.php%3Fchallengeid%3D11%26amp%3Bcompid%3D4%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fhost.robots.ox.ac.uk%3A8080%2Fleaderboard%2Fdisplaylb.php%3Fchallengeid%3D11%26amp%3Bcompid%3D4%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22GBDNet_534%22+target%3D%22_blank%22%3E%3C%2Fa%3EGBD-Net%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Gated_Bidirectional_CNN_for_Object_Detection_535%22+target%3D%22_blank%22%3E%3C%2Fa%3EGated+Bi-directional+CNN+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+The+Chinese+University+of+Hong+Kong+%26amp%3B+Sensetime+Group+Limited%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-319-46478-7_22%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-319-46478-7_22%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Emirror%3A+%3Ca+href%3D%22https%3A%2F%2Fpan.baidu.com%2Fs%2F1dFohO7v%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fpan.baidu.com%2Fs%2F1dFohO7v%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Crafting_GBDNet_for_Object_Detection_541%22+target%3D%22_blank%22%3E%3C%2Fa%3ECrafting+GBD-Net+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+winner+of+the+ImageNet+object+detection+challenge+of+2016.+CUImage+and+CUVideo%3C%2Fli%3E+%0A+++%3Cli%3Eintro%3A+gated+bi-directional+CNN+%28GBD-Net%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1610.02579%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1610.02579%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FcraftGBD%2FcraftGBD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FcraftGBD%2FcraftGBD%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22StuffNet_Using_Stuff_to_Improve_Object_Detection_549%22+target%3D%22_blank%22%3E%3C%2Fa%3EStuffNet%3A+Using+%E2%80%98Stuff%E2%80%99+to+Improve+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1610.05861%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1610.05861%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Generalized_Haar_Filter_based_Deep_Networks_for_RealTime_Object_Detection_in_Traffic_Scene_554%22+target%3D%22_blank%22%3E%3C%2Fa%3EGeneralized+Haar+Filter+based+Deep+Networks+for+Real-Time+Object+Detection+in+Traffic+Scene%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1610.09609%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1610.09609%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Hierarchical_Object_Detection_with_Deep_Reinforcement_Learning_559%22+target%3D%22_blank%22%3E%3C%2Fa%3EHierarchical+Object+Detection+with+Deep+Reinforcement+Learning%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Deep+Reinforcement+Learning+Workshop+%28NIPS+2016%29%3C%2Fli%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22https%3A%2F%2Fimatge-upc.github.io%2Fdetection-2016-nipsws%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fimatge-upc.github.io%2Fdetection-2016-nipsws%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.03718%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.03718%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.slideshare.net%2Fxavigiro%2Fhierarchical-object-detection-with-deep-reinforcement-learning%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.slideshare.net%2Fxavigiro%2Fhierarchical-object-detection-with-deep-reinforcement-learning%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fimatge-upc%2Fdetection-2016-nipsws%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fimatge-upc%2Fdetection-2016-nipsws%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eblog%3A+%3Ca+href%3D%22http%3A%2F%2Fjorditorres.org%2Fnips%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fjorditorres.org%2Fnips%2F%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_to_detect_and_localize_many_objects_from_few_examples_569%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+to+detect+and+localize+many+objects+from+few+examples%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.05664%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.05664%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Speedaccuracy_tradeoffs_for_modern_convolutional_object_detectors_574%22+target%3D%22_blank%22%3E%3C%2Fa%3ESpeed%2Faccuracy+trade-offs+for+modern+convolutional+object+detectors%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2017.+Google+Research%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.10012%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.10012%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca+id%3D%22SqueezeDet_Unified_Small_Low_Power_Fully_Convolutional_Neural_Networks_for_RealTime_Object_Detection_for_Autonomous_Driving_580%22+target%3D%22_blank%22%3E%3C%2Fa%3ESqueezeDet%3A+Unified%2C+Small%2C+Low+Power+Fully+Convolutional+Neural+Networks+for+Real-Time+Object+Detection+for+Autonomous+Driving%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.01051%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.01051%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBichenWuUCB%2FsqueezeDet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBichenWuUCB%2FsqueezeDet%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ffregu856%2F2D_detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ffregu856%2F2D_detection%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Feature_Pyramid_Network_FPN_587%22+target%3D%22_blank%22%3E%3C%2Fa%3EFeature+Pyramid+Network+%28FPN%29%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Feature_Pyramid_Networks_for_Object_Detection_588%22+target%3D%22_blank%22%3E%3C%2Fa%3EFeature+Pyramid+Networks+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Facebook+AI+Research%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.03144%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.03144%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ActionDriven_Object_Detection_with_TopDown_Visual_Attentions_594%22+target%3D%22_blank%22%3E%3C%2Fa%3EAction-Driven+Object+Detection+with+Top-Down+Visual+Attentions%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.06704%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.06704%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Beyond_Skip_Connections_TopDown_Modulation_for_Object_Detection_598%22+target%3D%22_blank%22%3E%3C%2Fa%3EBeyond+Skip+Connections%3A+Top-Down+Modulation+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CMU+%26amp%3B+UC+Berkeley+%26amp%3B+Google+Research%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.06851%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.06851%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22WideResidualInception_Networks_for_Realtime_Object_Detection_603%22+target%3D%22_blank%22%3E%3C%2Fa%3EWide-Residual-Inception+Networks+for+Real-time+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Inha+University%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.01243%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.01243%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Attentional_Network_for_Visual_Object_Detection_608%22+target%3D%22_blank%22%3E%3C%2Fa%3EAttentional+Network+for+Visual+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+University+of+Maryland+%26amp%3B+Mitsubishi+Electric+Research+Laboratories%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.01478%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.01478%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Chained_Deep_Features_and_Classifiers_for_Cascade_in_Object_Detection_613%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Chained+Deep+Features+and+Classifiers+for+Cascade+in+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Ekeykwords%3A+CC-Net%3C%2Fli%3E+%0A+++%3Cli%3Eintro%3A+chained+cascade+network+%28CC-Net%29.+81.1%25+mAP+on+PASCAL+VOC+2007%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.07054%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.07054%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeNet_Scalable_Realtime_Object_Detection_with_Directed_Sparse_Sampling_620%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeNet%3A+Scalable+Real-time+Object+Detection+with+Directed+Sparse+Sampling%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2017+%28poster%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1703.10295%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1703.10295%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Discriminative_Bimodal_Networks_for_Visual_Localization_and_Detection_with_Natural_Language_Queries_626%22+target%3D%22_blank%22%3E%3C%2Fa%3EDiscriminative+Bimodal+Networks+for+Visual+Localization+and+Detection+with+Natural+Language+Queries%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2017%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.03944%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.03944%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Spatial_Memory_for_Context_Reasoning_in_Object_Detection_632%22+target%3D%22_blank%22%3E%3C%2Fa%3ESpatial+Memory+for+Context+Reasoning+in+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.04224%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.04224%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Accurate_Single_Stage_Detector_Using_Recurrent_Rolling_Convolution_637%22+target%3D%22_blank%22%3E%3C%2Fa%3EAccurate+Single+Stage+Detector+Using+Recurrent+Rolling+Convolution%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2017.+SenseTime%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+Recurrent+Rolling+Convolution+%28RRC%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.05776%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.05776%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FxiaohaoChen%2Frrc_detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FxiaohaoChen%2Frrc_detection%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Occlusion_Reasoning_for_MultiCamera_MultiTarget_Detection_645%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Occlusion+Reasoning+for+Multi-Camera+Multi-Target+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.05775%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.05775%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LCDet_LowComplexity_FullyConvolutional_Neural_Networks_for_Object_Detection_in_Embedded_Systems_649%22+target%3D%22_blank%22%3E%3C%2Fa%3ELCDet%3A+Low-Complexity+Fully-Convolutional+Neural+Networks+for+Object+Detection+in+Embedded+Systems%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Embedded+Vision+Workshop+in+CVPR.+UC+San+Diego+%26amp%3B+Qualcomm+Inc%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1705.05922%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1705.05922%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Point_Linking_Network_for_Object_Detection_655%22+target%3D%22_blank%22%3E%3C%2Fa%3EPoint+Linking+Network+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Point+Linking+Network+%28PLN%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.03646%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.03646%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Perceptual_Generative_Adversarial_Networks_for_Small_Object_Detection_661%22+target%3D%22_blank%22%3E%3C%2Fa%3EPerceptual+Generative+Adversarial+Networks+for+Small+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.05274%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.05274%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fewshot_Object_Detection_665%22+target%3D%22_blank%22%3E%3C%2Fa%3EFew-shot+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.08249%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.08249%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22YesNet_An_effective_Detector_Based_on_Global_Information_669%22+target%3D%22_blank%22%3E%3C%2Fa%3EYes-Net%3A+An+effective+Detector+Based+on+Global+Information%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.09180%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.09180%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SMC_Faster_RCNN_Toward_a_scenespecialized_multiobject_detector_673%22+target%3D%22_blank%22%3E%3C%2Fa%3ESMC+Faster+R-CNN%3A+Toward+a+scene-specialized+multi-object+detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.10217%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.10217%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Towards_lightweight_convolutional_neural_networks_for_object_detection_677%22+target%3D%22_blank%22%3E%3C%2Fa%3ETowards+lightweight+convolutional+neural+networks+for+object+detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.01395%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.01395%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22RON_Reverse_Connection_with_Objectness_Prior_Networks_for_Object_Detection_681%22+target%3D%22_blank%22%3E%3C%2Fa%3ERON%3A+Reverse+Connection+with+Objectness+Prior+Networks+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2017%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.01691%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.01691%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ftaokong%2FRON%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ftaokong%2FRON%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Mimicking_Very_Efficient_Network_for_Object_Detection_688%22+target%3D%22_blank%22%3E%3C%2Fa%3EMimicking+Very+Efficient+Network+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2017.+SenseTime+%26amp%3B+Beihang+University%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_cvpr_2017%2Fpapers%2FLi_Mimicking_Very_Efficient_CVPR_2017_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_cvpr_2017%2Fpapers%2FLi_Mimicking_Very_Efficient_CVPR_2017_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Residual_Features_and_Unified_Prediction_Network_for_Single_Stage_Detection_693%22+target%3D%22_blank%22%3E%3C%2Fa%3EResidual+Features+and+Unified+Prediction+Network+for+Single+Stage+Detection%3C%2Fh3%3E+%0A++%3Cp%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.05031%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.05031%3C%2Fa%3E%3C%2Fp%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deformable_Partbased_Fully_Convolutional_Network_for_Object_Detection_697%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeformable+Part-based+Fully+Convolutional+Network+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+BMVC+2017+%28oral%29.+Sorbonne+Universit%C3%A9s+%26amp%3B+CEDRIC%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.06175%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.06175%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Adaptive_Feeding_Achieving_Fast_and_Accurate_Detections_by_Adaptively_Combining_Object_Detectors_702%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdaptive+Feeding%3A+Achieving+Fast+and+Accurate+Detections+by+Adaptively+Combining+Object+Detectors%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2017%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.06399%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.06399%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Recurrent_Scale_Approximation_for_Object_Detection_in_CNN_707%22+target%3D%22_blank%22%3E%3C%2Fa%3ERecurrent+Scale+Approximation+for+Object+Detection+in+CNN%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2017%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+Recurrent+Scale+Approximation+%28RSA%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.09531%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.09531%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fsciencefans%2FRSA-for-object-detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fsciencefans%2FRSA-for-object-detection%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DSOD_715%22+target%3D%22_blank%22%3E%3C%2Fa%3EDSOD%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DSOD_Learning_Deeply_Supervised_Object_Detectors_from_Scratch_716%22+target%3D%22_blank%22%3E%3C%2Fa%3EDSOD%3A+Learning+Deeply+Supervised+Object+Detectors+from+Scratch%3C%2Fh3%3E+%0A++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F201808211443428%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2017.+Fudan+University+%26amp%3B+Tsinghua+University+%26amp%3B+Intel+Labs+China%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.01241%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.01241%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fszq0214%2FDSOD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fszq0214%2FDSOD%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_from_Scratch_with_Deep_Supervision_722%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+from+Scratch+with+Deep+Supervision%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.09294%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.09294%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Cp%3E%23%23RetinaNet%3C%2Fp%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Focal_Loss_for_Dense_Object_Detection_727%22+target%3D%22_blank%22%3E%3C%2Fa%3EFocal+Loss+for+Dense+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2017+Best+student+paper+award.+Facebook+AI+Research%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+RetinaNet%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.02002%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.02002%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Focal_Loss_Dense_Detector_for_Vehicle_Surveillance_733%22+target%3D%22_blank%22%3E%3C%2Fa%3EFocal+Loss+Dense+Detector+for+Vehicle+Surveillance%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.01114%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.01114%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CoupleNet_Coupling_Global_Structure_with_Local_Parts_for_Object_Detection_737%22+target%3D%22_blank%22%3E%3C%2Fa%3ECoupleNet%3A+Coupling+Global+Structure+with+Local+Parts+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2017%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.02863%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.02863%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Incremental_Learning_of_Object_Detectors_without_Catastrophic_Forgetting_742%22+target%3D%22_blank%22%3E%3C%2Fa%3EIncremental+Learning+of+Object+Detectors+without+Catastrophic+Forgetting%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2017.+Inria%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.06977%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.06977%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Zoom_OutandIn_Network_with_Map_Attention_Decision_for_Region_Proposal_and_Object_Detection_747%22+target%3D%22_blank%22%3E%3C%2Fa%3EZoom+Out-and-In+Network+with+Map+Attention+Decision+for+Region+Proposal+and+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1709.04347%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1709.04347%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22StairNet_TopDown_Semantic_Aggregation_for_Accurate_One_Shot_Detection_751%22+target%3D%22_blank%22%3E%3C%2Fa%3EStairNet%3A+Top-Down+Semantic+Aggregation+for+Accurate+One+Shot+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1709.05788%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1709.05788%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Dynamic_Zoomin_Network_for_Fast_Object_Detection_in_Large_Images_755%22+target%3D%22_blank%22%3E%3C%2Fa%3EDynamic+Zoom-in+Network+for+Fast+Object+Detection+in+Large+Images%3C%2Fh3%3E+%0A++%3Cp%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.05187%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.05187%3C%2Fa%3E%3C%2Fp%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroAnnotation_Object_Detection_with_Web_Knowledge_Transfer_759%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Annotation+Object+Detection+with+Web+Knowledge+Transfer%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+NTU%2C+Singapore+%26amp%3B+Amazon%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+multi-instance+multi-label+domain+adaption+learning+framework%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.05954%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.05954%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MegDet_767%22+target%3D%22_blank%22%3E%3C%2Fa%3EMegDet%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MegDet_A_Large_MiniBatch_Object_Detector_768%22+target%3D%22_blank%22%3E%3C%2Fa%3EMegDet%3A+A+Large+Mini-Batch+Object+Detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Peking+University+%26amp%3B+Tsinghua+University+%26amp%3B+Megvii+Inc%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.07240%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.07240%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SingleShot_Refinement_Neural_Network_for_Object_Detection_773%22+target%3D%22_blank%22%3E%3C%2Fa%3ESingle-Shot+Refinement+Neural+Network+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.06897%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.06897%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fsfzhang15%2FRefineDet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fsfzhang15%2FRefineDet%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FMTCloudVision%2FRefineDet-Mxnet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FMTCloudVision%2FRefineDet-Mxnet%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Receptive_Field_Block_Net_for_Accurate_and_Fast_Object_Detection_779%22+target%3D%22_blank%22%3E%3C%2Fa%3EReceptive+Field+Block+Net+for+Accurate+and+Fast+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+RFBNet%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.07767%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.07767%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2F%2Fruinmessi%2FRFBNet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2F%2Fruinmessi%2FRFBNet%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22An_Analysis_of_Scale_Invariance_in_Object_Detection__SNIP_785%22+target%3D%22_blank%22%3E%3C%2Fa%3EAn+Analysis+of+Scale+Invariance+in+Object+Detection+-+SNIP%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.08189%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.08189%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fsnip%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fsnip%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Feature_Selective_Networks_for_Object_Detection_791%22+target%3D%22_blank%22%3E%3C%2Fa%3EFeature+Selective+Networks+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.08879%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.08879%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_a_Rotation_Invariant_Detector_with_Rotatable_Bounding_Box_795%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+a+Rotation+Invariant+Detector+with+Rotatable+Bounding+Box%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.09405%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.09405%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28official%2C+Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fliulei01%2FDRBox%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fliulei01%2FDRBox%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Scalable_Object_Detection_for_Stylized_Objects_800%22+target%3D%22_blank%22%3E%3C%2Fa%3EScalable+Object+Detection+for+Stylized+Objects%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Microsoft+AI+%26amp%3B+Research+Munich%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.09822%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.09822%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Object_Detectors_from_Scratch_with_Gated_Recurrent_Feature_Pyramids_805%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Object+Detectors+from+Scratch+with+Gated+Recurrent+Feature+Pyramids%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.00886%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.00886%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fszq0214%2FGRP-DSOD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fszq0214%2FGRP-DSOD%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Regionlets_for_Object_Detection_810%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Regionlets+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Ekeywords%3A+region+selection+network%2C+gating+network%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.02408%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.02408%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Training_and_Testing_Object_Detectors_with_Virtual_Images_815%22+target%3D%22_blank%22%3E%3C%2Fa%3ETraining+and+Testing+Object+Detectors+with+Virtual+Images%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+IEEE%2FCAA+Journal+of+Automatica+Sinica%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.08470%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.08470%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LargeScale_Object_Discovery_and_Detector_Adaptation_from_Unlabeled_Video_821%22+target%3D%22_blank%22%3E%3C%2Fa%3ELarge-Scale+Object+Discovery+and+Detector+Adaptation+from+Unlabeled+Video%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Ekeywords%3A+object+mining%2C+object+tracking%2C+unsupervised+object+discovery+by+appearance-based+clustering%2C+self-supervised+detector+adaptation%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.08832%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.08832%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Spot_the_Difference_by_Object_Detection_827%22+target%3D%22_blank%22%3E%3C%2Fa%3ESpot+the+Difference+by+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Tsinghua+University+%26amp%3B+JD+Group%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.01051%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.01051%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LocalizationAware_Active_Learning_for_Object_Detection_832%22+target%3D%22_blank%22%3E%3C%2Fa%3ELocalization-Aware+Active+Learning+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.05124%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.05124%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_with_Maskbased_Feature_Encoding_837%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+with+Mask-based+Feature+Encoding%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1802.03934%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1802.03934%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22LSTD_A_LowShot_Transfer_Detector_for_Object_Detection_841%22+target%3D%22_blank%22%3E%3C%2Fa%3ELSTD%3A+A+Low-Shot+Transfer+Detector+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+AAAI+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.01529%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.01529%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Domain_Adaptive_Faster_RCNN_for_Object_Detection_in_the_Wild_846%22+target%3D%22_blank%22%3E%3C%2Fa%3EDomain+Adaptive+Faster+R-CNN+for+Object+Detection+in+the+Wild%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2018.+ETH+Zurich+%26amp%3B+ESAT%2FPSI%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.03243%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.03243%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28official.+Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fyuhuayc%2Fda-faster-rcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fyuhuayc%2Fda-faster-rcnn%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Pseudo_Mask_Augmented_Object_Detection_852%22+target%3D%22_blank%22%3E%3C%2Fa%3EPseudo+Mask+Augmented+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.05858%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.05858%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Revisiting_RCNN_On_Awakening_the_Classification_Power_of_Faster_RCNN_856%22+target%3D%22_blank%22%3E%3C%2Fa%3ERevisiting+RCNN%3A+On+Awakening+the+Classification+Power+of+Faster+RCNN%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+DCR+V1%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.06799%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.06799%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28official%2C+MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbowenc0221%2FDecoupled-Classification-Refinement%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbowenc0221%2FDecoupled-Classification-Refinement%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Decoupled_Classification_Refinement_Hard_False_Positive_Suppression_for_Object_Detection_862%22+target%3D%22_blank%22%3E%3C%2Fa%3EDecoupled+Classification+Refinement%3A+Hard+False+Positive+Suppression+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Ekeywords%3A+DCR+V2%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1810.04002%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1810.04002%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28official%2C+MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbowenc0221%2FDecoupled-Classification-Refinement%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbowenc0221%2FDecoupled-Classification-Refinement%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Region_Features_for_Object_Detection_867%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Region+Features+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Peking+University+%26amp%3B+MSRA%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.07066%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.07066%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SingleShot_Bidirectional_Pyramid_Networks_for_HighQuality_Object_Detection_872%22+target%3D%22_blank%22%3E%3C%2Fa%3ESingle-Shot+Bidirectional+Pyramid+Networks+for+High-Quality+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Singapore+Management+University+%26amp%3B+Zhejiang+University%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.08208%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.08208%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_for_Comics_using_Manga109_Annotations_877%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+for+Comics+using+Manga109+Annotations%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+University+of+Tokyo+%26amp%3B+National+Institute+of+Informatics%2C+Japan%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.08670%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.08670%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22TaskDriven_Super_Resolution_Object_Detection_in_Lowresolution_Images_882%22+target%3D%22_blank%22%3E%3C%2Fa%3ETask-Driven+Super+Resolution%3A+Object+Detection+in+Low-resolution+Images%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.11316%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.11316%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Transferring_CommonSense_Knowledge_for_Object_Detection_886%22+target%3D%22_blank%22%3E%3C%2Fa%3ETransferring+Common-Sense+Knowledge+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.01077%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.01077%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Multiscale_Locationaware_Kernel_Representation_for_Object_Detection_890%22+target%3D%22_blank%22%3E%3C%2Fa%3EMulti-scale+Location-aware+Kernel+Representation+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.00428%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.00428%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FHwang64%2FMLKP%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FHwang64%2FMLKP%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Loss_Rank_Mining_A_General_Hard_Example_Mining_Method_for_Realtime_Detectors_896%22+target%3D%22_blank%22%3E%3C%2Fa%3ELoss+Rank+Mining%3A+A+General+Hard+Example+Mining+Method+for+Real-time+Detectors%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+National+University+of+Defense+Technology%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.04606%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.04606%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DetNet_A_Backbone_network_for_Object_Detection_901%22+target%3D%22_blank%22%3E%3C%2Fa%3EDetNet%3A+A+Backbone+network+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Tsinghua+University+%26amp%3B+Megvii+Inc%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.06215%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.06215%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Robust_Physical_Adversarial_Attack_on_Faster_RCNN_Object_Detector_906%22+target%3D%22_blank%22%3E%3C%2Fa%3ERobust+Physical+Adversarial+Attack+on+Faster+R-CNN+Object+Detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.05810%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.05810%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22AdvDetPatch_Attacking_Object_Detectors_with_Adversarial_Patches_910%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdvDetPatch%3A+Attacking+Object+Detectors+with+Adversarial+Patches%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1806.02299%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1806.02299%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Attacking_Object_Detectors_via_Imperceptible_Patches_on_Background_914%22+target%3D%22_blank%22%3E%3C%2Fa%3EAttacking+Object+Detectors+via+Imperceptible+Patches+on+Background%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.05966%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.05966%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Physical_Adversarial_Examples_for_Object_Detectors_918%22+target%3D%22_blank%22%3E%3C%2Fa%3EPhysical+Adversarial+Examples+for+Object+Detectors%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+WOOT+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.07769%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.07769%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Quantization_Mimic_Towards_Very_Tiny_CNN_for_Object_Detection_923%22+target%3D%22_blank%22%3E%3C%2Fa%3EQuantization+Mimic%3A+Towards+Very+Tiny+CNN+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.02152%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.02152%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_detection_at_200_Frames_Per_Second_927%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+detection+at+200+Frames+Per+Second%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+United+Technologies+Research+Center-Ireland%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.06361%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.06361%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca+id%3D%22Object_Detection_using_Domain_Randomization_and_Generative_Adversarial_Refinement_of_Synthetic_Images_932%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+using+Domain+Randomization+and+Generative+Adversarial+Refinement+of+Synthetic+Images%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2018+Deep+Vision+Workshop%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.11778%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.11778%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SNIPER_Efficient_MultiScale_Training_937%22+target%3D%22_blank%22%3E%3C%2Fa%3ESNIPER%3A+Efficient+Multi-Scale+Training%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.09300%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.09300%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmahyarnajibi%2FSNIPER%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmahyarnajibi%2FSNIPER%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Soft_Sampling_for_Robust_Object_Detection_942%22+target%3D%22_blank%22%3E%3C%2Fa%3ESoft+Sampling+for+Robust+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1806.06986%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1806.06986%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MetaAnchor_Learning_to_Detect_Objects_with_Customized_Anchors_946%22+target%3D%22_blank%22%3E%3C%2Fa%3EMetaAnchor%3A+Learning+to+Detect+Objects+with+Customized+Anchors%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Megvii+Inc+%28Face%2B%2B%29+%26amp%3B+Fudan+University%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.00980%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.00980%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Localization_Recall_Precision_LRP_A_New_Performance_Metric_for_Object_Detection_951%22+target%3D%22_blank%22%3E%3C%2Fa%3ELocalization+Recall+Precision+%28LRP%29%3A+A+New+Performance+Metric+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2018.+Middle+East+Technical+University%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.01696%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.01696%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fcancam%2FLRP%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fcancam%2FLRP%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22AutoContext_RCNN_957%22+target%3D%22_blank%22%3E%3C%2Fa%3EAuto-Context+R-CNN%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Rejected+by+ECCV18%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.02842%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.02842%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Pooling_Pyramid_Network_for_Object_Detection_962%22+target%3D%22_blank%22%3E%3C%2Fa%3EPooling+Pyramid+Network+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Google+AI+Perception%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.03284%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.03284%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Modeling_Visual_Context_is_Key_to_Augmenting_Object_Detection_Datasets_967%22+target%3D%22_blank%22%3E%3C%2Fa%3EModeling+Visual+Context+is+Key+to+Augmenting+Object+Detection+Datasets%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.07428%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.07428%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Dual_Refinement_Network_for_SingleShot_Object_Detection_972%22+target%3D%22_blank%22%3E%3C%2Fa%3EDual+Refinement+Network+for+Single-Shot+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.08638%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.08638%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Acquisition_of_Localization_Confidence_for_Accurate_Object_Detection_976%22+target%3D%22_blank%22%3E%3C%2Fa%3EAcquisition+of+Localization+Confidence+for+Accurate+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.11590%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.11590%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egihtub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fvacancy%2FPreciseRoIPooling%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fvacancy%2FPreciseRoIPooling%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CornerNet_Detecting_Objects_as_Paired_Keypoints_982%22+target%3D%22_blank%22%3E%3C%2Fa%3ECornerNet%3A+Detecting+Objects+as+Paired+Keypoints%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+IoU-Net%2C+PreciseRoIPooling%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1808.01244%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1808.01244%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fumich-vl%2FCornerNet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fumich-vl%2FCornerNet%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Unsupervised_Hard_Example_Mining_from_Videos_for_Improved_Object_Detection_989%22+target%3D%22_blank%22%3E%3C%2Fa%3EUnsupervised+Hard+Example+Mining+from+Videos+for+Improved+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1808.04285%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1808.04285%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SAN_Learning_Relationship_between_Convolutional_Features_for_MultiScale_Object_Detection_995%22+target%3D%22_blank%22%3E%3C%2Fa%3ESAN%3A+Learning+Relationship+between+Convolutional+Features+for+Multi-Scale+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1808.04974%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1808.04974%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Survey_of_Modern_Object_Detection_Literature_using_Deep_Learning_999%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Survey+of+Modern+Object+Detection+Literature+using+Deep+Learning%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1808.07256%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1808.07256%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22TinyDSOD_Lightweight_Object_Detection_for_ResourceRestricted_Usages_1003%22+target%3D%22_blank%22%3E%3C%2Fa%3ETiny-DSOD%3A+Lightweight+Object+Detection+for+Resource-Restricted+Usages%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+BMVC+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.11013%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.11013%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Flyxok1%2FTiny-DSOD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Flyxok1%2FTiny-DSOD%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Feature_Pyramid_Reconfiguration_for_Object_Detection_1008%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Feature+Pyramid+Reconfiguration+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1808.07993%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1808.07993%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MDCN_MultiScale_Deep_Inception_Convolutional_Neural_Networks_for_Efficient_Object_Detection_1012%22+target%3D%22_blank%22%3E%3C%2Fa%3EMDCN%3A+Multi-Scale%2C+Deep+Inception+Convolutional+Neural+Networks+for+Efficient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICPR+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.01791%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.01791%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Recent_Advances_in_Object_Detection_in_the_Age_of_Deep_Convolutional_Neural_Networks_1016%22+target%3D%22_blank%22%3E%3C%2Fa%3ERecent+Advances+in+Object+Detection+in+the+Age+of+Deep+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.03193%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.03193%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Learning_for_Generic_Object_Detection_A_Survey_1019%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Learning+for+Generic+Object+Detection%3A+A+Survey%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.02165%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.02165%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Training_ConfidenceCalibrated_Classifier_for_Detecting_OutofDistribution_Samples_1022%22+target%3D%22_blank%22%3E%3C%2Fa%3ETraining+Confidence-Calibrated+Classifier+for+Detecting+Out-of-Distribution+Samples%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICLR+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Falinlab%2FConfident_classifier%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Falinlab%2FConfident_classifier%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ScratchDetExploring_to_Train_SingleShot_Object_Detectors_from_Scratch_1026%22+target%3D%22_blank%22%3E%3C%2Fa%3EScratchDet%3AExploring+to+Train+Single-Shot+Object+Detectors+from+Scratch%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1810.08425%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1810.08425%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FKimSoybean%2FScratchDet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FKimSoybean%2FScratchDet%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fast_and_accurate_object_detection_in_high_resolution_4K_and_8K_video_using_GPUs_1030%22+target%3D%22_blank%22%3E%3C%2Fa%3EFast+and+accurate+object+detection+in+high+resolution+4K+and+8K+video+using+GPUs%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Best+Paper+Finalist+at+IEEE+High+Performance+Extreme+Computing+Conference+%28HPEC%29+2018%3C%2Fli%3E+%0A+++%3Cli%3Eintro%3A+Carnegie+Mellon+University%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1810.10551%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1810.10551%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Hybrid_Knowledge_Routed_Modules_for_Largescale_Object_Detection_1035%22+target%3D%22_blank%22%3E%3C%2Fa%3EHybrid+Knowledge+Routed+Modules+for+Large-scale+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+NIPS+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1810.12681%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1810.12681%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28official%2C+PyTorch%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fchanyn%2FHKRM%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fchanyn%2FHKRM%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Gradient_Harmonized_Singlestage_Detector_1040%22+target%3D%22_blank%22%3E%3C%2Fa%3EGradient+Harmonized+Single-stage+Detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+AAAI+2019%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.05181%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.05181%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22M2Det_A_SingleShot_Object_Detector_based_on_MultiLevel_Feature_Pyramid_Network_1044%22+target%3D%22_blank%22%3E%3C%2Fa%3EM2Det%3A+A+Single-Shot+Object+Detector+based+on+Multi-Level+Feature+Pyramid+Network%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+AAAI+2019%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.04533%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.04533%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fqijiezhao%2FM2Det%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fqijiezhao%2FM2Det%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22BAN_Focusing_on_Boundary_Context_for_Object_Detection_1049%22+target%3D%22_blank%22%3E%3C%2Fa%3EBAN%3A+Focusing+on+Boundary+Context+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.05243%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.05243%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Multilayer_Pruning_Framework_for_Compressing_Single_Shot_MultiBox_Detector_1052%22+target%3D%22_blank%22%3E%3C%2Fa%3EMulti-layer+Pruning+Framework+for+Compressing+Single+Shot+MultiBox+Detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+WACV+2019%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.08342%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.08342%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22R2CNN_MultiDimensional_Attention_Based_Rotation_Invariant_Detector_with_Robust_Anchor_Strategy_1056%22+target%3D%22_blank%22%3E%3C%2Fa%3ER2CNN%2B%2B%3A+Multi-Dimensional+Attention+Based+Rotation+Invariant+Detector+with+Robust+Anchor+Strategy%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.07126%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.07126%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FDetectionTeamUCAS%2FR2CNN-Plus-Plus_Tensorflow%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FDetectionTeamUCAS%2FR2CNN-Plus-Plus_Tensorflow%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeRPN_Taking_a_further_step_toward_more_general_object_detection_1060%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeRPN%3A+Taking+a+further+step+toward+more+general+object+detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+AAAI+2019%3C%2Fli%3E+%0A+++%3Cli%3Eintro%3A+South+China+University+of+Technology%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.06700%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.06700%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FHCIILAB%2FDeRPN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FHCIILAB%2FDeRPN%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fast_Efficient_Object_Detection_Using_Selective_Attention_1066%22+target%3D%22_blank%22%3E%3C%2Fa%3EFast+Efficient+Object+Detection+Using+Selective+Attention%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.07502%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.07502%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Sampling_Techniques_for_LargeScale_Object_Detection_from_Sparsely_Annotated_Objects_1070%22+target%3D%22_blank%22%3E%3C%2Fa%3ESampling+Techniques+for+Large-Scale+Object+Detection+from+Sparsely+Annotated+Objects%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.10862%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.10862%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22NonMaximum_Suppression_NMS_1074%22+target%3D%22_blank%22%3E%3C%2Fa%3ENon-Maximum+Suppression+%28NMS%29%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca+id%3D%22EndtoEnd_Integration_of_a_Convolutional_Network_Deformable_Parts_Model_and_NonMaximum_Suppression_1075%22+target%3D%22_blank%22%3E%3C%2Fa%3EEnd-to-End+Integration+of+a+Convolutional+Network%2C+Deformable+Parts+Model+and+Non-Maximum+Suppression%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2015%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1411.5309%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1411.5309%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2015%2Fpapers%2FWan_End-to-End_Integration_of_2015_CVPR_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2015%2Fpapers%2FWan_End-to-End_Integration_of_2015_CVPR_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_convnet_for_nonmaximum_suppression_1081%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+convnet+for+non-maximum+suppression%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1511.06437%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1511.06437%3C%2Fa%3E%3Cbr%3E+Improving+Object+Detection+With+One+Line+of+Code%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SoftNMS__Improving_Object_Detection_With_One_Line_of_Code_1086%22+target%3D%22_blank%22%3E%3C%2Fa%3ESoft-NMS+%E2%80%93+Improving+Object+Detection+With+One+Line+of+Code%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2017.+University+of+Maryland%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+Soft-NMS%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.04503%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.04503%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fsoft-nms%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fbharatsingh430%2Fsoft-nms%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_nonmaximum_suppression_1093%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+non-maximum+suppression%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2017%3C%2Fli%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.mpi-inf.mpg.de%2Fdepartments%2Fcomputer-vision-and-multimodal-computing%2Fresearch%2Fobject-recognition-and-scene-understanding%2Flearning-nms%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.mpi-inf.mpg.de%2Fdepartments%2Fcomputer-vision-and-multimodal-computing%2Fresearch%2Fobject-recognition-and-scene-understanding%2Flearning-nms%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1705.02950%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1705.02950%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fhosang%2Fgossipnet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fhosang%2Fgossipnet%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Relation_Networks_for_Object_Detection_1101%22+target%3D%22_blank%22%3E%3C%2Fa%3ERelation+Networks+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2018+oral%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.11575%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.11575%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28official%2C+MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmsracver%2FRelation-Networks-for-Object-Detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmsracver%2FRelation-Networks-for-Object-Detection%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Adversarial_Examples_1108%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdversarial+Examples%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Adversarial_Examples_that_Fool_Detectors_1109%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdversarial+Examples+that+Fool+Detectors%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+University+of+Illinois%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.02494%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.02494%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Adversarial_Examples_Are_Not_Easily_Detected_Bypassing_Ten_Detection_Methods_1114%22+target%3D%22_blank%22%3E%3C%2Fa%3EAdversarial+Examples+Are+Not+Easily+Detected%3A+Bypassing+Ten+Detection+Methods%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fnicholas.carlini.com%2Fcode%2Fnn_breaking_detection%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fnicholas.carlini.com%2Fcode%2Fnn_breaking_detection%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1705.07263%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1705.07263%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fcarlini%2Fnn_breaking_detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fcarlini%2Fnn_breaking_detection%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Weakly_Supervised_Object_Detection_1121%22+target%3D%22_blank%22%3E%3C%2Fa%3EWeakly+Supervised+Object+Detection%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca+id%3D%22Track_and_Transfer_Watching_Videos_to_Simulate_Strong_Human_Supervision_for_WeaklySupervised_Object_Detection_1122%22+target%3D%22_blank%22%3E%3C%2Fa%3ETrack+and+Transfer%3A+Watching+Videos+to+Simulate+Strong+Human+Supervision+for+Weakly-Supervised+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2016%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.05766%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.05766%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Weakly_supervised_object_detection_using_pseudostrong_labels_1127%22+target%3D%22_blank%22%3E%3C%2Fa%3EWeakly+supervised+object+detection+using+pseudo-strong+labels%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.04731%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.04731%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Saliency_Guided_EndtoEnd_Learning_for_Weakly_Supervised_Object_Detection_1131%22+target%3D%22_blank%22%3E%3C%2Fa%3ESaliency+Guided+End-to-End+Learning+for+Weakly+Supervised+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+IJCAI+2017%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.06768%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.06768%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_and_Semantic_Knowledge_Transfer_for_Large_Scale_Semisupervised_Object_Detection_1136%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+and+Semantic+Knowledge+Transfer+for+Large+Scale+Semi-supervised+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+TPAMI+2017.+National+Institutes+of+Health+%28NIH%29+Clinical+Center%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.03145%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.03145%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Video_Object_Detection_1142%22+target%3D%22_blank%22%3E%3C%2Fa%3EVideo+Object+Detection%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Object_Class_Detectors_from_Weakly_Annotated_Video_1143%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Object+Class+Detectors+from+Weakly+Annotated+Video%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2012%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.vision.ee.ethz.ch%2Fpublications%2Fpapers%2Fproceedings%2Feth_biwi_00905.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.vision.ee.ethz.ch%2Fpublications%2Fpapers%2Fproceedings%2Feth_biwi_00905.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Analysing_domain_shift_factors_between_videos_and_images_for_object_detection_1148%22+target%3D%22_blank%22%3E%3C%2Fa%3EAnalysing+domain+shift+factors+between+videos+and+images+for+object+detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1501.01186%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1501.01186%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Video_Object_Recognition_1152%22+target%3D%22_blank%22%3E%3C%2Fa%3EVideo+Object+Recognition%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fvision.princeton.edu%2Fcourses%2FCOS598%2F2015sp%2Fslides%2FVideoRecog%2FVideo%2520Object%2520Recognition.pptx%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fvision.princeton.edu%2Fcourses%2FCOS598%2F2015sp%2Fslides%2FVideoRecog%2FVideo+Object+Recognition.pptx%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Learning_for_Saliency_Prediction_in_Natural_Video_1156%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Learning+for+Saliency+Prediction+in+Natural+Video%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Submitted+on+12+Jan+2016%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+Deep+learning%2C+saliency+map%2C+optical+flow%2C+convolution+network%2C+contrast+features%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fhal.archives-ouvertes.fr%2Fhal-01251614%2Fdocument%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fhal.archives-ouvertes.fr%2Fhal-01251614%2Fdocument%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22TCNN_Tubelets_with_Convolutional_Neural_Networks_for_Object_Detection_from_Videos_1162%22+target%3D%22_blank%22%3E%3C%2Fa%3ET-CNN%3A+Tubelets+with+Convolutional+Neural+Networks+for+Object+Detection+from+Videos%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Winning+solution+in+ILSVRC2015+Object+Detection+from+Video%28VID%29+Task%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.02532%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.02532%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmyfavouritekk%2FT-CNN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmyfavouritekk%2FT-CNN%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_from_Video_Tubelets_with_Convolutional_Neural_Networks_1168%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+from+Video+Tubelets+with+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2016+Spotlight+paper%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1604.04053%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1604.04053%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Ewlouyang%2FPapers%2FKangVideoDet_CVPR16.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Ewlouyang%2FPapers%2FKangVideoDet_CVPR16.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egihtub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fmyfavouritekk%2Fvdetlib%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fmyfavouritekk%2Fvdetlib%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_in_Videos_with_Tubelets_and_Multicontext_Cues_1175%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+in+Videos+with+Tubelets+and+Multi-context+Cues%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+SenseTime+Group%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Exgwang%2FCUvideo.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.ee.cuhk.edu.hk%2F%7Exgwang%2FCUvideo.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks%2FObject%2520Detection%2520in%2520Videos%2520with%2520Tubelets%2520and%2520Multi-context%2520Cues%2520-%2520Final.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks%2FObject+Detection+in+Videos+with+Tubelets+and+Multi-context+Cues+-+Final.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Context_Matters_Refining_Object_Detection_in_Video_with_Recurrent_Neural_Networks_1181%22+target%3D%22_blank%22%3E%3C%2Fa%3EContext+Matters%3A+Refining+Object+Detection+in+Video+with+Recurrent+Neural+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+BMVC+2016%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+pseudo-labeler%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.04648%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.04648%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fvision.cornell.edu%2Fse3%2Fwp-content%2Fuploads%2F2016%2F07%2Fvideo_object_detection_BMVC.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fvision.cornell.edu%2Fse3%2Fwp-content%2Fuploads%2F2016%2F07%2Fvideo_object_detection_BMVC.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CNN_Based_Object_Detection_in_Large_Video_Images_1188%22+target%3D%22_blank%22%3E%3C%2Fa%3ECNN+Based+Object+Detection+in+Large+Video+Images%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+WangTao+%40+%E7%88%B1%E5%A5%87%E8%89%BA%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+object+retrieval%2C+object+detection%2C+scene+classification%3C%2Fli%3E+%0A+++%3Cli%3Eslides%3A+%3Ca+href%3D%22http%3A%2F%2Fon-demand.gputechconf.com%2Fgtc%2F2016%2Fpresentation%2Fs6362-wang-tao-cnn-based-object-detection-large-video-images.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fon-demand.gputechconf.com%2Fgtc%2F2016%2Fpresentation%2Fs6362-wang-tao-cnn-based-object-detection-large-video-images.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_in_Videos_with_Tubelet_Proposal_Networks_1194%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+in+Videos+with+Tubelet+Proposal+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.06355%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.06355%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22FlowGuided_Feature_Aggregation_for_Video_Object_Detection_1198%22+target%3D%22_blank%22%3E%3C%2Fa%3EFlow-Guided+Feature+Aggregation+for+Video+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+MSRA%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1703.10025%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1703.10025%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Video_Object_Detection_using_Faster_RCNN_1203%22+target%3D%22_blank%22%3E%3C%2Fa%3EVideo+Object+Detection+using+Faster+R-CNN%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eblog%3A+%3Ca+href%3D%22http%3A%2F%2Fandrewliao11.github.io%2Fobject_detection%2Ffaster_rcnn%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fandrewliao11.github.io%2Fobject_detection%2Ffaster_rcnn%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fandrewliao11%2Fpy-faster-rcnn-imagenet%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fandrewliao11%2Fpy-faster-rcnn-imagenet%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Improving_Context_Modeling_for_Video_Object_Detection_and_Tracking_1208%22+target%3D%22_blank%22%3E%3C%2Fa%3EImproving+Context+Modeling+for+Video+Object+Detection+and+Tracking%3C%2Fh3%3E+%0A++%3Cp%3E%3Ca+href%3D%22http%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks_2017%2Filsvrc2017_short%28poster%29.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fimage-net.org%2Fchallenges%2Ftalks_2017%2Filsvrc2017_short%28poster%29.pdf%3C%2Fa%3E%3C%2Fp%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Temporal_Dynamic_Graph_LSTM_for_Actiondriven_Video_Object_Detection_1212%22+target%3D%22_blank%22%3E%3C%2Fa%3ETemporal+Dynamic+Graph+LSTM+for+Action-driven+Video+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2017%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.00666%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.00666%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Mobile_Video_Object_Detection_with_TemporallyAware_Feature_Maps_1217%22+target%3D%22_blank%22%3E%3C%2Fa%3EMobile+Video+Object+Detection+with+Temporally-Aware+Feature+Maps%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.06368%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.06368%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Towards_High_Performance_Video_Object_Detection_1221%22+target%3D%22_blank%22%3E%3C%2Fa%3ETowards+High+Performance+Video+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.11577%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.11577%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Impression_Network_for_Video_Object_Detection_1225%22+target%3D%22_blank%22%3E%3C%2Fa%3EImpression+Network+for+Video+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.05896%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.05896%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SpatialTemporal_Memory_Networks_for_Video_Object_Detection_1229%22+target%3D%22_blank%22%3E%3C%2Fa%3ESpatial-Temporal+Memory+Networks+for+Video+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1712.06317%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1712.06317%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%223DDETNet_a_Single_Stage_VideoBased_Vehicle_Detector_1233%22+target%3D%22_blank%22%3E%3C%2Fa%3E3D-DETNet%3A+a+Single+Stage+Video-Based+Vehicle+Detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.01769%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.01769%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_in_Videos_by_Short_and_Long_Range_Object_Linking_1237%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+in+Videos+by+Short+and+Long+Range+Object+Linking%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.09823%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.09823%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_in_Video_with_Spatiotemporal_Sampling_Networks_1241%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+in+Video+with+Spatiotemporal+Sampling+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+University+of+Pennsylvania%2C+2Dartmouth+College%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.05549%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.05549%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Towards_High_Performance_Video_Object_Detection_for_Mobiles_1246%22+target%3D%22_blank%22%3E%3C%2Fa%3ETowards+High+Performance+Video+Object+Detection+for+Mobiles%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Microsoft+Research+Asia%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.05830%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.05830%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Optimizing_Video_Object_Detection_via_a_ScaleTime_Lattice_1251%22+target%3D%22_blank%22%3E%3C%2Fa%3EOptimizing+Video+Object+Detection+via+a+Scale-Time+Lattice%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2018%3C%2Fli%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fmmlab.ie.cuhk.edu.hk%2Fprojects%2FST-Lattice%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fmmlab.ie.cuhk.edu.hk%2Fprojects%2FST-Lattice%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.05472%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.05472%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fhellock%2Fscale-time-lattice%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fhellock%2Fscale-time-lattice%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Pack_and_Detect_Fast_Object_Detection_in_Videos_Using_RegionofInterest_Packing_1258%22+target%3D%22_blank%22%3E%3C%2Fa%3EPack+and+Detect%3A+Fast+Object+Detection+in+Videos+Using+Region-of-Interest+Packing%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3E%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.01701%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.01701%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Fast_Object_Detection_in_Compressed_Video_1261%22+target%3D%22_blank%22%3E%3C%2Fa%3EFast+Object+Detection+in+Compressed+Video%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%EF%BC%9A%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1811.11057%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1811.11057%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_on_Mobile_Devices_1266%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+on+Mobile+Devices%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Pelee_A_RealTime_Object_Detection_System_on_Mobile_Devices_1267%22+target%3D%22_blank%22%3E%3C%2Fa%3EPelee%3A+A+Real-Time+Object+Detection+System+on+Mobile+Devices%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICLR+2018+workshop+track%3C%2Fli%3E+%0A+++%3Cli%3Eintro%3A+based+on+the+SSD%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.06882%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.06882%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FRobert-JunWang%2FPelee%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FRobert-JunWang%2FPelee%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_in_3D_1276%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+in+3D%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Vote3Deep_Fast_Object_Detection_in_3D_Point_Clouds_Using_Efficient_Convolutional_Neural_Networks_1277%22+target%3D%22_blank%22%3E%3C%2Fa%3EVote3Deep%3A+Fast+Object+Detection+in+3D+Point+Clouds+Using+Efficient+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1609.06666%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1609.06666%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ComplexYOLO_Realtime_3D_Object_Detection_on_Point_Clouds_1281%22+target%3D%22_blank%22%3E%3C%2Fa%3EComplex-YOLO%3A+Real-time+3D+Object+Detection+on+Point+Clouds%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Valeo+Schalter+und+Sensoren+GmbH+%26amp%3B+Ilmenau+University+of+Technology%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.06199%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.06199%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Focal_Loss_in_3D_Object_Detection_1286%22+target%3D%22_blank%22%3E%3C%2Fa%3EFocal+Loss+in+3D+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.06065%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.06065%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fpyun-ram%2FFL3D%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fpyun-ram%2FFL3D%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Object_Detection_on_RGBD_1291%22+target%3D%22_blank%22%3E%3C%2Fa%3EObject+Detection+on+RGB-D%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Rich_Features_from_RGBD_Images_for_Object_Detection_and_Segmentation_1292%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Rich+Features+from+RGB-D+Images+for+Object+Detection+and+Segmentation%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1407.5736%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1407.5736%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Differential_Geometry_Boosts_Convolutional_Neural_Networks_for_Object_Detection_1296%22+target%3D%22_blank%22%3E%3C%2Fa%3EDifferential+Geometry+Boosts+Convolutional+Neural+Networks+for+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2016%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016_workshops%2Fw23%2Fhtml%2FWang_Differential_Geometry_Boosts_CVPR_2016_paper.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016_workshops%2Fw23%2Fhtml%2FWang_Differential_Geometry_Boosts_CVPR_2016_paper.html%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca+id%3D%22A_Selfsupervised_Learning_System_for_Object_Detection_using_Physics_Simulation_and_Multiview_Pose_Estimation_1301%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Self-supervised+Learning+System+for+Object+Detection+using+Physics+Simulation+and+Multi-view+Pose+Estimation%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1703.03347%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1703.03347%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroShot_Object_Detection_1307%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Shot+Object+Detection%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroShot_Detection_1308%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Shot+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Australian+National+University%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+YOLO%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.07113%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.07113%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroShot_Object_Detection_1314%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Shot+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.04340%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.04340%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroShot_Object_Detection_Learning_to_Simultaneously_Recognize_and_Localize_Novel_Concepts_1318%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Shot+Object+Detection%3A+Learning+to+Simultaneously+Recognize+and+Localize+Novel+Concepts%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Australian+National+University%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.06049%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.06049%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ZeroShot_Object_Detection_by_Hybrid_Region_Embedding_1323%22+target%3D%22_blank%22%3E%3C%2Fa%3EZero-Shot+Object+Detection+by+Hybrid+Region+Embedding%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Middle+East+Technical+University+%26amp%3B+Hacettepe+University%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1805.06157%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1805.06157%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Salient_Object_Detection_1330%22+target%3D%22_blank%22%3E%3C%2Fa%3ESalient+Object+Detection%3C%2Fh2%3E+%0A++%3Cp%3EThis+task+involves+predicting+the+salient+regions+of+an+image+given+by+human+eye+fixations.%3C%2Fp%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Best_Deep_Saliency_Detection_Models_CVPR_2016__2015_1333%22+target%3D%22_blank%22%3E%3C%2Fa%3EBest+Deep+Saliency+Detection+Models+%28CVPR+2016+%26amp%3B+2015%29%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Epage%3A+%3Ca+href%3D%22http%3A%2F%2Fi.cs.hku.hk%2F%7Eyzyu%2Fvision.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fi.cs.hku.hk%2F%7Eyzyu%2Fvision.html%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Largescale_optimization_of_hierarchical_features_for_saliency_prediction_in_natural_images_1337%22+target%3D%22_blank%22%3E%3C%2Fa%3ELarge-scale+optimization+of+hierarchical+features+for+saliency+prediction+in+natural+images%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fcoxlab.org%2Fpdfs%2Fcvpr2014_vig_saliency.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcoxlab.org%2Fpdfs%2Fcvpr2014_vig_saliency.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Predicting_Eye_Fixations_using_Convolutional_Neural_Networks_1341%22+target%3D%22_blank%22%3E%3C%2Fa%3EPredicting+Eye+Fixations+using+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.escience.cn%2Fsystem%2Ffile%3FfileId%3D72648%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.escience.cn%2Fsystem%2Ffile%3FfileId%3D72648%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Saliency_Detection_by_MultiContext_Deep_Learning_1345%22+target%3D%22_blank%22%3E%3C%2Fa%3ESaliency+Detection+by+Multi-Context+Deep+Learning%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2015%2Fpapers%2FZhao_Saliency_Detection_by_2015_CVPR_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2015%2Fpapers%2FZhao_Saliency_Detection_by_2015_CVPR_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeepSaliency_MultiTask_Deep_Neural_Network_Model_for_Salient_Object_Detection_1349%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeepSaliency%3A+Multi-Task+Deep+Neural+Network+Model+for+Salient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1510.05484%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1510.05484%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SuperCNN_A_Superpixelwise_Convolutional_Neural_Network_for_Salient_Object_Detection_1353%22+target%3D%22_blank%22%3E%3C%2Fa%3ESuperCNN%3A+A+Superpixelwise+Convolutional+Neural+Network+for+Salient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.shengfenghe.com%2Fsupercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ewww.shengfenghe.com%2Fsupercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Shallow_and_Deep_Convolutional_Networks_for_Saliency_Prediction_1357%22+target%3D%22_blank%22%3E%3C%2Fa%3EShallow+and+Deep+Convolutional+Networks+for+Saliency+Prediction%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2016%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1603.00845%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1603.00845%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fimatge-upc%2Fsaliency-2016-cvpr%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fimatge-upc%2Fsaliency-2016-cvpr%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Recurrent_Attentional_Networks_for_Saliency_Detection_1363%22+target%3D%22_blank%22%3E%3C%2Fa%3ERecurrent+Attentional+Networks+for+Saliency+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2016.+recurrent+attentional+convolutional-deconvolution+network+%28RACDNN%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1604.03227%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1604.03227%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22TwoStream_Convolutional_Networks_for_Dynamic_Saliency_Prediction_1368%22+target%3D%22_blank%22%3E%3C%2Fa%3ETwo-Stream+Convolutional+Networks+for+Dynamic+Saliency+Prediction%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.04730%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.04730%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Unconstrained_Salient_Object_Detection_1374%22+target%3D%22_blank%22%3E%3C%2Fa%3EUnconstrained+Salient+Object+Detection%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Unconstrained_Salient_Object_Detection_via_Proposal_Subset_Optimization_1375%22+target%3D%22_blank%22%3E%3C%2Fa%3EUnconstrained+Salient+Object+Detection+via+Proposal+Subset+Optimization%3C%2Fh3%3E+%0A++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F20180821144418842%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2016%3C%2Fli%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2Fsod.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2Fsod.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2FSOD%2FCVPR16SOD_camera_ready.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2FSOD%2FCVPR16SOD_camera_ready.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fjimmie33%2FSOD%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fjimmie33%2FSOD%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Ecaffe+model+zoo%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fwiki%2FModel-Zoo%23cnn-object-proposal-models-for-salient-object-detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fwiki%2FModel-Zoo%23cnn-object-proposal-models-for-salient-object-detection%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DHSNet_Deep_Hierarchical_Saliency_Network_for_Salient_Object_Detection_1383%22+target%3D%22_blank%22%3E%3C%2Fa%3EDHSNet%3A+Deep+Hierarchical+Saliency+Network+for+Salient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FLiu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2016%2Fpapers%2FLiu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Salient_Object_Subitizing_1387%22+target%3D%22_blank%22%3E%3C%2Fa%3ESalient+Object+Subitizing%3C%2Fh3%3E+%0A++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F20180821144443814%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2015%3C%2Fli%3E+%0A+++%3Cli%3Eintro%3A+predicting+the+existence+and+the+number+of+salient+objects+in+an+image+using+holistic+cues%3C%2Fli%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2Fsos.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2Fsos.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.07525%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.07525%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2FSOS%2FSOS_preprint.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fcs-people.bu.edu%2Fjmzhang%2FSOS%2FSOS_preprint.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Ecaffe+model+zoo%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fwiki%2FModel-Zoo%23cnn-models-for-salient-object-subitizing%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBVLC%2Fcaffe%2Fwiki%2FModel-Zoo%23cnn-models-for-salient-object-subitizing%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22DeeplySupervised_Recurrent_Convolutional_Neural_Network_for_Saliency_Detection_1396%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeeply-Supervised+Recurrent+Convolutional+Neural+Network+for+Saliency+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ACMMM+2016.+deeply-supervised+recurrent+convolutional+neural+network+%28DSRCNN%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.05177%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.05177%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Saliency_Detection_via_Combining_RegionLevel_and_PixelLevel_Predictions_with_CNNs_1401%22+target%3D%22_blank%22%3E%3C%2Fa%3ESaliency+Detection+via+Combining+Region-Level+and+Pixel-Level+Predictions+with+CNNs%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2016%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.05186%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.05186%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Edge_Preserving_and_MultiScale_Contextual_Neural_Network_for_Salient_Object_Detection_1406%22+target%3D%22_blank%22%3E%3C%2Fa%3EEdge+Preserving+and+Multi-Scale+Contextual+Neural+Network+for+Salient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.08029%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.08029%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Deep_MultiLevel_Network_for_Saliency_Prediction_1410%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Deep+Multi-Level+Network+for+Saliency+Prediction%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1609.01064%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1609.01064%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_Saliency_Detection_Based_on_Multiscale_Deep_CNN_Features_1414%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+Saliency+Detection+Based+on+Multiscale+Deep+CNN+Features%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+IEEE+Transactions+on+Image+Processing%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1609.02077%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1609.02077%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Deep_Spatial_Contextual_Longterm_Recurrent_Convolutional_Network_for_Saliency_Detection_1419%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Deep+Spatial+Contextual+Long-term+Recurrent+Convolutional+Network+for+Saliency+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+DSCLRCN%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1610.01708%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1610.01708%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deeply_supervised_salient_object_detection_with_short_connections_1424%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeeply+supervised+salient+object+detection+with+short+connections%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+IEEE+TPAMI+2018+%28IEEE+CVPR+2017%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.04849%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.04849%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28official%2C+Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FAndrew-Qibin%2FDSS%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FAndrew-Qibin%2FDSS%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Tensorflow%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FJoker316701882%2FSalient-Object-Detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FJoker316701882%2FSalient-Object-Detection%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Weakly_Supervised_Topdown_Salient_Object_Detection_1431%22+target%3D%22_blank%22%3E%3C%2Fa%3EWeakly+Supervised+Top-down+Salient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Nanyang+Technological+University%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1611.05345%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1611.05345%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22SalGAN_Visual_Saliency_Prediction_with_Generative_Adversarial_Networks_1436%22+target%3D%22_blank%22%3E%3C%2Fa%3ESalGAN%3A+Visual+Saliency+Prediction+with+Generative+Adversarial+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22https%3A%2F%2Fimatge-upc.github.io%2Fsaliency-salgan-2017%2F%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fimatge-upc.github.io%2Fsaliency-salgan-2017%2F%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1701.01081%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1701.01081%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_Saliency_Prediction_Using_a_Mixture_of_Deep_Neural_Networks_1441%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+Saliency+Prediction+Using+a+Mixture+of+Deep+Neural+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.00372%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.00372%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Fast_and_Compact_Salient_Score_Regression_Network_Based_on_Fully_Convolutional_Network_1445%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Fast+and+Compact+Salient+Score+Regression+Network+Based+on+Fully+Convolutional+Network%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.00615%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.00615%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Saliency_Detection_by_Forward_and_Backward_Cues_in_DeepCNNs_1449%22+target%3D%22_blank%22%3E%3C%2Fa%3ESaliency+Detection+by+Forward+and+Backward+Cues+in+Deep-CNNs%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1703.00152%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1703.00152%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Supervised_Adversarial_Networks_for_Image_Saliency_Detection_1453%22+target%3D%22_blank%22%3E%3C%2Fa%3ESupervised+Adversarial+Networks+for+Image+Saliency+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.07242%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.07242%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Groupwise_Deep_Cosaliency_Detection_1457%22+target%3D%22_blank%22%3E%3C%2Fa%3EGroup-wise+Deep+Co-saliency+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1707.07381%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1707.07381%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Towards_the_Success_Rate_of_One_Realtime_Unconstrained_Salient_Object_Detection_1461%22+target%3D%22_blank%22%3E%3C%2Fa%3ETowards+the+Success+Rate+of+One%3A+Real-time+Unconstrained+Salient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+University+of+Maryland+College+Park+%26amp%3B+eBay+Inc%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.00079%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.00079%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Amulet_Aggregating_Multilevel_Convolutional_Features_for_Salient_Object_Detection_1466%22+target%3D%22_blank%22%3E%3C%2Fa%3EAmulet%3A+Aggregating+Multi-level+Convolutional+Features+for+Salient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2017%3C%2Fli%3E+%0A+++%3Cli%3Earixv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.02001%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.02001%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Learning_Uncertain_Convolutional_Features_for_Accurate_Saliency_Detection_1471%22+target%3D%22_blank%22%3E%3C%2Fa%3ELearning+Uncertain+Convolutional+Features+for+Accurate+Saliency+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Accepted+as+a+poster+in+ICCV+2017%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.02031%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.02031%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_EdgeAware_Saliency_Detection_1476%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Edge-Aware+Saliency+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.04366%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.04366%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Selfexplanatory_Deep_Salient_Object_Detection_1480%22+target%3D%22_blank%22%3E%3C%2Fa%3ESelf-explanatory+Deep+Salient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+National+University+of+Defense+Technology%2C+China+%26amp%3B+National+University+of+Singapore%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.05595%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.05595%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca+id%3D%22PiCANet_Learning_Pixelwise_Contextual_Attention_in_ConvNets_and_Its_Application_in_Saliency_Detection_1485%22+target%3D%22_blank%22%3E%3C%2Fa%3EPiCANet%3A+Learning+Pixel-wise+Contextual+Attention+in+ConvNets+and+Its+Application+in+Saliency+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.06433%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.06433%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca+id%3D%22DeepFeat_A_Bottom_Up_and_Top_Down_Saliency_Model_Based_on_Deep_Features_of_Convolutional_Neural_Nets_1489%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeepFeat%3A+A+Bottom+Up+and+Top+Down+Saliency+Model+Based+on+Deep+Features+of+Convolutional+Neural+Nets%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1709.02495%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1709.02495%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Recurrently_Aggregating_Deep_Features_for_Salient_Object_Detection_1493%22+target%3D%22_blank%22%3E%3C%2Fa%3ERecurrently+Aggregating+Deep+Features+for+Salient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+AAAI+2018%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FAAAI%2FAAAI18%2Fpaper%2Fview%2F16775%2F16281%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FAAAI%2FAAAI18%2Fpaper%2Fview%2F16775%2F16281%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_saliency_What_is_learnt_by_a_deep_network_about_saliency_1498%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+saliency%3A+What+is+learnt+by+a+deep+network+about+saliency%3F%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+2nd+Workshop+on+Visualisation+for+Deep+Learning+in+the+34th+International+Conference+On+Machine+Learning%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1801.04261%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1801.04261%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ContrastOriented_Deep_Neural_Networks_for_Salient_Object_Detection_1503%22+target%3D%22_blank%22%3E%3C%2Fa%3EContrast-Oriented+Deep+Neural+Networks+for+Salient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+TNNLS%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1803.11395%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1803.11395%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Salient_Object_Detection_by_Lossless_Feature_Reflection_1508%22+target%3D%22_blank%22%3E%3C%2Fa%3ESalient+Object+Detection+by+Lossless+Feature+Reflection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+IJCAI+2018%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1802.06527%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1802.06527%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22HyperFusionNet_Densely_Reflective_Fusion_for_Salient_Object_Detection_1513%22+target%3D%22_blank%22%3E%3C%2Fa%3EHyperFusion-Net%3A+Densely+Reflective+Fusion+for+Salient+Object+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1804.05142%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1804.05142%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Video_Saliency_Detection_1520%22+target%3D%22_blank%22%3E%3C%2Fa%3EVideo+Saliency+Detection%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Learning_For_Video_Saliency_Detection_1521%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Learning+For+Video+Saliency+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.00871%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.00871%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Video_Salient_Object_Detection_Using_Spatiotemporal_Deep_Features_1525%22+target%3D%22_blank%22%3E%3C%2Fa%3EVideo+Salient+Object+Detection+Using+Spatiotemporal+Deep+Features%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.01447%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.01447%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Predicting_Video_Saliency_with_ObjecttoMotion_CNN_and_Twolayer_Convolutional_LSTM_1529%22+target%3D%22_blank%22%3E%3C%2Fa%3EPredicting+Video+Saliency+with+Object-to-Motion+CNN+and+Two-layer+Convolutional+LSTM%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1709.06316%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1709.06316%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_Relationship_Detection_1536%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+Relationship+Detection%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_Relationship_Detection_with_Language_Priors_1537%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+Relationship+Detection+with+Language+Priors%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2016+oral%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22https%3A%2F%2Fcs.stanford.edu%2Fpeople%2Franjaykrishna%2Fvrd%2Fvrd.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fcs.stanford.edu%2Fpeople%2Franjaykrishna%2Fvrd%2Fvrd.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FProf-Lu-Cewu%2FVisual-Relationship-Detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FProf-Lu-Cewu%2FVisual-Relationship-Detection%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ViPCNN_A_Visual_Phrase_Reasoning_Convolutional_Neural_Network_for_Visual_Relationship_Detection_1543%22+target%3D%22_blank%22%3E%3C%2Fa%3EViP-CNN%3A+A+Visual+Phrase+Reasoning+Convolutional+Neural+Network+for+Visual+Relationship+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Visual+Phrase+reasoning+Convolutional+Neural+Network+%28ViP-CNN%29%2C+Visual+Phrase+Reasoning+Structure+%28VPRS%29%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1702.07191%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1702.07191%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Visual_Translation_Embedding_Network_for_Visual_Relation_Detection_1548%22+target%3D%22_blank%22%3E%3C%2Fa%3EVisual+Translation+Embedding+Network+for+Visual+Relation+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.arxiv.org%2Fabs%2F1702.08319%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.arxiv.org%2Fabs%2F1702.08319%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Deep_Variationstructured_Reinforcement_Learning_for_Visual_Relationship_and_Attribute_Detection_1552%22+target%3D%22_blank%22%3E%3C%2Fa%3EDeep+Variation-structured+Reinforcement+Learning+for+Visual+Relationship+and+Attribute+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2017+spotlight+paper%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1703.03054%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1703.03054%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Detecting_Visual_Relationships_with_Deep_Relational_Networks_1557%22+target%3D%22_blank%22%3E%3C%2Fa%3EDetecting+Visual+Relationships+with+Deep+Relational+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CVPR+2017+oral.+The+Chinese+University+of+Hong+Kong%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1704.03114%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1704.03114%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Identifying_Spatial_Relations_in_Images_using_Convolutional_Neural_Networks_1562%22+target%3D%22_blank%22%3E%3C%2Fa%3EIdentifying+Spatial+Relations+in+Images+using+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1706.04215%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1706.04215%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22PPRFCN_Weakly_Supervised_Visual_Relation_Detection_via_Parallel_Pairwise_RFCN_1566%22+target%3D%22_blank%22%3E%3C%2Fa%3EPPR-FCN%3A+Weakly+Supervised+Visual+Relation+Detection+via+Parallel+Pairwise+R-FCN%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1708.01956%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1708.01956%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Natural_Language_Guided_Visual_Relationship_Detection_1571%22+target%3D%22_blank%22%3E%3C%2Fa%3ENatural+Language+Guided+Visual+Relationship+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1711.06032%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1711.06032%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Detecting_Visual_Relationships_Using_Box_Attention_1575%22+target%3D%22_blank%22%3E%3C%2Fa%3EDetecting+Visual+Relationships+Using+Box+Attention%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Google+AI+%26amp%3B+IST+Austria%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1807.02136%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1807.02136%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Google_AI_Open_Images__Visual_Relationship_Track_1580%22+target%3D%22_blank%22%3E%3C%2Fa%3EGoogle+AI+Open+Images+-+Visual+Relationship+Track%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Detect+pairs+of+objects+in+particular+relationships%3C%2Fli%3E+%0A+++%3Cli%3Ekaggle%3A+%3Ca+href%3D%22https%3A%2F%2Fwww.kaggle.com%2Fc%2Fgoogle-ai-open-images-visual-relationship-track%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fwww.kaggle.com%2Fc%2Fgoogle-ai-open-images-visual-relationship-track%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22ContextDependent_Diffusion_Network_for_Visual_Relationship_Detection_1585%22+target%3D%22_blank%22%3E%3C%2Fa%3EContext-Dependent+Diffusion+Network+for+Visual+Relationship+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+2018+ACM+Multimedia+Conference%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.06213%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.06213%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_Problem_Reduction_Approach_for_Visual_Relationships_Detection_1589%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Problem+Reduction+Approach+for+Visual+Relationships+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2018+Workshop%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1809.09828%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1809.09828%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Face_Deteciton_1595%22+target%3D%22_blank%22%3E%3C%2Fa%3EFace+Deteciton%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Multiview_Face_Detection_Using_Deep_Convolutional_Neural_Networks_1596%22+target%3D%22_blank%22%3E%3C%2Fa%3EMulti-view+Face+Detection+Using+Deep+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+Yahoo%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1502.02766%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1502.02766%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fguoyilin%2FFaceDetection_CNN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fguoyilin%2FFaceDetection_CNN%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22From_Facial_Parts_Responses_to_Face_Detection_A_Deep_Learning_Approach_1602%22+target%3D%22_blank%22%3E%3C%2Fa%3EFrom+Facial+Parts+Responses+to+Face+Detection%3A+A+Deep+Learning+Approach%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICCV+2015.+CUHK%3C%2Fli%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22http%3A%2F%2Fpersonal.ie.cuhk.edu.hk%2F%7Eys014%2Fprojects%2FFaceness%2FFaceness.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fpersonal.ie.cuhk.edu.hk%2F%7Eys014%2Fprojects%2FFaceness%2FFaceness.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1509.06451%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1509.06451%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_iccv_2015%2Fpapers%2FYang_From_Facial_Parts_ICCV_2015_paper.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_iccv_2015%2Fpapers%2FYang_From_Facial_Parts_ICCV_2015_paper.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Compact_Convolutional_Neural_Network_Cascade_for_Face_Detection_1611%22+target%3D%22_blank%22%3E%3C%2Fa%3ECompact+Convolutional+Neural+Network+Cascade+for+Face+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1508.01292%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1508.01292%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBkmz21%2FFD-Evaluation%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBkmz21%2FFD-Evaluation%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FBkmz21%2FCompactCNNCascade%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FBkmz21%2FCompactCNNCascade%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Face_Detection_with_EndtoEnd_Integration_of_a_ConvNet_and_a_3D_Model_1617%22+target%3D%22_blank%22%3E%3C%2Fa%3EFace+Detection+with+End-to-End+Integration+of+a+ConvNet+and+a+3D+Model%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2016%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1606.00850%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1606.00850%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Ftfwu%2FFaceDetection-ConvNet-3D%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Ftfwu%2FFaceDetection-ConvNet-3D%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22CMSRCNN_Contextual_MultiScale_Regionbased_CNN_for_Unconstrained_Face_Detection_1623%22+target%3D%22_blank%22%3E%3C%2Fa%3ECMS-RCNN%3A+Contextual+Multi-Scale+Region-based+CNN+for+Unconstrained+Face+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+CMU%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1606.05413%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1606.05413%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Towards_a_Deep_Learning_Framework_for_Unconstrained_Face_Detection_1628%22+target%3D%22_blank%22%3E%3C%2Fa%3ETowards+a+Deep+Learning+Framework+for+Unconstrained+Face+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+overlap+with+CMS-RCNN%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1612.05322%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1612.05322%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Supervised_Transformer_Network_for_Efficient_Face_Detection_1633%22+target%3D%22_blank%22%3E%3C%2Fa%3ESupervised+Transformer+Network+for+Efficient+Face+Detection%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1607.05477%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1607.05477%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22UnitBox_An_Advanced_Object_Detection_Network_1637%22+target%3D%22_blank%22%3E%3C%2Fa%3EUnitBox%3A+An+Advanced+Object+Detection+Network%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ACM+MM+2016%3C%2Fli%3E+%0A+++%3Cli%3Ekeywords%3A+IOULoss%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.01471%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.01471%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Bootstrapping_Face_Detection_with_Hard_Negative_Examples_1643%22+target%3D%22_blank%22%3E%3C%2Fa%3EBootstrapping+Face+Detection+with+Hard+Negative+Examples%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eauthor%3A+%E4%B8%87%E9%9F%B6%E5%8D%8E+%40+%E5%B0%8F%E7%B1%B3.%3C%2Fli%3E+%0A+++%3Cli%3Eintro%3A+Faster+R-CNN%2C+hard+negative+mining.+state-of-the-art+on+the+FDDB+dataset%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1608.02236%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1608.02236%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Grid_Loss_Detecting_Occluded_Faces_1649%22+target%3D%22_blank%22%3E%3C%2Fa%3EGrid+Loss%3A+Detecting+Occluded+Faces%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ECCV+2016%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1609.00129%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1609.00129%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Epaper%3A+%3Ca+href%3D%22http%3A%2F%2Flrs.icg.tugraz.at%2Fpubs%2Fopitz_eccv_16.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Flrs.icg.tugraz.at%2Fpubs%2Fopitz_eccv_16.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Eposter%3A+%3Ca+href%3D%22http%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-2A-34.pdf%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Fwww.eccv2016.org%2Ffiles%2Fposters%2FP-2A-34.pdf%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22A_MultiScale_Cascade_Fully_Convolutional_Network_Face_Detector_1656%22+target%3D%22_blank%22%3E%3C%2Fa%3EA+Multi-Scale+Cascade+Fully+Convolutional+Network+Face+Detector%3C%2Fh3%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eintro%3A+ICPR+2016%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22http%3A%2F%2Farxiv.org%2Fabs%2F1609.03536%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttp%3A%2F%2Farxiv.org%2Fabs%2F1609.03536%3C%2Fa%3E%3C%2Fli%3E+%0A++%3C%2Ful%3E+%0A++%3Chr%3E+%0A++%3Ch2%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22MTCNN_1664%22+target%3D%22_blank%22%3E%3C%2Fa%3EMTCNN%3C%2Fh2%3E+%0A++%3Ch3%3E%3Ca%3E%3C%2Fa%3E%3Ca+target%3D%22_blank%22%3E%3C%2Fa%3E%3Ca+id%3D%22Joint_Face_Detection_and_Alignment_using_Multitask_Cascaded_Convolutional_Neural_Networks_1665%22+target%3D%22_blank%22%3E%3C%2Fa%3EJoint+Face+Detection+and+Alignment+using+Multi-task+Cascaded+Convolutional+Neural+Networks%3C%2Fh3%3E+%0A++%3Cp%3E%3Cimg+src%3D%22https%3A%2F%2Fblog.uzzz.org.cn%2F_p%3Fhttps%3A%2F%2Fimg-blog.csdn.net%2F20180821144506912%3Fwatermark%2F2%2Ftext%2FaHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h3NTIyNjM0OQ%3D%3D%2Ffont%2F5a6L5L2T%2Ffontsize%2F400%2Ffill%2FI0JBQkFCMA%3D%3D%2Fdissolve%2F70%22+alt%3D%22%E8%BF%99%E9%87%8C%E5%86%99%E5%9B%BE%E7%89%87%E6%8F%8F%E8%BF%B0%22%3E%3C%2Fp%3E+%0A++%3Cul%3E+%0A+++%3Cli%3Eproject+page%3A+%3Ca+href%3D%22https%3A%2F%2Fkpzhang93.github.io%2FMTCNN_face_detection_alignment%2Findex.html%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fkpzhang93.github.io%2FMTCNN_face_detection_alignment%2Findex.html%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Earxiv%3A+%3Ca+href%3D%22https%3A%2F%2Farxiv.org%2Fabs%2F1604.02878%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Farxiv.org%2Fabs%2F1604.02878%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28official%2C+Matlab%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fkpzhang93%2FMTCNN_face_detection_alignment%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fkpzhang93%2FMTCNN_face_detection_alignment%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2Fpangyupo%2Fmxnet_mtcnn_face_detection%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2Fpangyupo%2Fmxnet_mtcnn_face_detection%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FDaFuCoding%2FMTCNN_Caffe%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FDaFuCoding%2FMTCNN_Caffe%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28MXNet%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FSeanlinx%2Fmtcnn%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FSeanlinx%2Fmtcnn%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FPi-DeepLearning%2FRaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FPi-DeepLearning%2FRaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Caffe%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FforeverYoungGitHub%2FMTCNN%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FforeverYoungGitHub%2FMTCNN%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FCongWeilin%2Fmtcnn-caffe%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FCongWeilin%2Fmtcnn-caffe%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28OpenCV%2BOpenBlas%29%3A+%3Ca+href%3D%22https%3A%2F%2Fgithub.com%2FAlphaQi%2FMTCNN-light%22+rel%3D%22nofollow%22+target%3D%22_blank%22%3Ehttps%3A%2F%2Fgithub.com%2FAlphaQi%2FMTCNN-light%3C%2Fa%3E%3C%2Fli%3E+%0A+++%3Cli%3Egithub%28Tensorflow%2Bgolang%29%3A+%3C%2Fli%3E%0A++%3C%2Ful%3E%0A+%3C%2Fdiv%3E%0A%3C%2Fdiv%3E" | url_decode}}