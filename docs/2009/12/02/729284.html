<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>网络爬虫当中暗网爬取初探 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="网络爬虫当中暗网爬取初探" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="众所周知，网络爬虫的最基本原理就是模拟HTTP协议向指定网站发送请求，从而从服务器端返回的网页源代码中抽取具有实用价值的信息（也可能下一次任务队列的地址）。这中间涉及到很多算法，根据网站不同域名，网站网页更新速度，网站结构深度，设定爬虫不同的爬取策略。爬虫可以从一些简单的网站上直接获取网页源代码，从而对网页源代码进行分析。但是对于一些需要用户登录的网站，要抓取网站当中被保护的数据具有一定的困难。今天要说就是一个从需要登录的网站上获取收保护数据的方法。 &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;按照现实生活中的方法，需要获取这些数据，我们首先需要登录这个网站，然后才能访问这个网站当中受保护的数据。好了，话题回来，我们想想，在服务器端网站是如何记录当前用户是否合法（是否已经登录了呢），很简单撒，session，这个我想是大家肯定都知道的方法了。原理很简单，我们访问一个网站，网站在我们首次访问的时候，会返回一个cookies信息。这个信息相当于我们在这个网站的一个登录信息。我们每次访问该网站的时候需要带上这个cookies信息，以便让该网站很容易的识别我们先前已经登录该网站了。 &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;其实步骤很简单，我们需要访问该网站，获取和这个网站会话的cookies信息。然后带这个这个合法的cookies信息重新登录该网站，登录成功后，会在服务器端产生一个保存用户信息的session。以后我们只需每次访问该网站带上合法的cookies信息，服务器端会根据cookies来判别我们是否是合法用户（当前用户信息是否存在session当中）。如果合法我们就可以顺利获取该受保护的网站内容。 &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; 用java来实现一个简单的代码。从指定网站当中获取需要的cookies信息 URL url_con; HttpURLConnection http_con = null; try { url_con = new URL(url); http_con = (HttpURLConnection) url_con.openConnection(); //构造请求信息，对于安全性稍微高一些的网站，构造此类的请求头信息是必须的，第一次请求不带正确的cookies信息 http_con.setRequestProperty(&quot;User-Agent&quot;, &quot;Mozilla/5.0 (Windows; U; Windows NT 6.0; zh-CN; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729)&quot;); http_con.setRequestProperty(&quot;Accept&quot;, &quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;); http_con.setRequestProperty(&quot;Accept-Charset&quot;, &quot;GB2312,utf-8;q=0.7,*;q=0.7&quot;); http_con.setRequestProperty(&quot;Accept-Language&quot;, &quot;zh-cn,zh;q=0.5&quot;); http_con.setRequestProperty(&quot;Host&quot;, &quot;search.cnipr.com&quot;); http_con.setRequestProperty(&quot;Accept-Encoding&quot;, &quot;gzip,deflate&quot;); http_con.setRequestProperty(&quot;Keep-Alive&quot;, &quot;300&quot;); http_con.setRequestProperty(&quot;Connection&quot;, &quot;keep-alive&quot;); http_con.setRequestProperty(&quot;Cookie&quot;, &quot;cizi=2&quot;); http_con.setRequestProperty(&quot;Referer&quot;,&quot;http://www.cnipr.com/top_js.htm&quot;); //设置请求方式为get请求 http_con.setDoInput(true); http_con.setDoOutput(true); http_con.setRequestMethod(&quot;GET&quot;); //建立和服务器之间的连接。 http_con.connect(); String cookieVal = http_con.getHeaderField(&quot;Set-Cookie&quot;); //获取服务器端传回的jsessionId(对于使用tomcat内核的服务器一般传回此类值)值 if (cookieVal != null) { cookiesId = cookieVal.substring(0, cookieVal.indexOf(&quot;;&quot;)); } } catch (Exception e) { e.printStackTrace(); getCookies() ; } &nbsp;很简单，我们获得cookies以后呢，重新登录该网站，当然别忘了在HTTP请求头加上需要的cookies信息啊。就是我们获取的cookiesId。这样就可以顺利登录。以后每次请求都需要带上该ID。这样我们就可以抓取需要的网页信息" />
<meta property="og:description" content="众所周知，网络爬虫的最基本原理就是模拟HTTP协议向指定网站发送请求，从而从服务器端返回的网页源代码中抽取具有实用价值的信息（也可能下一次任务队列的地址）。这中间涉及到很多算法，根据网站不同域名，网站网页更新速度，网站结构深度，设定爬虫不同的爬取策略。爬虫可以从一些简单的网站上直接获取网页源代码，从而对网页源代码进行分析。但是对于一些需要用户登录的网站，要抓取网站当中被保护的数据具有一定的困难。今天要说就是一个从需要登录的网站上获取收保护数据的方法。 &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;按照现实生活中的方法，需要获取这些数据，我们首先需要登录这个网站，然后才能访问这个网站当中受保护的数据。好了，话题回来，我们想想，在服务器端网站是如何记录当前用户是否合法（是否已经登录了呢），很简单撒，session，这个我想是大家肯定都知道的方法了。原理很简单，我们访问一个网站，网站在我们首次访问的时候，会返回一个cookies信息。这个信息相当于我们在这个网站的一个登录信息。我们每次访问该网站的时候需要带上这个cookies信息，以便让该网站很容易的识别我们先前已经登录该网站了。 &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;其实步骤很简单，我们需要访问该网站，获取和这个网站会话的cookies信息。然后带这个这个合法的cookies信息重新登录该网站，登录成功后，会在服务器端产生一个保存用户信息的session。以后我们只需每次访问该网站带上合法的cookies信息，服务器端会根据cookies来判别我们是否是合法用户（当前用户信息是否存在session当中）。如果合法我们就可以顺利获取该受保护的网站内容。 &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; 用java来实现一个简单的代码。从指定网站当中获取需要的cookies信息 URL url_con; HttpURLConnection http_con = null; try { url_con = new URL(url); http_con = (HttpURLConnection) url_con.openConnection(); //构造请求信息，对于安全性稍微高一些的网站，构造此类的请求头信息是必须的，第一次请求不带正确的cookies信息 http_con.setRequestProperty(&quot;User-Agent&quot;, &quot;Mozilla/5.0 (Windows; U; Windows NT 6.0; zh-CN; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729)&quot;); http_con.setRequestProperty(&quot;Accept&quot;, &quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;); http_con.setRequestProperty(&quot;Accept-Charset&quot;, &quot;GB2312,utf-8;q=0.7,*;q=0.7&quot;); http_con.setRequestProperty(&quot;Accept-Language&quot;, &quot;zh-cn,zh;q=0.5&quot;); http_con.setRequestProperty(&quot;Host&quot;, &quot;search.cnipr.com&quot;); http_con.setRequestProperty(&quot;Accept-Encoding&quot;, &quot;gzip,deflate&quot;); http_con.setRequestProperty(&quot;Keep-Alive&quot;, &quot;300&quot;); http_con.setRequestProperty(&quot;Connection&quot;, &quot;keep-alive&quot;); http_con.setRequestProperty(&quot;Cookie&quot;, &quot;cizi=2&quot;); http_con.setRequestProperty(&quot;Referer&quot;,&quot;http://www.cnipr.com/top_js.htm&quot;); //设置请求方式为get请求 http_con.setDoInput(true); http_con.setDoOutput(true); http_con.setRequestMethod(&quot;GET&quot;); //建立和服务器之间的连接。 http_con.connect(); String cookieVal = http_con.getHeaderField(&quot;Set-Cookie&quot;); //获取服务器端传回的jsessionId(对于使用tomcat内核的服务器一般传回此类值)值 if (cookieVal != null) { cookiesId = cookieVal.substring(0, cookieVal.indexOf(&quot;;&quot;)); } } catch (Exception e) { e.printStackTrace(); getCookies() ; } &nbsp;很简单，我们获得cookies以后呢，重新登录该网站，当然别忘了在HTTP请求头加上需要的cookies信息啊。就是我们获取的cookiesId。这样就可以顺利登录。以后每次请求都需要带上该ID。这样我们就可以抓取需要的网页信息" />
<link rel="canonical" href="https://mlh.app/2009/12/02/729284.html" />
<meta property="og:url" content="https://mlh.app/2009/12/02/729284.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2009-12-02T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"众所周知，网络爬虫的最基本原理就是模拟HTTP协议向指定网站发送请求，从而从服务器端返回的网页源代码中抽取具有实用价值的信息（也可能下一次任务队列的地址）。这中间涉及到很多算法，根据网站不同域名，网站网页更新速度，网站结构深度，设定爬虫不同的爬取策略。爬虫可以从一些简单的网站上直接获取网页源代码，从而对网页源代码进行分析。但是对于一些需要用户登录的网站，要抓取网站当中被保护的数据具有一定的困难。今天要说就是一个从需要登录的网站上获取收保护数据的方法。 &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;按照现实生活中的方法，需要获取这些数据，我们首先需要登录这个网站，然后才能访问这个网站当中受保护的数据。好了，话题回来，我们想想，在服务器端网站是如何记录当前用户是否合法（是否已经登录了呢），很简单撒，session，这个我想是大家肯定都知道的方法了。原理很简单，我们访问一个网站，网站在我们首次访问的时候，会返回一个cookies信息。这个信息相当于我们在这个网站的一个登录信息。我们每次访问该网站的时候需要带上这个cookies信息，以便让该网站很容易的识别我们先前已经登录该网站了。 &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;其实步骤很简单，我们需要访问该网站，获取和这个网站会话的cookies信息。然后带这个这个合法的cookies信息重新登录该网站，登录成功后，会在服务器端产生一个保存用户信息的session。以后我们只需每次访问该网站带上合法的cookies信息，服务器端会根据cookies来判别我们是否是合法用户（当前用户信息是否存在session当中）。如果合法我们就可以顺利获取该受保护的网站内容。 &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; 用java来实现一个简单的代码。从指定网站当中获取需要的cookies信息 URL url_con; HttpURLConnection http_con = null; try { url_con = new URL(url); http_con = (HttpURLConnection) url_con.openConnection(); //构造请求信息，对于安全性稍微高一些的网站，构造此类的请求头信息是必须的，第一次请求不带正确的cookies信息 http_con.setRequestProperty(&quot;User-Agent&quot;, &quot;Mozilla/5.0 (Windows; U; Windows NT 6.0; zh-CN; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729)&quot;); http_con.setRequestProperty(&quot;Accept&quot;, &quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;); http_con.setRequestProperty(&quot;Accept-Charset&quot;, &quot;GB2312,utf-8;q=0.7,*;q=0.7&quot;); http_con.setRequestProperty(&quot;Accept-Language&quot;, &quot;zh-cn,zh;q=0.5&quot;); http_con.setRequestProperty(&quot;Host&quot;, &quot;search.cnipr.com&quot;); http_con.setRequestProperty(&quot;Accept-Encoding&quot;, &quot;gzip,deflate&quot;); http_con.setRequestProperty(&quot;Keep-Alive&quot;, &quot;300&quot;); http_con.setRequestProperty(&quot;Connection&quot;, &quot;keep-alive&quot;); http_con.setRequestProperty(&quot;Cookie&quot;, &quot;cizi=2&quot;); http_con.setRequestProperty(&quot;Referer&quot;,&quot;http://www.cnipr.com/top_js.htm&quot;); //设置请求方式为get请求 http_con.setDoInput(true); http_con.setDoOutput(true); http_con.setRequestMethod(&quot;GET&quot;); //建立和服务器之间的连接。 http_con.connect(); String cookieVal = http_con.getHeaderField(&quot;Set-Cookie&quot;); //获取服务器端传回的jsessionId(对于使用tomcat内核的服务器一般传回此类值)值 if (cookieVal != null) { cookiesId = cookieVal.substring(0, cookieVal.indexOf(&quot;;&quot;)); } } catch (Exception e) { e.printStackTrace(); getCookies() ; } &nbsp;很简单，我们获得cookies以后呢，重新登录该网站，当然别忘了在HTTP请求头加上需要的cookies信息啊。就是我们获取的cookiesId。这样就可以顺利登录。以后每次请求都需要带上该ID。这样我们就可以抓取需要的网页信息","@type":"BlogPosting","url":"https://mlh.app/2009/12/02/729284.html","headline":"网络爬虫当中暗网爬取初探","dateModified":"2009-12-02T00:00:00+08:00","datePublished":"2009-12-02T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2009/12/02/729284.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>网络爬虫当中暗网爬取初探</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p><span> </span>众所周知，网络爬虫的最基本原理就是模拟HTTP协议向指定网站发送请求，从而从服务器端返回的网页源代码中抽取具有实用价值的信息（也可能下一次任务队列的地址）。这中间涉及到很多算法，根据网站不同域名，网站网页更新速度，网站结构深度，设定爬虫不同的爬取策略。爬虫可以从一些简单的网站上直接获取网页源代码，从而对网页源代码进行分析。但是对于一些需要用户登录的网站，要抓取网站当中被保护的数据具有一定的困难。今天要说就是一个从需要登录的网站上获取收保护数据的方法。</p> 
  <p>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;按照现实生活中的方法，需要获取这些数据，我们首先需要登录这个网站，然后才能访问这个网站当中受保护的数据。好了，话题回来，我们想想，在服务器端网站是如何记录当前用户是否合法（是否已经登录了呢），很简单撒，session，这个我想是大家肯定都知道的方法了。原理很简单，我们访问一个网站，网站在我们首次访问的时候，会返回一个cookies信息。这个信息相当于我们在这个网站的一个登录信息。我们每次访问该网站的时候需要带上这个cookies信息，以便让该网站很容易的识别我们先前已经登录该网站了。</p> 
  <p>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp;其实步骤很简单，我们需要访问该网站，获取和这个网站会话的cookies信息。然后带这个这个合法的cookies信息重新登录该网站，登录成功后，会在服务器端产生一个保存用户信息的session。以后我们只需每次访问该网站带上合法的cookies信息，服务器端会根据cookies来判别我们是否是合法用户（当前用户信息是否存在session当中）。如果合法我们就可以顺利获取该受保护的网站内容。</p> 
  <p>&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; 用java来实现一个简单的代码。从指定网站当中获取需要的cookies信息</p> 
  <p> </p> 
  <pre><code class="language-java">URL url_con;
		HttpURLConnection http_con = null;
		try {
			url_con = new URL(url);
			http_con = (HttpURLConnection) url_con.openConnection();
			//构造请求信息，对于安全性稍微高一些的网站，构造此类的请求头信息是必须的，第一次请求不带正确的cookies信息
			http_con.setRequestProperty("User-Agent", "Mozilla/5.0 (Windows; U; Windows NT 6.0; zh-CN; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 (.NET CLR 3.5.30729)");
			http_con.setRequestProperty("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8");
			http_con.setRequestProperty("Accept-Charset", "GB2312,utf-8;q=0.7,*;q=0.7");
			http_con.setRequestProperty("Accept-Language", "zh-cn,zh;q=0.5");
			http_con.setRequestProperty("Host", "search.cnipr.com");
			http_con.setRequestProperty("Accept-Encoding", "gzip,deflate");
			http_con.setRequestProperty("Keep-Alive", "300");
			http_con.setRequestProperty("Connection", "keep-alive");
			http_con.setRequestProperty("Cookie", "cizi=2");
			http_con.setRequestProperty("Referer","http://www.cnipr.com/top_js.htm");
			//设置请求方式为get请求
			http_con.setDoInput(true);
			http_con.setDoOutput(true);
			http_con.setRequestMethod("GET");
			//建立和服务器之间的连接。
			http_con.connect();
			String cookieVal = http_con.getHeaderField("Set-Cookie");
			//获取服务器端传回的jsessionId(对于使用tomcat内核的服务器一般传回此类值)值
			if (cookieVal != null) {
				cookiesId = cookieVal.substring(0, cookieVal.indexOf(";"));
			}
		} catch (Exception e) {
			e.printStackTrace();
			getCookies() ;
		}</code></pre> &nbsp;很简单，我们获得cookies以后呢，重新登录该网站，当然别忘了在HTTP请求头加上需要的cookies信息啊。就是我们获取的cookiesId。这样就可以顺利登录。以后每次请求都需要带上该ID。这样我们就可以抓取需要的网页信息 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
