<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>美团DB数据同步到数据仓库的架构与实践 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="美团DB数据同步到数据仓库的架构与实践" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="总第310篇 2018年 第102篇 本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入数仓。 By the way，如果你对我们的工作内容比较感兴趣，欢迎扫描并识别下方二维码加入我们，一起致力于解决海量数据采集和传输的问题中来吧！ 扫码查看职位详情，一键投递简历 背景 在数据仓库建模中，未经任何加工处理的原始业务层数据，我们称之为ODS（Operational Data Store）数据。在互联网企业中，常见的ODS数据有业务日志数据（Log）和业务DB数据（DB）两类。对于业务DB数据来说，从MySQL等关系型数据库的业务数据进行采集，然后导入到Hive中，是进行数据仓库生产的重要环节。 如何准确、高效地把MySQL数据同步到Hive中？一般常用的解决方案是批量取数并Load：直连MySQL去Select表中的数据，然后存到本地文件作为中间存储，最后把文件Load到Hive表中。这种方案的优点是实现简单，但是随着业务的发展，缺点也逐渐暴露出来： 性能瓶颈：随着业务规模的增长，Select From MySQL -&gt; Save to Localfile -&gt; Load to Hive这种数据流花费的时间越来越长，无法满足下游数仓生产的时间要求。 直接从MySQL中Select大量数据，对MySQL的影响非常大，容易造成慢查询，影响业务线上的正常服务。 由于Hive本身的语法不支持更新、删除等SQL原语，对于MySQL中发生Update/Delete的数据无法很好地进行支持。 为了彻底解决这些问题，我们逐步转向CDC（Change Data Capture）+ Merge的技术方案，即实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。Binlog是MySQL的二进制日志，记录了MySQL中发生的所有数据变更，MySQL集群自身的主从同步就是基于Binlog做的。 本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入数仓。 整体架构 整体的架构如上图所示。在Binlog实时采集方面，我们采用了阿里巴巴的开源项目Canal，负责从MySQL实时拉取Binlog并完成适当解析。Binlog采集后会暂存到Kafka上供下游消费。整体实时采集部分如图中红色箭头所示。 离线处理Binlog的部分，如图中黑色箭头所示，通过下面的步骤在Hive上还原一张MySQL表： 采用Linkedin的开源项目Camus，负责每小时把Kafka上的Binlog数据拉取到Hive上。 对每张ODS表，首先需要一次性制作快照（Snapshot），把MySQL里的存量数据读取到Hive上，这一过程底层采用直连MySQL去Select数据的方式。 对每张ODS表，每天基于存量数据和当天增量产生的Binlog做Merge，从而还原出业务数据。 我们回过头来看看，背景中介绍的批量取数并Load方案遇到的各种问题，为什么用这种方案能解决上面的问题呢？ 首先，Binlog是流式产生的，通过对Binlog的实时采集，把部分数据处理需求由每天一次的批处理分摊到实时流上。无论从性能上还是对MySQL的访问压力上，都会有明显地改善。 第二，Binlog本身记录了数据变更的类型（Insert/Update/Delete），通过一些语义方面的处理，完全能够做到精准的数据还原。 Binlog实时采集 对Binlog的实时采集包含两个主要模块：一是CanalManager，主要负责采集任务的分配、监控报警、元数据管理以及和外部依赖系统的对接；二是真正执行采集任务的Canal和CanalClient。 当用户提交某个DB的Binlog采集请求时，CanalManager首先会调用DBA平台的相关接口，获取这一DB所在MySQL实例的相关信息，目的是从中选出最适合Binlog采集的机器。然后把采集实例（Canal Instance）分发到合适的Canal服务器上，即CanalServer上。在选择具体的CanalServer时，CanalManager会考虑负载均衡、跨机房传输等因素，优先选择负载较低且同地域传输的机器。 CanalServer收到采集请求后，会在ZooKeeper上对收集信息进行注册。注册的内容包括： 以Instance名称命名的永久节点。 在该永久节点下注册以自身ip:port命名的临时节点。 这样做的目的有两个： 高可用：CanalManager对Instance进行分发时，会选择两台CanalServer，一台是Running节点，另一台作为Standby节点。Standby节点会对该Instance进行监听，当Running节点出现故障后，临时节点消失，然后Standby节点进行抢占。这样就达到了容灾的目的。 与CanalClient交互：CanalClient检测到自己负责的Instance所在的Running CanalServer后，便会进行连接，从而接收到CanalServer发来的Binlog数据。 对Binlog的订阅以MySQL的DB为粒度，一个DB的Binlog对应了一个Kafka Topic。底层实现时，一个MySQL实例下所有订阅的DB，都由同一个Canal Instance进行处理。这是因为Binlog的产生是以MySQL实例为粒度的。CanalServer会抛弃掉未订阅的Binlog数据，然后CanalClient将接收到的Binlog按DB粒度分发到Kafka上。 离线还原MySQL数据 完成Binlog采集后，下一步就是利用Binlog来还原业务数据。首先要解决的第一个问题是把Binlog从Kafka同步到Hive上。 Kafka2Hive 整个Kafka2Hive任务的管理，在美团数据平台的ETL框架下进行，包括任务原语的表达和调度机制等，都同其他ETL类似。而底层采用LinkedIn的开源项目Camus，并进行了有针对性的二次开发，来完成真正的Kafka2Hive数据传输工作。 对Camus的二次开发 Kafka上存储的Binlog未带Schema，而Hive表必须有Schema，并且其分区、字段等的设计，都要便于下游的高效消费。对Camus做的第一个改造，便是将Kafka上的Binlog解析成符合目标Schema的格式。 对Camus做的第二个改造，由美团的ETL框架所决定。在我们的任务调度系统中，目前只对同调度队列的任务做上下游依赖关系的解析，跨调度队列是不能建立依赖关系的。而在MySQL2Hive的整个流程中，Kafka2Hive的任务需要每小时执行一次（小时队列），Merge任务每天执行一次（天队列）。而Merge任务的启动必须要严格依赖小时Kafka2Hive任务的完成。 为了解决这一问题，我们引入了Checkdone任务。Checkdone任务是天任务，主要负责检测前一天的Kafka2Hive是否成功完成。如果成功完成了，则Checkdone任务执行成功，这样下游的Merge任务就可以正确启动了。 Checkdone的检测逻辑 Checkdone是怎样检测的呢？每个Kafka2Hive任务成功完成数据传输后，由Camus负责在相应的HDFS目录下记录该任务的启动时间。Checkdone会扫描前一天的所有时间戳，如果最大的时间戳已经超过了0点，就说明前一天的Kafka2Hive任务都成功完成了，这样Checkdone就完成了检测。 此外，由于Camus本身只是完成了读Kafka然后写HDFS文件的过程，还必须完成对Hive分区的加载才能使下游查询到。因此，整个Kafka2Hive任务的最后一步是加载Hive分区。这样，整个任务才算成功执行。 每个Kafka2Hive任务负责读取一个特定的Topic，把Binlog数据写入original_binlog库下的一张表中，即前面图中的original_binlog.db，其中存储的是对应到一个MySQL DB的全部Binlog。 上图说明了一个Kafka2Hive完成后，文件在HDFS上的目录结构。假如一个MySQL DB叫做user，对应的Binlog存储在original_binlog.user表中。ready目录中，按天存储了当天所有成功执行的Kafka2Hive任务的启动时间，供Checkdone使用。每张表的Binlog，被组织到一个分区中，例如userinfo表的Binlog，存储在table_name=userinfo这一分区中。每个table_name一级分区下，按dt组织二级分区。图中的xxx.lzo和xxx.lzo.index文件，存储的是经过lzo压缩的Binlog数据。 Merge Binlog成功入仓后，下一步要做的就是基于Binlog对MySQL数据进行还原。Merge流程做了两件事，首先把当天生成的Binlog数据存放到Delta表中，然后和已有的存量数据做一个基于主键的Merge。Delta表中的数据是当天的最新数据，当一条数据在一天内发生多次变更时，Delta表中只存储最后一次变更后的数据。 把Delta数据和存量数据进行Merge的过程中，需要有唯一键来判定是否是同一条数据。如果同一条数据既出现在存量表中，又出现在Delta表中，说明这一条数据发生了更新，则选取Delta表的数据作为最终结果；否则说明没有发生任何变动，保留原来存量表中的数据作为最终结果。Merge的结果数据会Insert Overwrite到原表中，即图中的origindb.table。 Merge流程举例 下面用一个例子来具体说明Merge的流程。 数据表共id、value两列，其中id是主键。在提取Delta数据时，对同一条数据的多次更新，只选择最后更新的一条。所以对id=1的数据，Delta表中记录最后一条更新后的值value=120。Delta数据和存量数据做Merge后，最终结果中，新插入一条数据（id=4），两条数据发生了更新（id=1和id=2），一条数据未变（id=3）。 默认情况下，我们采用MySQL表的主键作为这一判重的唯一键，业务也可以根据实际情况配置不同于MySQL的唯一键。 上面介绍了基于Binlog的数据采集和ODS数据还原的整体架构。下面主要从两个方面介绍我们解决的实际业务问题。 实践一：分库分表的支持 随着业务规模的扩大，MySQL的分库分表情况越来越多，很多业务的分表数目都在几千个这样的量级。而一般数据开发同学需要把这些数据聚合到一起进行分析。如果对每个分表都进行手动同步，再在Hive上进行聚合，这个成本很难被我们接受。因此，我们需要在ODS层就完成分表的聚合。 首先，在Binlog实时采集时，我们支持把不同DB的Binlog写入到同一个Kafka Topic。用户可以在申请Binlog采集时，同时勾选同一个业务逻辑下的多个物理DB。通过在Binlog采集层的汇集，所有分库的Binlog会写入到同一张Hive表中，这样下游在进行Merge时，依然只需要读取一张Hive表。 第二，Merge任务的配置支持正则匹配。通过配置符合业务分表命名规则的正则表达式，Merge任务就能了解自己需要聚合哪些MySQL表的Binlog，从而选取相应分区的数据来执行。 这样通过两个层面的工作，就完成了分库分表在ODS层的合并。 这里面有一个技术上的优化，在进行Kafka2Hive时，我们按业务分表规则对表名进行了处理，把物理表名转换成了逻辑表名。例如userinfo123这张表名会被转换为userinfo，其Binlog数据存储在original_binlog.user表的table_name=userinfo分区中。这样做的目的是防止过多的HDFS小文件和Hive分区造成的底层压力。 实践二：删除事件的支持 Delete操作在MySQL中非常常见，由于Hive不支持Delete，如果想把MySQL中删除的数据在Hive中删掉，需要采用“迂回”的方式进行。 对需要处理Delete事件的Merge流程，采用如下两个步骤： 首先，提取出发生了Delete事件的数据，由于Binlog本身记录了事件类型，这一步很容易做到。将存量数据（表A）与被删掉的数据（表B）在主键上做左外连接（Left outer join），如果能够全部join到双方的数据，说明该条数据被删掉了。因此，选择结果中表B对应的记录为NULL的数据，即是应当被保留的数据。 然后，对上面得到的被保留下来的数据，按照前面描述的流程做常规的Merge。 总结与展望 作为数据仓库生产的基础，美团数据平台提供的基于Binlog的MySQL2Hive服务，基本覆盖了美团内部的各个业务线，目前已经能够满足绝大部分业务的数据同步需求，实现DB数据准确、高效地入仓。在后面的发展中，我们会集中解决CanalManager的单点问题，并构建跨机房容灾的架构，从而更加稳定地支撑业务的发展。 本文主要从Binlog流式采集和基于Binlog的ODS数据还原两方面，介绍了这一服务的架构，并介绍了我们在实践中遇到的一些典型问题和解决方案。希望能够给其他开发者一些参考价值，同时也欢迎大家和我们一起交流。 欢迎加入美团数据库技术交流群，跟作者零距离交流。进群方式：请加美美同学微信（微信号：MTDPtech02），回复：数据库，美美会自动拉你进群。 活动推荐 《美团技术沙龙第47期·北京：运营效率系统架构演进之道》12月8日周六下午将在望京恒电大厦C座美团点评北京总部1层恒基咖啡举办。精选了供应链、商家赋能和数据智能方面的典型业务和系统，与业内同仁一起分享交流，同时，我们还邀请京东的优秀工程师分享京东在运营方面的经验。 更多活动详情，请戳：活动报名链接 ----------&nbsp; END&nbsp; ---------- 也许你还想看 新一代数据库TiDB在美团的实践 美团点评基于Storm的实时数据处理实践 美团点评基于 Flink 的实时数仓建设实践" />
<meta property="og:description" content="总第310篇 2018年 第102篇 本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入数仓。 By the way，如果你对我们的工作内容比较感兴趣，欢迎扫描并识别下方二维码加入我们，一起致力于解决海量数据采集和传输的问题中来吧！ 扫码查看职位详情，一键投递简历 背景 在数据仓库建模中，未经任何加工处理的原始业务层数据，我们称之为ODS（Operational Data Store）数据。在互联网企业中，常见的ODS数据有业务日志数据（Log）和业务DB数据（DB）两类。对于业务DB数据来说，从MySQL等关系型数据库的业务数据进行采集，然后导入到Hive中，是进行数据仓库生产的重要环节。 如何准确、高效地把MySQL数据同步到Hive中？一般常用的解决方案是批量取数并Load：直连MySQL去Select表中的数据，然后存到本地文件作为中间存储，最后把文件Load到Hive表中。这种方案的优点是实现简单，但是随着业务的发展，缺点也逐渐暴露出来： 性能瓶颈：随着业务规模的增长，Select From MySQL -&gt; Save to Localfile -&gt; Load to Hive这种数据流花费的时间越来越长，无法满足下游数仓生产的时间要求。 直接从MySQL中Select大量数据，对MySQL的影响非常大，容易造成慢查询，影响业务线上的正常服务。 由于Hive本身的语法不支持更新、删除等SQL原语，对于MySQL中发生Update/Delete的数据无法很好地进行支持。 为了彻底解决这些问题，我们逐步转向CDC（Change Data Capture）+ Merge的技术方案，即实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。Binlog是MySQL的二进制日志，记录了MySQL中发生的所有数据变更，MySQL集群自身的主从同步就是基于Binlog做的。 本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入数仓。 整体架构 整体的架构如上图所示。在Binlog实时采集方面，我们采用了阿里巴巴的开源项目Canal，负责从MySQL实时拉取Binlog并完成适当解析。Binlog采集后会暂存到Kafka上供下游消费。整体实时采集部分如图中红色箭头所示。 离线处理Binlog的部分，如图中黑色箭头所示，通过下面的步骤在Hive上还原一张MySQL表： 采用Linkedin的开源项目Camus，负责每小时把Kafka上的Binlog数据拉取到Hive上。 对每张ODS表，首先需要一次性制作快照（Snapshot），把MySQL里的存量数据读取到Hive上，这一过程底层采用直连MySQL去Select数据的方式。 对每张ODS表，每天基于存量数据和当天增量产生的Binlog做Merge，从而还原出业务数据。 我们回过头来看看，背景中介绍的批量取数并Load方案遇到的各种问题，为什么用这种方案能解决上面的问题呢？ 首先，Binlog是流式产生的，通过对Binlog的实时采集，把部分数据处理需求由每天一次的批处理分摊到实时流上。无论从性能上还是对MySQL的访问压力上，都会有明显地改善。 第二，Binlog本身记录了数据变更的类型（Insert/Update/Delete），通过一些语义方面的处理，完全能够做到精准的数据还原。 Binlog实时采集 对Binlog的实时采集包含两个主要模块：一是CanalManager，主要负责采集任务的分配、监控报警、元数据管理以及和外部依赖系统的对接；二是真正执行采集任务的Canal和CanalClient。 当用户提交某个DB的Binlog采集请求时，CanalManager首先会调用DBA平台的相关接口，获取这一DB所在MySQL实例的相关信息，目的是从中选出最适合Binlog采集的机器。然后把采集实例（Canal Instance）分发到合适的Canal服务器上，即CanalServer上。在选择具体的CanalServer时，CanalManager会考虑负载均衡、跨机房传输等因素，优先选择负载较低且同地域传输的机器。 CanalServer收到采集请求后，会在ZooKeeper上对收集信息进行注册。注册的内容包括： 以Instance名称命名的永久节点。 在该永久节点下注册以自身ip:port命名的临时节点。 这样做的目的有两个： 高可用：CanalManager对Instance进行分发时，会选择两台CanalServer，一台是Running节点，另一台作为Standby节点。Standby节点会对该Instance进行监听，当Running节点出现故障后，临时节点消失，然后Standby节点进行抢占。这样就达到了容灾的目的。 与CanalClient交互：CanalClient检测到自己负责的Instance所在的Running CanalServer后，便会进行连接，从而接收到CanalServer发来的Binlog数据。 对Binlog的订阅以MySQL的DB为粒度，一个DB的Binlog对应了一个Kafka Topic。底层实现时，一个MySQL实例下所有订阅的DB，都由同一个Canal Instance进行处理。这是因为Binlog的产生是以MySQL实例为粒度的。CanalServer会抛弃掉未订阅的Binlog数据，然后CanalClient将接收到的Binlog按DB粒度分发到Kafka上。 离线还原MySQL数据 完成Binlog采集后，下一步就是利用Binlog来还原业务数据。首先要解决的第一个问题是把Binlog从Kafka同步到Hive上。 Kafka2Hive 整个Kafka2Hive任务的管理，在美团数据平台的ETL框架下进行，包括任务原语的表达和调度机制等，都同其他ETL类似。而底层采用LinkedIn的开源项目Camus，并进行了有针对性的二次开发，来完成真正的Kafka2Hive数据传输工作。 对Camus的二次开发 Kafka上存储的Binlog未带Schema，而Hive表必须有Schema，并且其分区、字段等的设计，都要便于下游的高效消费。对Camus做的第一个改造，便是将Kafka上的Binlog解析成符合目标Schema的格式。 对Camus做的第二个改造，由美团的ETL框架所决定。在我们的任务调度系统中，目前只对同调度队列的任务做上下游依赖关系的解析，跨调度队列是不能建立依赖关系的。而在MySQL2Hive的整个流程中，Kafka2Hive的任务需要每小时执行一次（小时队列），Merge任务每天执行一次（天队列）。而Merge任务的启动必须要严格依赖小时Kafka2Hive任务的完成。 为了解决这一问题，我们引入了Checkdone任务。Checkdone任务是天任务，主要负责检测前一天的Kafka2Hive是否成功完成。如果成功完成了，则Checkdone任务执行成功，这样下游的Merge任务就可以正确启动了。 Checkdone的检测逻辑 Checkdone是怎样检测的呢？每个Kafka2Hive任务成功完成数据传输后，由Camus负责在相应的HDFS目录下记录该任务的启动时间。Checkdone会扫描前一天的所有时间戳，如果最大的时间戳已经超过了0点，就说明前一天的Kafka2Hive任务都成功完成了，这样Checkdone就完成了检测。 此外，由于Camus本身只是完成了读Kafka然后写HDFS文件的过程，还必须完成对Hive分区的加载才能使下游查询到。因此，整个Kafka2Hive任务的最后一步是加载Hive分区。这样，整个任务才算成功执行。 每个Kafka2Hive任务负责读取一个特定的Topic，把Binlog数据写入original_binlog库下的一张表中，即前面图中的original_binlog.db，其中存储的是对应到一个MySQL DB的全部Binlog。 上图说明了一个Kafka2Hive完成后，文件在HDFS上的目录结构。假如一个MySQL DB叫做user，对应的Binlog存储在original_binlog.user表中。ready目录中，按天存储了当天所有成功执行的Kafka2Hive任务的启动时间，供Checkdone使用。每张表的Binlog，被组织到一个分区中，例如userinfo表的Binlog，存储在table_name=userinfo这一分区中。每个table_name一级分区下，按dt组织二级分区。图中的xxx.lzo和xxx.lzo.index文件，存储的是经过lzo压缩的Binlog数据。 Merge Binlog成功入仓后，下一步要做的就是基于Binlog对MySQL数据进行还原。Merge流程做了两件事，首先把当天生成的Binlog数据存放到Delta表中，然后和已有的存量数据做一个基于主键的Merge。Delta表中的数据是当天的最新数据，当一条数据在一天内发生多次变更时，Delta表中只存储最后一次变更后的数据。 把Delta数据和存量数据进行Merge的过程中，需要有唯一键来判定是否是同一条数据。如果同一条数据既出现在存量表中，又出现在Delta表中，说明这一条数据发生了更新，则选取Delta表的数据作为最终结果；否则说明没有发生任何变动，保留原来存量表中的数据作为最终结果。Merge的结果数据会Insert Overwrite到原表中，即图中的origindb.table。 Merge流程举例 下面用一个例子来具体说明Merge的流程。 数据表共id、value两列，其中id是主键。在提取Delta数据时，对同一条数据的多次更新，只选择最后更新的一条。所以对id=1的数据，Delta表中记录最后一条更新后的值value=120。Delta数据和存量数据做Merge后，最终结果中，新插入一条数据（id=4），两条数据发生了更新（id=1和id=2），一条数据未变（id=3）。 默认情况下，我们采用MySQL表的主键作为这一判重的唯一键，业务也可以根据实际情况配置不同于MySQL的唯一键。 上面介绍了基于Binlog的数据采集和ODS数据还原的整体架构。下面主要从两个方面介绍我们解决的实际业务问题。 实践一：分库分表的支持 随着业务规模的扩大，MySQL的分库分表情况越来越多，很多业务的分表数目都在几千个这样的量级。而一般数据开发同学需要把这些数据聚合到一起进行分析。如果对每个分表都进行手动同步，再在Hive上进行聚合，这个成本很难被我们接受。因此，我们需要在ODS层就完成分表的聚合。 首先，在Binlog实时采集时，我们支持把不同DB的Binlog写入到同一个Kafka Topic。用户可以在申请Binlog采集时，同时勾选同一个业务逻辑下的多个物理DB。通过在Binlog采集层的汇集，所有分库的Binlog会写入到同一张Hive表中，这样下游在进行Merge时，依然只需要读取一张Hive表。 第二，Merge任务的配置支持正则匹配。通过配置符合业务分表命名规则的正则表达式，Merge任务就能了解自己需要聚合哪些MySQL表的Binlog，从而选取相应分区的数据来执行。 这样通过两个层面的工作，就完成了分库分表在ODS层的合并。 这里面有一个技术上的优化，在进行Kafka2Hive时，我们按业务分表规则对表名进行了处理，把物理表名转换成了逻辑表名。例如userinfo123这张表名会被转换为userinfo，其Binlog数据存储在original_binlog.user表的table_name=userinfo分区中。这样做的目的是防止过多的HDFS小文件和Hive分区造成的底层压力。 实践二：删除事件的支持 Delete操作在MySQL中非常常见，由于Hive不支持Delete，如果想把MySQL中删除的数据在Hive中删掉，需要采用“迂回”的方式进行。 对需要处理Delete事件的Merge流程，采用如下两个步骤： 首先，提取出发生了Delete事件的数据，由于Binlog本身记录了事件类型，这一步很容易做到。将存量数据（表A）与被删掉的数据（表B）在主键上做左外连接（Left outer join），如果能够全部join到双方的数据，说明该条数据被删掉了。因此，选择结果中表B对应的记录为NULL的数据，即是应当被保留的数据。 然后，对上面得到的被保留下来的数据，按照前面描述的流程做常规的Merge。 总结与展望 作为数据仓库生产的基础，美团数据平台提供的基于Binlog的MySQL2Hive服务，基本覆盖了美团内部的各个业务线，目前已经能够满足绝大部分业务的数据同步需求，实现DB数据准确、高效地入仓。在后面的发展中，我们会集中解决CanalManager的单点问题，并构建跨机房容灾的架构，从而更加稳定地支撑业务的发展。 本文主要从Binlog流式采集和基于Binlog的ODS数据还原两方面，介绍了这一服务的架构，并介绍了我们在实践中遇到的一些典型问题和解决方案。希望能够给其他开发者一些参考价值，同时也欢迎大家和我们一起交流。 欢迎加入美团数据库技术交流群，跟作者零距离交流。进群方式：请加美美同学微信（微信号：MTDPtech02），回复：数据库，美美会自动拉你进群。 活动推荐 《美团技术沙龙第47期·北京：运营效率系统架构演进之道》12月8日周六下午将在望京恒电大厦C座美团点评北京总部1层恒基咖啡举办。精选了供应链、商家赋能和数据智能方面的典型业务和系统，与业内同仁一起分享交流，同时，我们还邀请京东的优秀工程师分享京东在运营方面的经验。 更多活动详情，请戳：活动报名链接 ----------&nbsp; END&nbsp; ---------- 也许你还想看 新一代数据库TiDB在美团的实践 美团点评基于Storm的实时数据处理实践 美团点评基于 Flink 的实时数仓建设实践" />
<link rel="canonical" href="https://mlh.app/2019/04/29/729116.html" />
<meta property="og:url" content="https://mlh.app/2019/04/29/729116.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-29T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"总第310篇 2018年 第102篇 本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入数仓。 By the way，如果你对我们的工作内容比较感兴趣，欢迎扫描并识别下方二维码加入我们，一起致力于解决海量数据采集和传输的问题中来吧！ 扫码查看职位详情，一键投递简历 背景 在数据仓库建模中，未经任何加工处理的原始业务层数据，我们称之为ODS（Operational Data Store）数据。在互联网企业中，常见的ODS数据有业务日志数据（Log）和业务DB数据（DB）两类。对于业务DB数据来说，从MySQL等关系型数据库的业务数据进行采集，然后导入到Hive中，是进行数据仓库生产的重要环节。 如何准确、高效地把MySQL数据同步到Hive中？一般常用的解决方案是批量取数并Load：直连MySQL去Select表中的数据，然后存到本地文件作为中间存储，最后把文件Load到Hive表中。这种方案的优点是实现简单，但是随着业务的发展，缺点也逐渐暴露出来： 性能瓶颈：随着业务规模的增长，Select From MySQL -&gt; Save to Localfile -&gt; Load to Hive这种数据流花费的时间越来越长，无法满足下游数仓生产的时间要求。 直接从MySQL中Select大量数据，对MySQL的影响非常大，容易造成慢查询，影响业务线上的正常服务。 由于Hive本身的语法不支持更新、删除等SQL原语，对于MySQL中发生Update/Delete的数据无法很好地进行支持。 为了彻底解决这些问题，我们逐步转向CDC（Change Data Capture）+ Merge的技术方案，即实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。Binlog是MySQL的二进制日志，记录了MySQL中发生的所有数据变更，MySQL集群自身的主从同步就是基于Binlog做的。 本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入数仓。 整体架构 整体的架构如上图所示。在Binlog实时采集方面，我们采用了阿里巴巴的开源项目Canal，负责从MySQL实时拉取Binlog并完成适当解析。Binlog采集后会暂存到Kafka上供下游消费。整体实时采集部分如图中红色箭头所示。 离线处理Binlog的部分，如图中黑色箭头所示，通过下面的步骤在Hive上还原一张MySQL表： 采用Linkedin的开源项目Camus，负责每小时把Kafka上的Binlog数据拉取到Hive上。 对每张ODS表，首先需要一次性制作快照（Snapshot），把MySQL里的存量数据读取到Hive上，这一过程底层采用直连MySQL去Select数据的方式。 对每张ODS表，每天基于存量数据和当天增量产生的Binlog做Merge，从而还原出业务数据。 我们回过头来看看，背景中介绍的批量取数并Load方案遇到的各种问题，为什么用这种方案能解决上面的问题呢？ 首先，Binlog是流式产生的，通过对Binlog的实时采集，把部分数据处理需求由每天一次的批处理分摊到实时流上。无论从性能上还是对MySQL的访问压力上，都会有明显地改善。 第二，Binlog本身记录了数据变更的类型（Insert/Update/Delete），通过一些语义方面的处理，完全能够做到精准的数据还原。 Binlog实时采集 对Binlog的实时采集包含两个主要模块：一是CanalManager，主要负责采集任务的分配、监控报警、元数据管理以及和外部依赖系统的对接；二是真正执行采集任务的Canal和CanalClient。 当用户提交某个DB的Binlog采集请求时，CanalManager首先会调用DBA平台的相关接口，获取这一DB所在MySQL实例的相关信息，目的是从中选出最适合Binlog采集的机器。然后把采集实例（Canal Instance）分发到合适的Canal服务器上，即CanalServer上。在选择具体的CanalServer时，CanalManager会考虑负载均衡、跨机房传输等因素，优先选择负载较低且同地域传输的机器。 CanalServer收到采集请求后，会在ZooKeeper上对收集信息进行注册。注册的内容包括： 以Instance名称命名的永久节点。 在该永久节点下注册以自身ip:port命名的临时节点。 这样做的目的有两个： 高可用：CanalManager对Instance进行分发时，会选择两台CanalServer，一台是Running节点，另一台作为Standby节点。Standby节点会对该Instance进行监听，当Running节点出现故障后，临时节点消失，然后Standby节点进行抢占。这样就达到了容灾的目的。 与CanalClient交互：CanalClient检测到自己负责的Instance所在的Running CanalServer后，便会进行连接，从而接收到CanalServer发来的Binlog数据。 对Binlog的订阅以MySQL的DB为粒度，一个DB的Binlog对应了一个Kafka Topic。底层实现时，一个MySQL实例下所有订阅的DB，都由同一个Canal Instance进行处理。这是因为Binlog的产生是以MySQL实例为粒度的。CanalServer会抛弃掉未订阅的Binlog数据，然后CanalClient将接收到的Binlog按DB粒度分发到Kafka上。 离线还原MySQL数据 完成Binlog采集后，下一步就是利用Binlog来还原业务数据。首先要解决的第一个问题是把Binlog从Kafka同步到Hive上。 Kafka2Hive 整个Kafka2Hive任务的管理，在美团数据平台的ETL框架下进行，包括任务原语的表达和调度机制等，都同其他ETL类似。而底层采用LinkedIn的开源项目Camus，并进行了有针对性的二次开发，来完成真正的Kafka2Hive数据传输工作。 对Camus的二次开发 Kafka上存储的Binlog未带Schema，而Hive表必须有Schema，并且其分区、字段等的设计，都要便于下游的高效消费。对Camus做的第一个改造，便是将Kafka上的Binlog解析成符合目标Schema的格式。 对Camus做的第二个改造，由美团的ETL框架所决定。在我们的任务调度系统中，目前只对同调度队列的任务做上下游依赖关系的解析，跨调度队列是不能建立依赖关系的。而在MySQL2Hive的整个流程中，Kafka2Hive的任务需要每小时执行一次（小时队列），Merge任务每天执行一次（天队列）。而Merge任务的启动必须要严格依赖小时Kafka2Hive任务的完成。 为了解决这一问题，我们引入了Checkdone任务。Checkdone任务是天任务，主要负责检测前一天的Kafka2Hive是否成功完成。如果成功完成了，则Checkdone任务执行成功，这样下游的Merge任务就可以正确启动了。 Checkdone的检测逻辑 Checkdone是怎样检测的呢？每个Kafka2Hive任务成功完成数据传输后，由Camus负责在相应的HDFS目录下记录该任务的启动时间。Checkdone会扫描前一天的所有时间戳，如果最大的时间戳已经超过了0点，就说明前一天的Kafka2Hive任务都成功完成了，这样Checkdone就完成了检测。 此外，由于Camus本身只是完成了读Kafka然后写HDFS文件的过程，还必须完成对Hive分区的加载才能使下游查询到。因此，整个Kafka2Hive任务的最后一步是加载Hive分区。这样，整个任务才算成功执行。 每个Kafka2Hive任务负责读取一个特定的Topic，把Binlog数据写入original_binlog库下的一张表中，即前面图中的original_binlog.db，其中存储的是对应到一个MySQL DB的全部Binlog。 上图说明了一个Kafka2Hive完成后，文件在HDFS上的目录结构。假如一个MySQL DB叫做user，对应的Binlog存储在original_binlog.user表中。ready目录中，按天存储了当天所有成功执行的Kafka2Hive任务的启动时间，供Checkdone使用。每张表的Binlog，被组织到一个分区中，例如userinfo表的Binlog，存储在table_name=userinfo这一分区中。每个table_name一级分区下，按dt组织二级分区。图中的xxx.lzo和xxx.lzo.index文件，存储的是经过lzo压缩的Binlog数据。 Merge Binlog成功入仓后，下一步要做的就是基于Binlog对MySQL数据进行还原。Merge流程做了两件事，首先把当天生成的Binlog数据存放到Delta表中，然后和已有的存量数据做一个基于主键的Merge。Delta表中的数据是当天的最新数据，当一条数据在一天内发生多次变更时，Delta表中只存储最后一次变更后的数据。 把Delta数据和存量数据进行Merge的过程中，需要有唯一键来判定是否是同一条数据。如果同一条数据既出现在存量表中，又出现在Delta表中，说明这一条数据发生了更新，则选取Delta表的数据作为最终结果；否则说明没有发生任何变动，保留原来存量表中的数据作为最终结果。Merge的结果数据会Insert Overwrite到原表中，即图中的origindb.table。 Merge流程举例 下面用一个例子来具体说明Merge的流程。 数据表共id、value两列，其中id是主键。在提取Delta数据时，对同一条数据的多次更新，只选择最后更新的一条。所以对id=1的数据，Delta表中记录最后一条更新后的值value=120。Delta数据和存量数据做Merge后，最终结果中，新插入一条数据（id=4），两条数据发生了更新（id=1和id=2），一条数据未变（id=3）。 默认情况下，我们采用MySQL表的主键作为这一判重的唯一键，业务也可以根据实际情况配置不同于MySQL的唯一键。 上面介绍了基于Binlog的数据采集和ODS数据还原的整体架构。下面主要从两个方面介绍我们解决的实际业务问题。 实践一：分库分表的支持 随着业务规模的扩大，MySQL的分库分表情况越来越多，很多业务的分表数目都在几千个这样的量级。而一般数据开发同学需要把这些数据聚合到一起进行分析。如果对每个分表都进行手动同步，再在Hive上进行聚合，这个成本很难被我们接受。因此，我们需要在ODS层就完成分表的聚合。 首先，在Binlog实时采集时，我们支持把不同DB的Binlog写入到同一个Kafka Topic。用户可以在申请Binlog采集时，同时勾选同一个业务逻辑下的多个物理DB。通过在Binlog采集层的汇集，所有分库的Binlog会写入到同一张Hive表中，这样下游在进行Merge时，依然只需要读取一张Hive表。 第二，Merge任务的配置支持正则匹配。通过配置符合业务分表命名规则的正则表达式，Merge任务就能了解自己需要聚合哪些MySQL表的Binlog，从而选取相应分区的数据来执行。 这样通过两个层面的工作，就完成了分库分表在ODS层的合并。 这里面有一个技术上的优化，在进行Kafka2Hive时，我们按业务分表规则对表名进行了处理，把物理表名转换成了逻辑表名。例如userinfo123这张表名会被转换为userinfo，其Binlog数据存储在original_binlog.user表的table_name=userinfo分区中。这样做的目的是防止过多的HDFS小文件和Hive分区造成的底层压力。 实践二：删除事件的支持 Delete操作在MySQL中非常常见，由于Hive不支持Delete，如果想把MySQL中删除的数据在Hive中删掉，需要采用“迂回”的方式进行。 对需要处理Delete事件的Merge流程，采用如下两个步骤： 首先，提取出发生了Delete事件的数据，由于Binlog本身记录了事件类型，这一步很容易做到。将存量数据（表A）与被删掉的数据（表B）在主键上做左外连接（Left outer join），如果能够全部join到双方的数据，说明该条数据被删掉了。因此，选择结果中表B对应的记录为NULL的数据，即是应当被保留的数据。 然后，对上面得到的被保留下来的数据，按照前面描述的流程做常规的Merge。 总结与展望 作为数据仓库生产的基础，美团数据平台提供的基于Binlog的MySQL2Hive服务，基本覆盖了美团内部的各个业务线，目前已经能够满足绝大部分业务的数据同步需求，实现DB数据准确、高效地入仓。在后面的发展中，我们会集中解决CanalManager的单点问题，并构建跨机房容灾的架构，从而更加稳定地支撑业务的发展。 本文主要从Binlog流式采集和基于Binlog的ODS数据还原两方面，介绍了这一服务的架构，并介绍了我们在实践中遇到的一些典型问题和解决方案。希望能够给其他开发者一些参考价值，同时也欢迎大家和我们一起交流。 欢迎加入美团数据库技术交流群，跟作者零距离交流。进群方式：请加美美同学微信（微信号：MTDPtech02），回复：数据库，美美会自动拉你进群。 活动推荐 《美团技术沙龙第47期·北京：运营效率系统架构演进之道》12月8日周六下午将在望京恒电大厦C座美团点评北京总部1层恒基咖啡举办。精选了供应链、商家赋能和数据智能方面的典型业务和系统，与业内同仁一起分享交流，同时，我们还邀请京东的优秀工程师分享京东在运营方面的经验。 更多活动详情，请戳：活动报名链接 ----------&nbsp; END&nbsp; ---------- 也许你还想看 新一代数据库TiDB在美团的实践 美团点评基于Storm的实时数据处理实践 美团点评基于 Flink 的实时数仓建设实践","@type":"BlogPosting","url":"https://mlh.app/2019/04/29/729116.html","headline":"美团DB数据同步到数据仓库的架构与实践","dateModified":"2019-04-29T00:00:00+08:00","datePublished":"2019-04-29T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/04/29/729116.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>美团DB数据同步到数据仓库的架构与实践</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <p style="white-space: normal;margin-left: 0em;margin-right: 0em;" data-mpa-powered-by="yiban.io"><img class="" data-copyright="0" data-ratio="0.10546875" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsV6LYkM3uK5TAnl8DxXwdR4zA3FUoOfW6b1icLsE77CELpkNLzriajHTdibqkqVFYoldIoffibgkOslZA/640?wx_fmt=png" data-type="png" data-w="1280" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsV6LYkM3uK5TAnl8DxXwdR4zA3FUoOfW6b1icLsE77CELpkNLzriajHTdibqkqVFYoldIoffibgkOslZA/640?wx_fmt=png"></p> 
<p style="white-space: normal;text-align: center;margin-left: 0em;margin-right: 0em;"><strong><span style="color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 1px;">总第310篇</span></strong></p> 
<p style="white-space: normal;text-align: center;margin-left: 0em;margin-right: 0em;"><strong><span style="color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 1px;">2018年 第102篇</span></strong></p> 
<p style="white-space: normal;text-align: center;margin-left: 0em;margin-right: 0em;"><strong><span style="color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 1px;"><br></span></strong></p> 
<section data-role="outer" label="Powered by 135editor.com" style="font-size: 16px;font-family: 微软雅黑;margin-left: 0em;margin-right: 0em;"> 
 <section data-role="outer" label="Powered by 135editor.com"> 
  <section class="_135editor" data-tools="135编辑器" data-id="127" style="border-width: 0px;border-style: none;border-color: initial;"> 
   <section class="_135editor" data-tools="135编辑器" data-id="127" style="border-width: 0px;border-style: none;border-color: initial;"> 
    <section style="margin: 60px 16px 16px;border-width: 1px;border-style: solid;border-color: rgb(235, 234, 225);text-align: center;border-radius: 8px;font-weight: inherit;text-decoration: inherit;"> 
     <section style="font-size: 18px;margin-top: -3.3em;margin-right: 5px;margin-left: 5px;color: inherit;"> 
      <section style="border-width: 2px;border-style: solid;border-color: rgb(235, 234, 225);width: 108px;clear: both;margin-right: auto;margin-left: auto;height: 108px;border-radius: 50%;box-shadow: rgb(201, 201, 201) 0px 2px 2px 2px;background-color: rgb(254, 254, 254);"> 
       <img border="0" class="" data-ratio="1" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsXnTI7oawW8yYMfCd5oyEEGpLQfCPYP5xPHgGAG5TqXoXCCanURVtuReGYV3hKwOcVicnkrGaUeLrA/640?wx_fmt=jpeg" data-type="jpeg" data-w="800" data-width="100%" height="84" opacity="" style="border-radius: 50%;color: inherit;display: inline-block;height: 84px;width: 84px;" title="undefined" width="84" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsXnTI7oawW8yYMfCd5oyEEGpLQfCPYP5xPHgGAG5TqXoXCCanURVtuReGYV3hKwOcVicnkrGaUeLrA/640?wx_fmt=jpeg"> 
      </section> 
     </section> 
     <section class="135brush" data-brushtype="text" data-style="text-align: left; font-size: 14px; color: inherit;" style="font-size: 18px;margin: 8px 15px;line-height: 1.4;color: inherit;"> 
      <p style="margin-right: 0em;margin-left: 0em;text-align: left;"><span style="font-size: 13px;color: rgb(127, 127, 127);">本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入数仓。</span></p> 
      <p style="margin-top: 10px;margin-right: 0em;margin-left: 0em;text-align: left;"><span style="font-size: 13px;color: rgb(127, 127, 127);">By the way，如果你对我们的工作内容比较感兴趣，欢迎扫描并识别下方二维码加入我们，一起致力于解决海量数据采集和传输的问题中来吧！</span></p> 
     </section> 
     <section style="margin: 10px 15px;border-top: 1px solid rgb(235, 234, 225);border-right-color: rgb(235, 234, 225);border-bottom-color: rgb(235, 234, 225);border-left-color: rgb(235, 234, 225);color: inherit;"> 
      <p style="color: inherit;"><strong><span style="font-size: 12.6px;letter-spacing: 1.5px;color: #25b7a7;">扫码查看职位详情，一键投递简历</span></strong></p> 
     </section> 
     <span style="font-size: 18px;margin-bottom: 5.4px;"><img class="" data-ratio="0.9726962457337884" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsXnTI7oawW8yYMfCd5oyEEGAVH6icAeZKCyibhcEjNmmicVB3pqhyWe1ZSo1XyLviaT5RaeiclC0MtgNiaQ/640?wx_fmt=jpeg" data-type="jpeg" data-w="293" height="89" style="color: inherit;margin-bottom: 5.4px;width: 92px;height: 89px;visibility: visible !important;" title="WechatIMG3.jpeg" width="92" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsXnTI7oawW8yYMfCd5oyEEGAVH6icAeZKCyibhcEjNmmicVB3pqhyWe1ZSo1XyLviaT5RaeiclC0MtgNiaQ/640?wx_fmt=jpeg"></span> 
    </section> 
   </section> 
  </section> 
 </section> 
</section> 
<section class="output_wrapper" style="margin-left: 0em;margin-right: 0em;"> 
 <h2><span style="color: rgb(37, 183, 167);font-size: 20px;"><strong>背景</strong></span></h2> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">在数据仓库建模中，未经任何加工处理的原始业务层数据，我们称之为ODS（</span><span style="font-size: 15px;color: rgb(136, 136, 136);">Operational Data Store</span><span style="font-size: 15px;">）数据。在互联网企业中，常见的ODS数据有业务日志数据（</span><span style="font-size: 15px;color: rgb(136, 136, 136);">Log</span><span style="font-size: 15px;">）和业务DB数据（</span><span style="font-size: 15px;color: rgb(136, 136, 136);">DB</span><span style="font-size: 15px;">）两类。对于业务DB数据来说，从MySQL等关系型数据库的业务数据进行采集，然后导入到Hive中，是进行数据仓库生产的重要环节。</span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">如何准确、高效地把MySQL数据同步到Hive中？一般常用的解决方案是批量取数并Load：直连MySQL去Select表中的数据，然后存到本地文件作为中间存储，最后把文件Load到Hive表中。这种方案的优点是实现简单，但是随着业务的发展，缺点也逐渐暴露出来：</span></p> 
 <ul style="" class=" list-paddingleft-2"> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">性能瓶颈：随着业务规模的增长，Select From MySQL -&gt; Save to Localfile -&gt; Load to Hive这种数据流花费的时间越来越长，无法满足下游数仓生产的时间要求。</span></p></li> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">直接从MySQL中Select大量数据，对MySQL的影响非常大，容易造成慢查询，影响业务线上的正常服务。</span></p></li> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">由于Hive本身的语法不支持更新、删除等SQL原语，对于MySQL中发生Update/Delete的数据无法很好地进行支持。</span></p></li> 
 </ul> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">为了彻底解决这些问题，我们逐步转向CDC（</span><span style="font-size: 15px;color: rgb(136, 136, 136);">Change Data Capture</span><span style="font-size: 15px;">）+ Merge的技术方案，即实时Binlog采集 + 离线处理Binlog还原业务数据这样一套解决方案。Binlog是MySQL的二进制日志，记录了MySQL中发生的所有数据变更，MySQL集群自身的主从同步就是基于Binlog做的。</span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">本文主要从Binlog实时采集和离线处理Binlog还原业务数据两个方面，来介绍如何实现DB数据准确、高效地进入数仓。</span></p> 
 <h2><span style="font-size: 15px;"><br></span></h2> 
 <h2><span style="color: rgb(37, 183, 167);font-size: 20px;"><strong>整体架构</strong></span></h2> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="1.1591355599214146" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0FtGGwyzWt9KgFmexk5B8oGBZTVyKAmG9ibcYMC0rP11YFib6vecicaxew/640?wx_fmt=png" data-type="png" data-w="509" style="" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0FtGGwyzWt9KgFmexk5B8oGBZTVyKAmG9ibcYMC0rP11YFib6vecicaxew/640?wx_fmt=png"></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">整体的架构如上图所示。在Binlog实时采集方面，我们采用了阿里巴巴的开源项目Canal，负责从MySQL实时拉取Binlog并完成适当解析。Binlog采集后会暂存到Kafka上供下游消费。整体实时采集部分如图中红色箭头所示。</span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">离线处理Binlog的部分，如图中黑色箭头所示，通过下面的步骤在Hive上还原一张MySQL表：</span></p> 
 <ol style="" class=" list-paddingleft-2"> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">采用Linkedin的开源项目Camus，负责每小时把Kafka上的Binlog数据拉取到Hive上。</span></p></li> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">对每张ODS表，首先需要一次性制作快照（</span><span style="line-height: inherit;font-size: 15px;color: rgb(136, 136, 136);">Snapshot</span><span style="color: inherit;line-height: inherit;font-size: 15px;">），把MySQL里的存量数据读取到Hive上，这一过程底层采用直连MySQL去Select数据的方式。</span></p></li> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">对每张ODS表，每天基于存量数据和当天增量产生的Binlog做Merge，从而还原出业务数据。</span></p></li> 
 </ol> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">我们回过头来看看，背景中介绍的批量取数并Load方案遇到的各种问题，为什么用这种方案能解决上面的问题呢？</span></p> 
 <ul style="" class=" list-paddingleft-2"> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">首先，Binlog是流式产生的，通过对Binlog的实时采集，把部分数据处理需求由每天一次的批处理分摊到实时流上。无论从性能上还是对MySQL的访问压力上，都会有明显地改善。</span></p></li> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">第二，Binlog本身记录了数据变更的类型（</span><span style="line-height: inherit;font-size: 15px;color: rgb(136, 136, 136);">Insert/Update/Delete</span><span style="color: inherit;line-height: inherit;font-size: 15px;">），通过一些语义方面的处理，完全能够做到精准的数据还原。</span></p></li> 
 </ul> 
 <h2><span style="font-size: 15px;"><br></span></h2> 
 <h2><span style="color: rgb(37, 183, 167);font-size: 20px;"><strong>Binlog实时采集</strong></span></h2> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">对Binlog的实时采集包含两个主要模块：一是CanalManager，主要负责采集任务的分配、监控报警、元数据管理以及和外部依赖系统的对接；二是真正执行采集任务的Canal和CanalClient。</span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.5220040622884224" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0icRdeDx5fLSRF0C4H2EfTpEibVXDg7Cu7yKHad1Zc0FfEYgvFVN7nOzg/640?wx_fmt=png" data-type="png" data-w="1477" style="" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0icRdeDx5fLSRF0C4H2EfTpEibVXDg7Cu7yKHad1Zc0FfEYgvFVN7nOzg/640?wx_fmt=png"></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">当用户提交某个DB的Binlog采集请求时，CanalManager首先会调用DBA平台的相关接口，获取这一DB所在MySQL实例的相关信息，目的是从中选出最适合Binlog采集的机器。然后把采集实例（</span><span style="font-size: 15px;color: rgb(136, 136, 136);">Canal Instance</span><span style="font-size: 15px;">）分发到合适的Canal服务器上，即CanalServer上。在选择具体的CanalServer时，CanalManager会考虑负载均衡、跨机房传输等因素，优先选择负载较低且同地域传输的机器。</span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">CanalServer收到采集请求后，会在ZooKeeper上对收集信息进行注册。注册的内容包括：</span></p> 
 <ul style="" class=" list-paddingleft-2"> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">以Instance名称命名的永久节点。</span></p></li> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">在该永久节点下注册以自身ip:port命名的临时节点。</span></p></li> 
 </ul> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">这样做的目的有两个：</span></p> 
 <ul style="" class=" list-paddingleft-2"> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">高可用：CanalManager对Instance进行分发时，会选择两台CanalServer，一台是Running节点，另一台作为Standby节点。Standby节点会对该Instance进行监听，当Running节点出现故障后，临时节点消失，然后Standby节点进行抢占。这样就达到了容灾的目的。</span></p></li> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">与CanalClient交互：CanalClient检测到自己负责的Instance所在的Running CanalServer后，便会进行连接，从而接收到CanalServer发来的Binlog数据。</span></p></li> 
 </ul> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">对Binlog的订阅以MySQL的DB为粒度，一个DB的Binlog对应了一个Kafka Topic。底层实现时，一个MySQL实例下所有订阅的DB，都由同一个Canal Instance进行处理。这是因为Binlog的产生是以MySQL实例为粒度的。CanalServer会抛弃掉未订阅的Binlog数据，然后CanalClient将接收到的Binlog按DB粒度分发到Kafka上。</span></p> 
 <h2><span style="font-size: 15px;"><br></span></h2> 
 <h2><span style="color: rgb(37, 183, 167);font-size: 20px;"><strong>离线还原MySQL数据</strong></span></h2> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">完成Binlog采集后，下一步就是利用Binlog来还原业务数据。首先要解决的第一个问题是把Binlog从Kafka同步到Hive上。</span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.5766509433962265" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0knNWYEams3ZgxVviccoNJOVKeNxLVbfRYf7wxdcQzA6mvzpNF3xRUGg/640?wx_fmt=png" data-type="png" data-w="848" style="" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0knNWYEams3ZgxVviccoNJOVKeNxLVbfRYf7wxdcQzA6mvzpNF3xRUGg/640?wx_fmt=png"></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><br></p> 
 <h3><span style="font-size: 18px;"><strong>Kafka2Hive</strong></span></h3> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">整个Kafka2Hive任务的管理，在美团数据平台的ETL框架下进行，包括任务原语的表达和调度机制等，都同其他ETL类似。而底层采用LinkedIn的开源项目Camus，并进行了有针对性的二次开发，来完成真正的Kafka2Hive数据传输工作。</span></p> 
 <h4><span style="font-size: 15px;"><br></span></h4> 
 <h4><span style="font-size: 16px;"><strong>对Camus的二次开发</strong></span></h4> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">Kafka上存储的Binlog未带Schema，而Hive表必须有Schema，并且其分区、字段等的设计，都要便于下游的高效消费。对Camus做的第一个改造，便是将Kafka上的Binlog解析成符合目标Schema的格式。</span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">对Camus做的第二个改造，由美团的ETL框架所决定。在我们的任务调度系统中，目前只对同调度队列的任务做上下游依赖关系的解析，跨调度队列是不能建立依赖关系的。而在MySQL2Hive的整个流程中，Kafka2Hive的任务需要每小时执行一次（</span><span style="font-size: 15px;color: rgb(136, 136, 136);">小时队列</span><span style="font-size: 15px;">），Merge任务每天执行一次（</span><span style="font-size: 15px;color: rgb(136, 136, 136);">天队列</span><span style="font-size: 15px;">）。而Merge任务的启动必须要严格依赖小时Kafka2Hive任务的完成。</span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">为了解决这一问题，我们引入了Checkdone任务。Checkdone任务是天任务，主要负责检测前一天的Kafka2Hive是否成功完成。如果成功完成了，则Checkdone任务执行成功，这样下游的Merge任务就可以正确启动了。</span></p> 
 <h4><span style="font-size: 15px;"><br></span></h4> 
 <h4><span style="font-size: 16px;"><strong>Checkdone的检测逻辑</strong></span></h4> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">Checkdone是怎样检测的呢？每个Kafka2Hive任务成功完成数据传输后，由Camus负责在相应的HDFS目录下记录该任务的启动时间。Checkdone会扫描前一天的所有时间戳，如果最大的时间戳已经超过了0点，就说明前一天的Kafka2Hive任务都成功完成了，这样Checkdone就完成了检测。</span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">此外，由于Camus本身只是完成了读Kafka然后写HDFS文件的过程，还必须完成对Hive分区的加载才能使下游查询到。因此，整个Kafka2Hive任务的最后一步是加载Hive分区。这样，整个任务才算成功执行。</span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">每个Kafka2Hive任务负责读取一个特定的Topic，把Binlog数据写入original_binlog库下的一张表中，即前面图中的original_binlog.<em style="font-size: inherit;color: inherit;line-height: inherit;">db</em>，其中存储的是对应到一个MySQL DB的全部Binlog。</span></p> 
 <p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.5038759689922481" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0AcfdiahGib55F0D5A6N9icCcvqgia3dptCDsDf5qn39w4v4tic6TPrgibboQ/640?wx_fmt=jpeg" data-type="jpeg" data-w="1032" style="" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0AcfdiahGib55F0D5A6N9icCcvqgia3dptCDsDf5qn39w4v4tic6TPrgibboQ/640?wx_fmt=jpeg"></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">上图说明了一个Kafka2Hive完成后，文件在HDFS上的目录结构。假如一个MySQL DB叫做user，对应的Binlog存储在original_binlog.user表中。ready目录中，按天存储了当天所有成功执行的Kafka2Hive任务的启动时间，供Checkdone使用。每张表的Binlog，被组织到一个分区中，例如userinfo表的Binlog，存储在table_name=userinfo这一分区中。每个table_name一级分区下，按dt组织二级分区。图中的xxx.lzo和xxx.lzo.index文件，存储的是经过lzo压缩的Binlog数据。</span></p> 
 <h3><span style="font-size: 18px;"><strong>Merge</strong></span></h3> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">Binlog成功入仓后，下一步要做的就是基于Binlog对MySQL数据进行还原。Merge流程做了两件事，首先把当天生成的Binlog数据存放到Delta表中，然后和已有的存量数据做一个基于主键的Merge。Delta表中的数据是当天的最新数据，当一条数据在一天内发生多次变更时，Delta表中只存储最后一次变更后的数据。</span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">把Delta数据和存量数据进行Merge的过程中，需要有唯一键来判定是否是同一条数据。如果同一条数据既出现在存量表中，又出现在Delta表中，说明这一条数据发生了更新，则选取Delta表的数据作为最终结果；否则说明没有发生任何变动，保留原来存量表中的数据作为最终结果。Merge的结果数据会Insert Overwrite到原表中，即图中的origindb.<em style="font-size: inherit;color: inherit;line-height: inherit;">table</em>。</span></p> 
 <h4><span style="font-size: 15px;"><br></span></h4> 
 <h4><span style="font-size: 16px;"><strong>Merge流程举例</strong></span></h4> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">下面用一个例子来具体说明Merge的流程。</span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.6800501882057717" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0fIdBwnnXXpaAHzWMUZ8A5h6HF7ibiaUKicgzag7su46duoDDkzlUzmy0w/640?wx_fmt=png" data-type="png" data-w="797" style="" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0fIdBwnnXXpaAHzWMUZ8A5h6HF7ibiaUKicgzag7su46duoDDkzlUzmy0w/640?wx_fmt=png"></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">数据表共id、value两列，其中id是主键。在提取Delta数据时，对同一条数据的多次更新，只选择最后更新的一条。所以对id=1的数据，Delta表中记录最后一条更新后的值value=120。Delta数据和存量数据做Merge后，最终结果中，新插入一条数据（</span><span style="font-size: 15px;color: rgb(136, 136, 136);">id=4</span><span style="font-size: 15px;">），两条数据发生了更新（</span><span style="font-size: 15px;color: rgb(136, 136, 136);">id=1和id=2</span><span style="font-size: 15px;">），一条数据未变（</span><span style="font-size: 15px;color: rgb(136, 136, 136);">id=3</span><span style="font-size: 15px;">）。</span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">默认情况下，我们采用MySQL表的主键作为这一判重的唯一键，业务也可以根据实际情况配置不同于MySQL的唯一键。</span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">上面介绍了基于Binlog的数据采集和ODS数据还原的整体架构。下面主要从两个方面介绍我们解决的实际业务问题。</span></p> 
 <h2><span style="font-size: 15px;"><br></span></h2> 
 <h2><span style="color: rgb(0, 0, 0);font-size: 18px;"><strong>实践一：分库分表的支持</strong></span></h2> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">随着业务规模的扩大，MySQL的分库分表情况越来越多，很多业务的分表数目都在几千个这样的量级。而一般数据开发同学需要把这些数据聚合到一起进行分析。如果对每个分表都进行手动同步，再在Hive上进行聚合，这个成本很难被我们接受。因此，我们需要在ODS层就完成分表的聚合。</span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="1.0668953687821612" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0veZpmTtUWn0rmM342ll7VNPPhB88cQRE8TMDNr70hoJ7lEV7oO6OVA/640?wx_fmt=png" data-type="png" data-w="583" style="" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck0veZpmTtUWn0rmM342ll7VNPPhB88cQRE8TMDNr70hoJ7lEV7oO6OVA/640?wx_fmt=png"></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">首先，在Binlog实时采集时，我们支持把不同DB的Binlog写入到同一个Kafka Topic。用户可以在申请Binlog采集时，同时勾选同一个业务逻辑下的多个物理DB。通过在Binlog采集层的汇集，所有分库的Binlog会写入到同一张Hive表中，这样下游在进行Merge时，依然只需要读取一张Hive表。</span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">第二，Merge任务的配置支持正则匹配。通过配置符合业务分表命名规则的正则表达式，Merge任务就能了解自己需要聚合哪些MySQL表的Binlog，从而选取相应分区的数据来执行。</span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">这样通过两个层面的工作，就完成了分库分表在ODS层的合并。</span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">这里面有一个技术上的优化，在进行Kafka2Hive时，我们按业务分表规则对表名进行了处理，把物理表名转换成了逻辑表名。例如userinfo123这张表名会被转换为userinfo，其Binlog数据存储在original_binlog.user表的table_name=userinfo分区中。这样做的目的是防止过多的HDFS小文件和Hive分区造成的底层压力。</span></p> 
 <h2><span style="font-size: 15px;"><br></span></h2> 
 <h2><span style="font-size: 18px;color: rgb(0, 0, 0);"><strong>实践二：删除事件的支持</strong></span></h2> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">Delete操作在MySQL中非常常见，由于Hive不支持Delete，如果想把MySQL中删除的数据在Hive中删掉，需要采用“迂回”的方式进行。</span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">对需要处理Delete事件的Merge流程，采用如下两个步骤：</span></p> 
 <ul style="" class=" list-paddingleft-2"> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">首先，提取出发生了Delete事件的数据，由于Binlog本身记录了事件类型，这一步很容易做到。将存量数据（</span><span style="line-height: inherit;font-size: 15px;color: rgb(136, 136, 136);">表A</span><span style="color: inherit;line-height: inherit;font-size: 15px;">）与被删掉的数据（</span><span style="line-height: inherit;font-size: 15px;color: rgb(136, 136, 136);">表B</span><span style="color: inherit;line-height: inherit;font-size: 15px;">）在主键上做左外连接（</span><span style="line-height: inherit;font-size: 15px;color: rgb(136, 136, 136);">Left outer join</span><span style="color: inherit;line-height: inherit;font-size: 15px;">），如果能够全部join到双方的数据，说明该条数据被删掉了。因此，选择结果中表B对应的记录为NULL的数据，即是应当被保留的数据。</span></p></li> 
  <li><p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="color: inherit;line-height: inherit;font-size: 15px;">然后，对上面得到的被保留下来的数据，按照前面描述的流程做常规的Merge。</span></p></li> 
 </ul> 
 <h2><span style="font-size: 15px;"><br></span></h2> 
 <p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.9235751295336787" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck001G66vGxrJ251w3V0ib45t7bAVEujyK39NKxlMtTZNspiaSejHbzWHQw/640?wx_fmt=png" data-type="png" data-w="772" style="" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVtCpmPKkdiaDK3WfZwlqck001G66vGxrJ251w3V0ib45t7bAVEujyK39NKxlMtTZNspiaSejHbzWHQw/640?wx_fmt=png"></p> 
 <h2><span style="color: rgb(37, 183, 167);font-size: 20px;"><strong>总结与展望</strong></span></h2> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"><br></span></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">作为数据仓库生产的基础，美团数据平台提供的基于Binlog的MySQL2Hive服务，基本覆盖了美团内部的各个业务线，目前已经能够满足绝大部分业务的数据同步需求，实现DB数据准确、高效地入仓。在后面的发展中，我们会集中解决CanalManager的单点问题，并构建跨机房容灾的架构，从而更加稳定地支撑业务的发展。</span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;">本文主要从Binlog流式采集和基于Binlog的ODS数据还原两方面，介绍了这一服务的架构，并介绍了我们在实践中遇到的一些典型问题和解决方案。希望能够给其他开发者一些参考价值，同时也欢迎大家和我们一起交流。</span></p> 
 <p style="font-size: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="font-size: 15px;"><span style="font-size: 15px;"><span style="color: rgb(136, 136, 136);font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;text-align: justify;background-color: rgb(255, 255, 255);">欢迎加入<strong style="color: rgb(51, 51, 51);"><span style="color: rgb(0, 0, 0);">美团数据库技术交流群</span></strong>，跟作者零距离交流。进群方式：请加美美同学<span style="letter-spacing: 0px;">微信（微信号：</span></span><span style="font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;letter-spacing: 0px;text-align: justify;background-color: rgb(255, 255, 255);"><strong style="color: rgb(136, 136, 136);">MTDPtech02</strong><strong style="color: rgb(136, 136, 136);">）</strong><span style="color:#888888;">，回复：</span><strong>数据库</strong></span></span><span style="color: rgb(136, 136, 136);letter-spacing: 0px;font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;text-align: justify;font-size: 15px;background-color: rgb(255, 255, 255);">，美美会自动拉你进群。</span></span></p> 
 <p style="font-size: inherit;color: inherit;line-height: inherit;margin: 1.5em 0.5em;"><span style="color: rgb(37, 183, 167);font-size: 18px;"><strong>活动推荐</strong></span></p> 
 <p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.5921875" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsX9YCOWAxribJWwL0jhxT5ID6WIs2p04IdOYv9SNMBc2DJuRicVay1aV1N5uuribhVia5Cm7KVplsIz0A/640?wx_fmt=jpeg" data-type="jpeg" data-w="1280" style="" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsX9YCOWAxribJWwL0jhxT5ID6WIs2p04IdOYv9SNMBc2DJuRicVay1aV1N5uuribhVia5Cm7KVplsIz0A/640?wx_fmt=jpeg"></p> 
 <p class="p1" style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;"></span><br></p> 
 <p class="p1" style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">《美团技术沙龙第47期·北京：运营效率系统架构演进之道》12月8日周六下午将在望京恒电大厦C座美团点评北京总部1层恒基咖啡举办。精选了供应链、商家赋能和数据智能方面的典型业务和系统，与业内同仁一起分享交流，同时，我们还邀请京东的优秀工程师分享京东在运营方面的经验。</span></p> 
 <p class="p1" style="margin-left: 0.5em;margin-right: 0.5em;"><br></p> 
 <p class="p1" style="margin-left: 0.5em;margin-right: 0.5em;"><strong><span style="font-size: 15px;">更多活动详情，请戳：</span><span style="font-size: 15px;"><a href="http://www.huodongxing.com/event/1467374863911?qd=weixin" target="_blank">活动报名链接</a></span></strong></p> 
 <p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="letter-spacing: 0px;font-family: -apple-system-font, system-ui, &quot;Helvetica Neue&quot;, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei UI&quot;, &quot;Microsoft YaHei&quot;, Arial, sans-serif;text-align: justify;font-size: 15px;color: rgb(136, 136, 136);background-color: rgb(255, 255, 255);"><br></span></p> 
</section> 
<p style="white-space: normal;color: rgb(51, 51, 51);text-align: center;margin-left: 0em;margin-right: 0em;"><span style="font-size: 15px;color: rgb(136, 136, 136);">----------&nbsp; END&nbsp; ----------</span></p> 
<p data-source-line="194" style="white-space: normal;margin-left: 0em;margin-right: 0em;"><br></p> 
<p style="white-space: normal;margin-left: 0em;margin-right: 0em;"><span style="font-size: 16px;"><strong><span style="color: rgb(49, 188, 173);">也许你还想看</span></strong></span></p> 
<p style="white-space: normal;margin-left: 0em;margin-right: 0em;"><span style="font-size: 16px;"><strong><span style="color: rgb(49, 188, 173);"><br></span></strong></span></p> 
<p style="white-space: normal;margin-left: 0em;margin-right: 0em;"><a href="http://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651749520&amp;idx=3&amp;sn=2390b6e44b871ace46ce2d678198d3d9&amp;chksm=bd12a5dd8a652ccb1d84feddce4849e996b3c3cac50f84b8bd4b7b06cbae24c1079f4b0dc2ea&amp;scene=21#wechat_redirect" target="_blank" style="text-decoration: underline;font-size: 14px;"><span style="font-size: 14px;">新一代数据库TiDB在美团的实践</span></a></p> 
<p style="white-space: normal;margin-left: 0em;margin-right: 0em;"><a href="http://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651747457&amp;idx=2&amp;sn=9e28ee0d12f3ed3dede92e48f8e43d0f&amp;chksm=bd12adcc8a6524daebe7aa3e8d56a8ec94d7413e6651a12886a424bb7bdecc2b888dd8b6b0c2&amp;scene=21#wechat_redirect" target="_blank" style="text-decoration: underline;font-size: 14px;"><span style="font-size: 14px;">美团点评基于Storm的实时数据处理实践</span></a></p> 
<p style="white-space: normal;margin-left: 0em;margin-right: 0em;"><a href="http://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651749037&amp;idx=1&amp;sn=4a448647b3dae50779bc9ec0e9c10275&amp;chksm=bd12a3e08a652af6ed8b305b0523716e08a81cf99296425cdaf2bbee1e9d8a6aca06c81cdcc1&amp;scene=21#wechat_redirect" target="_blank" style="font-size: 14px;text-decoration: underline;"><span style="font-size: 14px;">美团点评基于 Flink 的实时数仓建设实践</span></a><br></p> 
<p style="white-space: normal;margin-left: 0em;margin-right: 0em;"><br><span style="font-size: 15px;"></span></p> 
<p style="white-space: normal;text-align: center;margin-left: 0em;margin-right: 0em;"><img class="" data-copyright="0" data-ratio="0.44533333333333336" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsV6LYkM3uK5TAnl8DxXwdR4YOAKWmYSpAtzV3P359bDG3cn3Vr4T6HMkvDSI8icUYsejmDnfa5CdpQ/640?wx_fmt=png" data-type="png" data-w="1875" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsV6LYkM3uK5TAnl8DxXwdR4YOAKWmYSpAtzV3P359bDG3cn3Vr4T6HMkvDSI8icUYsejmDnfa5CdpQ/640?wx_fmt=png"></p>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
