<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>大众点评搜索基于知识图谱的深度学习排序实践 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="大众点评搜索基于知识图谱的深度学习排序实践" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="总第327篇 2019年 第005篇 本文介绍了大众点评搜索核心排序层模型的演化之路，包括结合知识图谱信息构建适合搜索场景的Listwise深度学习排序模型LambdaDNN以及特征工程实践和相关工具建设。 1. 引言 挑战与思路 搜索是大众点评App上用户进行信息查找的最大入口，是连接用户和信息的重要纽带。而用户搜索的方式和场景非常多样，并且由于对接业务种类多，流量差异大，为大众点评搜索（下文简称点评搜索）带来了巨大的挑战，具体体现在如下几个方面： 意图多样：用户查找的信息类型和方式多样。信息类型包括POI、榜单、UGC、攻略、达人等。以找店为例，查找方式包括按距离、按热度、按菜品和按地理位置等多种方式。例如用户按照品牌进行搜索时，大概率是需要寻找距离最近或者常去的某家分店；但用户搜索菜品时，会对菜品推荐人数更加敏感，而距离因素会弱化。 业务多样：不同业务之间，用户的使用频率、选择难度以及业务诉求均不一样。例如家装场景用户使用频次很低，行为非常稀疏，距离因素弱，并且选择周期可能会很长；而美食多为即时消费场景，用户行为数据多，距离敏感。 用户类型多样：不同的用户对价格、距离、口味以及偏好的类目之间差异很大；搜索需要能深度挖掘到用户的各种偏好，实现定制化的“千人千面”的搜索。 LBS的搜索：相比电商和通用搜索，LBS的升维效应极大地增加了搜索场景的复杂性。例如对于旅游用户和常驻地用户来说，前者在搜索美食的时候可能会更加关心当地的知名特色商户，而对于距离相对不敏感。 上述的各项特性，叠加上时间、空间、场景等维度，使得点评搜索面临比通用搜索引擎更加独特的挑战。而解决这些挑战的方法，就需要升级NLP（Natural Language Processing，自然语言处理）技术，进行深度查询理解以及深度评价分析，并依赖知识图谱技术和深度学习技术对搜索架构进行整体升级。在美团NLP中心以及大众点评搜索智能中心两个团队的紧密合作之下，经过短短半年时间，点评搜索核心KPI在高位基础上仍然大幅提升，是过去一年半涨幅的六倍之多，提前半年完成全年目标。 基于知识图谱的搜索架构重塑 美团NLP中心正在构建全世界最大的餐饮娱乐知识图谱——美团大脑（相关信息请参见《美团大脑：知识图谱的建模方法及其应用》）。它充分挖掘关联各个场景数据，用NLP技术让机器“阅读”用户公开评论，理解用户在菜品、价格、服务、环境等方面的喜好，构建人、店、商品、场景之间的知识关联，从而形成一个“知识大脑”[1]。通过将知识图谱信息加入到搜索各个流程中，我们对点评搜索的整体架构进行了升级重塑，图1为点评搜索基于知识图谱搭建的5层搜索架构。本篇文章是“美团大脑”系列文章第二篇（系列首篇文章请参见《美团餐饮娱乐知识图谱——美团大脑揭秘》），主要介绍点评搜索5层架构中核心排序层的演变过程，文章主要分为如下3个部分： 核心排序从传统机器学习模型到大规模深度学习模型的演进。 搜索场景深度学习排序模型的特征工程实践。 适用于搜索场景的深度学习Listwise排序算法——LambdaDNN。 图1 基于知识图谱的点评搜索5层架构 2. 排序模型探索与实践 搜索排序问题在机器学习领域有一个单独的分支，Learning to Rank（L2R）。主要分类如下： 根据样本生成方法和Loss Function的不同，L2R可以分为Pointwise、Pairwise、Listwise。 按照模型结构划分，可以分为线性排序模型、树模型、深度学习模型，它们之间的组合（GBDT+LR，Deep&amp;Wide等）。 在排序模型方面，点评搜索也经历了业界比较普遍的迭代过程：从早期的线性模型LR，到引入自动二阶交叉特征的FM和FFM，到非线性树模型GBDT和GBDT+LR，到最近全面迁移至大规模深度学习排序模型。下面先简单介绍下传统机器学习模型（LR、FM、GBDT）的应用和优缺点，然后详细介绍深度模型的探索实践过程。 传统机器学习模型 图2 几种传统机器学习模型结构 LR可以视作单层单节点的线性网络结构。模型优点是可解释性强。通常而言，良好的解释性是工业界应用实践比较注重的一个指标，它意味着更好的可控性，同时也能指导工程师去分析问题优化模型。但是LR需要依赖大量的人工特征挖掘投入，有限的特征组合自然无法提供较强的表达能力。 FM可以看做是在LR的基础上增加了一部分二阶交叉项。引入自动的交叉特征有助于减少人工挖掘的投入，同时增加模型的非线性，捕捉更多信息。FM能够自动学习两两特征间的关系，但更高量级的特征交叉仍然无法满足。 GBDT是一个Boosting的模型，通过组合多个弱模型逐步拟合残差得到一个强模型。树模型具有天然的优势，能够很好的挖掘组合高阶统计特征，兼具较优的可解释性。GBDT的主要缺陷是依赖连续型的统计特征，对于高维度稀疏特征、时间序列特征不能很好的处理。 深度神经网络模型 随着业务的发展，在传统模型上取得指标收益变得愈发困难。同时业务的复杂性要求我们引入海量用户历史数据，超大规模知识图谱特征等多维度信息源，以实现精准个性化的排序。因此我们从2018年下半年开始，全力推进L2核心排序层的主模型迁移至深度学习排序模型。深度模型优势体现在如下几个方面： 强大的模型拟合能力：深度学习网络包含多个隐藏层和隐藏结点，配合上非线性的激活函数，理论上可以拟合任何函数，因此十分适用于点评搜索这种复杂的场景。 强大的特征表征和泛化能力：深度学习模型可以处理很多传统模型无法处理的特征。例如深度网络可以直接中从海量训练样本中学习到高维稀疏ID的隐含信息，并通过Embedding的方式去表征；另外对于文本、序列特征以及图像特征，深度网络均有对应的结构或者单元去处理。 自动组合和发现特征的能力：华为提出的DeepFM，以及Google提出的DeepCrossNetwork可以自动进行特征组合，代替大量人工组合特征的工作。 下图是我们基于Google提出的Wide&amp;Deep模型搭建的网络结构[2]。其中Wide部分输入的是LR、GBDT阶段常用的一些细粒度统计特征。通过较长周期统计的高频行为特征，能够提供很好的记忆能力。Deep部分通过深层的神经网络学习Low-Order、高维度稀疏的Categorical型特征，拟合样本中的长尾部分，发现新的特征组合，提高模型的泛化能力。同时对于文本、头图等传统机器学习模型难以刻画的特征，我们可以通过End-to-End的方式，利用相应的子网络模型进行预处理表示，然后进行融合学习。 图3 Deep&amp;Wide模型结构图 3. 搜索深度排序模型的特征工程实践 深度学习的横空出世，将算法工程师从很多人工挖掘和组合特征的事情中解放出来。甚至有一种论调，专做特征工程的算法工程师可能面临着失业的风险。但是深度学习的自动特征学习目前主要集中体现在CV领域，CV领域的特征数据是图片的像素点——稠密的低阶特征，深度学习通过卷积层这个强力工具，可以自动对低阶特征进行组合和变换，相比之前人工定义的图像特征从效果上来说确实更加显著。在NLP领域因为Transformer的出现，在自动特征挖掘上也有了长足的进步，BERT利用Transformer在多个NLP Task中取得了State-of-The-Art的效果。 但是对于CTR预估和排序学习的领域，目前深度学习尚未在自动特征挖掘上对人工特征工程形成碾压之势，因此人工特征工程依然很重要。当然，深度学习在特征工程上与传统模型的特征工程也存在着一些区别，我们的工作主要集中在如下几个方面。 3.1 特征预处理 特征归一化：深度网络的学习几乎都是基于反向传播，而此类梯度优化的方法对于特征的尺度非常敏感。因此，需要对特征进行归一化或者标准化以促使模型更好的收敛。 特征离散化：工业界一般很少直接使用连续值作为特征，而是将特征离散化后再输入到模型中。一方面因为离散化特征对于异常值具有更好的鲁棒性，其次可以为特征引入非线性的能力。并且，离散化可以更好的进行Embedding，我们主要使用如下两种离散化方法： 等频分桶：按样本频率进行等频切分，缺失值可以选择给一个默认桶值或者单独设置分桶。 树模型分桶：等频离散化的方式在特征分布特别不均匀的时候效果往往不好。此时可以利用单特征结合Label训练树模型，以树的分叉点做为切分值，相应的叶子节点作为桶号。 特征组合：基于业务场景对基础特征进行组合，形成更丰富的行为表征，为模型提供先验信息，可加速模型的收敛速度。典型示例如下： 用户性别与类目之间的交叉特征，能够刻画出不同性别的用户在类目上的偏好差异，比如男性用户可能会较少关注“丽人”相关的商户。 时间与类目之间的交叉特征，能够刻画出不同类目商户在时间上的差异，例如，酒吧在夜间会更容易被点击。 3.2 万物皆可Embedding 深度学习最大的魅力在于其强大的特征表征能力，在点评搜索场景下，我们有海量的用户行为数据，有丰富的商户UGC信息以及美团大脑提供的多维度细粒度标签数据。我们利用深度学习将这些信息Embedding到多个向量空间中，通过Embedding去表征用户的个性化偏好和商户的精准画像。同时向量化的Embedding也便于深度模型进一步的泛化、组合以及进行相似度的计算。 3.2.1 用户行为序列的Embedding 用户行为序列（搜索词序列、点击商户序列、筛选行为序列）包含了用户丰富的偏好信息。例如用户筛选了“距离优先”时，我们能够知道当前用户很有可能是一个即时消费的场景，并且对距离较为敏感。行为序列特征一般有如下图所示的三种接入方式: - Pooling：序列Embedding后接入Sum/Average Pooling层。此方式接入成本低，但忽略了行为的时序关系。 - RNN：LSTM/GRU接入，利用循环网络进行聚合。此方式能够考虑行为序列的时序关系；代价是增大了模型复杂度，影响线上预测性能。 - Attention：序列Embedding后引入Attention机制，表现为加权的Sum Pooling；相比LSTM/GRU计算开销更低[4]。 图4 行为序列特征接入的几种方法 同时，为了突显用户长期偏好和短期偏好对于排序的不同影响，我们按照时间维度对行为序列进行了划分：Session、半小时、一天、一周等粒度，也在线上取得了收益。 3.2.2 用户ID的Embedding 一种更常见的刻画用户偏好的方式，是直接将用户ID经过Embedding后作为特征接入到模型中，但是最后上线的效果却不尽如人意。通过分析用户的行为数据，我们发现相当一部分用户ID的行为数据较为稀疏，导致用户ID的Embedding没有充分收敛，未能充分刻画用户的偏好信息。 Airbnb发表在KDD 2018上的文章为这种问题提供了一种解决思路[9]——利用用户基础画像和行为数据对用户ID进行聚类。Airbnb的主要场景是为旅游用户提供民宿短租服务，一般用户一年旅游的次数在1-2次之间，因此Airbnb的用户行为数据相比点评搜索会更为稀疏一些。 图5 按照用户画像和行为信息聚类 如上图所示，将用户画像特征和行为特征进行离散分桶，拼接特征名和所属桶号，得到的聚类ID为：US_lt1_pn3_pg3_r3_5s4_c2_b1_bd2_bt2_nu3。 我们也采取了类似Airbnb的方案，稀疏性的问题得到了很好的解决，并且这样做还获得了一些额外的收益。大众点评作为一个本地化的生活信息服务平台，大部分用户的行为都集中自己的常驻地，导致用户到达一个新地方时，排序个性化明显不足。通过这种聚类的方式，将异地有相同行为的用户聚集在一起，也能解决一部分跨站的个性化问题。 3.2.3 商户信息Embedding 商户Embedding除了可以直接将商户ID加入模型中之外，美团大脑也利用深度学习技术对UGC进行大量挖掘，对商家的口味、特色等细粒度情感进行充分刻画，例如下图所示的“好停车”、“菜品精致”、“愿意再次光顾”等标签。 图6 美团大脑提供的商家细粒度情感标签 这些信息与单纯的商户星级、点评数相比，刻画的角度更多，粒度也更细。我们将这些标签也进行Embedding并输入到模型中： 直连：将标签特征做Pooling后直接输入模型。这种接入方式适合端到端的学习方式；但受输入层大小限制，只能取Top的标签，容易损失抽象实体信息。 分组直连：类似于直连接入的方式，但是先对标签进行分类，如菜品/风格/口味等类别；每个分类取Top N的实体后进行Pooling生成不同维度的语义向量。与不分组的直连相比，能够保留更多抽象信息。 子模型接入：可以利用DSSM模型，以标签作为商户输入学习商户的Embedding表达。此种方式能够最大化保留标签的抽象信息，但是线上实现和计算成本较高。 3.2.4 加速Embedding特征的收敛 在我们的深度学习排序模型中，除了Embedding特征，也存在大量Query、Shop和用户维度的强记忆特征，能够很快收敛。而Embedding特征是更为稀疏的弱特征，收敛速度较慢，为了加速Embedding特征的收敛，我们尝试了如下几种方案： 低频过滤：针对出现频率较低的特征进行过滤，可以很大程度上减少参数量，避免过拟合。 预训练：利用多类模型对稀疏Embedding特征进行预训练，然后进入模型进行微调： 通过无监督模型如Word2vec、Fasttext对用户-商户点击关系建模，生成共现关系下的商户Embedding。 利用DSSM等监督模型对Query-商户点击行为建模得到Query和商户的Embedding。 Multi-Task：针对稀疏的Embedding特征，单独设置一个子损失函数，如下图所示。此时Embedding特征的更新依赖两个损失函数的梯度，而子损失函数脱离了对强特征的依赖，可以加快Embedding特征的收敛。 图7 Multi-Task加速Embedding特征收敛 3.3 图片特征 图片在搜索结果页中占据了很大的展示面积，图片质量的好坏会直接影响用户的体验和点击，而点评商户首图来自于商户和用户上传的图片，质量参差不齐。因此，图片特征也是排序模型中较为重要的一类。目前点评搜索主要用了以下几类图片特征： 基础特征：提取图片的亮度、色度饱和度等基础信息，进行特征离散化后得到图片基础特征。 泛化特征：使用ResNet50进行图片特征提取[3]，通过聚类得到图片的泛化特征。 质量特征：使用自研的图片质量模型，提取中间层输出，作为图片质量的Embedding特征。 标签特征：提取图片是否是食物、环境、价目表、Logo等作为图片分类和标签特征。 图8 图片特征接入 4. 适用于搜索场景的深度学习Listwise排序算法：LambdaDNN 4.1 搜索业务指标与模型优化目标的Gap 通常模型的预测目标与业务指标总会存在一些Gap。如果模型的预测目标越贴近业务目标，越能保证模型优化的同时业务指标也能够有相应的提升；反之则会出现模型离线指标提升，但线上关键业务指标提升不明显，甚至出现负向的问题。工业届大部分深度学习排序采用Pointwise的Log Loss作为损失函数，与搜索业务指标有较大的Gap。体现在如下两个方面： 搜索业务常用的指标有QV_CTR或者SSR(Session Success Rate)，更关心的是用户搜索的成功率（有没有发生点击行为）；而Pointwise的Log Loss更多是关注单个Item的点击率。 搜索业务更关心排在页面头部结果的好坏，而Pointwise的方法则对于所有位置的样本一视同仁。 图9 Pointwise和Listwise优化目标的区别 基于上述理由，我们对于深度学习模型的损失函数进行了优化。 4.2 优化目标改进：从Log Loss到NDCG 为了让排序模型的优化目标尽量贴近搜索业务指标，需要按照Query计算损失，且不同位置的样本具有不同的权重。搜索系统常用的指标NDCG(Normalized Discounted Cumulative Gain)相较于Log Loss显然更贴近搜索业务的要求，NDCG计算公式如下： 累加部分为DCG(Discounted Cumulative Gain)表示按照位置折损的收益，对于Query下的结果列表l，函数G表示对应Doc的相关度分值，通常取指数函数，即G(lj)=2lj-1（lj表示的是相关度水平，如{0，1，2}）；函数 η 即位置折损，一般采用 η(j)=1/log(j+1)，Doc与Query的相关度越高且位置越靠前则DCG值会越大。另外，通常我们仅关注排序列表页前k位的效果，Zk 表示 DCG@k 的可能最大值，以此进行归一化处理后得到的就是NDCG@k。 问题在于NDCG是一个处处非平滑的函数，直接以它为目标函数进行优化是不可行的。LambdaRank提供了一种思路：绕过目标函数本身，直接构造一个特殊的梯度，按照梯度的方向修正模型参数，最终能达到拟合NDCG的方法[6]。因此，如果我们能将该梯度通过深度网络进行反向传播，则能训练一个优化NDCG的深度网络，该梯度我们称之为Lambda梯度，通过该梯度构造出的深度学习网络称之为LambdaDNN。 要了解Lambda梯度需要引入LambdaRank。LambdaRank模型是通过Pairwise来构造的，通常将同Query下有点击样本和无点击样本构造成一个样本Pair。模型的基本假设如下式所示，令Pij为同一个Query下Doci相比Docj更相关的概率，其中si和sj分别为Doci和Docj的模型得分： 使用交叉熵为损失函数，令Sij表示样本Pair的真实标记，当Doci比Docj更相关时（即Doci有被用户点击，而Docj没有被点击），有Sij=1，否则为-1；则损失函数可以表示为： 在构造样本Pair时，我们可以始终令i为更相关的文档，此时始终有Sij≡1，代入上式并进行求导，则损失函数的梯度为： 到目前为止，损失函数的计算过程中并未考虑样本所在的位置信息。因此进一步对梯度进行改造，考虑Doci和Docj交换位置时的NDCG值变化，下式即为前述的Lambda梯度。可以证明，通过此种方式构造出来的梯度经过迭代更新，最终可以达到优化NDCG的目的。 Lambda梯度的物理意义如下图所示。其中蓝色表示更相关（用户点击过）的文档，则Lambda梯度更倾向于位置靠上的Doc得到的提升更大（如红色箭头所示）。有了Lambda梯度的计算方法，训练中我们利用深度网络预测同Query下的Doc得分，根据用户实际点击Doc的情况计算Lambda梯度并反向传播回深度网络，则可以得到一个直接预测NDCG的深度网络。 图10 Lambda梯度的物理意义 4.3 LambdaDNN的工程实施 我们利用TensorFlow分布式框架训练LambdaDNN模型。如前文所述，Lambda梯度需要对同Query下的样本进行计算，但是正常情况下所有的样本是随机Shuffle到各个Worker的。因此我们需要对样本进行预处理： 通过QueryId进行Shuffle，将同一个Query的样本聚合在一起，同一个Query的样本打包进一个TFRecord。 由于每次请求Query召回的Doc数不一样，对于可变Size的Query样本在拉取数据进行训练时需要注意，TF会自动补齐Mini-Batch内每个样本大小一致，导致输入数据中存在大量无意义的默认值样本。这里我们提供两点处理方式： MR过程中对Key进行处理，使得多个Query的样本聚合在一起，然后在训练的时候进行动态切分。 读取到补齐的样本，根据设定的补齐标记获取索引位，去除补齐数据。 图11 Lambda梯度的分布式实现 为了提升训练效率，我们与基础研发平台数据平台中心紧密协同，一起探索并验证了多项优化操作： 将ID类特征的映射等操作一并在预处理中完成，减少多轮Training过程中的重复计算。 将样本转TfRecord，利用RecordDataSet方式读取数据并计算处理，Worker的计算性能大概提升了10倍。 Concat多个Categorical特征，组合成Multi-Hot的Tensor进行一次Embedding_Lookup操作，减少Map操作的同时有助于参数做分片存储计算。 稀疏Tensor在计算梯度以及正则化处理时保留索引值，仅对有数值的部分进行更新操作。 多个PS服务器间进行分片存储大规模Tensor变量，减少Worker同步更新的通讯压力，减少更新阻塞，达到更平滑的梯度更新效果。 整体下来，对于30亿左右的样本量、上亿级别的特征维度，一轮迭代大概在半小时内完成。适当的增加并行计算的资源，可以达到分钟级的训练任务。 4.4 进一步改进优化目标 NDCG的计算公式中，折损的权重是随着位置呈指数变化的。然而实际曝光点击率随位置变化的曲线与NDCG的理论折损值存在着较大的差异。 对于移动端的场景来说，用户在下拉滑动列表进行浏览时，视觉的焦点会随着滑屏、翻页而发生变动。例如用户翻到第二页时，往往会重新聚焦，因此，会发现第二页头部的曝光点击率实际上是高于第一页尾部位置的。我们尝试了两种方案去微调NDCG中的指数位置折损： 根据实际曝光点击率拟合折损曲线：根据实际统计到的曝光点击率数据，拟合公式替代NDCG中的指数折损公式，绘制的曲线如图12所示。 计算Position Bias作为位置折损：Position Bias在业界有较多的讨论，其中[7][8]将用户点击商户的过程分为观察和点击两个步骤：a.用户需要首先看到该商户，而看到商户的概率取决于所在的位置；b.看到商户后点击商户的概率只与商户的相关性有关。步骤a计算的概率即为Position Bias，这块内容可以讨论的东西很多，这里不再详述。 图12 真实位置折损与理论折损的差别 经过上述对NDCG计算改造训练出的LambdaDNN模型，相较Base树模型和Pointwise DNN模型，在业务指标上有了非常显著的提升。 图13 LambdaDNN离线NDCG指标与线上PvCtr效果对比 4.5 Lambda深度排序框架 Lambda梯度除了与DNN网络相结合外，事实上可以与绝大部分常见的网络结构相结合。为了进一步学习到更多交叉特征，我们在LambdaDNN的基础上分别尝试了LambdaDeepFM和LambdaDCN网络；其中DCN网络是一种加入Cross的并行网络结构，交叉的网络每一层的输出特征与第一层的原始输入特征进行显性的两两交叉，相当于每一层学习特征交叉的映射去拟合层之间的残差。 图14 DCN模型结构 离线的对比实验表明，Lambda梯度与DCN网络结合之后充分发挥了DCN网络的特点，简洁的多项式交叉设计有效地提升模型的训练效果。NDCG指标对比效果如下图所示： 图15 Lambda Loss与DCN网络结果的效果 5. 深度学习排序诊断系统 深度学习排序模型虽然给业务指标带来了大幅度的提升，但由于深度学习模型的“黑盒属性”导致了巨大的解释性成本，也给搜索业务带来了一些问题： 日常搜索Bad Case无法快速响应：搜索业务日常需要应对大量来自于用户、业务和老板们的“灵魂拷问”，“为何这个排序是这样的”，“为什么这家商户质量跟我差不多，但是会排在我的前面”。刚切换到深度学习排序模型的时候，我们对于这样的问题显得手足无措，需要花费大量的时间去定位问题。 无法从Bad Case中学习总结规律持续优化：如果不明白为什么排序模型会得出一个很坏的排序结果，自然也无法定位模型到底出了什么问题，也就无法根据Bad Case总结规律，从而确定模型和特征将来的优化方向。 模型和特征是否充分学习无从得知：新挖掘一些特征之后，通常我们会根据离线评测指标是否有提升决定特征是否上线。但是，即使一个有提升的特征，我们也无法知道这个特征是否性能足够好。例如，模型拟合的距离特征，会不会在特定的距离段出现距离越远反而打分越高的情况。 这些问题都会潜在带来一些用户无法理解的排序结果。我们需要对深度排序模型清晰地诊断并解释。 关于机器学习模型的可解释性研究，业界已经有了一些探索。Lime(Local Interpretable Model-Agnostic Explanations)是其中的一种，如下图所示：通过对单个样本的特征生成扰动产生近邻样本，观察模型的预测行为。根据这些扰动的数据点距离原始数据的距离分配权重，基于它们学习得到一个可解释的模型和预测结果[5]。举个例子，如果需要解释一个情感分类模型是如何预测“我讨厌这部电影”为负面情感的，我们通过丢掉部分词或者乱序构造一些样本预测情感，最终会发现，决定“我讨厌这部电影”为负面情感的是因为“讨厌”这个词。 图16 Lime解释器的工作原理 基于Lime解释器的思想，我们开发了一套深度模型解释器工具——雅典娜系统。目前雅典娜系统支持两种工作模式，Pairwise和Listwise模式： Pairwise模式用来解释同一个列表中两个结果之间的相对排序。通过对样本的特征进行重新赋值或者替换等操作，观察样本打分和排序位次的变化趋势，诊断出当前样本排序是否符合预期。如下图所示，通过右侧的特征位次面板可以快速诊断出为什么“南京大牌档”的排序比“金时代顺风港湾”要更靠前。第一行的特征位次信息显示，若将“金时代顺风港湾”的1.3km的距离特征用“南京大牌档”的0.2km的距离特征进行替换，排序位次将上升10位；由此得出，“南京大牌档”排在前面的决定性因素是因为距离近。 Listwise模式与Lime的工作模式基本类似，通过整个列表的样本生成扰动样本，训练线性分类器模型输出特征重要度，从而达到对模型进行解释的目的。 图17 深度学习排序诊断系统：雅典娜 6. 总结与展望 2018年下半年，点评搜索完成了从树模型到大规模深度学习排序模型的全面升级。团队在深度学习特征工程、模型结构、优化目标以及工程实践上都进行了一些探索，在核心指标上取得了较为显著的收益。当然，未来依然有不少可以探索的点。 在特征层面，大量知识图谱提供的标签信息尚未充分挖掘。从使用方式上看，简单以文本标签的形式接入，损失了知识图谱的结构信息，因此，Graph Embedding也是未来需要尝试的方向。同时团队也会利用BERT在Query和商户文本的深层语义表达上做一些工作。 模型结构层面，目前线上依然以全连接的DNN网络结构为主，但DNN网络结构在低秩数据的学习上不如DeepFM和DCN。目前LambdaDeepFM和LambdaDCN在离线上已经取得了收益，未来会在网络结构上做进一步优化。 在模型优化目标上，Lambda Loss计算损失的时候，只会考虑Query内部有点击和无点击的样本对，大量无点击的Query被丢弃，同时，同一个用户短时间内在不同Query下的行为也包含着一些信息可以利用。因此，目前团队正在探索综合考虑Log Loss和Lambda Loss的模型，通过Multi-Task和按照不同维度Shuffle样本让模型充分学习，目前我们已经在线下取得了一些收益。 最后，近期Google开源的TF Ranking提出的Groupwise模型也对我们有一些启发。目前绝大部分的Listwise方法只是体现在模型训练阶段，在打分预测阶段依然是Pointwise的，即只会考虑当前商户相关的特征，而不会考虑列表上下文的结果，未来我们也会在这个方向上进行一些探索。 参考资料 美团大脑：知识图谱的建模方法及其应用 Wide &amp; Deep Learning for Recommender Systems Deep Residual Learning for Image Recognition Attention Is All You Need Local Interpretable Mode l- Agnostic Explanations: LIME From RankNet to LambdaRank to LambdaMART: An Overview A Novel Algorithm for Unbiased Learning to Rank Unbiased Learning-to-Rank with Biased Feedback Real-time Personalization using Embeddings for Search Ranking at Airbnb 作者简介 非易，2016年加入美团点评，高级算法工程师，目前主要负责点评搜索核心排序层的研发工作。 祝升，2016年加入美团点评，高级算法工程师，目前负责点评搜索核心排序层的研发工作。 汤彪，2013年加入美团点评，高级算法专家，点评平台搜索技术负责人，致力于深层次查询理解和大规模深度学习排序的技术落地。 张弓，2012年加入美团点评，美团点评研究员。目前主要负责点评搜索业务演进，及集团搜索公共服务平台建设。 仲远，博士，美团AI平台部NLP中心负责人，点评搜索智能中心负责人。在国际顶级学术会议发表论文30余篇，获得ICDE 2015最佳论文奖，并是ACL 2016 Tutorial “Understanding Short Texts”主讲人，出版学术专著3部，获得美国专利5项。此前，博士曾担任微软亚洲研究院主管研究员，以及美国Facebook公司Research Scientist。曾负责微软研究院知识图谱、对话机器人项目和Facebook产品级NLP Service。 欢迎加入美团机器学习技术交流群，跟项目维护者零距离交流。进群方式：请加美美同学微信（微信号：MTDPtech02），回复：深度学习，美美会自动拉你进群。 ----------&nbsp; END&nbsp; ---------- 也许你还想看 美团餐饮娱乐知识图谱——美团大脑揭秘 美团大脑：知识图谱的建模方法及其应用 美团如何基于深度学习实现图像的智能审核？" />
<meta property="og:description" content="总第327篇 2019年 第005篇 本文介绍了大众点评搜索核心排序层模型的演化之路，包括结合知识图谱信息构建适合搜索场景的Listwise深度学习排序模型LambdaDNN以及特征工程实践和相关工具建设。 1. 引言 挑战与思路 搜索是大众点评App上用户进行信息查找的最大入口，是连接用户和信息的重要纽带。而用户搜索的方式和场景非常多样，并且由于对接业务种类多，流量差异大，为大众点评搜索（下文简称点评搜索）带来了巨大的挑战，具体体现在如下几个方面： 意图多样：用户查找的信息类型和方式多样。信息类型包括POI、榜单、UGC、攻略、达人等。以找店为例，查找方式包括按距离、按热度、按菜品和按地理位置等多种方式。例如用户按照品牌进行搜索时，大概率是需要寻找距离最近或者常去的某家分店；但用户搜索菜品时，会对菜品推荐人数更加敏感，而距离因素会弱化。 业务多样：不同业务之间，用户的使用频率、选择难度以及业务诉求均不一样。例如家装场景用户使用频次很低，行为非常稀疏，距离因素弱，并且选择周期可能会很长；而美食多为即时消费场景，用户行为数据多，距离敏感。 用户类型多样：不同的用户对价格、距离、口味以及偏好的类目之间差异很大；搜索需要能深度挖掘到用户的各种偏好，实现定制化的“千人千面”的搜索。 LBS的搜索：相比电商和通用搜索，LBS的升维效应极大地增加了搜索场景的复杂性。例如对于旅游用户和常驻地用户来说，前者在搜索美食的时候可能会更加关心当地的知名特色商户，而对于距离相对不敏感。 上述的各项特性，叠加上时间、空间、场景等维度，使得点评搜索面临比通用搜索引擎更加独特的挑战。而解决这些挑战的方法，就需要升级NLP（Natural Language Processing，自然语言处理）技术，进行深度查询理解以及深度评价分析，并依赖知识图谱技术和深度学习技术对搜索架构进行整体升级。在美团NLP中心以及大众点评搜索智能中心两个团队的紧密合作之下，经过短短半年时间，点评搜索核心KPI在高位基础上仍然大幅提升，是过去一年半涨幅的六倍之多，提前半年完成全年目标。 基于知识图谱的搜索架构重塑 美团NLP中心正在构建全世界最大的餐饮娱乐知识图谱——美团大脑（相关信息请参见《美团大脑：知识图谱的建模方法及其应用》）。它充分挖掘关联各个场景数据，用NLP技术让机器“阅读”用户公开评论，理解用户在菜品、价格、服务、环境等方面的喜好，构建人、店、商品、场景之间的知识关联，从而形成一个“知识大脑”[1]。通过将知识图谱信息加入到搜索各个流程中，我们对点评搜索的整体架构进行了升级重塑，图1为点评搜索基于知识图谱搭建的5层搜索架构。本篇文章是“美团大脑”系列文章第二篇（系列首篇文章请参见《美团餐饮娱乐知识图谱——美团大脑揭秘》），主要介绍点评搜索5层架构中核心排序层的演变过程，文章主要分为如下3个部分： 核心排序从传统机器学习模型到大规模深度学习模型的演进。 搜索场景深度学习排序模型的特征工程实践。 适用于搜索场景的深度学习Listwise排序算法——LambdaDNN。 图1 基于知识图谱的点评搜索5层架构 2. 排序模型探索与实践 搜索排序问题在机器学习领域有一个单独的分支，Learning to Rank（L2R）。主要分类如下： 根据样本生成方法和Loss Function的不同，L2R可以分为Pointwise、Pairwise、Listwise。 按照模型结构划分，可以分为线性排序模型、树模型、深度学习模型，它们之间的组合（GBDT+LR，Deep&amp;Wide等）。 在排序模型方面，点评搜索也经历了业界比较普遍的迭代过程：从早期的线性模型LR，到引入自动二阶交叉特征的FM和FFM，到非线性树模型GBDT和GBDT+LR，到最近全面迁移至大规模深度学习排序模型。下面先简单介绍下传统机器学习模型（LR、FM、GBDT）的应用和优缺点，然后详细介绍深度模型的探索实践过程。 传统机器学习模型 图2 几种传统机器学习模型结构 LR可以视作单层单节点的线性网络结构。模型优点是可解释性强。通常而言，良好的解释性是工业界应用实践比较注重的一个指标，它意味着更好的可控性，同时也能指导工程师去分析问题优化模型。但是LR需要依赖大量的人工特征挖掘投入，有限的特征组合自然无法提供较强的表达能力。 FM可以看做是在LR的基础上增加了一部分二阶交叉项。引入自动的交叉特征有助于减少人工挖掘的投入，同时增加模型的非线性，捕捉更多信息。FM能够自动学习两两特征间的关系，但更高量级的特征交叉仍然无法满足。 GBDT是一个Boosting的模型，通过组合多个弱模型逐步拟合残差得到一个强模型。树模型具有天然的优势，能够很好的挖掘组合高阶统计特征，兼具较优的可解释性。GBDT的主要缺陷是依赖连续型的统计特征，对于高维度稀疏特征、时间序列特征不能很好的处理。 深度神经网络模型 随着业务的发展，在传统模型上取得指标收益变得愈发困难。同时业务的复杂性要求我们引入海量用户历史数据，超大规模知识图谱特征等多维度信息源，以实现精准个性化的排序。因此我们从2018年下半年开始，全力推进L2核心排序层的主模型迁移至深度学习排序模型。深度模型优势体现在如下几个方面： 强大的模型拟合能力：深度学习网络包含多个隐藏层和隐藏结点，配合上非线性的激活函数，理论上可以拟合任何函数，因此十分适用于点评搜索这种复杂的场景。 强大的特征表征和泛化能力：深度学习模型可以处理很多传统模型无法处理的特征。例如深度网络可以直接中从海量训练样本中学习到高维稀疏ID的隐含信息，并通过Embedding的方式去表征；另外对于文本、序列特征以及图像特征，深度网络均有对应的结构或者单元去处理。 自动组合和发现特征的能力：华为提出的DeepFM，以及Google提出的DeepCrossNetwork可以自动进行特征组合，代替大量人工组合特征的工作。 下图是我们基于Google提出的Wide&amp;Deep模型搭建的网络结构[2]。其中Wide部分输入的是LR、GBDT阶段常用的一些细粒度统计特征。通过较长周期统计的高频行为特征，能够提供很好的记忆能力。Deep部分通过深层的神经网络学习Low-Order、高维度稀疏的Categorical型特征，拟合样本中的长尾部分，发现新的特征组合，提高模型的泛化能力。同时对于文本、头图等传统机器学习模型难以刻画的特征，我们可以通过End-to-End的方式，利用相应的子网络模型进行预处理表示，然后进行融合学习。 图3 Deep&amp;Wide模型结构图 3. 搜索深度排序模型的特征工程实践 深度学习的横空出世，将算法工程师从很多人工挖掘和组合特征的事情中解放出来。甚至有一种论调，专做特征工程的算法工程师可能面临着失业的风险。但是深度学习的自动特征学习目前主要集中体现在CV领域，CV领域的特征数据是图片的像素点——稠密的低阶特征，深度学习通过卷积层这个强力工具，可以自动对低阶特征进行组合和变换，相比之前人工定义的图像特征从效果上来说确实更加显著。在NLP领域因为Transformer的出现，在自动特征挖掘上也有了长足的进步，BERT利用Transformer在多个NLP Task中取得了State-of-The-Art的效果。 但是对于CTR预估和排序学习的领域，目前深度学习尚未在自动特征挖掘上对人工特征工程形成碾压之势，因此人工特征工程依然很重要。当然，深度学习在特征工程上与传统模型的特征工程也存在着一些区别，我们的工作主要集中在如下几个方面。 3.1 特征预处理 特征归一化：深度网络的学习几乎都是基于反向传播，而此类梯度优化的方法对于特征的尺度非常敏感。因此，需要对特征进行归一化或者标准化以促使模型更好的收敛。 特征离散化：工业界一般很少直接使用连续值作为特征，而是将特征离散化后再输入到模型中。一方面因为离散化特征对于异常值具有更好的鲁棒性，其次可以为特征引入非线性的能力。并且，离散化可以更好的进行Embedding，我们主要使用如下两种离散化方法： 等频分桶：按样本频率进行等频切分，缺失值可以选择给一个默认桶值或者单独设置分桶。 树模型分桶：等频离散化的方式在特征分布特别不均匀的时候效果往往不好。此时可以利用单特征结合Label训练树模型，以树的分叉点做为切分值，相应的叶子节点作为桶号。 特征组合：基于业务场景对基础特征进行组合，形成更丰富的行为表征，为模型提供先验信息，可加速模型的收敛速度。典型示例如下： 用户性别与类目之间的交叉特征，能够刻画出不同性别的用户在类目上的偏好差异，比如男性用户可能会较少关注“丽人”相关的商户。 时间与类目之间的交叉特征，能够刻画出不同类目商户在时间上的差异，例如，酒吧在夜间会更容易被点击。 3.2 万物皆可Embedding 深度学习最大的魅力在于其强大的特征表征能力，在点评搜索场景下，我们有海量的用户行为数据，有丰富的商户UGC信息以及美团大脑提供的多维度细粒度标签数据。我们利用深度学习将这些信息Embedding到多个向量空间中，通过Embedding去表征用户的个性化偏好和商户的精准画像。同时向量化的Embedding也便于深度模型进一步的泛化、组合以及进行相似度的计算。 3.2.1 用户行为序列的Embedding 用户行为序列（搜索词序列、点击商户序列、筛选行为序列）包含了用户丰富的偏好信息。例如用户筛选了“距离优先”时，我们能够知道当前用户很有可能是一个即时消费的场景，并且对距离较为敏感。行为序列特征一般有如下图所示的三种接入方式: - Pooling：序列Embedding后接入Sum/Average Pooling层。此方式接入成本低，但忽略了行为的时序关系。 - RNN：LSTM/GRU接入，利用循环网络进行聚合。此方式能够考虑行为序列的时序关系；代价是增大了模型复杂度，影响线上预测性能。 - Attention：序列Embedding后引入Attention机制，表现为加权的Sum Pooling；相比LSTM/GRU计算开销更低[4]。 图4 行为序列特征接入的几种方法 同时，为了突显用户长期偏好和短期偏好对于排序的不同影响，我们按照时间维度对行为序列进行了划分：Session、半小时、一天、一周等粒度，也在线上取得了收益。 3.2.2 用户ID的Embedding 一种更常见的刻画用户偏好的方式，是直接将用户ID经过Embedding后作为特征接入到模型中，但是最后上线的效果却不尽如人意。通过分析用户的行为数据，我们发现相当一部分用户ID的行为数据较为稀疏，导致用户ID的Embedding没有充分收敛，未能充分刻画用户的偏好信息。 Airbnb发表在KDD 2018上的文章为这种问题提供了一种解决思路[9]——利用用户基础画像和行为数据对用户ID进行聚类。Airbnb的主要场景是为旅游用户提供民宿短租服务，一般用户一年旅游的次数在1-2次之间，因此Airbnb的用户行为数据相比点评搜索会更为稀疏一些。 图5 按照用户画像和行为信息聚类 如上图所示，将用户画像特征和行为特征进行离散分桶，拼接特征名和所属桶号，得到的聚类ID为：US_lt1_pn3_pg3_r3_5s4_c2_b1_bd2_bt2_nu3。 我们也采取了类似Airbnb的方案，稀疏性的问题得到了很好的解决，并且这样做还获得了一些额外的收益。大众点评作为一个本地化的生活信息服务平台，大部分用户的行为都集中自己的常驻地，导致用户到达一个新地方时，排序个性化明显不足。通过这种聚类的方式，将异地有相同行为的用户聚集在一起，也能解决一部分跨站的个性化问题。 3.2.3 商户信息Embedding 商户Embedding除了可以直接将商户ID加入模型中之外，美团大脑也利用深度学习技术对UGC进行大量挖掘，对商家的口味、特色等细粒度情感进行充分刻画，例如下图所示的“好停车”、“菜品精致”、“愿意再次光顾”等标签。 图6 美团大脑提供的商家细粒度情感标签 这些信息与单纯的商户星级、点评数相比，刻画的角度更多，粒度也更细。我们将这些标签也进行Embedding并输入到模型中： 直连：将标签特征做Pooling后直接输入模型。这种接入方式适合端到端的学习方式；但受输入层大小限制，只能取Top的标签，容易损失抽象实体信息。 分组直连：类似于直连接入的方式，但是先对标签进行分类，如菜品/风格/口味等类别；每个分类取Top N的实体后进行Pooling生成不同维度的语义向量。与不分组的直连相比，能够保留更多抽象信息。 子模型接入：可以利用DSSM模型，以标签作为商户输入学习商户的Embedding表达。此种方式能够最大化保留标签的抽象信息，但是线上实现和计算成本较高。 3.2.4 加速Embedding特征的收敛 在我们的深度学习排序模型中，除了Embedding特征，也存在大量Query、Shop和用户维度的强记忆特征，能够很快收敛。而Embedding特征是更为稀疏的弱特征，收敛速度较慢，为了加速Embedding特征的收敛，我们尝试了如下几种方案： 低频过滤：针对出现频率较低的特征进行过滤，可以很大程度上减少参数量，避免过拟合。 预训练：利用多类模型对稀疏Embedding特征进行预训练，然后进入模型进行微调： 通过无监督模型如Word2vec、Fasttext对用户-商户点击关系建模，生成共现关系下的商户Embedding。 利用DSSM等监督模型对Query-商户点击行为建模得到Query和商户的Embedding。 Multi-Task：针对稀疏的Embedding特征，单独设置一个子损失函数，如下图所示。此时Embedding特征的更新依赖两个损失函数的梯度，而子损失函数脱离了对强特征的依赖，可以加快Embedding特征的收敛。 图7 Multi-Task加速Embedding特征收敛 3.3 图片特征 图片在搜索结果页中占据了很大的展示面积，图片质量的好坏会直接影响用户的体验和点击，而点评商户首图来自于商户和用户上传的图片，质量参差不齐。因此，图片特征也是排序模型中较为重要的一类。目前点评搜索主要用了以下几类图片特征： 基础特征：提取图片的亮度、色度饱和度等基础信息，进行特征离散化后得到图片基础特征。 泛化特征：使用ResNet50进行图片特征提取[3]，通过聚类得到图片的泛化特征。 质量特征：使用自研的图片质量模型，提取中间层输出，作为图片质量的Embedding特征。 标签特征：提取图片是否是食物、环境、价目表、Logo等作为图片分类和标签特征。 图8 图片特征接入 4. 适用于搜索场景的深度学习Listwise排序算法：LambdaDNN 4.1 搜索业务指标与模型优化目标的Gap 通常模型的预测目标与业务指标总会存在一些Gap。如果模型的预测目标越贴近业务目标，越能保证模型优化的同时业务指标也能够有相应的提升；反之则会出现模型离线指标提升，但线上关键业务指标提升不明显，甚至出现负向的问题。工业届大部分深度学习排序采用Pointwise的Log Loss作为损失函数，与搜索业务指标有较大的Gap。体现在如下两个方面： 搜索业务常用的指标有QV_CTR或者SSR(Session Success Rate)，更关心的是用户搜索的成功率（有没有发生点击行为）；而Pointwise的Log Loss更多是关注单个Item的点击率。 搜索业务更关心排在页面头部结果的好坏，而Pointwise的方法则对于所有位置的样本一视同仁。 图9 Pointwise和Listwise优化目标的区别 基于上述理由，我们对于深度学习模型的损失函数进行了优化。 4.2 优化目标改进：从Log Loss到NDCG 为了让排序模型的优化目标尽量贴近搜索业务指标，需要按照Query计算损失，且不同位置的样本具有不同的权重。搜索系统常用的指标NDCG(Normalized Discounted Cumulative Gain)相较于Log Loss显然更贴近搜索业务的要求，NDCG计算公式如下： 累加部分为DCG(Discounted Cumulative Gain)表示按照位置折损的收益，对于Query下的结果列表l，函数G表示对应Doc的相关度分值，通常取指数函数，即G(lj)=2lj-1（lj表示的是相关度水平，如{0，1，2}）；函数 η 即位置折损，一般采用 η(j)=1/log(j+1)，Doc与Query的相关度越高且位置越靠前则DCG值会越大。另外，通常我们仅关注排序列表页前k位的效果，Zk 表示 DCG@k 的可能最大值，以此进行归一化处理后得到的就是NDCG@k。 问题在于NDCG是一个处处非平滑的函数，直接以它为目标函数进行优化是不可行的。LambdaRank提供了一种思路：绕过目标函数本身，直接构造一个特殊的梯度，按照梯度的方向修正模型参数，最终能达到拟合NDCG的方法[6]。因此，如果我们能将该梯度通过深度网络进行反向传播，则能训练一个优化NDCG的深度网络，该梯度我们称之为Lambda梯度，通过该梯度构造出的深度学习网络称之为LambdaDNN。 要了解Lambda梯度需要引入LambdaRank。LambdaRank模型是通过Pairwise来构造的，通常将同Query下有点击样本和无点击样本构造成一个样本Pair。模型的基本假设如下式所示，令Pij为同一个Query下Doci相比Docj更相关的概率，其中si和sj分别为Doci和Docj的模型得分： 使用交叉熵为损失函数，令Sij表示样本Pair的真实标记，当Doci比Docj更相关时（即Doci有被用户点击，而Docj没有被点击），有Sij=1，否则为-1；则损失函数可以表示为： 在构造样本Pair时，我们可以始终令i为更相关的文档，此时始终有Sij≡1，代入上式并进行求导，则损失函数的梯度为： 到目前为止，损失函数的计算过程中并未考虑样本所在的位置信息。因此进一步对梯度进行改造，考虑Doci和Docj交换位置时的NDCG值变化，下式即为前述的Lambda梯度。可以证明，通过此种方式构造出来的梯度经过迭代更新，最终可以达到优化NDCG的目的。 Lambda梯度的物理意义如下图所示。其中蓝色表示更相关（用户点击过）的文档，则Lambda梯度更倾向于位置靠上的Doc得到的提升更大（如红色箭头所示）。有了Lambda梯度的计算方法，训练中我们利用深度网络预测同Query下的Doc得分，根据用户实际点击Doc的情况计算Lambda梯度并反向传播回深度网络，则可以得到一个直接预测NDCG的深度网络。 图10 Lambda梯度的物理意义 4.3 LambdaDNN的工程实施 我们利用TensorFlow分布式框架训练LambdaDNN模型。如前文所述，Lambda梯度需要对同Query下的样本进行计算，但是正常情况下所有的样本是随机Shuffle到各个Worker的。因此我们需要对样本进行预处理： 通过QueryId进行Shuffle，将同一个Query的样本聚合在一起，同一个Query的样本打包进一个TFRecord。 由于每次请求Query召回的Doc数不一样，对于可变Size的Query样本在拉取数据进行训练时需要注意，TF会自动补齐Mini-Batch内每个样本大小一致，导致输入数据中存在大量无意义的默认值样本。这里我们提供两点处理方式： MR过程中对Key进行处理，使得多个Query的样本聚合在一起，然后在训练的时候进行动态切分。 读取到补齐的样本，根据设定的补齐标记获取索引位，去除补齐数据。 图11 Lambda梯度的分布式实现 为了提升训练效率，我们与基础研发平台数据平台中心紧密协同，一起探索并验证了多项优化操作： 将ID类特征的映射等操作一并在预处理中完成，减少多轮Training过程中的重复计算。 将样本转TfRecord，利用RecordDataSet方式读取数据并计算处理，Worker的计算性能大概提升了10倍。 Concat多个Categorical特征，组合成Multi-Hot的Tensor进行一次Embedding_Lookup操作，减少Map操作的同时有助于参数做分片存储计算。 稀疏Tensor在计算梯度以及正则化处理时保留索引值，仅对有数值的部分进行更新操作。 多个PS服务器间进行分片存储大规模Tensor变量，减少Worker同步更新的通讯压力，减少更新阻塞，达到更平滑的梯度更新效果。 整体下来，对于30亿左右的样本量、上亿级别的特征维度，一轮迭代大概在半小时内完成。适当的增加并行计算的资源，可以达到分钟级的训练任务。 4.4 进一步改进优化目标 NDCG的计算公式中，折损的权重是随着位置呈指数变化的。然而实际曝光点击率随位置变化的曲线与NDCG的理论折损值存在着较大的差异。 对于移动端的场景来说，用户在下拉滑动列表进行浏览时，视觉的焦点会随着滑屏、翻页而发生变动。例如用户翻到第二页时，往往会重新聚焦，因此，会发现第二页头部的曝光点击率实际上是高于第一页尾部位置的。我们尝试了两种方案去微调NDCG中的指数位置折损： 根据实际曝光点击率拟合折损曲线：根据实际统计到的曝光点击率数据，拟合公式替代NDCG中的指数折损公式，绘制的曲线如图12所示。 计算Position Bias作为位置折损：Position Bias在业界有较多的讨论，其中[7][8]将用户点击商户的过程分为观察和点击两个步骤：a.用户需要首先看到该商户，而看到商户的概率取决于所在的位置；b.看到商户后点击商户的概率只与商户的相关性有关。步骤a计算的概率即为Position Bias，这块内容可以讨论的东西很多，这里不再详述。 图12 真实位置折损与理论折损的差别 经过上述对NDCG计算改造训练出的LambdaDNN模型，相较Base树模型和Pointwise DNN模型，在业务指标上有了非常显著的提升。 图13 LambdaDNN离线NDCG指标与线上PvCtr效果对比 4.5 Lambda深度排序框架 Lambda梯度除了与DNN网络相结合外，事实上可以与绝大部分常见的网络结构相结合。为了进一步学习到更多交叉特征，我们在LambdaDNN的基础上分别尝试了LambdaDeepFM和LambdaDCN网络；其中DCN网络是一种加入Cross的并行网络结构，交叉的网络每一层的输出特征与第一层的原始输入特征进行显性的两两交叉，相当于每一层学习特征交叉的映射去拟合层之间的残差。 图14 DCN模型结构 离线的对比实验表明，Lambda梯度与DCN网络结合之后充分发挥了DCN网络的特点，简洁的多项式交叉设计有效地提升模型的训练效果。NDCG指标对比效果如下图所示： 图15 Lambda Loss与DCN网络结果的效果 5. 深度学习排序诊断系统 深度学习排序模型虽然给业务指标带来了大幅度的提升，但由于深度学习模型的“黑盒属性”导致了巨大的解释性成本，也给搜索业务带来了一些问题： 日常搜索Bad Case无法快速响应：搜索业务日常需要应对大量来自于用户、业务和老板们的“灵魂拷问”，“为何这个排序是这样的”，“为什么这家商户质量跟我差不多，但是会排在我的前面”。刚切换到深度学习排序模型的时候，我们对于这样的问题显得手足无措，需要花费大量的时间去定位问题。 无法从Bad Case中学习总结规律持续优化：如果不明白为什么排序模型会得出一个很坏的排序结果，自然也无法定位模型到底出了什么问题，也就无法根据Bad Case总结规律，从而确定模型和特征将来的优化方向。 模型和特征是否充分学习无从得知：新挖掘一些特征之后，通常我们会根据离线评测指标是否有提升决定特征是否上线。但是，即使一个有提升的特征，我们也无法知道这个特征是否性能足够好。例如，模型拟合的距离特征，会不会在特定的距离段出现距离越远反而打分越高的情况。 这些问题都会潜在带来一些用户无法理解的排序结果。我们需要对深度排序模型清晰地诊断并解释。 关于机器学习模型的可解释性研究，业界已经有了一些探索。Lime(Local Interpretable Model-Agnostic Explanations)是其中的一种，如下图所示：通过对单个样本的特征生成扰动产生近邻样本，观察模型的预测行为。根据这些扰动的数据点距离原始数据的距离分配权重，基于它们学习得到一个可解释的模型和预测结果[5]。举个例子，如果需要解释一个情感分类模型是如何预测“我讨厌这部电影”为负面情感的，我们通过丢掉部分词或者乱序构造一些样本预测情感，最终会发现，决定“我讨厌这部电影”为负面情感的是因为“讨厌”这个词。 图16 Lime解释器的工作原理 基于Lime解释器的思想，我们开发了一套深度模型解释器工具——雅典娜系统。目前雅典娜系统支持两种工作模式，Pairwise和Listwise模式： Pairwise模式用来解释同一个列表中两个结果之间的相对排序。通过对样本的特征进行重新赋值或者替换等操作，观察样本打分和排序位次的变化趋势，诊断出当前样本排序是否符合预期。如下图所示，通过右侧的特征位次面板可以快速诊断出为什么“南京大牌档”的排序比“金时代顺风港湾”要更靠前。第一行的特征位次信息显示，若将“金时代顺风港湾”的1.3km的距离特征用“南京大牌档”的0.2km的距离特征进行替换，排序位次将上升10位；由此得出，“南京大牌档”排在前面的决定性因素是因为距离近。 Listwise模式与Lime的工作模式基本类似，通过整个列表的样本生成扰动样本，训练线性分类器模型输出特征重要度，从而达到对模型进行解释的目的。 图17 深度学习排序诊断系统：雅典娜 6. 总结与展望 2018年下半年，点评搜索完成了从树模型到大规模深度学习排序模型的全面升级。团队在深度学习特征工程、模型结构、优化目标以及工程实践上都进行了一些探索，在核心指标上取得了较为显著的收益。当然，未来依然有不少可以探索的点。 在特征层面，大量知识图谱提供的标签信息尚未充分挖掘。从使用方式上看，简单以文本标签的形式接入，损失了知识图谱的结构信息，因此，Graph Embedding也是未来需要尝试的方向。同时团队也会利用BERT在Query和商户文本的深层语义表达上做一些工作。 模型结构层面，目前线上依然以全连接的DNN网络结构为主，但DNN网络结构在低秩数据的学习上不如DeepFM和DCN。目前LambdaDeepFM和LambdaDCN在离线上已经取得了收益，未来会在网络结构上做进一步优化。 在模型优化目标上，Lambda Loss计算损失的时候，只会考虑Query内部有点击和无点击的样本对，大量无点击的Query被丢弃，同时，同一个用户短时间内在不同Query下的行为也包含着一些信息可以利用。因此，目前团队正在探索综合考虑Log Loss和Lambda Loss的模型，通过Multi-Task和按照不同维度Shuffle样本让模型充分学习，目前我们已经在线下取得了一些收益。 最后，近期Google开源的TF Ranking提出的Groupwise模型也对我们有一些启发。目前绝大部分的Listwise方法只是体现在模型训练阶段，在打分预测阶段依然是Pointwise的，即只会考虑当前商户相关的特征，而不会考虑列表上下文的结果，未来我们也会在这个方向上进行一些探索。 参考资料 美团大脑：知识图谱的建模方法及其应用 Wide &amp; Deep Learning for Recommender Systems Deep Residual Learning for Image Recognition Attention Is All You Need Local Interpretable Mode l- Agnostic Explanations: LIME From RankNet to LambdaRank to LambdaMART: An Overview A Novel Algorithm for Unbiased Learning to Rank Unbiased Learning-to-Rank with Biased Feedback Real-time Personalization using Embeddings for Search Ranking at Airbnb 作者简介 非易，2016年加入美团点评，高级算法工程师，目前主要负责点评搜索核心排序层的研发工作。 祝升，2016年加入美团点评，高级算法工程师，目前负责点评搜索核心排序层的研发工作。 汤彪，2013年加入美团点评，高级算法专家，点评平台搜索技术负责人，致力于深层次查询理解和大规模深度学习排序的技术落地。 张弓，2012年加入美团点评，美团点评研究员。目前主要负责点评搜索业务演进，及集团搜索公共服务平台建设。 仲远，博士，美团AI平台部NLP中心负责人，点评搜索智能中心负责人。在国际顶级学术会议发表论文30余篇，获得ICDE 2015最佳论文奖，并是ACL 2016 Tutorial “Understanding Short Texts”主讲人，出版学术专著3部，获得美国专利5项。此前，博士曾担任微软亚洲研究院主管研究员，以及美国Facebook公司Research Scientist。曾负责微软研究院知识图谱、对话机器人项目和Facebook产品级NLP Service。 欢迎加入美团机器学习技术交流群，跟项目维护者零距离交流。进群方式：请加美美同学微信（微信号：MTDPtech02），回复：深度学习，美美会自动拉你进群。 ----------&nbsp; END&nbsp; ---------- 也许你还想看 美团餐饮娱乐知识图谱——美团大脑揭秘 美团大脑：知识图谱的建模方法及其应用 美团如何基于深度学习实现图像的智能审核？" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-29T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"总第327篇 2019年 第005篇 本文介绍了大众点评搜索核心排序层模型的演化之路，包括结合知识图谱信息构建适合搜索场景的Listwise深度学习排序模型LambdaDNN以及特征工程实践和相关工具建设。 1. 引言 挑战与思路 搜索是大众点评App上用户进行信息查找的最大入口，是连接用户和信息的重要纽带。而用户搜索的方式和场景非常多样，并且由于对接业务种类多，流量差异大，为大众点评搜索（下文简称点评搜索）带来了巨大的挑战，具体体现在如下几个方面： 意图多样：用户查找的信息类型和方式多样。信息类型包括POI、榜单、UGC、攻略、达人等。以找店为例，查找方式包括按距离、按热度、按菜品和按地理位置等多种方式。例如用户按照品牌进行搜索时，大概率是需要寻找距离最近或者常去的某家分店；但用户搜索菜品时，会对菜品推荐人数更加敏感，而距离因素会弱化。 业务多样：不同业务之间，用户的使用频率、选择难度以及业务诉求均不一样。例如家装场景用户使用频次很低，行为非常稀疏，距离因素弱，并且选择周期可能会很长；而美食多为即时消费场景，用户行为数据多，距离敏感。 用户类型多样：不同的用户对价格、距离、口味以及偏好的类目之间差异很大；搜索需要能深度挖掘到用户的各种偏好，实现定制化的“千人千面”的搜索。 LBS的搜索：相比电商和通用搜索，LBS的升维效应极大地增加了搜索场景的复杂性。例如对于旅游用户和常驻地用户来说，前者在搜索美食的时候可能会更加关心当地的知名特色商户，而对于距离相对不敏感。 上述的各项特性，叠加上时间、空间、场景等维度，使得点评搜索面临比通用搜索引擎更加独特的挑战。而解决这些挑战的方法，就需要升级NLP（Natural Language Processing，自然语言处理）技术，进行深度查询理解以及深度评价分析，并依赖知识图谱技术和深度学习技术对搜索架构进行整体升级。在美团NLP中心以及大众点评搜索智能中心两个团队的紧密合作之下，经过短短半年时间，点评搜索核心KPI在高位基础上仍然大幅提升，是过去一年半涨幅的六倍之多，提前半年完成全年目标。 基于知识图谱的搜索架构重塑 美团NLP中心正在构建全世界最大的餐饮娱乐知识图谱——美团大脑（相关信息请参见《美团大脑：知识图谱的建模方法及其应用》）。它充分挖掘关联各个场景数据，用NLP技术让机器“阅读”用户公开评论，理解用户在菜品、价格、服务、环境等方面的喜好，构建人、店、商品、场景之间的知识关联，从而形成一个“知识大脑”[1]。通过将知识图谱信息加入到搜索各个流程中，我们对点评搜索的整体架构进行了升级重塑，图1为点评搜索基于知识图谱搭建的5层搜索架构。本篇文章是“美团大脑”系列文章第二篇（系列首篇文章请参见《美团餐饮娱乐知识图谱——美团大脑揭秘》），主要介绍点评搜索5层架构中核心排序层的演变过程，文章主要分为如下3个部分： 核心排序从传统机器学习模型到大规模深度学习模型的演进。 搜索场景深度学习排序模型的特征工程实践。 适用于搜索场景的深度学习Listwise排序算法——LambdaDNN。 图1 基于知识图谱的点评搜索5层架构 2. 排序模型探索与实践 搜索排序问题在机器学习领域有一个单独的分支，Learning to Rank（L2R）。主要分类如下： 根据样本生成方法和Loss Function的不同，L2R可以分为Pointwise、Pairwise、Listwise。 按照模型结构划分，可以分为线性排序模型、树模型、深度学习模型，它们之间的组合（GBDT+LR，Deep&amp;Wide等）。 在排序模型方面，点评搜索也经历了业界比较普遍的迭代过程：从早期的线性模型LR，到引入自动二阶交叉特征的FM和FFM，到非线性树模型GBDT和GBDT+LR，到最近全面迁移至大规模深度学习排序模型。下面先简单介绍下传统机器学习模型（LR、FM、GBDT）的应用和优缺点，然后详细介绍深度模型的探索实践过程。 传统机器学习模型 图2 几种传统机器学习模型结构 LR可以视作单层单节点的线性网络结构。模型优点是可解释性强。通常而言，良好的解释性是工业界应用实践比较注重的一个指标，它意味着更好的可控性，同时也能指导工程师去分析问题优化模型。但是LR需要依赖大量的人工特征挖掘投入，有限的特征组合自然无法提供较强的表达能力。 FM可以看做是在LR的基础上增加了一部分二阶交叉项。引入自动的交叉特征有助于减少人工挖掘的投入，同时增加模型的非线性，捕捉更多信息。FM能够自动学习两两特征间的关系，但更高量级的特征交叉仍然无法满足。 GBDT是一个Boosting的模型，通过组合多个弱模型逐步拟合残差得到一个强模型。树模型具有天然的优势，能够很好的挖掘组合高阶统计特征，兼具较优的可解释性。GBDT的主要缺陷是依赖连续型的统计特征，对于高维度稀疏特征、时间序列特征不能很好的处理。 深度神经网络模型 随着业务的发展，在传统模型上取得指标收益变得愈发困难。同时业务的复杂性要求我们引入海量用户历史数据，超大规模知识图谱特征等多维度信息源，以实现精准个性化的排序。因此我们从2018年下半年开始，全力推进L2核心排序层的主模型迁移至深度学习排序模型。深度模型优势体现在如下几个方面： 强大的模型拟合能力：深度学习网络包含多个隐藏层和隐藏结点，配合上非线性的激活函数，理论上可以拟合任何函数，因此十分适用于点评搜索这种复杂的场景。 强大的特征表征和泛化能力：深度学习模型可以处理很多传统模型无法处理的特征。例如深度网络可以直接中从海量训练样本中学习到高维稀疏ID的隐含信息，并通过Embedding的方式去表征；另外对于文本、序列特征以及图像特征，深度网络均有对应的结构或者单元去处理。 自动组合和发现特征的能力：华为提出的DeepFM，以及Google提出的DeepCrossNetwork可以自动进行特征组合，代替大量人工组合特征的工作。 下图是我们基于Google提出的Wide&amp;Deep模型搭建的网络结构[2]。其中Wide部分输入的是LR、GBDT阶段常用的一些细粒度统计特征。通过较长周期统计的高频行为特征，能够提供很好的记忆能力。Deep部分通过深层的神经网络学习Low-Order、高维度稀疏的Categorical型特征，拟合样本中的长尾部分，发现新的特征组合，提高模型的泛化能力。同时对于文本、头图等传统机器学习模型难以刻画的特征，我们可以通过End-to-End的方式，利用相应的子网络模型进行预处理表示，然后进行融合学习。 图3 Deep&amp;Wide模型结构图 3. 搜索深度排序模型的特征工程实践 深度学习的横空出世，将算法工程师从很多人工挖掘和组合特征的事情中解放出来。甚至有一种论调，专做特征工程的算法工程师可能面临着失业的风险。但是深度学习的自动特征学习目前主要集中体现在CV领域，CV领域的特征数据是图片的像素点——稠密的低阶特征，深度学习通过卷积层这个强力工具，可以自动对低阶特征进行组合和变换，相比之前人工定义的图像特征从效果上来说确实更加显著。在NLP领域因为Transformer的出现，在自动特征挖掘上也有了长足的进步，BERT利用Transformer在多个NLP Task中取得了State-of-The-Art的效果。 但是对于CTR预估和排序学习的领域，目前深度学习尚未在自动特征挖掘上对人工特征工程形成碾压之势，因此人工特征工程依然很重要。当然，深度学习在特征工程上与传统模型的特征工程也存在着一些区别，我们的工作主要集中在如下几个方面。 3.1 特征预处理 特征归一化：深度网络的学习几乎都是基于反向传播，而此类梯度优化的方法对于特征的尺度非常敏感。因此，需要对特征进行归一化或者标准化以促使模型更好的收敛。 特征离散化：工业界一般很少直接使用连续值作为特征，而是将特征离散化后再输入到模型中。一方面因为离散化特征对于异常值具有更好的鲁棒性，其次可以为特征引入非线性的能力。并且，离散化可以更好的进行Embedding，我们主要使用如下两种离散化方法： 等频分桶：按样本频率进行等频切分，缺失值可以选择给一个默认桶值或者单独设置分桶。 树模型分桶：等频离散化的方式在特征分布特别不均匀的时候效果往往不好。此时可以利用单特征结合Label训练树模型，以树的分叉点做为切分值，相应的叶子节点作为桶号。 特征组合：基于业务场景对基础特征进行组合，形成更丰富的行为表征，为模型提供先验信息，可加速模型的收敛速度。典型示例如下： 用户性别与类目之间的交叉特征，能够刻画出不同性别的用户在类目上的偏好差异，比如男性用户可能会较少关注“丽人”相关的商户。 时间与类目之间的交叉特征，能够刻画出不同类目商户在时间上的差异，例如，酒吧在夜间会更容易被点击。 3.2 万物皆可Embedding 深度学习最大的魅力在于其强大的特征表征能力，在点评搜索场景下，我们有海量的用户行为数据，有丰富的商户UGC信息以及美团大脑提供的多维度细粒度标签数据。我们利用深度学习将这些信息Embedding到多个向量空间中，通过Embedding去表征用户的个性化偏好和商户的精准画像。同时向量化的Embedding也便于深度模型进一步的泛化、组合以及进行相似度的计算。 3.2.1 用户行为序列的Embedding 用户行为序列（搜索词序列、点击商户序列、筛选行为序列）包含了用户丰富的偏好信息。例如用户筛选了“距离优先”时，我们能够知道当前用户很有可能是一个即时消费的场景，并且对距离较为敏感。行为序列特征一般有如下图所示的三种接入方式: - Pooling：序列Embedding后接入Sum/Average Pooling层。此方式接入成本低，但忽略了行为的时序关系。 - RNN：LSTM/GRU接入，利用循环网络进行聚合。此方式能够考虑行为序列的时序关系；代价是增大了模型复杂度，影响线上预测性能。 - Attention：序列Embedding后引入Attention机制，表现为加权的Sum Pooling；相比LSTM/GRU计算开销更低[4]。 图4 行为序列特征接入的几种方法 同时，为了突显用户长期偏好和短期偏好对于排序的不同影响，我们按照时间维度对行为序列进行了划分：Session、半小时、一天、一周等粒度，也在线上取得了收益。 3.2.2 用户ID的Embedding 一种更常见的刻画用户偏好的方式，是直接将用户ID经过Embedding后作为特征接入到模型中，但是最后上线的效果却不尽如人意。通过分析用户的行为数据，我们发现相当一部分用户ID的行为数据较为稀疏，导致用户ID的Embedding没有充分收敛，未能充分刻画用户的偏好信息。 Airbnb发表在KDD 2018上的文章为这种问题提供了一种解决思路[9]——利用用户基础画像和行为数据对用户ID进行聚类。Airbnb的主要场景是为旅游用户提供民宿短租服务，一般用户一年旅游的次数在1-2次之间，因此Airbnb的用户行为数据相比点评搜索会更为稀疏一些。 图5 按照用户画像和行为信息聚类 如上图所示，将用户画像特征和行为特征进行离散分桶，拼接特征名和所属桶号，得到的聚类ID为：US_lt1_pn3_pg3_r3_5s4_c2_b1_bd2_bt2_nu3。 我们也采取了类似Airbnb的方案，稀疏性的问题得到了很好的解决，并且这样做还获得了一些额外的收益。大众点评作为一个本地化的生活信息服务平台，大部分用户的行为都集中自己的常驻地，导致用户到达一个新地方时，排序个性化明显不足。通过这种聚类的方式，将异地有相同行为的用户聚集在一起，也能解决一部分跨站的个性化问题。 3.2.3 商户信息Embedding 商户Embedding除了可以直接将商户ID加入模型中之外，美团大脑也利用深度学习技术对UGC进行大量挖掘，对商家的口味、特色等细粒度情感进行充分刻画，例如下图所示的“好停车”、“菜品精致”、“愿意再次光顾”等标签。 图6 美团大脑提供的商家细粒度情感标签 这些信息与单纯的商户星级、点评数相比，刻画的角度更多，粒度也更细。我们将这些标签也进行Embedding并输入到模型中： 直连：将标签特征做Pooling后直接输入模型。这种接入方式适合端到端的学习方式；但受输入层大小限制，只能取Top的标签，容易损失抽象实体信息。 分组直连：类似于直连接入的方式，但是先对标签进行分类，如菜品/风格/口味等类别；每个分类取Top N的实体后进行Pooling生成不同维度的语义向量。与不分组的直连相比，能够保留更多抽象信息。 子模型接入：可以利用DSSM模型，以标签作为商户输入学习商户的Embedding表达。此种方式能够最大化保留标签的抽象信息，但是线上实现和计算成本较高。 3.2.4 加速Embedding特征的收敛 在我们的深度学习排序模型中，除了Embedding特征，也存在大量Query、Shop和用户维度的强记忆特征，能够很快收敛。而Embedding特征是更为稀疏的弱特征，收敛速度较慢，为了加速Embedding特征的收敛，我们尝试了如下几种方案： 低频过滤：针对出现频率较低的特征进行过滤，可以很大程度上减少参数量，避免过拟合。 预训练：利用多类模型对稀疏Embedding特征进行预训练，然后进入模型进行微调： 通过无监督模型如Word2vec、Fasttext对用户-商户点击关系建模，生成共现关系下的商户Embedding。 利用DSSM等监督模型对Query-商户点击行为建模得到Query和商户的Embedding。 Multi-Task：针对稀疏的Embedding特征，单独设置一个子损失函数，如下图所示。此时Embedding特征的更新依赖两个损失函数的梯度，而子损失函数脱离了对强特征的依赖，可以加快Embedding特征的收敛。 图7 Multi-Task加速Embedding特征收敛 3.3 图片特征 图片在搜索结果页中占据了很大的展示面积，图片质量的好坏会直接影响用户的体验和点击，而点评商户首图来自于商户和用户上传的图片，质量参差不齐。因此，图片特征也是排序模型中较为重要的一类。目前点评搜索主要用了以下几类图片特征： 基础特征：提取图片的亮度、色度饱和度等基础信息，进行特征离散化后得到图片基础特征。 泛化特征：使用ResNet50进行图片特征提取[3]，通过聚类得到图片的泛化特征。 质量特征：使用自研的图片质量模型，提取中间层输出，作为图片质量的Embedding特征。 标签特征：提取图片是否是食物、环境、价目表、Logo等作为图片分类和标签特征。 图8 图片特征接入 4. 适用于搜索场景的深度学习Listwise排序算法：LambdaDNN 4.1 搜索业务指标与模型优化目标的Gap 通常模型的预测目标与业务指标总会存在一些Gap。如果模型的预测目标越贴近业务目标，越能保证模型优化的同时业务指标也能够有相应的提升；反之则会出现模型离线指标提升，但线上关键业务指标提升不明显，甚至出现负向的问题。工业届大部分深度学习排序采用Pointwise的Log Loss作为损失函数，与搜索业务指标有较大的Gap。体现在如下两个方面： 搜索业务常用的指标有QV_CTR或者SSR(Session Success Rate)，更关心的是用户搜索的成功率（有没有发生点击行为）；而Pointwise的Log Loss更多是关注单个Item的点击率。 搜索业务更关心排在页面头部结果的好坏，而Pointwise的方法则对于所有位置的样本一视同仁。 图9 Pointwise和Listwise优化目标的区别 基于上述理由，我们对于深度学习模型的损失函数进行了优化。 4.2 优化目标改进：从Log Loss到NDCG 为了让排序模型的优化目标尽量贴近搜索业务指标，需要按照Query计算损失，且不同位置的样本具有不同的权重。搜索系统常用的指标NDCG(Normalized Discounted Cumulative Gain)相较于Log Loss显然更贴近搜索业务的要求，NDCG计算公式如下： 累加部分为DCG(Discounted Cumulative Gain)表示按照位置折损的收益，对于Query下的结果列表l，函数G表示对应Doc的相关度分值，通常取指数函数，即G(lj)=2lj-1（lj表示的是相关度水平，如{0，1，2}）；函数 η 即位置折损，一般采用 η(j)=1/log(j+1)，Doc与Query的相关度越高且位置越靠前则DCG值会越大。另外，通常我们仅关注排序列表页前k位的效果，Zk 表示 DCG@k 的可能最大值，以此进行归一化处理后得到的就是NDCG@k。 问题在于NDCG是一个处处非平滑的函数，直接以它为目标函数进行优化是不可行的。LambdaRank提供了一种思路：绕过目标函数本身，直接构造一个特殊的梯度，按照梯度的方向修正模型参数，最终能达到拟合NDCG的方法[6]。因此，如果我们能将该梯度通过深度网络进行反向传播，则能训练一个优化NDCG的深度网络，该梯度我们称之为Lambda梯度，通过该梯度构造出的深度学习网络称之为LambdaDNN。 要了解Lambda梯度需要引入LambdaRank。LambdaRank模型是通过Pairwise来构造的，通常将同Query下有点击样本和无点击样本构造成一个样本Pair。模型的基本假设如下式所示，令Pij为同一个Query下Doci相比Docj更相关的概率，其中si和sj分别为Doci和Docj的模型得分： 使用交叉熵为损失函数，令Sij表示样本Pair的真实标记，当Doci比Docj更相关时（即Doci有被用户点击，而Docj没有被点击），有Sij=1，否则为-1；则损失函数可以表示为： 在构造样本Pair时，我们可以始终令i为更相关的文档，此时始终有Sij≡1，代入上式并进行求导，则损失函数的梯度为： 到目前为止，损失函数的计算过程中并未考虑样本所在的位置信息。因此进一步对梯度进行改造，考虑Doci和Docj交换位置时的NDCG值变化，下式即为前述的Lambda梯度。可以证明，通过此种方式构造出来的梯度经过迭代更新，最终可以达到优化NDCG的目的。 Lambda梯度的物理意义如下图所示。其中蓝色表示更相关（用户点击过）的文档，则Lambda梯度更倾向于位置靠上的Doc得到的提升更大（如红色箭头所示）。有了Lambda梯度的计算方法，训练中我们利用深度网络预测同Query下的Doc得分，根据用户实际点击Doc的情况计算Lambda梯度并反向传播回深度网络，则可以得到一个直接预测NDCG的深度网络。 图10 Lambda梯度的物理意义 4.3 LambdaDNN的工程实施 我们利用TensorFlow分布式框架训练LambdaDNN模型。如前文所述，Lambda梯度需要对同Query下的样本进行计算，但是正常情况下所有的样本是随机Shuffle到各个Worker的。因此我们需要对样本进行预处理： 通过QueryId进行Shuffle，将同一个Query的样本聚合在一起，同一个Query的样本打包进一个TFRecord。 由于每次请求Query召回的Doc数不一样，对于可变Size的Query样本在拉取数据进行训练时需要注意，TF会自动补齐Mini-Batch内每个样本大小一致，导致输入数据中存在大量无意义的默认值样本。这里我们提供两点处理方式： MR过程中对Key进行处理，使得多个Query的样本聚合在一起，然后在训练的时候进行动态切分。 读取到补齐的样本，根据设定的补齐标记获取索引位，去除补齐数据。 图11 Lambda梯度的分布式实现 为了提升训练效率，我们与基础研发平台数据平台中心紧密协同，一起探索并验证了多项优化操作： 将ID类特征的映射等操作一并在预处理中完成，减少多轮Training过程中的重复计算。 将样本转TfRecord，利用RecordDataSet方式读取数据并计算处理，Worker的计算性能大概提升了10倍。 Concat多个Categorical特征，组合成Multi-Hot的Tensor进行一次Embedding_Lookup操作，减少Map操作的同时有助于参数做分片存储计算。 稀疏Tensor在计算梯度以及正则化处理时保留索引值，仅对有数值的部分进行更新操作。 多个PS服务器间进行分片存储大规模Tensor变量，减少Worker同步更新的通讯压力，减少更新阻塞，达到更平滑的梯度更新效果。 整体下来，对于30亿左右的样本量、上亿级别的特征维度，一轮迭代大概在半小时内完成。适当的增加并行计算的资源，可以达到分钟级的训练任务。 4.4 进一步改进优化目标 NDCG的计算公式中，折损的权重是随着位置呈指数变化的。然而实际曝光点击率随位置变化的曲线与NDCG的理论折损值存在着较大的差异。 对于移动端的场景来说，用户在下拉滑动列表进行浏览时，视觉的焦点会随着滑屏、翻页而发生变动。例如用户翻到第二页时，往往会重新聚焦，因此，会发现第二页头部的曝光点击率实际上是高于第一页尾部位置的。我们尝试了两种方案去微调NDCG中的指数位置折损： 根据实际曝光点击率拟合折损曲线：根据实际统计到的曝光点击率数据，拟合公式替代NDCG中的指数折损公式，绘制的曲线如图12所示。 计算Position Bias作为位置折损：Position Bias在业界有较多的讨论，其中[7][8]将用户点击商户的过程分为观察和点击两个步骤：a.用户需要首先看到该商户，而看到商户的概率取决于所在的位置；b.看到商户后点击商户的概率只与商户的相关性有关。步骤a计算的概率即为Position Bias，这块内容可以讨论的东西很多，这里不再详述。 图12 真实位置折损与理论折损的差别 经过上述对NDCG计算改造训练出的LambdaDNN模型，相较Base树模型和Pointwise DNN模型，在业务指标上有了非常显著的提升。 图13 LambdaDNN离线NDCG指标与线上PvCtr效果对比 4.5 Lambda深度排序框架 Lambda梯度除了与DNN网络相结合外，事实上可以与绝大部分常见的网络结构相结合。为了进一步学习到更多交叉特征，我们在LambdaDNN的基础上分别尝试了LambdaDeepFM和LambdaDCN网络；其中DCN网络是一种加入Cross的并行网络结构，交叉的网络每一层的输出特征与第一层的原始输入特征进行显性的两两交叉，相当于每一层学习特征交叉的映射去拟合层之间的残差。 图14 DCN模型结构 离线的对比实验表明，Lambda梯度与DCN网络结合之后充分发挥了DCN网络的特点，简洁的多项式交叉设计有效地提升模型的训练效果。NDCG指标对比效果如下图所示： 图15 Lambda Loss与DCN网络结果的效果 5. 深度学习排序诊断系统 深度学习排序模型虽然给业务指标带来了大幅度的提升，但由于深度学习模型的“黑盒属性”导致了巨大的解释性成本，也给搜索业务带来了一些问题： 日常搜索Bad Case无法快速响应：搜索业务日常需要应对大量来自于用户、业务和老板们的“灵魂拷问”，“为何这个排序是这样的”，“为什么这家商户质量跟我差不多，但是会排在我的前面”。刚切换到深度学习排序模型的时候，我们对于这样的问题显得手足无措，需要花费大量的时间去定位问题。 无法从Bad Case中学习总结规律持续优化：如果不明白为什么排序模型会得出一个很坏的排序结果，自然也无法定位模型到底出了什么问题，也就无法根据Bad Case总结规律，从而确定模型和特征将来的优化方向。 模型和特征是否充分学习无从得知：新挖掘一些特征之后，通常我们会根据离线评测指标是否有提升决定特征是否上线。但是，即使一个有提升的特征，我们也无法知道这个特征是否性能足够好。例如，模型拟合的距离特征，会不会在特定的距离段出现距离越远反而打分越高的情况。 这些问题都会潜在带来一些用户无法理解的排序结果。我们需要对深度排序模型清晰地诊断并解释。 关于机器学习模型的可解释性研究，业界已经有了一些探索。Lime(Local Interpretable Model-Agnostic Explanations)是其中的一种，如下图所示：通过对单个样本的特征生成扰动产生近邻样本，观察模型的预测行为。根据这些扰动的数据点距离原始数据的距离分配权重，基于它们学习得到一个可解释的模型和预测结果[5]。举个例子，如果需要解释一个情感分类模型是如何预测“我讨厌这部电影”为负面情感的，我们通过丢掉部分词或者乱序构造一些样本预测情感，最终会发现，决定“我讨厌这部电影”为负面情感的是因为“讨厌”这个词。 图16 Lime解释器的工作原理 基于Lime解释器的思想，我们开发了一套深度模型解释器工具——雅典娜系统。目前雅典娜系统支持两种工作模式，Pairwise和Listwise模式： Pairwise模式用来解释同一个列表中两个结果之间的相对排序。通过对样本的特征进行重新赋值或者替换等操作，观察样本打分和排序位次的变化趋势，诊断出当前样本排序是否符合预期。如下图所示，通过右侧的特征位次面板可以快速诊断出为什么“南京大牌档”的排序比“金时代顺风港湾”要更靠前。第一行的特征位次信息显示，若将“金时代顺风港湾”的1.3km的距离特征用“南京大牌档”的0.2km的距离特征进行替换，排序位次将上升10位；由此得出，“南京大牌档”排在前面的决定性因素是因为距离近。 Listwise模式与Lime的工作模式基本类似，通过整个列表的样本生成扰动样本，训练线性分类器模型输出特征重要度，从而达到对模型进行解释的目的。 图17 深度学习排序诊断系统：雅典娜 6. 总结与展望 2018年下半年，点评搜索完成了从树模型到大规模深度学习排序模型的全面升级。团队在深度学习特征工程、模型结构、优化目标以及工程实践上都进行了一些探索，在核心指标上取得了较为显著的收益。当然，未来依然有不少可以探索的点。 在特征层面，大量知识图谱提供的标签信息尚未充分挖掘。从使用方式上看，简单以文本标签的形式接入，损失了知识图谱的结构信息，因此，Graph Embedding也是未来需要尝试的方向。同时团队也会利用BERT在Query和商户文本的深层语义表达上做一些工作。 模型结构层面，目前线上依然以全连接的DNN网络结构为主，但DNN网络结构在低秩数据的学习上不如DeepFM和DCN。目前LambdaDeepFM和LambdaDCN在离线上已经取得了收益，未来会在网络结构上做进一步优化。 在模型优化目标上，Lambda Loss计算损失的时候，只会考虑Query内部有点击和无点击的样本对，大量无点击的Query被丢弃，同时，同一个用户短时间内在不同Query下的行为也包含着一些信息可以利用。因此，目前团队正在探索综合考虑Log Loss和Lambda Loss的模型，通过Multi-Task和按照不同维度Shuffle样本让模型充分学习，目前我们已经在线下取得了一些收益。 最后，近期Google开源的TF Ranking提出的Groupwise模型也对我们有一些启发。目前绝大部分的Listwise方法只是体现在模型训练阶段，在打分预测阶段依然是Pointwise的，即只会考虑当前商户相关的特征，而不会考虑列表上下文的结果，未来我们也会在这个方向上进行一些探索。 参考资料 美团大脑：知识图谱的建模方法及其应用 Wide &amp; Deep Learning for Recommender Systems Deep Residual Learning for Image Recognition Attention Is All You Need Local Interpretable Mode l- Agnostic Explanations: LIME From RankNet to LambdaRank to LambdaMART: An Overview A Novel Algorithm for Unbiased Learning to Rank Unbiased Learning-to-Rank with Biased Feedback Real-time Personalization using Embeddings for Search Ranking at Airbnb 作者简介 非易，2016年加入美团点评，高级算法工程师，目前主要负责点评搜索核心排序层的研发工作。 祝升，2016年加入美团点评，高级算法工程师，目前负责点评搜索核心排序层的研发工作。 汤彪，2013年加入美团点评，高级算法专家，点评平台搜索技术负责人，致力于深层次查询理解和大规模深度学习排序的技术落地。 张弓，2012年加入美团点评，美团点评研究员。目前主要负责点评搜索业务演进，及集团搜索公共服务平台建设。 仲远，博士，美团AI平台部NLP中心负责人，点评搜索智能中心负责人。在国际顶级学术会议发表论文30余篇，获得ICDE 2015最佳论文奖，并是ACL 2016 Tutorial “Understanding Short Texts”主讲人，出版学术专著3部，获得美国专利5项。此前，博士曾担任微软亚洲研究院主管研究员，以及美国Facebook公司Research Scientist。曾负责微软研究院知识图谱、对话机器人项目和Facebook产品级NLP Service。 欢迎加入美团机器学习技术交流群，跟项目维护者零距离交流。进群方式：请加美美同学微信（微信号：MTDPtech02），回复：深度学习，美美会自动拉你进群。 ----------&nbsp; END&nbsp; ---------- 也许你还想看 美团餐饮娱乐知识图谱——美团大脑揭秘 美团大脑：知识图谱的建模方法及其应用 美团如何基于深度学习实现图像的智能审核？","@type":"BlogPosting","url":"/2019/04/29/729136.html","headline":"大众点评搜索基于知识图谱的深度学习排序实践","dateModified":"2019-04-29T00:00:00+08:00","datePublished":"2019-04-29T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/04/29/729136.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>大众点评搜索基于知识图谱的深度学习排序实践</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <p style="white-space: normal;margin-left: 0.5em;margin-right: 0.5em;" data-mpa-powered-by="yiban.io"><img class="" data-copyright="0" data-ratio="0.10546875" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsV6LYkM3uK5TAnl8DxXwdR4zA3FUoOfW6b1icLsE77CELpkNLzriajHTdibqkqVFYoldIoffibgkOslZA/640?wx_fmt=png" data-type="png" data-w="1280" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsV6LYkM3uK5TAnl8DxXwdR4zA3FUoOfW6b1icLsE77CELpkNLzriajHTdibqkqVFYoldIoffibgkOslZA/640?wx_fmt=png"></p> 
<p style="white-space: normal;text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><strong><span style="color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 1px;">总第327篇</span></strong></p> 
<p style="white-space: normal;text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><strong><span style="color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 1px;">2019年 第005篇</span></strong></p> 
<p style="white-space: normal;text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><strong><span style="color: rgb(136, 136, 136);font-size: 12px;letter-spacing: 1px;"><br></span></strong></p> 
<section data-role="outer" label="Powered by 135editor.com" style="font-size:16px;font-family:微软雅黑;"> 
 <section data-role="outer" label="Powered by 135editor.com"> 
  <section class="_135editor" data-tools="135编辑器" data-id="127" style="border-width: 0px;border-style: none;border-color: initial;box-sizing: border-box;"> 
   <section class="_135editor" data-tools="135编辑器" data-id="127" style="border-width: 0px;border-style: none;border-color: initial;box-sizing: border-box;"> 
    <section style="margin: 60px 16px 16px;border-width: 1px;border-style: solid;border-color: rgb(235, 234, 225);text-align: center;border-radius: 8px;font-size: 18px;font-weight: inherit;text-decoration: inherit;box-sizing: border-box;"> 
     <section style="margin-top: -3.3em;margin-right: 5px;margin-left: 5px;color: inherit;"> 
      <section style="border-width: 2px;border-style: solid;border-color: rgb(235, 234, 225);box-sizing: border-box;width: 108px;clear: both;margin-right: auto;margin-left: auto;height: 108px;border-radius: 50%;box-shadow: rgb(201, 201, 201) 0px 2px 2px 2px;background-color: rgb(254, 254, 254);"> 
       <img border="0" class="" data-ratio="1" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsXPiabnOCOibwq6W3OrlOJk0bticmbLiaNRY2EuaP6xLf175nvxAnroN0ycZTycP17hWOdLVy4eZylBMg/640?wx_fmt=jpeg" data-type="jpeg" data-w="800" data-width="100%" height="auto" opacity="" style="border-radius: 50%;box-sizing: border-box;color: inherit;display: inline-block;" title="undefined" width="100%" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsXPiabnOCOibwq6W3OrlOJk0bticmbLiaNRY2EuaP6xLf175nvxAnroN0ycZTycP17hWOdLVy4eZylBMg/640?wx_fmt=jpeg"> 
      </section> 
     </section> 
     <section class="135brush" data-brushtype="text" data-style="text-align: left; font-size: 14px; color: inherit;" style="margin: 8px 15px;line-height: 1.4;box-sizing: border-box;color: inherit;"> 
      <p style="text-align: left;"><span style="color: #a5a5a5;font-size: 13px;">本文介绍了大众点评搜索核心排序层模型的演化之路，包括结合知识图谱信息构建适合搜索场景的Listwise深度学习排序模型LambdaDNN以及特征工程实践和相关工具建设。</span></p> 
     </section> 
    </section> 
   </section> 
  </section> 
 </section> 
</section> 
<p style="white-space: normal;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);box-sizing: border-box !important;"><strong style="box-sizing: border-box !important;"></strong></span></p> 
<h2 style="color: rgb(0, 0, 0);font-size: 18px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-bottom: 15px;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 20px;color: rgb(37, 183, 167);">1. 引言</span></h2> 
<h3 style="margin-top: 10px;color: rgb(0, 0, 0);font-weight: bold;line-height: 1.5;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 18px;">挑战与思路</span></h3> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">搜索是大众点评App上用户进行信息查找的最大入口，是连接用户和信息的重要纽带。而用户搜索的方式和场景非常多样，并且由于对接业务种类多，流量差异大，为大众点评搜索（<span style="color: rgb(136, 136, 136);">下文简称点评搜索</span>）带来了巨大的挑战，具体体现在如下几个方面：</p> 
<ul class=" list-paddingleft-2" style="list-style-type: disc;"> 
 <li><p><span style="font-size: 14px;"><strong>意图多样</strong>：用户查找的信息类型和方式多样。信息类型包括POI、榜单、UGC、攻略、达人等。以找店为例，查找方式包括按距离、按热度、按菜品和按地理位置等多种方式。例如用户按照品牌进行搜索时，大概率是需要寻找距离最近或者常去的某家分店；但用户搜索菜品时，会对菜品推荐人数更加敏感，而距离因素会弱化。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>业务多样</strong>：不同业务之间，用户的使用频率、选择难度以及业务诉求均不一样。例如家装场景用户使用频次很低，行为非常稀疏，距离因素弱，并且选择周期可能会很长；而美食多为即时消费场景，用户行为数据多，距离敏感。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>用户类型多样</strong>：不同的用户对价格、距离、口味以及偏好的类目之间差异很大；搜索需要能深度挖掘到用户的各种偏好，实现定制化的“千人千面”的搜索。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>LBS的搜索</strong>：相比电商和通用搜索，LBS的升维效应极大地增加了搜索场景的复杂性。例如对于旅游用户和常驻地用户来说，前者在搜索美食的时候可能会更加关心当地的知名特色商户，而对于距离相对不敏感。</span></p></li> 
</ul> 
<p style="font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-top: 15px;margin-left: 0.5em;margin-right: 0.5em;">上述的各项特性，叠加上时间、空间、场景等维度，使得点评搜索面临比通用搜索引擎更加独特的挑战。而解决这些挑战的方法，就需要升级NLP（<span style="color: rgb(136, 136, 136);">Natural Language Processing，自然语言处理</span>）技术，进行深度查询理解以及深度评价分析，并依赖知识图谱技术和深度学习技术对搜索架构进行整体升级。在美团NLP中心以及大众点评搜索智能中心两个团队的紧密合作之下，经过短短半年时间，点评搜索核心KPI在高位基础上仍然大幅提升，是过去一年半涨幅的六倍之多，提前半年完成全年目标。</p> 
<h3 style="margin: 30px 0.5em 15px;color: rgb(0, 0, 0);font-weight: bold;line-height: 1.5;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="font-size: 18px;">基于知识图谱的搜索架构重塑</span></h3> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">美团NLP中心正在构建全世界最大的餐饮娱乐知识图谱——美团大脑（<span style="color: rgb(136, 136, 136);">相关信息请参见《</span><a href="http://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651749250&amp;idx=1&amp;sn=6f382206bed8d5e79999a8c957857ba5&amp;chksm=bd12a2cf8a652bd92e9bd226405fd9110a696077a5ec17f08f6be3f699afc5c4873008e923ec&amp;scene=21#wechat_redirect" target="_blank" style="color: rgb(136, 136, 136);text-decoration: underline;" data-linktype="2"><span style="color: rgb(136, 136, 136);">美团大脑：知识图谱的建模方法及其应用</span></a><span style="color: rgb(136, 136, 136);">》</span>）。它充分挖掘关联各个场景数据，用NLP技术让机器“阅读”用户公开评论，理解用户在菜品、价格、服务、环境等方面的喜好，构建人、店、商品、场景之间的知识关联，从而形成一个“知识大脑”[1]。通过将知识图谱信息加入到搜索各个流程中，我们对点评搜索的整体架构进行了升级重塑，图1为点评搜索基于知识图谱搭建的5层搜索架构。本篇文章是“美团大脑”系列文章第二篇（<span style="color: rgb(136, 136, 136);">系列首篇文章请参见《</span><a href="http://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651749520&amp;idx=1&amp;sn=e905e4ecefca3be58b46f59dcfb4b202&amp;chksm=bd12a5dd8a652ccb499914b029b898651946e21b5d7acc3fc2229c7afbc3d2f490ba685cbef0&amp;scene=21#wechat_redirect" target="_blank" style="color: rgb(136, 136, 136);text-decoration: underline;" data-linktype="2"><span style="color: rgb(136, 136, 136);">美团餐饮娱乐知识图谱——美团大脑揭秘</span></a><span style="color: rgb(136, 136, 136);">》</span><span style="color: rgb(0, 0, 0);">）</span>，主要介绍点评搜索5层架构中核心排序层的演变过程，文章主要分为如下3个部分：</p> 
<ul class=" list-paddingleft-2" style="list-style-type: disc;"> 
 <li><p><span style="font-size: 14px;">核心排序从传统机器学习模型到大规模深度学习模型的演进。</span></p></li> 
 <li><p><span style="font-size: 14px;">搜索场景深度学习排序模型的特征工程实践。</span></p></li> 
 <li><p style="margin-bottom: 15px;"><span style="font-size: 14px;">适用于搜索场景的深度学习Listwise排序算法——LambdaDNN。</span></p></li> 
</ul> 
<p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.5950570342205324" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXPiabnOCOibwq6W3OrlOJk0b7ibfjWibFbLIlLT2e5liaOEd0GgZFa4ZIz00TdfJ3Gw7nq6wcDEovTNvQ/640?wx_fmt=png" data-type="png" data-w="1052" style="width: 394px;height: 234px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXPiabnOCOibwq6W3OrlOJk0b7ibfjWibFbLIlLT2e5liaOEd0GgZFa4ZIz00TdfJ3Gw7nq6wcDEovTNvQ/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图1 基于知识图谱的点评搜索5层架构</span></p> 
<h2 style="margin: 30px 0.5em 15px;color: rgb(0, 0, 0);font-size: 18px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="color: rgb(37, 183, 167);font-size: 20px;">2. 排序模型探索与实践</span></h2> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">搜索排序问题在机器学习领域有一个单独的分支，Learning to Rank（<span style="color: rgb(136, 136, 136);">L2R</span>）。主要分类如下：</p> 
<ul class=" list-paddingleft-2" style="list-style-type: disc;"> 
 <li><p><span style="font-size: 14px;">根据样本生成方法和Loss Function的不同，L2R可以分为Pointwise、Pairwise、Listwise。</span></p></li> 
 <li><p><span style="font-size: 14px;">按照模型结构划分，可以分为线性排序模型、树模型、深度学习模型，它们之间的组合（</span><span style="font-size: 14px;color: rgb(136, 136, 136);">GBDT+LR，Deep&amp;Wide等</span><span style="font-size: 14px;">）。</span></p></li> 
</ul> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">在排序模型方面，点评搜索也经历了业界比较普遍的迭代过程：从早期的线性模型LR，到引入自动二阶交叉特征的FM和FFM，到非线性树模型GBDT和GBDT+LR，到最近全面迁移至大规模深度学习排序模型。下面先简单介绍下传统机器学习模型（<span style="color: rgb(136, 136, 136);">LR、FM、GBDT</span>）的应用和优缺点，然后详细介绍深度模型的探索实践过程。</p> 
<h4 style="margin: 20px 0.5em 15px;color: rgb(0, 0, 0);font-size: 14px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="font-size: 15px;">传统机器学习模型</span></h4> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.33444444444444443" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnvfxbdpyIVL66PDpoibUFDfVBQcuGOlUZSbUtiaqPPTX3CsRG1kQsjDxA/640?wx_fmt=png" data-type="png" data-w="900" style="width: 494px;height: 165px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnvfxbdpyIVL66PDpoibUFDfVBQcuGOlUZSbUtiaqPPTX3CsRG1kQsjDxA/640?wx_fmt=png"></p> 
<p style="margin-left: 0.5em;margin-right: 0.5em;text-align: center;margin-bottom: 15px;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图2 几种传统机器学习模型结构</span></p> 
<ul class=" list-paddingleft-2" style="list-style-type: disc;"> 
 <li><p><span style="font-size: 14px;">LR可以视作单层单节点的线性网络结构。模型优点是可解释性强。通常而言，良好的解释性是工业界应用实践比较注重的一个指标，它意味着更好的可控性，同时也能指导工程师去分析问题优化模型。但是LR需要依赖大量的人工特征挖掘投入，有限的特征组合自然无法提供较强的表达能力。</span></p></li> 
 <li><p><span style="font-size: 14px;">FM可以看做是在LR的基础上增加了一部分二阶交叉项。引入自动的交叉特征有助于减少人工挖掘的投入，同时增加模型的非线性，捕捉更多信息。FM能够自动学习两两特征间的关系，但更高量级的特征交叉仍然无法满足。</span></p></li> 
 <li><p><span style="font-size: 14px;">GBDT是一个Boosting的模型，通过组合多个弱模型逐步拟合残差得到一个强模型。树模型具有天然的优势，能够很好的挖掘组合高阶统计特征，兼具较优的可解释性。GBDT的主要缺陷是依赖连续型的统计特征，对于高维度稀疏特征、时间序列特征不能很好的处理。</span></p></li> 
</ul> 
<h4 style="margin-top: 20px;color: rgb(0, 0, 0);font-size: 14px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">深度神经网络模型</span></h4> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">随着业务的发展，在传统模型上取得指标收益变得愈发困难。同时业务的复杂性要求我们引入海量用户历史数据，超大规模知识图谱特征等多维度信息源，以实现精准个性化的排序。因此我们从2018年下半年开始，全力推进L2核心排序层的主模型迁移至深度学习排序模型。深度模型优势体现在如下几个方面：</p> 
<ul class=" list-paddingleft-2" style="list-style-type: disc;"> 
 <li><p><span style="font-size: 14px;"><strong>强大的模型拟合能力</strong>：深度学习网络包含多个隐藏层和隐藏结点，配合上非线性的激活函数，理论上可以拟合任何函数，因此十分适用于点评搜索这种复杂的场景。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>强大的特征表征和泛化能力</strong>：深度学习模型可以处理很多传统模型无法处理的特征。例如深度网络可以直接中从海量训练样本中学习到高维稀疏ID的隐含信息，并通过Embedding的方式去表征；另外对于文本、序列特征以及图像特征，深度网络均有对应的结构或者单元去处理。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>自动组合和发现特征的能力</strong>：华为提出的DeepFM，以及Google提出的DeepCrossNetwork可以自动进行特征组合，代替大量人工组合特征的工作。</span></p></li> 
</ul> 
<p style="font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-top: 15px;margin-left: 0.5em;margin-right: 0.5em;">下图是我们基于Google提出的Wide&amp;Deep模型搭建的网络结构[2]。其中Wide部分输入的是LR、GBDT阶段常用的一些细粒度统计特征。通过较长周期统计的高频行为特征，能够提供很好的记忆能力。Deep部分通过深层的神经网络学习Low-Order、高维度稀疏的Categorical型特征，拟合样本中的长尾部分，发现新的特征组合，提高模型的泛化能力。同时对于文本、头图等传统机器学习模型难以刻画的特征，我们可以通过End-to-End的方式，利用相应的子网络模型进行预处理表示，然后进行融合学习。</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.5411255411255411" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodn11sNytCRANZVKfeaMZAucwYPUrdjWukDATmNRShTwMwhQ18yASxcGw/640?wx_fmt=png" data-type="png" data-w="693" style="width: 414px;height: 224px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodn11sNytCRANZVKfeaMZAucwYPUrdjWukDATmNRShTwMwhQ18yASxcGw/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图3 Deep&amp;Wide模型结构图</span></p> 
<h2 style="margin: 30px 0.5em 15px;color: rgb(0, 0, 0);font-size: 18px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="color: rgb(37, 183, 167);font-size: 20px;">3. 搜索深度排序模型的特征工程实践</span></h2> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">深度学习的横空出世，将算法工程师从很多人工挖掘和组合特征的事情中解放出来。甚至有一种论调，专做特征工程的算法工程师可能面临着失业的风险。但是深度学习的自动特征学习目前主要集中体现在CV领域，CV领域的特征数据是图片的像素点——稠密的低阶特征，深度学习通过卷积层这个强力工具，可以自动对低阶特征进行组合和变换，相比之前人工定义的图像特征从效果上来说确实更加显著。在NLP领域因为Transformer的出现，在自动特征挖掘上也有了长足的进步，BERT利用Transformer在多个NLP Task中取得了State-of-The-Art的效果。</p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">但是对于CTR预估和排序学习的领域，目前深度学习尚未在自动特征挖掘上对人工特征工程形成碾压之势，因此人工特征工程依然很重要。当然，深度学习在特征工程上与传统模型的特征工程也存在着一些区别，我们的工作主要集中在如下几个方面。</p> 
<h3 style="margin: 30px 0.5em 15px;color: rgb(0, 0, 0);font-weight: bold;line-height: 1.5;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="font-size: 18px;">3.1 特征预处理</span></h3> 
<ul style="margin-left: 0.5em;margin-right: 0.5em;" class=" list-paddingleft-2"> 
 <li><p><span style="font-size: 14px;"><strong>特征归一化</strong>：深度网络的学习几乎都是基于反向传播，而此类梯度优化的方法对于特征的尺度非常敏感。因此，需要对特征进行归一化或者标准化以促使模型更好的收敛。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>特征离散化</strong>：工业界一般很少直接使用连续值作为特征，而是将特征离散化后再输入到模型中。一方面因为离散化特征对于异常值具有更好的鲁棒性，其次可以为特征引入非线性的能力。并且，离散化可以更好的进行Embedding，我们主要使用如下两种离散化方法：</span></p></li> 
 <ul class=" list-paddingleft-2" style="list-style-type: circle;"> 
  <li><p><span style="font-size: 14px;">等频分桶：按样本频率进行等频切分，缺失值可以选择给一个默认桶值或者单独设置分桶。</span></p></li> 
  <li><p><span style="font-size: 14px;">树模型分桶：等频离散化的方式在特征分布特别不均匀的时候效果往往不好。此时可以利用单特征结合Label训练树模型，以树的分叉点做为切分值，相应的叶子节点作为桶号。</span></p></li> 
 </ul> 
 <li><p><span style="font-size: 14px;"><strong>特征组合</strong>：基于业务场景对基础特征进行组合，形成更丰富的行为表征，为模型提供先验信息，可加速模型的收敛速度。典型示例如下：</span></p></li> 
 <ul class=" list-paddingleft-2" style="list-style-type: circle;"> 
  <li><p><span style="font-size: 14px;">用户性别与类目之间的交叉特征，能够刻画出不同性别的用户在类目上的偏好差异，比如男性用户可能会较少关注“丽人”相关的商户。</span></p></li> 
  <li><p><span style="font-size: 14px;">时间与类目之间的交叉特征，能够刻画出不同类目商户在时间上的差异，例如，酒吧在夜间会更容易被点击。</span></p></li> 
 </ul> 
</ul> 
<h3 style="margin-top: 30px;color: rgb(0, 0, 0);font-weight: bold;line-height: 1.5;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 18px;">3.2 万物皆可Embedding</span></h3> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">深度学习最大的魅力在于其强大的特征表征能力，在点评搜索场景下，我们有海量的用户行为数据，有丰富的商户UGC信息以及美团大脑提供的多维度细粒度标签数据。我们利用深度学习将这些信息Embedding到多个向量空间中，通过Embedding去表征用户的个性化偏好和商户的精准画像。同时向量化的Embedding也便于深度模型进一步的泛化、组合以及进行相似度的计算。</p> 
<h4 style="margin: 20px 0.5em 15px;color: rgb(0, 0, 0);font-size: 14px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="font-size: 15px;">3.2.1 用户行为序列的Embedding</span></h4> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">用户行为序列（<span style="color: rgb(136, 136, 136);">搜索词序列、点击商户序列、筛选行为序列</span>）包含了用户丰富的偏好信息。例如用户筛选了“距离优先”时，我们能够知道当前用户很有可能是一个即时消费的场景，并且对距离较为敏感。行为序列特征一般有如下图所示的三种接入方式:</p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">- <strong>Pooling</strong>：序列Embedding后接入Sum/Average Pooling层。此方式接入成本低，但忽略了行为的时序关系。</p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">- <strong>RNN</strong>：LSTM/GRU接入，利用循环网络进行聚合。此方式能够考虑行为序列的时序关系；代价是增大了模型复杂度，影响线上预测性能。</p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">- <strong>Attention</strong>：序列Embedding后引入Attention机制，表现为加权的Sum Pooling；相比LSTM/GRU计算开销更低[4]。</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.5492001938923897" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnicfw4aWrnicZxDRCqAjYdnHoibz0gQNTe5DmtwPl5NkaokCn6GAfPTRMQ/640?wx_fmt=png" data-type="png" data-w="2063" style="width: 303px;height: 166px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnicfw4aWrnicZxDRCqAjYdnHoibz0gQNTe5DmtwPl5NkaokCn6GAfPTRMQ/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图4 行为序列特征接入的几种方法</span></p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">同时，为了突显用户长期偏好和短期偏好对于排序的不同影响，我们按照时间维度对行为序列进行了划分：Session、半小时、一天、一周等粒度，也在线上取得了收益。</p> 
<h4 style="margin-top: 20px;color: rgb(0, 0, 0);font-size: 14px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">3.2.2 用户ID的Embedding</span></h4> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">一种更常见的刻画用户偏好的方式，是直接将用户ID经过Embedding后作为特征接入到模型中，但是最后上线的效果却不尽如人意。通过分析用户的行为数据，我们发现相当一部分用户ID的行为数据较为稀疏，导致用户ID的Embedding没有充分收敛，未能充分刻画用户的偏好信息。</p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">Airbnb发表在KDD 2018上的文章为这种问题提供了一种解决思路[9]——利用用户基础画像和行为数据对用户ID进行聚类。Airbnb的主要场景是为旅游用户提供民宿短租服务，一般用户一年旅游的次数在1-2次之间，因此Airbnb的用户行为数据相比点评搜索会更为稀疏一些。</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-croporisrc="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnBZwicjAGXqfRJSkL3Dg4WCJialDwbLc4uIFgyt2ouBCSH5CAAk8ZzGcw/0?wx_fmt=png" data-cropx1="0" data-cropx2="1530.1151079136691" data-cropy1="0" data-cropy2="660.9208633093525" data-ratio="0.43125" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnPtjhsfrsSuJ6icyUallKVOXqNbLicUXEGKQbb4pnkS8cLgDRHsLvbKBA/640?wx_fmt=jpeg" data-type="jpeg" data-w="1280" style="width: 439px;height: 189px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnPtjhsfrsSuJ6icyUallKVOXqNbLicUXEGKQbb4pnkS8cLgDRHsLvbKBA/640?wx_fmt=jpeg"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图5 按照用户画像和行为信息聚类</span></p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">如上图所示，将用户画像特征和行为特征进行离散分桶，拼接特征名和所属桶号，得到的聚类ID为：US_lt1_pn3_pg3_r3_5s4_c2_b1_bd2_bt2_nu3。</p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">我们也采取了类似Airbnb的方案，稀疏性的问题得到了很好的解决，并且这样做还获得了一些额外的收益。<span style="font-size: 15px;"><span style="">大众点评作为一个本地化的生活信息服务平台</span>，</span>大部分用户的行为都集中自己的常驻地，导致用户到达一个新地方时，排序个性化明显不足。通过这种聚类的方式，将异地有相同行为的用户聚集在一起，也能解决一部分跨站的个性化问题。</p> 
<h4 style="margin: 20px 0.5em 15px;color: rgb(0, 0, 0);font-size: 14px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="font-size: 15px;">3.2.3 商户信息Embedding</span></h4> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">商户Embedding除了可以直接将商户ID加入模型中之外，美团大脑也利用深度学习技术对UGC进行大量挖掘，对商家的口味、特色等细粒度情感进行充分刻画，例如下图所示的“好停车”、“菜品精致”、“愿意再次光顾”等标签。</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="1.684507042253521" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnyUylQrgiaNiaZzJbpUb54AS5qqhgdsibsuIO00GTACPhVLxpPia9diaLwMQ/640?wx_fmt=png" data-type="png" data-w="710" style="width: 261px;height: 440px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnyUylQrgiaNiaZzJbpUb54AS5qqhgdsibsuIO00GTACPhVLxpPia9diaLwMQ/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图6 美团大脑提供的商家细粒度情感标签</span></p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">这些信息与单纯的商户星级、点评数相比，刻画的角度更多，粒度也更细。我们将这些标签也进行Embedding并输入到模型中：</p> 
<ul style="margin-left: 0.5em;margin-right: 0.5em;" class=" list-paddingleft-2"> 
 <li><p><span style="font-size: 14px;"><strong>直连</strong>：将标签特征做Pooling后直接输入模型。这种接入方式适合端到端的学习方式；但受输入层大小限制，只能取Top的标签，容易损失抽象实体信息。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>分组直连</strong>：类似于直连接入的方式，但是先对标签进行分类，如菜品/风格/口味等类别；每个分类取Top N的实体后进行Pooling生成不同维度的语义向量。与不分组的直连相比，能够保留更多抽象信息。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>子模型接入</strong>：可以利用DSSM模型，以标签作为商户输入学习商户的Embedding表达。此种方式能够最大化保留标签的抽象信息，但是线上实现和计算成本较高。</span></p></li> 
</ul> 
<h4 style="margin-top: 20px;color: rgb(0, 0, 0);font-size: 14px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;">3.2.4 加速Embedding特征的收敛</span></h4> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">在我们的深度学习排序模型中，除了Embedding特征，也存在大量Query、Shop和用户维度的强记忆特征，能够很快收敛。而Embedding特征是更为稀疏的弱特征，收敛速度较慢，为了加速Embedding特征的收敛，我们尝试了如下几种方案：</p> 
<ul style="margin-left: 0.5em;margin-right: 0.5em;" class=" list-paddingleft-2"> 
 <li><p><span style="font-size: 14px;"><strong>低频过滤</strong>：针对出现频率较低的特征进行过滤，可以很大程度上减少参数量，避免过拟合。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>预训练</strong>：利用多类模型对稀疏Embedding特征进行预训练，然后进入模型进行微调：</span></p></li> 
 <ul class=" list-paddingleft-2" style="list-style-type: circle;"> 
  <li><p><span style="font-size: 14px;">通过无监督模型如Word2vec、Fasttext对用户-商户点击关系建模，生成共现关系下的商户Embedding。</span></p></li> 
  <li><p><span style="font-size: 14px;">利用DSSM等监督模型对Query-商户点击行为建模得到Query和商户的Embedding。</span></p></li> 
 </ul> 
 <li><p style="margin-bottom: 15px;"><span style="font-size: 14px;"><strong>Multi-Task</strong>：针对稀疏的Embedding特征，单独设置一个子损失函数，如下图所示。此时Embedding特征的更新依赖两个损失函数的梯度，而子损失函数脱离了对强特征的依赖，可以加快Embedding特征的收敛。</span></p></li> 
</ul> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.6516007532956686" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnicBRCiaK92uibW2WhDV9AxBwSia7FHAn8M00nZlHNAxp6z1WoGZnQc0FMg/640?wx_fmt=png" data-type="png" data-w="1062" style="width: 312px;height: 203px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnicBRCiaK92uibW2WhDV9AxBwSia7FHAn8M00nZlHNAxp6z1WoGZnQc0FMg/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图7 Multi-Task加速Embedding特征收敛</span></p> 
<h3 style="margin-top: 30px;color: rgb(0, 0, 0);font-weight: bold;line-height: 1.5;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 18px;">3.3 图片特征</span></h3> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">图片在搜索结果页中占据了很大的展示面积，图片质量的好坏会直接影响用户的体验和点击，而点评商户首图来自于商户和用户上传的图片，质量参差不齐。因此，图片特征也是排序模型中较为重要的一类。目前点评搜索主要用了以下几类图片特征：</p> 
<ul style="margin-left: 0.5em;margin-right: 0.5em;" class=" list-paddingleft-2"> 
 <li><p><span style="font-size: 14px;"><strong>基础特征</strong>：提取图片的亮度、色度饱和度等基础信息，进行特征离散化后得到图片基础特征。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>泛化特征</strong>：使用ResNet50进行图片特征提取[3]，通过聚类得到图片的泛化特征。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>质量特征</strong>：使用自研的图片质量模型，提取中间层输出，作为图片质量的Embedding特征。</span></p></li> 
 <li><p style="margin-bottom: 15px;"><span style="font-size: 14px;"><strong>标签特征</strong>：提取图片是否是食物、环境、价目表、Logo等作为图片分类和标签特征。</span></p></li> 
</ul> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.15599343185550082" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnwiaCibznPN3Kt0vmKL5diaEkfBQevcTOTibPCicnVHQBR8iaSGcoOYVW0GmQ/640?wx_fmt=png" data-type="png" data-w="1218" style="width: 492px;height: 77px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnwiaCibznPN3Kt0vmKL5diaEkfBQevcTOTibPCicnVHQBR8iaSGcoOYVW0GmQ/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图8 图片特征接入</span></p> 
<h2 style="margin-top: 30px;color: rgb(0, 0, 0);font-size: 18px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(37, 183, 167);font-size: 20px;">4. 适用于搜索场景的深度学习Listwise排序算法：LambdaDNN</span></h2> 
<h3 style="margin: 10px 0.5em 15px;color: rgb(0, 0, 0);font-weight: bold;line-height: 1.5;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="font-size: 18px;">4.1 搜索业务指标与模型优化目标的Gap</span></h3> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">通常模型的预测目标与业务指标总会存在一些Gap。如果模型的预测目标越贴近业务目标，越能保证模型优化的同时业务指标也能够有相应的提升；反之则会出现模型离线指标提升，但线上关键业务指标提升不明显，甚至出现负向的问题。工业届大部分深度学习排序采用Pointwise的Log Loss作为损失函数，与搜索业务指标有较大的Gap。体现在如下两个方面：</p> 
<ul class=" list-paddingleft-2" style="list-style-type: disc;"> 
 <li><p><span style="font-size: 14px;">搜索业务常用的指标有QV_CTR或者SSR(</span><span style="font-size: 14px;color: rgb(136, 136, 136);">Session Success Rate</span><span style="font-size: 14px;">)，更关心的是用户搜索的成功率（</span><span style="font-size: 14px;color: rgb(136, 136, 136);">有没有发生点击行为</span><span style="font-size: 14px;">）；而Pointwise的Log Loss更多是关注单个Item的点击率。</span></p></li> 
 <li><p><span style="font-size: 14px;">搜索业务更关心排在页面头部结果的好坏，而Pointwise的方法则对于所有位置的样本一视同仁。</span></p></li> 
</ul> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.6598557692307693" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnAWwBVFME06VSTibO7Voib3ZGibeGBdO8XylqvmjicvqGb7MkNKvKyyJehw/640?wx_fmt=png" data-type="png" data-w="1664" style="width: 389px;height: 257px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnAWwBVFME06VSTibO7Voib3ZGibeGBdO8XylqvmjicvqGb7MkNKvKyyJehw/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图9 Pointwise和Listwise优化目标的区别</span></p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">基于上述理由，我们对于深度学习模型的损失函数进行了优化。</p> 
<h3 style="margin: 30px 0.5em 15px;color: rgb(0, 0, 0);font-weight: bold;line-height: 1.5;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="font-size: 18px;">4.2 优化目标改进：从Log Loss到NDCG</span></h3> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">为了让排序模型的优化目标尽量贴近搜索业务指标，需要按照Query计算损失，且不同位置的样本具有不同的权重。搜索系统常用的指标NDCG(<span style="color: rgb(136, 136, 136);">Normalized Discounted Cumulative Gain</span>)相较于Log Loss显然更贴近搜索业务的要求，NDCG计算公式如下：</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.26011560693641617" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnV3gwd3dV8YC4VvCzw4Fr1eKTlpPudpvN0d1y7z6cibghjSbTq5yN2mA/640?wx_fmt=png" data-type="png" data-w="2076" style="width: 223px;height: 58px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnV3gwd3dV8YC4VvCzw4Fr1eKTlpPudpvN0d1y7z6cibghjSbTq5yN2mA/640?wx_fmt=png"></p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">累加部分为DCG(<span style="color: rgb(136, 136, 136);">Discounted Cumulative Gain</span>)表示按照位置折损的收益，对于Query下的结果列表l，函数G表示对应Doc的相关度分值，通常取指数函数，即G(<span style="color: rgb(0, 0, 0);">l<sub>j</sub></span>)=2<sup>l<sub>j</sub></sup>-1（<span style="color: rgb(136, 136, 136);">l<sub>j</sub>表示的是相关度水平，如{0，1，2}</span>）；函数 η 即位置折损，一般采用 η(<span style="color: rgb(0, 0, 0);">j</span>)=1/log(<span style="color: rgb(0, 0, 0);">j+1</span>)，Doc与Query的相关度越高且位置越靠前则DCG值会越大。另外，通常我们仅关注排序列表页前k位的效果，Z<sub>k</sub> 表示 DCG@k 的可能最大值，以此进行归一化处理后得到的就是NDCG@k。</p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">问题在于NDCG是一个处处非平滑的函数，直接以它为目标函数进行优化是不可行的。LambdaRank提供了一种思路：绕过目标函数本身，直接构造一个特殊的梯度，按照梯度的方向修正模型参数，最终能达到拟合NDCG的方法[6]。因此，如果我们能将该梯度通过深度网络进行反向传播，则能训练一个优化NDCG的深度网络，该梯度我们称之为Lambda梯度，通过该梯度构造出的深度学习网络称之为LambdaDNN。</p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">要了解Lambda梯度需要引入LambdaRank。LambdaRank模型是通过Pairwise来构造的，通常将同Query下有点击样本和无点击样本构造成一个样本Pair。模型的基本假设如下式所示，令P<sub>ij</sub>为同一个Query下Doc<sub>i</sub>相比Doc<sub>j</sub>更相关的概率，其中s<sub>i</sub>和s<sub>j</sub>分别为Doc<sub>i</sub>和Doc<sub>j</sub>的模型得分：</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.22589531680440772" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnGGrxOZZ0SvEDVbxlmmvibZibfNOXHOOZyUW8T6iasXYs8ZG5KFY0TuB0g/640?wx_fmt=png" data-type="png" data-w="726" style="width: 263px;height: 59px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnGGrxOZZ0SvEDVbxlmmvibZibfNOXHOOZyUW8T6iasXYs8ZG5KFY0TuB0g/640?wx_fmt=png"></p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">使用交叉熵为损失函数，令S<sub>ij</sub>表示样本Pair的真实标记，当Doc<sub>i</sub>比Doc<sub>j</sub>更相关时（<span style="color: rgb(136, 136, 136);">即Doc<sub>i</sub>有被用户点击，而Doc<sub>j</sub>没有被点击</span>），有S<sub>ij</sub>=1，否则为-1；则损失函数可以表示为：</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.17062634989200864" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnsqbnpulaGNiccVITedEOricrJEGqZLD3eR6L9YKYl4YEM3kQIdsWlk3g/640?wx_fmt=png" data-type="png" data-w="926" style="width: 269px;height: 46px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnsqbnpulaGNiccVITedEOricrJEGqZLD3eR6L9YKYl4YEM3kQIdsWlk3g/640?wx_fmt=png"></p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">在构造样本Pair时，我们可以始终令i为更相关的文档，此时始终有S<sub>ij</sub>≡1，代入上式并进行求导，则损失函数的梯度为：</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.24923076923076923" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnnbAfcTBAVGwF6DI70BPwcNmXoMzF9rkicr3fOfIcCicKZXZI5PAU7hbg/640?wx_fmt=png" data-type="png" data-w="650" style="width: 238px;height: 59px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnnbAfcTBAVGwF6DI70BPwcNmXoMzF9rkicr3fOfIcCicKZXZI5PAU7hbg/640?wx_fmt=png"></p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">到目前为止，损失函数的计算过程中并未考虑样本所在的位置信息。因此进一步对梯度进行改造，考虑Doc<sub>i</sub>和Doc<sub>j</sub>交换位置时的NDCG值变化，下式即为前述的Lambda梯度。可以证明，通过此种方式构造出来的梯度经过迭代更新，最终可以达到优化NDCG的目的。</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.19158878504672897" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnCTolRly1XnPL4KUsQOSVkl2SfP63A27ow5bBTyicUEX19DeAkrwNuHw/640?wx_fmt=png" data-type="png" data-w="856" style="width: 288px;height: 55px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnCTolRly1XnPL4KUsQOSVkl2SfP63A27ow5bBTyicUEX19DeAkrwNuHw/640?wx_fmt=png"></p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">Lambda梯度的物理意义如下图所示。其中蓝色表示更相关（<span style="color: rgb(136, 136, 136);">用户点击过</span>）的文档，则Lambda梯度更倾向于位置靠上的Doc得到的提升更大（<span style="color: rgb(136, 136, 136);">如红色箭头所示</span>）。有了Lambda梯度的计算方法，训练中我们利用深度网络预测同Query下的Doc得分，根据用户实际点击Doc的情况计算Lambda梯度并反向传播回深度网络，则可以得到一个直接预测NDCG的深度网络。</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-croporisrc="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnxhW07F9Vcm4gdb9HibhK1ibql8K9QSeibo22gDssKVDPlr26riacfIxj4w/0?wx_fmt=png" data-cropx1="0" data-cropx2="658.7608695652174" data-cropy1="38.608695652173914" data-cropy2="576.7173913043479" data-ratio="0.8161094224924013" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnbXwPfZPjHwI7DmMFtYTiczYGZQIrTIoiaEiawPjLgbuN34LELvjgBdfHQ/640?wx_fmt=jpeg" data-type="jpeg" data-w="658" style="width: 273px;height: 223px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnbXwPfZPjHwI7DmMFtYTiczYGZQIrTIoiaEiawPjLgbuN34LELvjgBdfHQ/640?wx_fmt=jpeg"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图10 Lambda梯度的物理意义</span></p> 
<h3 style="margin-top: 30px;color: rgb(0, 0, 0);font-weight: bold;line-height: 1.5;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 18px;">4.3 LambdaDNN的工程实施</span></h3> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">我们利用TensorFlow分布式框架训练LambdaDNN模型。如前文所述，Lambda梯度需要对同Query下的样本进行计算，但是正常情况下所有的样本是随机Shuffle到各个Worker的。因此我们需要对样本进行预处理：</p> 
<ul class=" list-paddingleft-2" style="list-style-type: disc;"> 
 <li><p><span style="font-size: 14px;">通过QueryId进行Shuffle，将同一个Query的样本聚合在一起，同一个Query的样本打包进一个TFRecord。</span></p></li> 
 <li><p><span style="font-size: 14px;">由于每次请求Query召回的Doc数不一样，对于可变Size的Query样本在拉取数据进行训练时需要注意，TF会自动补齐Mini-Batch内每个样本大小一致，导致输入数据中存在大量无意义的默认值样本。这里我们提供两点处理方式：</span></p></li> 
</ul> 
<ol style="margin-left: 0.5em;margin-right: 0.5em;" class=" list-paddingleft-2"> 
 <ul class=" list-paddingleft-2" style=""> 
  <li><p><span style="font-size: 14px;">MR过程中对Key进行处理，使得多个Query的样本聚合在一起，然后在训练的时候进行动态切分。</span></p></li> 
  <li><p style="margin-bottom: 15px;"><span style="font-size: 14px;">读取到补齐的样本，根据设定的补齐标记获取索引位，去除补齐数据。</span></p></li> 
 </ul> 
</ol> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.5370813397129187" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnXlaaoumBKWSC5xmazcicpuZ9mrUpWoHEDxicOAvn9omp6gQ7Za3DYYIQ/640?wx_fmt=png" data-type="png" data-w="1672" style="width: 348px;height: 187px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnXlaaoumBKWSC5xmazcicpuZ9mrUpWoHEDxicOAvn9omp6gQ7Za3DYYIQ/640?wx_fmt=png"></p> 
<p style="margin-left: 0.5em;margin-right: 0.5em;text-align: center;margin-bottom: 15px;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图11 Lambda梯度的分布式实现</span></p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">为了提升训练效率，我们与基础研发平台数据平台中心紧密协同，一起探索并验证了多项优化操作：</p> 
<ul class=" list-paddingleft-2" style="list-style-type: disc;"> 
 <li><p><span style="font-size: 14px;">将ID类特征的映射等操作一并在预处理中完成，减少多轮Training过程中的重复计算。</span></p></li> 
 <li><p><span style="font-size: 14px;">将样本转TfRecord，利用RecordDataSet方式读取数据并计算处理，Worker的计算性能大概提升了10倍。</span></p></li> 
 <li><p><span style="font-size: 14px;">Concat多个Categorical特征，组合成Multi-Hot的Tensor进行一次Embedding_Lookup操作，减少Map操作的同时有助于参数做分片存储计算。</span></p></li> 
 <li><p><span style="font-size: 14px;">稀疏Tensor在计算梯度以及正则化处理时保留索引值，仅对有数值的部分进行更新操作。</span></p></li> 
 <li><p><span style="font-size: 14px;">多个PS服务器间进行分片存储大规模Tensor变量，减少Worker同步更新的通讯压力，减少更新阻塞，达到更平滑的梯度更新效果。</span></p></li> 
</ul> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">整体下来，对于30亿左右的样本量、上亿级别的特征维度，一轮迭代大概在半小时内完成。适当的增加并行计算的资源，可以达到分钟级的训练任务。</p> 
<h3 style="margin: 30px 0.5em 15px;color: rgb(0, 0, 0);font-weight: bold;line-height: 1.5;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="font-size: 18px;">4.4 进一步改进优化目标</span></h3> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">NDCG的计算公式中，折损的权重是随着位置呈指数变化的。然而实际曝光点击率随位置变化的曲线与NDCG的理论折损值存在着较大的差异。</p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">对于移动端的场景来说，用户在下拉滑动列表进行浏览时，视觉的焦点会随着滑屏、翻页而发生变动。例如用户翻到第二页时，往往会重新聚焦，因此，会发现第二页头部的曝光点击率实际上是高于第一页尾部位置的。我们尝试了两种方案去微调NDCG中的指数位置折损：</p> 
<ul class=" list-paddingleft-2" style="list-style-type: disc;"> 
 <li><p><span style="font-size: 14px;"><strong>根据实际曝光点击率拟合折损曲线</strong>：根据实际统计到的曝光点击率数据，拟合公式替代NDCG中的指数折损公式，绘制的曲线如图12所示。</span></p></li> 
 <li><p style="margin-bottom: 15px;"><span style="font-size: 14px;"><strong>计算Position Bias作为位置折损</strong>：Position Bias在业界有较多的讨论，其中[7][8]将用户点击商户的过程分为观察和点击两个步骤：a.用户需要首先看到该商户，而看到商户的概率取决于所在的位置；b.看到商户后点击商户的概率只与商户的相关性有关。步骤a计算的概率即为Position Bias，这块内容可以讨论的东西很多，这里不再详述。</span></p></li> 
</ul> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.6568627450980392" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnsMW9DXX9hxk0c6jia6iaib2Rg9orK6KLd8Hgf7lAXRSxwLT6OFibvfZUEg/640?wx_fmt=png" data-type="png" data-w="1224" style="width: 339px;height: 223px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnsMW9DXX9hxk0c6jia6iaib2Rg9orK6KLd8Hgf7lAXRSxwLT6OFibvfZUEg/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图12 真实位置折损与理论折损的差别</span></p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">经过上述对NDCG计算改造训练出的LambdaDNN模型，相较Base树模型和Pointwise DNN模型，在业务指标上有了非常显著的提升。</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.24893314366998578" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnhp734SN73r2GjHHebAgEcibjtfqsUlKcekubgRK1e63NedymXkXJ0pw/640?wx_fmt=png" data-type="png" data-w="1406" style="width: 420px;height: 105px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnhp734SN73r2GjHHebAgEcibjtfqsUlKcekubgRK1e63NedymXkXJ0pw/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图13 LambdaDNN离线NDCG指标与线上PvCtr效果对比</span></p> 
<h3 style="margin: 30px 0.5em 15px;color: rgb(0, 0, 0);font-weight: bold;line-height: 1.5;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="font-size: 18px;">4.5 Lambda深度排序框架</span></h3> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">Lambda梯度除了与DNN网络相结合外，事实上可以与绝大部分常见的网络结构相结合。为了进一步学习到更多交叉特征，我们在LambdaDNN的基础上分别尝试了LambdaDeepFM和LambdaDCN网络；其中DCN网络是一种加入Cross的并行网络结构，交叉的网络每一层的输出特征与第一层的原始输入特征进行显性的两两交叉，相当于每一层学习特征交叉的映射去拟合层之间的残差。</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.818639798488665" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnVLPBWiajYeypbicAFfN4KhVkoiatuxvNeCRibw3rJTdheZw3UP9V9tyJiag/640?wx_fmt=png" data-type="png" data-w="397" style="width: 249px;height: 204px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnVLPBWiajYeypbicAFfN4KhVkoiatuxvNeCRibw3rJTdheZw3UP9V9tyJiag/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图14 DCN模型结构</span></p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">离线的对比实验表明，Lambda梯度与DCN网络结合之后充分发挥了DCN网络的特点，简洁的多项式交叉设计有效地提升模型的训练效果。NDCG指标对比效果如下图所示：</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.5360501567398119" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnhjrOLGGAoorX5vUgYEpcbOL1He7WtF4d6zHbAbZWziaia0hqp3icNfh5A/640?wx_fmt=png" data-type="png" data-w="638" style="width: 335px;height: 180px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnhjrOLGGAoorX5vUgYEpcbOL1He7WtF4d6zHbAbZWziaia0hqp3icNfh5A/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图15 Lambda Loss与DCN网络结果的效果</span></p> 
<h2 style="margin-top: 30px;color: rgb(0, 0, 0);font-size: 18px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(37, 183, 167);font-size: 20px;">5. 深度学习排序诊断系统</span></h2> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">深度学习排序模型虽然给业务指标带来了大幅度的提升，但由于深度学习模型的“黑盒属性”导致了巨大的解释性成本，也给搜索业务带来了一些问题：</p> 
<ul class=" list-paddingleft-2" style="list-style-type: disc;"> 
 <li><p><span style="font-size: 14px;"><strong>日常搜索Bad Case无法快速响应</strong>：搜索业务日常需要应对大量来自于用户、业务和老板们的“灵魂拷问”，“为何这个排序是这样的”，“为什么这家商户质量跟我差不多，但是会排在我的前面”。刚切换到深度学习排序模型的时候，我们对于这样的问题显得手足无措，需要花费大量的时间去定位问题。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>无法从Bad Case中学习总结规律持续优化</strong>：如果不明白为什么排序模型会得出一个很坏的排序结果，自然也无法定位模型到底出了什么问题，也就无法根据Bad Case总结规律，从而确定模型和特征将来的优化方向。</span></p></li> 
 <li><p><span style="font-size: 14px;"><strong>模型和特征是否充分学习无从得知</strong>：新挖掘一些特征之后，通常我们会根据离线评测指标是否有提升决定特征是否上线。但是，即使一个有提升的特征，我们也无法知道这个特征是否性能足够好。例如，模型拟合的距离特征，会不会在特定的距离段出现距离越远反而打分越高的情况。</span></p></li> 
</ul> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">这些问题都会潜在带来一些用户无法理解的排序结果。我们需要对深度排序模型清晰地诊断并解释。</p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">关于机器学习模型的可解释性研究，业界已经有了一些探索。Lime(<span style="color: rgb(136, 136, 136);">Local Interpretable Model-Agnostic Explanations</span>)是其中的一种，如下图所示：通过对单个样本的特征生成扰动产生近邻样本，观察模型的预测行为。根据这些扰动的数据点距离原始数据的距离分配权重，基于它们学习得到一个可解释的模型和预测结果[5]。举个例子，如果需要解释一个情感分类模型是如何预测“我讨厌这部电影”为负面情感的，我们通过丢掉部分词或者乱序构造一些样本预测情感，最终会发现，决定“我讨厌这部电影”为负面情感的是因为“讨厌”这个词。</p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.6220614828209765" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnCJPOqkH1U7H7ibxetBXfoCQT9icot8VrJDic3xdyUPXdia4JzLKH5s9gOw/640?wx_fmt=png" data-type="png" data-w="553" style="width: 382px;height: 238px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodnCJPOqkH1U7H7ibxetBXfoCQT9icot8VrJDic3xdyUPXdia4JzLKH5s9gOw/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图16 Lime解释器的工作原理</span></p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">基于Lime解释器的思想，我们开发了一套深度模型解释器工具——雅典娜系统。目前雅典娜系统支持两种工作模式，Pairwise和Listwise模式：</p> 
<ul class=" list-paddingleft-2" style="list-style-type: disc;"> 
 <li><p><span style="font-size: 14px;">Pairwise模式用来解释同一个列表中两个结果之间的相对排序。通过对样本的特征进行重新赋值或者替换等操作，观察样本打分和排序位次的变化趋势，诊断出当前样本排序是否符合预期。如下图所示，通过右侧的特征位次面板可以快速诊断出为什么“南京大牌档”的排序比“金时代顺风港湾”要更靠前。第一行的特征位次信息显示，若将“金时代顺风港湾”的1.3km的距离特征用“南京大牌档”的0.2km的距离特征进行替换，排序位次将上升10位；由此得出，“南京大牌档”排在前面的决定性因素是因为距离近。</span></p></li> 
 <li><p style="margin-bottom: 15px;"><span style="font-size: 14px;">Listwise模式与Lime的工作模式基本类似，通过整个列表的样本生成扰动样本，训练线性分类器模型输出特征重要度，从而达到对模型进行解释的目的。</span></p></li> 
</ul> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.7122153209109731" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodn5pPf3wag9gUxddJytbmgj95X38OI4oonib2zEwaqh28vEyt8GcanwBg/640?wx_fmt=png" data-type="png" data-w="966" style="width: 441px;height: 314px;" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsXnxL4iaoF4MCQ0gGeZncodn5pPf3wag9gUxddJytbmgj95X38OI4oonib2zEwaqh28vEyt8GcanwBg/640?wx_fmt=png"></p> 
<p style="text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="color: rgb(136, 136, 136);font-size: 13px;">图17 深度学习排序诊断系统：雅典娜</span></p> 
<h2 style="margin: 30px 0.5em 15px;color: rgb(0, 0, 0);font-size: 18px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="color: rgb(37, 183, 167);font-size: 20px;">6. 总结与展望</span></h2> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">2018年下半年，点评搜索完成了从树模型到大规模深度学习排序模型的全面升级。团队在深度学习特征工程、模型结构、优化目标以及工程实践上都进行了一些探索，在核心指标上取得了较为显著的收益。当然，未来依然有不少可以探索的点。</p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">在特征层面，大量知识图谱提供的标签信息尚未充分挖掘。从使用方式上看，简单以文本标签的形式接入，损失了知识图谱的结构信息，因此，Graph Embedding也是未来需要尝试的方向。同时团队也会利用BERT在Query和商户文本的深层语义表达上做一些工作。</p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">模型结构层面，目前线上依然以全连接的DNN网络结构为主，但DNN网络结构在低秩数据的学习上不如DeepFM和DCN。<span style="font-size: 15px;">目前LambdaDeepFM和LambdaDCN在离线上已经取得了收益，未来会在网络结构上做进一步优化。</span></p> 
<p style="margin: 10px 0.5em 15px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;">在模型优化目标上，Lambda Loss计算损失的时候，只会考虑Query内部有点击和无点击的样本对，大量无点击的Query被丢弃，同时，同一个用户短时间内在不同Query下的行为也包含着一些信息可以利用。因此，目前团队正在探索综合考虑Log Loss和Lambda Loss的模型，通过Multi-Task和按照不同维度Shuffle样本让模型充分学习，目前我们已经在线下取得了一些收益。</p> 
<p style="margin-top: 10px;font-size: 15px;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;margin-left: 0.5em;margin-right: 0.5em;">最后，近期Google开源的TF Ranking提出的Groupwise模型也对我们有一些启发。目前绝大部分的Listwise方法只是体现在模型训练阶段，在打分预测阶段依然是Pointwise的，即只会考虑当前商户相关的特征，而不会考虑列表上下文的结果，未来我们也会在这个方向上进行一些探索。</p> 
<h2 style="margin: 30px 0.5em 15px;color: rgb(0, 0, 0);font-size: 18px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="color: rgb(37, 183, 167);">参考资料</span></h2> 
<ol class=" list-paddingleft-2" style="list-style-type: decimal;"> 
 <li><p><a href="https://tech.meituan.com/meituan_AI_NLP.html" style="color: rgb(53, 114, 176);text-decoration: underline;font-size: 10px;" data-linktype="2"><span style="font-size: 10px;">美团大脑：知识图谱的建模方法及其应用</span></a></p></li> 
 <li><p><a href="https://arxiv.org/pdf/1606.07792.pdf" style="color: rgb(53, 114, 176);text-decoration: underline;font-size: 10px;" data-linktype="2"><span style="font-size: 10px;">Wide &amp; Deep Learning for Recommender Systems</span></a></p></li> 
 <li><p><a href="https://arxiv.org/abs/1512.03385" style="color: rgb(53, 114, 176);text-decoration: underline;font-size: 10px;" data-linktype="2"><span style="font-size: 10px;">Deep Residual Learning for Image Recognition</span></a></p></li> 
 <li><p><a href="https://arxiv.org/abs/1706.03762" style="color: rgb(53, 114, 176);text-decoration: underline;font-size: 10px;" data-linktype="2"><span style="font-size: 10px;">Attention Is All You Need</span></a></p></li> 
 <li><p><a href="https://arxiv.org/pdf/1602.04938v1.pdf" style="color: rgb(53, 114, 176);text-decoration: underline;font-size: 10px;" data-linktype="2"><span style="font-size: 10px;">Local Interpretable Mode l- Agnostic Explanations: LIME</span></a></p></li> 
 <li><p><a href="https://pdfs.semanticscholar.org/0df9/c70875783a73ce1e933079f328e8cf5e9ea2.pdf" style="color: rgb(53, 114, 176);text-decoration: underline;font-size: 10px;" data-linktype="2"><span style="font-size: 10px;">From RankNet to LambdaRank to LambdaMART: An Overview</span></a></p></li> 
 <li><p><a href="https://arxiv.org/abs/1809.05818" style="color: rgb(53, 114, 176);text-decoration: underline;font-size: 10px;" data-linktype="2"><span style="font-size: 10px;">A Novel Algorithm for Unbiased Learning to Rank</span></a></p></li> 
 <li><p><a href="https://www.ijcai.org/proceedings/2018/0738.pdf" style="color: rgb(53, 114, 176);text-decoration: underline;font-size: 10px;" data-linktype="2"><span style="font-size: 10px;">Unbiased Learning-to-Rank with Biased Feedback</span></a></p></li> 
 <li><p><a href="https://astro.temple.edu/%7Etua95067/kdd2018.pdf" style="color: rgb(53, 114, 176);text-decoration: underline;font-size: 10px;" data-linktype="2"><span style="font-size: 10px;">Real-time Personalization using Embeddings for Search Ranking at Airbnb</span></a></p></li> 
</ol> 
<h2 style="margin: 30px 0.5em 15px;color: rgb(0, 0, 0);font-size: 18px;font-weight: bold;line-height: 1.25;font-family: Arial, sans-serif;text-align: start;white-space: pre-wrap;"><span style="color: rgb(37, 183, 167);">作者简介</span></h2> 
<p style="margin-bottom: 10px;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 14px;">非易，2016年加入美团点评，高级算法工程师，目前主要负责点评搜索核心排序层的研发工作。</span></p> 
<p style="margin-bottom: 10px;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 14px;">祝升，2016年加入美团点评，高级算法工程师，目前负责点评搜索核心排序层的研发工作。</span></p> 
<p style="margin-bottom: 10px;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 14px;">汤彪，2013年加入美团点评，高级算法专家，点评平台搜索技术负责人，致力于深层次查询理解和大规模深度学习排序的技术落地。</span></p> 
<p style="margin-bottom: 10px;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 14px;">张弓，2012年加入美团点评，美团点评研究员。目前主要负责点评搜索业务演进，及集团搜索公共服务平台建设。</span></p> 
<p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 14px;">仲远，博士，美团AI平台部NLP中心负责人，点评搜索智能中心负责人。在国际顶级学术会议发表论文30余篇，获得ICDE 2015最佳论文奖，并是ACL 2016 Tutorial “Understanding Short Texts”主讲人，出版学术专著3部，获得美国专利5项。此前，博士曾担任微软亚洲研究院主管研究员，以及美国Facebook公司Research Scientist。曾负责微软研究院知识图谱、对话机器人项目和Facebook产品级NLP Service。</span></p> 
<p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 14px;"><br></span></p> 
<p style="margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 14px;"><span style="letter-spacing: 0.544px;background-color: rgb(255, 255, 255);color: rgb(136, 136, 136);font-size: 15px;box-sizing: border-box !important;">欢迎加入</span><strong style="letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;"><span style="font-size: 15px;color: rgb(0, 0, 0);box-sizing: border-box !important;">美团机器学习技术交流群</span></strong><span style="letter-spacing: 0.544px;background-color: rgb(255, 255, 255);color: rgb(136, 136, 136);font-size: 15px;box-sizing: border-box !important;">，跟项目维护者零距离交流。进群方式：请加美美同学微信（<strong style="box-sizing: border-box !important;">微信号：MTDPtech02</strong>），回复：</span><span style="letter-spacing: 0.544px;background-color: rgb(255, 255, 255);font-size: 15px;color: rgb(0, 0, 0);box-sizing: border-box !important;"><strong style="box-sizing: border-box !important;">深度学习</strong></span><span style="letter-spacing: 0.544px;background-color: rgb(255, 255, 255);color: rgb(136, 136, 136);font-size: 15px;box-sizing: border-box !important;">，美美会自动拉你进群。</span></span></p> 
<p style="white-space: normal;text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 15px;color: rgb(136, 136, 136);box-sizing: border-box !important;">----------&nbsp; END&nbsp; ----------</span></p> 
<p data-source-line="194" style="white-space: normal;margin-left: 0.5em;margin-right: 0.5em;"><br></p> 
<p style="white-space: normal;margin-bottom: 15px;margin-left: 0.5em;margin-right: 0.5em;"><span style="font-size: 16px;"><strong><span style="color: rgb(49, 188, 173);">也许你还想看</span></strong></span></p> 
<p style="white-space: normal;margin-left: 0.5em;margin-right: 0.5em;"><a href="http://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651749520&amp;idx=1&amp;sn=e905e4ecefca3be58b46f59dcfb4b202&amp;chksm=bd12a5dd8a652ccb499914b029b898651946e21b5d7acc3fc2229c7afbc3d2f490ba685cbef0&amp;scene=21#wechat_redirect" target="_blank" style="font-size: 14px;text-decoration: underline;" data-linktype="2"><span style="font-size: 14px;">美团餐饮娱乐知识图谱——美团大脑揭秘</span></a><br></p> 
<p style="white-space: normal;margin-left: 0.5em;margin-right: 0.5em;"><a href="http://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651749250&amp;idx=1&amp;sn=6f382206bed8d5e79999a8c957857ba5&amp;chksm=bd12a2cf8a652bd92e9bd226405fd9110a696077a5ec17f08f6be3f699afc5c4873008e923ec&amp;scene=21#wechat_redirect" target="_blank" style="font-size: 14px;text-decoration: underline;" data-linktype="2"><span style="font-size: 14px;">美团大脑：知识图谱的建模方法及其应用</span></a><br></p> 
<p style="white-space: normal;margin-left: 0.5em;margin-right: 0.5em;"><a href="http://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651748367&amp;idx=3&amp;sn=8b55db4ab7b5e39426103ff1decd37c9&amp;chksm=bd12a1428a652854805fe23766c5d4b19adb6a0691f2371b1d33f9b91b4be14ca03c585e36e7&amp;scene=21#wechat_redirect" target="_blank" style="font-size: 14px;text-decoration: underline;" data-linktype="2"><span style="font-size: 14px;">美团如何基于深度学习实现图像的智能审核？</span></a><br></p> 
<p style="white-space: normal;margin-left: 0.5em;margin-right: 0.5em;"><br></p> 
<p style="white-space: normal;text-align: center;margin-left: 0.5em;margin-right: 0.5em;"><img class="" data-copyright="0" data-ratio="0.44533333333333336" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsV6LYkM3uK5TAnl8DxXwdR4YOAKWmYSpAtzV3P359bDG3cn3Vr4T6HMkvDSI8icUYsejmDnfa5CdpQ/640?wx_fmt=png" data-type="png" data-w="1875" src="https://uzshare.com/_p?https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsV6LYkM3uK5TAnl8DxXwdR4YOAKWmYSpAtzV3P359bDG3cn3Vr4T6HMkvDSI8icUYsejmDnfa5CdpQ/640?wx_fmt=png"></p>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
