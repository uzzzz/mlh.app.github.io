<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>基于深度学习的文本情感分析（Keras VS Pytorch） | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="基于深度学习的文本情感分析（Keras VS Pytorch）" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="版权声明：未经授权，不得转载。 https://blog.csdn.net/Captain_F_/article/details/89331133 &nbsp; 实验任务：利用不同的深度学习框架对微博短文本进行情感分析，并将情感分为三类，分别是正、负、中。 &nbsp; 所用语言及相应的工具包：Python 3.6, Keras 2.2.4, Torch 1.0.1 &nbsp; 数据分布：&nbsp;{&#39;pos&#39;: 712, &#39;neu&#39;: 768, &#39;neg&#39;: 521} &nbsp; 技术路线： 本次实验利用词向量来表示文本，单条文本的形状为[50, 100]. &nbsp;利用Keras对处理好的文本进行情感识别： import pickle from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Bidirectional, LSTM, GRU from keras.callbacks import TensorBoard def load_f(path): with open(path, &#39;rb&#39;)as f: data = pickle.load(f) return data path_1 = r&#39;G:/Multimodal/nlp/w2v_weibo_data.pickle&#39; path_2 = r&#39;G:/Multimodal/labels.pickle&#39; txts = load_f(path_1) labels = load_f(path_2) train_X, test_X, train_Y, test_Y = train_test_split(txts, labels, test_size= 0.2, random_state= 46) #build model; tensorboard = TensorBoard(log_dir= r&#39;G:\pytorch&#39;) model = Sequential() model.add(LSTM(128, input_shape = (None, 100))) #model.add(GRU(128, input_shape = (None, 100))) model.add(Dense(3, activation= &#39;softmax&#39;)) model.compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = [&#39;accuracy&#39;]) model.fit(train_X, train_Y, epochs= 50, validation_data = (test_X, test_Y), batch_size= 128, verbose= 1, callbacks= [tensorboard]) y_pre = model.predict(test_X, batch_size= 128) print(classification_report(test_Y.argmax(axis= 1), y_pre.argmax(axis= 1), digits= 5)) &nbsp; 训练后的结果： &nbsp;利用Pytorch对处理好的文本进行情感识别： import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import pickle from sklearn.model_selection import train_test_split import torch.utils.data as U from tqdm import tqdm from sklearn.metrics import accuracy_score, classification_report #define Hyper parameters; Input_size = 100 Hidden_size = 128 Epochs = 50 batch_size = 128 class lstm(nn.Module): def __init__(self): super(lstm, self).__init__() self.lstm = nn.LSTM( input_size=Input_size, hidden_size=Hidden_size, batch_first=True) self.fc = nn.Linear(128, 3) def forward(self, x): out, (h_0, c_0) = self.lstm(x) out = out[:, -1, :] out = self.fc(out) out = F.softmax(out, dim= 1) return out, h_0 model = lstm() optimizer = torch.optim.Adam(model.parameters()) def load_f(path): with open(path, &#39;rb&#39;)as f: data = pickle.load(f) return data path_1 = r&#39;G:/Multimodal/nlp/w2v_weibo_data.pickle&#39; path_2 = r&#39;G:/Multimodal/labels.pickle&#39; txts = load_f(path_1) labels = load_f(path_2) train_X, test_X, train_Y, test_Y = train_test_split(txts, labels, test_size= 0.2, random_state= 46) train_Y = train_Y.argmax(axis=1) test_Y = test_Y.argmax(axis=1) train_X = torch.from_numpy(train_X) train_Y = torch.from_numpy(train_Y) test_X = torch.from_numpy(test_X) test_Y = torch.from_numpy(test_Y) train_data = U.TensorDataset(train_X, train_Y) test_data = U.TensorDataset(test_X, test_Y) train_loader = U.DataLoader(train_data, batch_size= batch_size, shuffle=True) test_loader = U.DataLoader(test_data, batch_size= batch_size, shuffle=False) def train(model, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() output, h_state = model(data) labels = output.argmax(dim= 1) acc = accuracy_score(target, labels) #梯度清零；反向传播； optimizer.zero_grad() loss = F.cross_entropy(output, target)#交叉熵损失函数； loss.backward() optimizer.step() if (batch_idx+1)%2 == 0: finish_rate = (batch_idx* len(data) / len(train_X))*100 print(&#39;Train Epoch: %s&#39;%str(epoch), #Train Epoch: 1 &#39;[%d/%d]--&#39;%((batch_idx* len(data)),len(train_X)), #[1450/60000] &#39;%.3f%%&#39;%finish_rate, #0.024 &#39;\t&#39;, &#39;acc: %.5f&#39;%acc, #acc: 0.98000 &#39; loss: %s&#39;%loss.item()) #loss: 1.4811499118804932 def valid(model, test_loader): model.eval() test_loss = 0 acc = 0 y_true = [] y_pred = [] with torch.no_grad(): for data, target in test_loader: output, h_state = model(data) test_loss += F.cross_entropy(output, target, reduction=&#39;sum&#39;).item() # 将一批的损失相加 output = output.argmax(dim = 1) y_true.extend(target) y_pred.extend(output) acc = accuracy_score(y_true, y_pred) #print(classification_report(y_true, y_pred, digits= 5)) test_loss /= len(test_X) print(&#39;Valid set: Avg Loss:%s&#39;%str(test_loss), &#39;\t&#39;, &#39;Avg acc:%s&#39;%str(acc)) def test(model, test_loader): model.eval() y_true = [] y_pre = [] for data, target in test_loader: output, h_state = model(data) output = output.argmax(dim= 1) y_true.extend(target) y_pre.extend(output) print(classification_report(y_true, y_pre, digits= 5)) for epoch in tqdm(range(1, 50)): train(model, train_loader, optimizer, epoch) valid(model, test_loader) print(&#39;============================================================================&#39;) print(&#39;**********************test set*************************&#39;) test(model, test_loader) &nbsp; 训练结果： 总结： &nbsp; &nbsp;总的来说，利用两个不同的深度学习框架进行训练，两者结果相差并不是很大（要说明的是，两者训练和测试的数据并不是一样的）。但是利用pytorch，可以更好的了解和体会所用的模型。 &nbsp; 在实验过程中，对于pytorch，我有一点很疑惑。同样一个模型，在两个深度学习框架中，所用的配置是完全一样的，在keras中，模型能够快速的拟合，达到一个高精度。但是在pytorch中，确是需要更多的epoch才能拟合。这是我非常困惑的地方，一度认为在模型构建中出了问题，如果有同学知道问题所在，请帮忙解惑，谢谢。 &nbsp;" />
<meta property="og:description" content="版权声明：未经授权，不得转载。 https://blog.csdn.net/Captain_F_/article/details/89331133 &nbsp; 实验任务：利用不同的深度学习框架对微博短文本进行情感分析，并将情感分为三类，分别是正、负、中。 &nbsp; 所用语言及相应的工具包：Python 3.6, Keras 2.2.4, Torch 1.0.1 &nbsp; 数据分布：&nbsp;{&#39;pos&#39;: 712, &#39;neu&#39;: 768, &#39;neg&#39;: 521} &nbsp; 技术路线： 本次实验利用词向量来表示文本，单条文本的形状为[50, 100]. &nbsp;利用Keras对处理好的文本进行情感识别： import pickle from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Bidirectional, LSTM, GRU from keras.callbacks import TensorBoard def load_f(path): with open(path, &#39;rb&#39;)as f: data = pickle.load(f) return data path_1 = r&#39;G:/Multimodal/nlp/w2v_weibo_data.pickle&#39; path_2 = r&#39;G:/Multimodal/labels.pickle&#39; txts = load_f(path_1) labels = load_f(path_2) train_X, test_X, train_Y, test_Y = train_test_split(txts, labels, test_size= 0.2, random_state= 46) #build model; tensorboard = TensorBoard(log_dir= r&#39;G:\pytorch&#39;) model = Sequential() model.add(LSTM(128, input_shape = (None, 100))) #model.add(GRU(128, input_shape = (None, 100))) model.add(Dense(3, activation= &#39;softmax&#39;)) model.compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = [&#39;accuracy&#39;]) model.fit(train_X, train_Y, epochs= 50, validation_data = (test_X, test_Y), batch_size= 128, verbose= 1, callbacks= [tensorboard]) y_pre = model.predict(test_X, batch_size= 128) print(classification_report(test_Y.argmax(axis= 1), y_pre.argmax(axis= 1), digits= 5)) &nbsp; 训练后的结果： &nbsp;利用Pytorch对处理好的文本进行情感识别： import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import pickle from sklearn.model_selection import train_test_split import torch.utils.data as U from tqdm import tqdm from sklearn.metrics import accuracy_score, classification_report #define Hyper parameters; Input_size = 100 Hidden_size = 128 Epochs = 50 batch_size = 128 class lstm(nn.Module): def __init__(self): super(lstm, self).__init__() self.lstm = nn.LSTM( input_size=Input_size, hidden_size=Hidden_size, batch_first=True) self.fc = nn.Linear(128, 3) def forward(self, x): out, (h_0, c_0) = self.lstm(x) out = out[:, -1, :] out = self.fc(out) out = F.softmax(out, dim= 1) return out, h_0 model = lstm() optimizer = torch.optim.Adam(model.parameters()) def load_f(path): with open(path, &#39;rb&#39;)as f: data = pickle.load(f) return data path_1 = r&#39;G:/Multimodal/nlp/w2v_weibo_data.pickle&#39; path_2 = r&#39;G:/Multimodal/labels.pickle&#39; txts = load_f(path_1) labels = load_f(path_2) train_X, test_X, train_Y, test_Y = train_test_split(txts, labels, test_size= 0.2, random_state= 46) train_Y = train_Y.argmax(axis=1) test_Y = test_Y.argmax(axis=1) train_X = torch.from_numpy(train_X) train_Y = torch.from_numpy(train_Y) test_X = torch.from_numpy(test_X) test_Y = torch.from_numpy(test_Y) train_data = U.TensorDataset(train_X, train_Y) test_data = U.TensorDataset(test_X, test_Y) train_loader = U.DataLoader(train_data, batch_size= batch_size, shuffle=True) test_loader = U.DataLoader(test_data, batch_size= batch_size, shuffle=False) def train(model, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() output, h_state = model(data) labels = output.argmax(dim= 1) acc = accuracy_score(target, labels) #梯度清零；反向传播； optimizer.zero_grad() loss = F.cross_entropy(output, target)#交叉熵损失函数； loss.backward() optimizer.step() if (batch_idx+1)%2 == 0: finish_rate = (batch_idx* len(data) / len(train_X))*100 print(&#39;Train Epoch: %s&#39;%str(epoch), #Train Epoch: 1 &#39;[%d/%d]--&#39;%((batch_idx* len(data)),len(train_X)), #[1450/60000] &#39;%.3f%%&#39;%finish_rate, #0.024 &#39;\t&#39;, &#39;acc: %.5f&#39;%acc, #acc: 0.98000 &#39; loss: %s&#39;%loss.item()) #loss: 1.4811499118804932 def valid(model, test_loader): model.eval() test_loss = 0 acc = 0 y_true = [] y_pred = [] with torch.no_grad(): for data, target in test_loader: output, h_state = model(data) test_loss += F.cross_entropy(output, target, reduction=&#39;sum&#39;).item() # 将一批的损失相加 output = output.argmax(dim = 1) y_true.extend(target) y_pred.extend(output) acc = accuracy_score(y_true, y_pred) #print(classification_report(y_true, y_pred, digits= 5)) test_loss /= len(test_X) print(&#39;Valid set: Avg Loss:%s&#39;%str(test_loss), &#39;\t&#39;, &#39;Avg acc:%s&#39;%str(acc)) def test(model, test_loader): model.eval() y_true = [] y_pre = [] for data, target in test_loader: output, h_state = model(data) output = output.argmax(dim= 1) y_true.extend(target) y_pre.extend(output) print(classification_report(y_true, y_pre, digits= 5)) for epoch in tqdm(range(1, 50)): train(model, train_loader, optimizer, epoch) valid(model, test_loader) print(&#39;============================================================================&#39;) print(&#39;**********************test set*************************&#39;) test(model, test_loader) &nbsp; 训练结果： 总结： &nbsp; &nbsp;总的来说，利用两个不同的深度学习框架进行训练，两者结果相差并不是很大（要说明的是，两者训练和测试的数据并不是一样的）。但是利用pytorch，可以更好的了解和体会所用的模型。 &nbsp; 在实验过程中，对于pytorch，我有一点很疑惑。同样一个模型，在两个深度学习框架中，所用的配置是完全一样的，在keras中，模型能够快速的拟合，达到一个高精度。但是在pytorch中，确是需要更多的epoch才能拟合。这是我非常困惑的地方，一度认为在模型构建中出了问题，如果有同学知道问题所在，请帮忙解惑，谢谢。 &nbsp;" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-16T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"版权声明：未经授权，不得转载。 https://blog.csdn.net/Captain_F_/article/details/89331133 &nbsp; 实验任务：利用不同的深度学习框架对微博短文本进行情感分析，并将情感分为三类，分别是正、负、中。 &nbsp; 所用语言及相应的工具包：Python 3.6, Keras 2.2.4, Torch 1.0.1 &nbsp; 数据分布：&nbsp;{&#39;pos&#39;: 712, &#39;neu&#39;: 768, &#39;neg&#39;: 521} &nbsp; 技术路线： 本次实验利用词向量来表示文本，单条文本的形状为[50, 100]. &nbsp;利用Keras对处理好的文本进行情感识别： import pickle from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Bidirectional, LSTM, GRU from keras.callbacks import TensorBoard def load_f(path): with open(path, &#39;rb&#39;)as f: data = pickle.load(f) return data path_1 = r&#39;G:/Multimodal/nlp/w2v_weibo_data.pickle&#39; path_2 = r&#39;G:/Multimodal/labels.pickle&#39; txts = load_f(path_1) labels = load_f(path_2) train_X, test_X, train_Y, test_Y = train_test_split(txts, labels, test_size= 0.2, random_state= 46) #build model; tensorboard = TensorBoard(log_dir= r&#39;G:\\pytorch&#39;) model = Sequential() model.add(LSTM(128, input_shape = (None, 100))) #model.add(GRU(128, input_shape = (None, 100))) model.add(Dense(3, activation= &#39;softmax&#39;)) model.compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = [&#39;accuracy&#39;]) model.fit(train_X, train_Y, epochs= 50, validation_data = (test_X, test_Y), batch_size= 128, verbose= 1, callbacks= [tensorboard]) y_pre = model.predict(test_X, batch_size= 128) print(classification_report(test_Y.argmax(axis= 1), y_pre.argmax(axis= 1), digits= 5)) &nbsp; 训练后的结果： &nbsp;利用Pytorch对处理好的文本进行情感识别： import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import pickle from sklearn.model_selection import train_test_split import torch.utils.data as U from tqdm import tqdm from sklearn.metrics import accuracy_score, classification_report #define Hyper parameters; Input_size = 100 Hidden_size = 128 Epochs = 50 batch_size = 128 class lstm(nn.Module): def __init__(self): super(lstm, self).__init__() self.lstm = nn.LSTM( input_size=Input_size, hidden_size=Hidden_size, batch_first=True) self.fc = nn.Linear(128, 3) def forward(self, x): out, (h_0, c_0) = self.lstm(x) out = out[:, -1, :] out = self.fc(out) out = F.softmax(out, dim= 1) return out, h_0 model = lstm() optimizer = torch.optim.Adam(model.parameters()) def load_f(path): with open(path, &#39;rb&#39;)as f: data = pickle.load(f) return data path_1 = r&#39;G:/Multimodal/nlp/w2v_weibo_data.pickle&#39; path_2 = r&#39;G:/Multimodal/labels.pickle&#39; txts = load_f(path_1) labels = load_f(path_2) train_X, test_X, train_Y, test_Y = train_test_split(txts, labels, test_size= 0.2, random_state= 46) train_Y = train_Y.argmax(axis=1) test_Y = test_Y.argmax(axis=1) train_X = torch.from_numpy(train_X) train_Y = torch.from_numpy(train_Y) test_X = torch.from_numpy(test_X) test_Y = torch.from_numpy(test_Y) train_data = U.TensorDataset(train_X, train_Y) test_data = U.TensorDataset(test_X, test_Y) train_loader = U.DataLoader(train_data, batch_size= batch_size, shuffle=True) test_loader = U.DataLoader(test_data, batch_size= batch_size, shuffle=False) def train(model, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() output, h_state = model(data) labels = output.argmax(dim= 1) acc = accuracy_score(target, labels) #梯度清零；反向传播； optimizer.zero_grad() loss = F.cross_entropy(output, target)#交叉熵损失函数； loss.backward() optimizer.step() if (batch_idx+1)%2 == 0: finish_rate = (batch_idx* len(data) / len(train_X))*100 print(&#39;Train Epoch: %s&#39;%str(epoch), #Train Epoch: 1 &#39;[%d/%d]--&#39;%((batch_idx* len(data)),len(train_X)), #[1450/60000] &#39;%.3f%%&#39;%finish_rate, #0.024 &#39;\\t&#39;, &#39;acc: %.5f&#39;%acc, #acc: 0.98000 &#39; loss: %s&#39;%loss.item()) #loss: 1.4811499118804932 def valid(model, test_loader): model.eval() test_loss = 0 acc = 0 y_true = [] y_pred = [] with torch.no_grad(): for data, target in test_loader: output, h_state = model(data) test_loss += F.cross_entropy(output, target, reduction=&#39;sum&#39;).item() # 将一批的损失相加 output = output.argmax(dim = 1) y_true.extend(target) y_pred.extend(output) acc = accuracy_score(y_true, y_pred) #print(classification_report(y_true, y_pred, digits= 5)) test_loss /= len(test_X) print(&#39;Valid set: Avg Loss:%s&#39;%str(test_loss), &#39;\\t&#39;, &#39;Avg acc:%s&#39;%str(acc)) def test(model, test_loader): model.eval() y_true = [] y_pre = [] for data, target in test_loader: output, h_state = model(data) output = output.argmax(dim= 1) y_true.extend(target) y_pre.extend(output) print(classification_report(y_true, y_pre, digits= 5)) for epoch in tqdm(range(1, 50)): train(model, train_loader, optimizer, epoch) valid(model, test_loader) print(&#39;============================================================================&#39;) print(&#39;**********************test set*************************&#39;) test(model, test_loader) &nbsp; 训练结果： 总结： &nbsp; &nbsp;总的来说，利用两个不同的深度学习框架进行训练，两者结果相差并不是很大（要说明的是，两者训练和测试的数据并不是一样的）。但是利用pytorch，可以更好的了解和体会所用的模型。 &nbsp; 在实验过程中，对于pytorch，我有一点很疑惑。同样一个模型，在两个深度学习框架中，所用的配置是完全一样的，在keras中，模型能够快速的拟合，达到一个高精度。但是在pytorch中，确是需要更多的epoch才能拟合。这是我非常困惑的地方，一度认为在模型构建中出了问题，如果有同学知道问题所在，请帮忙解惑，谢谢。 &nbsp;","@type":"BlogPosting","url":"/2019/04/16/728238.html","headline":"基于深度学习的文本情感分析（Keras VS Pytorch）","dateModified":"2019-04-16T00:00:00+08:00","datePublished":"2019-04-16T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/04/16/728238.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>基于深度学习的文本情感分析（Keras VS Pytorch）</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div class="article-copyright"> 
  <svg class="icon" title="CSDN认证原创" aria-hidden="true" style="width:53px; height: 18px; vertical-align: -4px;"> 
   <use xlink:href="#CSDN_Cert"></use> 
  </svg> 版权声明：未经授权，不得转载。 https://blog.csdn.net/Captain_F_/article/details/89331133 
 </div> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>&nbsp; 实验任务：利用不同的深度学习框架对微博短文本进行情感分析，并将情感分为三类，分别是正、负、中。</p> 
  <p>&nbsp; 所用语言及相应的工具包：Python 3.6, Keras 2.2.4, Torch 1.0.1</p> 
  <p>&nbsp; 数据分布：&nbsp;{'pos': 712, 'neu': 768, 'neg': 521}</p> 
  <p>&nbsp; 技术路线：</p> 
  <p><img alt="" class="has" height="213" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190416131215729.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NhcHRhaW5fRl8=,size_16,color_FFFFFF,t_70" width="251"></p> 
  <p>本次实验利用词向量来表示文本，单条文本的形状为[50, 100].</p> 
  <p>&nbsp;利用Keras对处理好的文本进行情感识别：</p> 
  <pre class="has">
<code>import pickle
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Bidirectional, LSTM, GRU
from keras.callbacks import TensorBoard

def load_f(path):
    with open(path, 'rb')as f:
        data = pickle.load(f)
    return data

path_1 = r'G:/Multimodal/nlp/w2v_weibo_data.pickle'
path_2 = r'G:/Multimodal/labels.pickle'

txts = load_f(path_1)
labels = load_f(path_2)

train_X, test_X, train_Y, test_Y = train_test_split(txts, labels, test_size= 0.2, random_state= 46)

#build model;

tensorboard = TensorBoard(log_dir= r'G:\pytorch')

model = Sequential()
model.add(LSTM(128, input_shape = (None, 100)))
#model.add(GRU(128, input_shape = (None, 100)))
model.add(Dense(3, activation= 'softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model.fit(train_X, train_Y, epochs= 50, validation_data = (test_X, test_Y), 
          batch_size= 128, verbose= 1, callbacks= [tensorboard])

y_pre = model.predict(test_X, batch_size= 128)
print(classification_report(test_Y.argmax(axis= 1), y_pre.argmax(axis= 1), digits= 5))
</code></pre> 
  <p>&nbsp; 训练后的结果：</p> 
  <p><img alt="" class="has" height="136" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190416130025769.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NhcHRhaW5fRl8=,size_16,color_FFFFFF,t_70" width="357"></p> 
  <p>&nbsp;利用Pytorch对处理好的文本进行情感识别：</p> 
  <pre class="has">
<code>import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import pickle
from sklearn.model_selection import train_test_split
import torch.utils.data as U
from tqdm import tqdm
from sklearn.metrics import accuracy_score, classification_report

#define Hyper parameters;

Input_size = 100
Hidden_size = 128 
Epochs = 50
batch_size = 128

class lstm(nn.Module):
    def __init__(self):
        super(lstm, self).__init__()
        self.lstm = nn.LSTM(
            input_size=Input_size,
            hidden_size=Hidden_size,
            batch_first=True)
        self.fc = nn.Linear(128, 3)
        
    def forward(self, x):
        out, (h_0, c_0) = self.lstm(x)
        out = out[:, -1, :]
        out = self.fc(out)
        out = F.softmax(out, dim= 1)
        return out, h_0

model = lstm()
optimizer = torch.optim.Adam(model.parameters())

def load_f(path):
    with open(path, 'rb')as f:
        data = pickle.load(f)
    return data

path_1 = r'G:/Multimodal/nlp/w2v_weibo_data.pickle'
path_2 = r'G:/Multimodal/labels.pickle'

txts = load_f(path_1)
labels = load_f(path_2)

train_X, test_X, train_Y, test_Y = train_test_split(txts, labels, test_size= 0.2, random_state= 46)

train_Y = train_Y.argmax(axis=1)
test_Y = test_Y.argmax(axis=1)


train_X = torch.from_numpy(train_X)
train_Y = torch.from_numpy(train_Y)
test_X = torch.from_numpy(test_X)
test_Y = torch.from_numpy(test_Y)


train_data = U.TensorDataset(train_X, train_Y)
test_data = U.TensorDataset(test_X, test_Y)

train_loader = U.DataLoader(train_data, batch_size= batch_size, shuffle=True)
test_loader = U.DataLoader(test_data, batch_size= batch_size, shuffle=False)

def train(model, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):        
        optimizer.zero_grad()
        output, h_state = model(data)
        labels = output.argmax(dim= 1)
        acc = accuracy_score(target, labels)
        
        #梯度清零；反向传播；
        optimizer.zero_grad()
        loss = F.cross_entropy(output, target)#交叉熵损失函数；
        loss.backward()
        optimizer.step()
        
        
        if (batch_idx+1)%2 == 0:
            finish_rate = (batch_idx* len(data) / len(train_X))*100
            print('Train Epoch: %s'%str(epoch), #Train Epoch: 1
                  '[%d/%d]--'%((batch_idx* len(data)),len(train_X)), #[1450/60000]
                  '%.3f%%'%finish_rate, #0.024
                 '\t', 'acc: %.5f'%acc, #acc: 0.98000
                 '  loss: %s'%loss.item()) #loss: 1.4811499118804932
            
def valid(model, test_loader):
    model.eval()
    test_loss = 0
    acc = 0
    y_true = []
    y_pred = []
    with torch.no_grad():
        for data, target in test_loader:
            output, h_state = model(data)
            test_loss += F.cross_entropy(output, target, reduction='sum').item() # 将一批的损失相加
            output = output.argmax(dim = 1)
            y_true.extend(target)
            y_pred.extend(output)

    acc = accuracy_score(y_true, y_pred)
    #print(classification_report(y_true, y_pred, digits= 5))
    test_loss /= len(test_X)
    
    print('Valid set: Avg Loss:%s'%str(test_loss),
         '\t', 'Avg acc:%s'%str(acc))
    
def test(model, test_loader):
    model.eval()
    y_true = []
    y_pre = []
    for data, target in test_loader:
        output, h_state = model(data)
        output = output.argmax(dim= 1)
        y_true.extend(target)
        y_pre.extend(output)
    print(classification_report(y_true, y_pre, digits= 5))


for epoch in tqdm(range(1, 50)):
    train(model, train_loader, optimizer, epoch)
    valid(model, test_loader)
    print('============================================================================')
print('**********************test set*************************')
test(model, test_loader)</code></pre> 
  <p>&nbsp; 训练结果：</p> 
  <p><img alt="" class="has" height="201" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190416130355590.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NhcHRhaW5fRl8=,size_16,color_FFFFFF,t_70" width="444"></p> 
  <p>总结：</p> 
  <p>&nbsp; &nbsp;总的来说，利用两个不同的深度学习框架进行训练，两者结果相差并不是很大（要说明的是，两者训练和测试的数据并不是一样的）。但是利用pytorch，可以更好的了解和体会所用的模型。</p> 
  <p>&nbsp; <strong>在实验过程中，对于pytorch，我有一点很疑惑。同样一个模型，在两个深度学习框架中，所用的配置是完全一样的，在keras中，模型能够快速的拟合，达到一个高精度。但是在pytorch中，确是需要更多的epoch才能拟合。这是我非常困惑的地方，一度认为在模型构建中出了问题，如果有同学知道问题所在，请帮忙解惑，谢谢。</strong></p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
