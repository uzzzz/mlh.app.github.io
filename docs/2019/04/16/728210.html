<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>NLTK库进行文本分类的过程 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="NLTK库进行文本分类的过程" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="本文针对英文文本，介绍的全部都是基于Python3.7， 利用NLTK库进行文本分类的过程. 文本分词 文本分词即将文本拆解成词语单元，英文文本以英文单词空格连接成句，分词过程较为简单。以下介绍几种方法。 正则表达式分词 1.以空格进行分词 import re text = &#39;I was just a kid, and loved it very much! What a fantastic song!&#39; print(re.split(r&#39; &#39;,text)) 2.re匹配符号进行分词 print(re.split(r&#39;\W+&#39;, text)) print(re.findall(r&#39;\w+|\S\w*&#39;, text)) print(re.findall(r&quot;\w+(?:[-&#39;]\w+)*|&#39;|[-.(]+|\S\w*&quot;, text)) 3.NLTK正则表达式分词器 import re import nltk text = &#39;I was just a kid, and loved it very much! What a fantastic song!&#39; pattern = r&quot;&quot;&quot;(?x) # set flag to allow verbose regexps (?:[A-Z]\.)+ # abbreviations, e.g. U.S.A. |\d+(?:\.\d+)?%? # numbers, incl. currency and percentages |\w+(?:[-&#39;]\w+)* # words w/ optional internal hyphens/apostrophe |\.\.\. # ellipsis |(?:[.,;&quot;&#39;?():-_`]) # special characters with meanings &quot;&quot;&quot; print(nltk.regexp_tokenize(text, pattern)) [‘I’, ‘was’, ‘just’, ‘a’, ‘kid’, ‘,’, ‘and’, ‘loved’, ‘it’, ‘very’, ‘much’, ‘What’, ‘a’, ‘fantastic’, ‘song’] 4.最大匹配算法（MaxMatch）分词 import nltk from nltk.corpus import words nltk.download(&#39;words&#39;) wordlist = set(words.words()) def max_match(text): pos2 = len(text) result = &#39;&#39; while len(text) &gt; 0: word = text[0:pos2] if word in wordlist: result = result + text[0:pos2] + &#39; &#39; text = text[pos2:] pos2 = len(text) else: pos2 = pos2-1 return result[0:-1] string = &#39;theyarebirds&#39; print(max_match(string)) 停用词去除 简单易懂，匹配词库中的停用词，去除！以消除冠词、连词等一些无意义无作用的词增加数据占用空间，并避免其为挖掘计算带来的干扰。 NLTK停用词库 import nltk from nltk.corpus import stopwords stopworddic = set(stopwords.words(&#39;english&#39;)) text = [&#39;I&#39;, &#39;was&#39;, &#39;just&#39;, &#39;a&#39;, &#39;kid&#39;, &#39;and&#39;, &#39;loved&#39;, &#39;it&#39;, &#39;very&#39;, &#39;much&#39;, &#39;What&#39;, &#39;a&#39;, &#39;fantastic&#39;, &#39;song&#39;] text = [i for i in text if i not in stopworddic ] print(text) 输出： [‘I’, ‘kid’, ‘loved’, ‘much’, ‘What’, ‘fantastic’, ‘song’] 词干抽取 将文本列表中的词语抽取其词干，以统一特征表征形式，特征降维以减少计算量。NLTK中提供了三种最常用的词干提取器接口，即 Porter stemmer, Lancaster Stemmer 和 Snowball Stemmer。抽取词的词干或词根形式（不一定能够表达完整语义） from nltk.stem.porter import PorterStemmer porter_stemmer = PorterStemmer() from nltk.stem.lancaster import LancasterStemmer lancaster_stemmer = LancasterStemmer() from nltk.stem import SnowballStemmer snowball_stemmer = SnowballStemmer(&#39;english&#39;) print(porter_stemmer.stem(&#39;maximum&#39;)) print(lancaster_stemmer.stem(&#39;maximum&#39;)) print(snowball_stemmer.stem(&#39;maximum&#39;)) 结果 maximum maxim maximum 词形还原 词形还原Lemmatization是把一个任何形式的语言词汇还原为一般形式（能表达完整语义）。相对而言，词干提取是简单的轻量级的词形归并方式，最后获得的结果为词干，并不一定具有实际意义。词形还原处理相对复杂，获得结果为词的原形，能够承载一定意义，与词干提取相比，更具有研究和应用价值。 from nltk.stem import WordNetLemmatizer wordnet_lemmatizer = WordNetLemmatizer() word = wordnet_lemmatizer.lemmatize(&#39;birds&#39;) print(word) 文本向量表征以及TF-IDF权重表示 这一部分是基于Python的Gensim库将文本特征抽取为词袋，并将词袋表征为id,以特征id以及文档频率表征成文本向量。TF-IDF权重是很可靠的权重表征方式，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。 #coding:utf-8 from gensim import corpora, models, similarities documents = [&quot;Shipment of gold damaged in a fire&quot;,&quot;Delivery of silver arrived in a silver truck&quot;,&quot;Shipment of gold arrived in a truck&quot;] #分词# texts = [[word for word in document.lower().split()] for document in documents] print (texts) #抽取词袋，将token映射为id dictionary = corpora.Dictionary(texts) print (dictionary.token2id) #由文档向量以及频率构成文档向量 corpus = [dictionary.doc2bow(text) for text in texts] print (corpus) #计算tfidf权重,注意在gensim的tfidf算法中到文档频率的求解过程中对数之后+1了 tfidf = models.TfidfModel(corpus) corpus_tfidf = tfidf[corpus] for doc in corpus_tfidf: print (doc) print(tfidf.dfs) print(tfidf.idfs) 特征选择 根据研究的需求进行特征的选择已达到特征降维，减小噪音的影响。常见的是根据词频（TF）、倒文档频率（IDF）、TFIDF权重等设定阈值进行筛选，无非是在TF/IDF/TFIDF权重计算结果的基础上设定阈值进行筛检。除此之外互信息、信息增益、X平方统计也是常见的方法。 如果你的研究是在给定类别名称的前提下进行语义文本分类，那么判断特征词与分类之间的语义相似度，从而进行筛选也是一种可行的方法。这里介绍的便是基于WordNet进行语义相似度的介绍。 WordNet计算语义相似度常见的包括两种主要方法： path_similarity(sense1,sense2) # 词在词典层次结构中的最短路径 wup_similarity(sense1, sense2) # Wu-Palmer 提出的最短路径 #coding:utf-8 import nltk from nltk.corpus import wordnet as wn from heapq import * from itertools import product nltk.download(&#39;wordnet&#39;) word1 = &#39;hen&#39; word2 = &#39;chicken&#39; sense1 = wn.synsets(word1) sense2 = wn.synsets(word2) sim_wup = [word1.wup_similarity(word2) for (word1, word2) in product(sense1, sense2)] ##里边含有None sim_wup = max(a if a!=None else 0 for a in sim_wup) print(sim_wup) sim_path = [word1.path_similarity(word2) for (word1, word2) in product(sense1, sense2)] sim_path = max(a if a!=None else 0 for a in sim_path) print(sim_path) 0.962962962963 0.5" />
<meta property="og:description" content="本文针对英文文本，介绍的全部都是基于Python3.7， 利用NLTK库进行文本分类的过程. 文本分词 文本分词即将文本拆解成词语单元，英文文本以英文单词空格连接成句，分词过程较为简单。以下介绍几种方法。 正则表达式分词 1.以空格进行分词 import re text = &#39;I was just a kid, and loved it very much! What a fantastic song!&#39; print(re.split(r&#39; &#39;,text)) 2.re匹配符号进行分词 print(re.split(r&#39;\W+&#39;, text)) print(re.findall(r&#39;\w+|\S\w*&#39;, text)) print(re.findall(r&quot;\w+(?:[-&#39;]\w+)*|&#39;|[-.(]+|\S\w*&quot;, text)) 3.NLTK正则表达式分词器 import re import nltk text = &#39;I was just a kid, and loved it very much! What a fantastic song!&#39; pattern = r&quot;&quot;&quot;(?x) # set flag to allow verbose regexps (?:[A-Z]\.)+ # abbreviations, e.g. U.S.A. |\d+(?:\.\d+)?%? # numbers, incl. currency and percentages |\w+(?:[-&#39;]\w+)* # words w/ optional internal hyphens/apostrophe |\.\.\. # ellipsis |(?:[.,;&quot;&#39;?():-_`]) # special characters with meanings &quot;&quot;&quot; print(nltk.regexp_tokenize(text, pattern)) [‘I’, ‘was’, ‘just’, ‘a’, ‘kid’, ‘,’, ‘and’, ‘loved’, ‘it’, ‘very’, ‘much’, ‘What’, ‘a’, ‘fantastic’, ‘song’] 4.最大匹配算法（MaxMatch）分词 import nltk from nltk.corpus import words nltk.download(&#39;words&#39;) wordlist = set(words.words()) def max_match(text): pos2 = len(text) result = &#39;&#39; while len(text) &gt; 0: word = text[0:pos2] if word in wordlist: result = result + text[0:pos2] + &#39; &#39; text = text[pos2:] pos2 = len(text) else: pos2 = pos2-1 return result[0:-1] string = &#39;theyarebirds&#39; print(max_match(string)) 停用词去除 简单易懂，匹配词库中的停用词，去除！以消除冠词、连词等一些无意义无作用的词增加数据占用空间，并避免其为挖掘计算带来的干扰。 NLTK停用词库 import nltk from nltk.corpus import stopwords stopworddic = set(stopwords.words(&#39;english&#39;)) text = [&#39;I&#39;, &#39;was&#39;, &#39;just&#39;, &#39;a&#39;, &#39;kid&#39;, &#39;and&#39;, &#39;loved&#39;, &#39;it&#39;, &#39;very&#39;, &#39;much&#39;, &#39;What&#39;, &#39;a&#39;, &#39;fantastic&#39;, &#39;song&#39;] text = [i for i in text if i not in stopworddic ] print(text) 输出： [‘I’, ‘kid’, ‘loved’, ‘much’, ‘What’, ‘fantastic’, ‘song’] 词干抽取 将文本列表中的词语抽取其词干，以统一特征表征形式，特征降维以减少计算量。NLTK中提供了三种最常用的词干提取器接口，即 Porter stemmer, Lancaster Stemmer 和 Snowball Stemmer。抽取词的词干或词根形式（不一定能够表达完整语义） from nltk.stem.porter import PorterStemmer porter_stemmer = PorterStemmer() from nltk.stem.lancaster import LancasterStemmer lancaster_stemmer = LancasterStemmer() from nltk.stem import SnowballStemmer snowball_stemmer = SnowballStemmer(&#39;english&#39;) print(porter_stemmer.stem(&#39;maximum&#39;)) print(lancaster_stemmer.stem(&#39;maximum&#39;)) print(snowball_stemmer.stem(&#39;maximum&#39;)) 结果 maximum maxim maximum 词形还原 词形还原Lemmatization是把一个任何形式的语言词汇还原为一般形式（能表达完整语义）。相对而言，词干提取是简单的轻量级的词形归并方式，最后获得的结果为词干，并不一定具有实际意义。词形还原处理相对复杂，获得结果为词的原形，能够承载一定意义，与词干提取相比，更具有研究和应用价值。 from nltk.stem import WordNetLemmatizer wordnet_lemmatizer = WordNetLemmatizer() word = wordnet_lemmatizer.lemmatize(&#39;birds&#39;) print(word) 文本向量表征以及TF-IDF权重表示 这一部分是基于Python的Gensim库将文本特征抽取为词袋，并将词袋表征为id,以特征id以及文档频率表征成文本向量。TF-IDF权重是很可靠的权重表征方式，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。 #coding:utf-8 from gensim import corpora, models, similarities documents = [&quot;Shipment of gold damaged in a fire&quot;,&quot;Delivery of silver arrived in a silver truck&quot;,&quot;Shipment of gold arrived in a truck&quot;] #分词# texts = [[word for word in document.lower().split()] for document in documents] print (texts) #抽取词袋，将token映射为id dictionary = corpora.Dictionary(texts) print (dictionary.token2id) #由文档向量以及频率构成文档向量 corpus = [dictionary.doc2bow(text) for text in texts] print (corpus) #计算tfidf权重,注意在gensim的tfidf算法中到文档频率的求解过程中对数之后+1了 tfidf = models.TfidfModel(corpus) corpus_tfidf = tfidf[corpus] for doc in corpus_tfidf: print (doc) print(tfidf.dfs) print(tfidf.idfs) 特征选择 根据研究的需求进行特征的选择已达到特征降维，减小噪音的影响。常见的是根据词频（TF）、倒文档频率（IDF）、TFIDF权重等设定阈值进行筛选，无非是在TF/IDF/TFIDF权重计算结果的基础上设定阈值进行筛检。除此之外互信息、信息增益、X平方统计也是常见的方法。 如果你的研究是在给定类别名称的前提下进行语义文本分类，那么判断特征词与分类之间的语义相似度，从而进行筛选也是一种可行的方法。这里介绍的便是基于WordNet进行语义相似度的介绍。 WordNet计算语义相似度常见的包括两种主要方法： path_similarity(sense1,sense2) # 词在词典层次结构中的最短路径 wup_similarity(sense1, sense2) # Wu-Palmer 提出的最短路径 #coding:utf-8 import nltk from nltk.corpus import wordnet as wn from heapq import * from itertools import product nltk.download(&#39;wordnet&#39;) word1 = &#39;hen&#39; word2 = &#39;chicken&#39; sense1 = wn.synsets(word1) sense2 = wn.synsets(word2) sim_wup = [word1.wup_similarity(word2) for (word1, word2) in product(sense1, sense2)] ##里边含有None sim_wup = max(a if a!=None else 0 for a in sim_wup) print(sim_wup) sim_path = [word1.path_similarity(word2) for (word1, word2) in product(sense1, sense2)] sim_path = max(a if a!=None else 0 for a in sim_path) print(sim_path) 0.962962962963 0.5" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-16T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"本文针对英文文本，介绍的全部都是基于Python3.7， 利用NLTK库进行文本分类的过程. 文本分词 文本分词即将文本拆解成词语单元，英文文本以英文单词空格连接成句，分词过程较为简单。以下介绍几种方法。 正则表达式分词 1.以空格进行分词 import re text = &#39;I was just a kid, and loved it very much! What a fantastic song!&#39; print(re.split(r&#39; &#39;,text)) 2.re匹配符号进行分词 print(re.split(r&#39;\\W+&#39;, text)) print(re.findall(r&#39;\\w+|\\S\\w*&#39;, text)) print(re.findall(r&quot;\\w+(?:[-&#39;]\\w+)*|&#39;|[-.(]+|\\S\\w*&quot;, text)) 3.NLTK正则表达式分词器 import re import nltk text = &#39;I was just a kid, and loved it very much! What a fantastic song!&#39; pattern = r&quot;&quot;&quot;(?x) # set flag to allow verbose regexps (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A. |\\d+(?:\\.\\d+)?%? # numbers, incl. currency and percentages |\\w+(?:[-&#39;]\\w+)* # words w/ optional internal hyphens/apostrophe |\\.\\.\\. # ellipsis |(?:[.,;&quot;&#39;?():-_`]) # special characters with meanings &quot;&quot;&quot; print(nltk.regexp_tokenize(text, pattern)) [‘I’, ‘was’, ‘just’, ‘a’, ‘kid’, ‘,’, ‘and’, ‘loved’, ‘it’, ‘very’, ‘much’, ‘What’, ‘a’, ‘fantastic’, ‘song’] 4.最大匹配算法（MaxMatch）分词 import nltk from nltk.corpus import words nltk.download(&#39;words&#39;) wordlist = set(words.words()) def max_match(text): pos2 = len(text) result = &#39;&#39; while len(text) &gt; 0: word = text[0:pos2] if word in wordlist: result = result + text[0:pos2] + &#39; &#39; text = text[pos2:] pos2 = len(text) else: pos2 = pos2-1 return result[0:-1] string = &#39;theyarebirds&#39; print(max_match(string)) 停用词去除 简单易懂，匹配词库中的停用词，去除！以消除冠词、连词等一些无意义无作用的词增加数据占用空间，并避免其为挖掘计算带来的干扰。 NLTK停用词库 import nltk from nltk.corpus import stopwords stopworddic = set(stopwords.words(&#39;english&#39;)) text = [&#39;I&#39;, &#39;was&#39;, &#39;just&#39;, &#39;a&#39;, &#39;kid&#39;, &#39;and&#39;, &#39;loved&#39;, &#39;it&#39;, &#39;very&#39;, &#39;much&#39;, &#39;What&#39;, &#39;a&#39;, &#39;fantastic&#39;, &#39;song&#39;] text = [i for i in text if i not in stopworddic ] print(text) 输出： [‘I’, ‘kid’, ‘loved’, ‘much’, ‘What’, ‘fantastic’, ‘song’] 词干抽取 将文本列表中的词语抽取其词干，以统一特征表征形式，特征降维以减少计算量。NLTK中提供了三种最常用的词干提取器接口，即 Porter stemmer, Lancaster Stemmer 和 Snowball Stemmer。抽取词的词干或词根形式（不一定能够表达完整语义） from nltk.stem.porter import PorterStemmer porter_stemmer = PorterStemmer() from nltk.stem.lancaster import LancasterStemmer lancaster_stemmer = LancasterStemmer() from nltk.stem import SnowballStemmer snowball_stemmer = SnowballStemmer(&#39;english&#39;) print(porter_stemmer.stem(&#39;maximum&#39;)) print(lancaster_stemmer.stem(&#39;maximum&#39;)) print(snowball_stemmer.stem(&#39;maximum&#39;)) 结果 maximum maxim maximum 词形还原 词形还原Lemmatization是把一个任何形式的语言词汇还原为一般形式（能表达完整语义）。相对而言，词干提取是简单的轻量级的词形归并方式，最后获得的结果为词干，并不一定具有实际意义。词形还原处理相对复杂，获得结果为词的原形，能够承载一定意义，与词干提取相比，更具有研究和应用价值。 from nltk.stem import WordNetLemmatizer wordnet_lemmatizer = WordNetLemmatizer() word = wordnet_lemmatizer.lemmatize(&#39;birds&#39;) print(word) 文本向量表征以及TF-IDF权重表示 这一部分是基于Python的Gensim库将文本特征抽取为词袋，并将词袋表征为id,以特征id以及文档频率表征成文本向量。TF-IDF权重是很可靠的权重表征方式，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。 #coding:utf-8 from gensim import corpora, models, similarities documents = [&quot;Shipment of gold damaged in a fire&quot;,&quot;Delivery of silver arrived in a silver truck&quot;,&quot;Shipment of gold arrived in a truck&quot;] #分词# texts = [[word for word in document.lower().split()] for document in documents] print (texts) #抽取词袋，将token映射为id dictionary = corpora.Dictionary(texts) print (dictionary.token2id) #由文档向量以及频率构成文档向量 corpus = [dictionary.doc2bow(text) for text in texts] print (corpus) #计算tfidf权重,注意在gensim的tfidf算法中到文档频率的求解过程中对数之后+1了 tfidf = models.TfidfModel(corpus) corpus_tfidf = tfidf[corpus] for doc in corpus_tfidf: print (doc) print(tfidf.dfs) print(tfidf.idfs) 特征选择 根据研究的需求进行特征的选择已达到特征降维，减小噪音的影响。常见的是根据词频（TF）、倒文档频率（IDF）、TFIDF权重等设定阈值进行筛选，无非是在TF/IDF/TFIDF权重计算结果的基础上设定阈值进行筛检。除此之外互信息、信息增益、X平方统计也是常见的方法。 如果你的研究是在给定类别名称的前提下进行语义文本分类，那么判断特征词与分类之间的语义相似度，从而进行筛选也是一种可行的方法。这里介绍的便是基于WordNet进行语义相似度的介绍。 WordNet计算语义相似度常见的包括两种主要方法： path_similarity(sense1,sense2) # 词在词典层次结构中的最短路径 wup_similarity(sense1, sense2) # Wu-Palmer 提出的最短路径 #coding:utf-8 import nltk from nltk.corpus import wordnet as wn from heapq import * from itertools import product nltk.download(&#39;wordnet&#39;) word1 = &#39;hen&#39; word2 = &#39;chicken&#39; sense1 = wn.synsets(word1) sense2 = wn.synsets(word2) sim_wup = [word1.wup_similarity(word2) for (word1, word2) in product(sense1, sense2)] ##里边含有None sim_wup = max(a if a!=None else 0 for a in sim_wup) print(sim_wup) sim_path = [word1.path_similarity(word2) for (word1, word2) in product(sense1, sense2)] sim_path = max(a if a!=None else 0 for a in sim_path) print(sim_path) 0.962962962963 0.5","@type":"BlogPosting","url":"/2019/04/16/728210.html","headline":"NLTK库进行文本分类的过程","dateModified":"2019-04-16T00:00:00+08:00","datePublished":"2019-04-16T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/04/16/728210.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>NLTK库进行文本分类的过程</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p>本文针对英文文本，介绍的全部都是基于Python3.7，<br> 利用NLTK库进行文本分类的过程.</p> 
  <h2><a id="_3"></a><strong>文本分词</strong></h2> 
  <p>文本分词即将文本拆解成词语单元，英文文本以英文单词空格连接成句，分词过程较为简单。以下介绍几种方法。</p> 
  <h2><a id="_7"></a>正则表达式分词</h2> 
  <p><strong>1.以空格进行分词</strong></p> 
  <pre><code>import re
text = 'I was just a kid, and loved it very much! What a fantastic song!'
print(re.split(r' ',text))
</code></pre> 
  <p><strong>2.re匹配符号进行分词</strong></p> 
  <pre><code>print(re.split(r'\W+', text))
print(re.findall(r'\w+|\S\w*', text))
print(re.findall(r"\w+(?:[-']\w+)*|'|[-.(]+|\S\w*", text))
</code></pre> 
  <p><strong>3.NLTK正则表达式分词器</strong></p> 
  <pre><code>import re
import nltk
text = 'I was just a kid, and loved it very much! What a fantastic song!'
pattern = r"""(?x)                   # set flag to allow verbose regexps 
	              (?:[A-Z]\.)+           # abbreviations, e.g. U.S.A. 
	              |\d+(?:\.\d+)?%?       # numbers, incl. currency and percentages 
	              |\w+(?:[-']\w+)*       # words w/ optional internal hyphens/apostrophe 
	              |\.\.\.                # ellipsis 
	              |(?:[.,;"'?():-_`])    # special characters with meanings 
	            """ 
print(nltk.regexp_tokenize(text, pattern))
</code></pre> 
  <p>[‘I’, ‘was’, ‘just’, ‘a’, ‘kid’, ‘,’, ‘and’, ‘loved’, ‘it’, ‘very’, ‘much’, ‘What’, ‘a’, ‘fantastic’, ‘song’]</p> 
  <p><strong>4.最大匹配算法（MaxMatch）分词</strong></p> 
  <pre><code>import nltk
from nltk.corpus import words  
nltk.download('words')
wordlist = set(words.words())   
def max_match(text):  
    pos2 = len(text)  
    result = ''  
    while len(text) &gt; 0:         
        word = text[0:pos2]
        if word in wordlist:  
            result = result + text[0:pos2] + ' '  
            text = text[pos2:]  
            pos2 = len(text)  
        else:  
            pos2 = pos2-1                 
    return result[0:-1]  
string = 'theyarebirds'  
print(max_match(string))
</code></pre> 
  <h2><a id="_60"></a>停用词去除</h2> 
  <p>简单易懂，匹配词库中的停用词，去除！以消除冠词、连词等一些无意义无作用的词增加数据占用空间，并避免其为挖掘计算带来的干扰。</p> 
  <p><strong>NLTK停用词库</strong></p> 
  <pre><code>import nltk
from nltk.corpus import stopwords
stopworddic = set(stopwords.words('english'))  
text = ['I', 'was', 'just', 'a', 'kid',  'and', 'loved', 'it', 'very', 'much', 'What', 'a', 'fantastic', 'song']
text = [i for i in text if i not in stopworddic ]
print(text)
</code></pre> 
  <p>输出：<br> [‘I’, ‘kid’, ‘loved’, ‘much’, ‘What’, ‘fantastic’, ‘song’]</p> 
  <h2><a id="_75"></a><strong>词干抽取</strong></h2> 
  <p>将文本列表中的词语抽取其词干，以统一特征表征形式，特征降维以减少计算量。NLTK中提供了三种最常用的词干提取器接口，即 Porter stemmer, Lancaster Stemmer 和 Snowball Stemmer。抽取词的词干或词根形式（不一定能够表达完整语义）</p> 
  <pre><code>from nltk.stem.porter import PorterStemmer
porter_stemmer = PorterStemmer()
from nltk.stem.lancaster import LancasterStemmer
lancaster_stemmer = LancasterStemmer()
from nltk.stem import SnowballStemmer
snowball_stemmer = SnowballStemmer('english')
print(porter_stemmer.stem('maximum'))
print(lancaster_stemmer.stem('maximum'))
print(snowball_stemmer.stem('maximum'))
</code></pre> 
  <p>结果<br> maximum<br> maxim<br> maximum</p> 
  <h2><a id="_94"></a><strong>词形还原</strong></h2> 
  <p>词形还原Lemmatization是把一个任何形式的语言词汇还原为一般形式（能表达完整语义）。相对而言，词干提取是简单的轻量级的词形归并方式，最后获得的结果为词干，并不一定具有实际意义。词形还原处理相对复杂，获得结果为词的原形，能够承载一定意义，与词干提取相比，更具有研究和应用价值。</p> 
  <pre><code>from nltk.stem import WordNetLemmatizer  
wordnet_lemmatizer = WordNetLemmatizer()  
word = wordnet_lemmatizer.lemmatize('birds') 
print(word)
</code></pre> 
  <h2><a id="TFIDF_103"></a>文本向量表征以及TF-IDF权重表示</h2> 
  <p>这一部分是基于Python的Gensim库将文本特征抽取为词袋，并将词袋表征为id,以特征id以及文档频率表征成文本向量。TF-IDF权重是很可靠的权重表征方式，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。</p> 
  <pre><code>#coding:utf-8
from gensim import corpora, models, similarities
documents = ["Shipment of gold damaged in a fire","Delivery of silver arrived in a silver truck","Shipment of gold arrived in a truck"]

#分词#
texts = [[word for word in document.lower().split()] for document in documents]
print (texts)

#抽取词袋，将token映射为id
dictionary = corpora.Dictionary(texts)
print (dictionary.token2id)
#由文档向量以及频率构成文档向量
corpus = [dictionary.doc2bow(text) for text in texts]
print (corpus)

#计算tfidf权重,注意在gensim的tfidf算法中到文档频率的求解过程中对数之后+1了
tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]
for doc in corpus_tfidf:
	print (doc)
print(tfidf.dfs)
print(tfidf.idfs)
</code></pre> 
  <h2><a id="_131"></a>特征选择</h2> 
  <p>根据研究的需求进行特征的选择已达到特征降维，减小噪音的影响。常见的是根据词频（TF）、倒文档频率（IDF）、TFIDF权重等设定阈值进行筛选，无非是在TF/IDF/TFIDF权重计算结果的基础上设定阈值进行筛检。除此之外互信息、信息增益、X平方统计也是常见的方法。<br> 如果你的研究是在给定类别名称的前提下进行语义文本分类，那么判断特征词与分类之间的语义相似度，从而进行筛选也是一种可行的方法。这里介绍的便是基于WordNet进行语义相似度的介绍。</p> 
  <p>WordNet计算语义相似度常见的包括两种主要方法： path_similarity(sense1,sense2) # 词在词典层次结构中的最短路径 wup_similarity(sense1, sense2) # Wu-Palmer 提出的最短路径</p> 
  <pre><code>#coding:utf-8
import nltk
from nltk.corpus import wordnet as wn
from heapq import *  
from itertools import product
nltk.download('wordnet')
word1 = 'hen'
word2 = 'chicken'
sense1 = wn.synsets(word1)
sense2 = wn.synsets(word2)
sim_wup = [word1.wup_similarity(word2) for (word1, word2) in product(sense1, sense2)]  ##里边含有None
sim_wup = max(a if a!=None else 0  for a in sim_wup)
print(sim_wup)
sim_path = [word1.path_similarity(word2) for (word1, word2) in product(sense1, sense2)]
sim_path = max(a if a!=None else 0 for a in sim_path)
print(sim_path)
</code></pre> 
  <p>0.962962962963<br> 0.5</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
