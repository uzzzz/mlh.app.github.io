<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>opencv机器学习ml模块简介 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="opencv机器学习ml模块简介" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 本文不涉原理，只介绍如何使用！ ml模块为opencv的机器学习(machine learning，ml)代码库，包含各种机器学习算法： 0, class CvStatModel ; class CvMLData; struct CvParamGrid; 1，Normal Bayes Classifier（贝叶斯分类）； 2，K-Nearest Neighbour Classifier（K-邻近算法）； 3，SVM,support vector machine(支持向量机)； 4，Expectation - Maximization （EM算法）； 5，Decision Tree（决策树）； 6，Random Trees Classifier（随机森林算法）； 7， Boosted tree classifier （Boost树算法）； 9，Stochastic Gradient Descent&nbsp;SVM&nbsp;classifier； 10，ANN,Artificial Neural Networks(人工神经网络)； ml模块中有一个cv::ml::TrainData类，用于生成训练数据 create函数，从内存数组创建训练数据。： static Ptr&lt;TrainData&gt; cv::ml::TrainData::create (InputArray samples, int layout, InputArray responses, InputArray varIdx = noArray(), InputArray sampleIdx = noArray(), InputArray sampleWeights = noArray(), InputArray varType = noArray() ) 参数 sample：样本矩阵样本。它应该具有CV_32F类型，原始数据一般是uchar型，所以要进行类型转化。 layout：有两种取值 ROW_SAMPLE&nbsp;&nbsp;&nbsp;一行为一个样本 COL_SAMPLE&nbsp;一列为一个样本 response：响应矩阵。如果响应是标量，则将它们存储为单行或单列。 矩阵应该具有类型CV_32F或CV_32S（在前一种情况下，响应在默认情况下被视为有序;在后一种情况下 - 作为分类） varIdx：vector，指定用于训练的变量。 它可以是包含基于0的变量索引的整数向量（CV_32S）或包含活动变量掩码的字节向量（CV_8U）。 sampleIdx：向量，指定用于训练的样本。 它可以是包含基于0的样本索引的整数向量（CV_32S）或包含训练样本掩码的字节向量（CV_8U）。 sampleWeights：权重向量，每个样本都有权重。 它应该具有CV_32F类型。 1、KNN &nbsp; &nbsp; &nbsp; OpenCV 3.4.3中给出了K-最近邻(KNN)算法的实现，即cv::ml::Knearest类，此类的声明在include/opecv2/ml.hpp文件中，实现在modules/ml/src/knearest.cpp文件中。Knearest类：继承自cv::ml::StateModel，而StateModel又继承自cv::Algorithm。 static Ptr&lt;KNearest&gt; cv::ml::KNearest::create () create函数：为静态成员函数，new一个KNearestImpl，用来创建一个KNearest对象，如下： cv::Ptr&lt;cv::ml::KNearest&gt; knn = cv::ml::KNearest::create(); setDefaultK/getDefaultK函数：在预测时，设置/获取的K值； setIsClassifier/getIsClassifier函数：设置/获取应用KNN是进行分类还是回归； setEmax/getEmax函数：在使用KDTree算法时，设置/获取Emax参数值； setAlgorithmType/getAlgorithmType函数：设置/获取KNN算法类型，目前支持两种：brute_force和KDTree； findNearest函数：根据输入预测分类/回归结果 virtual float cv::ml::KNearest::findNearest ( InputArray samples, int k, OutputArray results, OutputArray neighborResponses = noArray(), OutputArray dist = noArray() ) const 参数说明： samples:为 样本数*特征数 的浮点二维矩阵； k：为寻找最近点的个数； results:预测结果； neighborResponse:为样本数*k的二维矩阵,代表每个样本的K个近邻的输出值； dist：为样本数*k的二维矩阵，代表每个样本的K个近邻的距离。 实例：opencv提供了一张手写数字图片给我们， 图片大小为1000*2000,有0-9的10个数字，每5行为一个数字，总共50行，共有5000个手写数字。我们首先要做的，就是把这5000个手写数字，一个个截取出来，每个数字块大小为20*20。直接将每个小图块进行序列化，因此最终得到一个5000*400的特征矩阵。样本数为5000，维度为400维。取其中前3000个样本进行训练。 注意：截取的时候，是按列截取。不然取前3000个样本进行训练就会出现后几个数字训练不到。 #include &quot;stdafx.h&quot; #include &quot;opencv2\opencv.hpp&quot; #include &lt;iostream&gt; using namespace std; using namespace cv; using namespace cv::ml; int main() { Mat img = imread(&quot;E:/opencv/opencv/sources/samples/data/digits.png&quot;); Mat gray; cvtColor(img, gray, CV_BGR2GRAY); int b = 20; int m = gray.rows / b; //原图为1000*2000 int n = gray.cols / b; //裁剪为5000个20*20的小图块 Mat data,labels; //特征矩阵 for (int i = 0; i &lt; n; i++) { int offsetCol = i*b; //列上的偏移量 for (int j = 0; j &lt; m; j++) { int offsetRow = j*b; //行上的偏移量 //截取20*20的小块 Mat tmp; gray(Range(offsetRow, offsetRow + b), Range(offsetCol, offsetCol + b)).copyTo(tmp); data.push_back(tmp.reshape(0,1)); //序列化后放入特征矩阵 labels.push_back((int)j / 5); //对应的标注 } } data.convertTo(data, CV_32F); //uchar型转换为cv_32f int samplesNum = data.rows; int trainNum = 3000; Mat trainData, trainLabels; trainData = data(Range(0, trainNum), Range::all()); //前3000个样本为训练数据 trainLabels = labels(Range(0, trainNum), Range::all()); //使用KNN算法 int K = 5; Ptr&lt;TrainData&gt; tData = TrainData::create(trainData, ROW_SAMPLE, trainLabels);//降训练数据封装成一个TrainData对象，送入train函数 Ptr&lt;KNearest&gt; model = KNearest::create(); model-&gt;setDefaultK(K); model-&gt;setIsClassifier(true); model-&gt;train(tData); //预测分类 double train_hr = 0, test_hr = 0; Mat response; // compute prediction error on train and test data for (int i = 0; i &lt; samplesNum; i++) { Mat sample = data.row(i); float r = model-&gt;predict(sample); //对所有行进行预测 //预测结果与原结果相比，相等为1，不等为0 r = std::abs(r - labels.at&lt;int&gt;(i)) &lt;= FLT_EPSILON ? 1.f : 0.f; if (i &lt; trainNum) train_hr += r; //累积正确数 else test_hr += r; } test_hr /= samplesNum - trainNum; train_hr = trainNum &gt; 0 ? train_hr / trainNum : 1.; printf(&quot;accuracy: train = %.1f%%, test = %.1f%%\n&quot;, train_hr*100., test_hr*100.); waitKey(0); return 0; } 代码来自：https://www.cnblogs.com/denny402/p/5033898.html 2、DTree &nbsp;create函数用来创建一个DTrees对象； static Ptr&lt;DTrees&gt; cv::ml::DTrees::create() (1)、setMaxCategories/getMaxCategories函数：设置/获取最大的类别数，默认值为10； (2)、setMaxDepth/getMaxDepth函数：设置/获取树的最大深度，默认值为INT_MAX； (3)、setMinSampleCount/getMinSampleCount函数：设置/获取最小训练样本数，如果节点中的样本数小于此参数，则不会拆分该节点。默认值为10； (4)、setCVFolds/getCVFolds函数：设置/获取CVFolds值，如果此值大于1，算法使用K-fold交叉验证程序修剪构建的决策树，其中K等于CVFolds。 默认值为10。 (5)、setTruncatePrunedTree/getTruncatedTree函数：设置/获取是否进行剪枝后移除操作，默认值为true； (6)、setRegressionAccuracy/getRegressionAccuracy函数：设置/获取回归时用于终止的标准，默认值为0.01； (7)、setPriors/getPriors函数：设置/获取先验概率数值，用于调整决策树的偏好，默认值为空的Mat； (8)、getRoots函数：获取根节点索引； (9)、getNodes函数：获取所有节点索引； (10)、getSplits函数：获取所有拆分索引； (11)、getSubsets函数：获取分类拆分的所有bitsets； (12)、load函数：load已序列化的model文件。 static Ptr&lt;DTrees&gt; cv::ml::DTrees::load (const String &amp; filepath,const String &amp; nodeName = String() ) 使用方法与KNN基本一致： cv::Ptr&lt;cv::ml::DTrees&gt; dtree = cv::ml::DTrees::create(); dtree-&gt;setMaxCategories(10);//设置分类数为10,0-9 dtree-&gt;setMaxDepth(10); dtree-&gt;setMinSampleCount(10); dtree-&gt;setCVFolds(0); dtree-&gt;setUseSurrogates(false); dtree-&gt;setUse1SERule(false); dtree-&gt;setTruncatePrunedTree(false); dtree-&gt;setRegressionAccuracy(0); dtree-&gt;setPriors(cv::Mat()); dtree-&gt;train(train_data, cv::ml::ROW_SAMPLE, train_labels); //在这里我们没有使用Ptr&lt;TrainData&gt;创建训练对象，而是直接将“train_data, cv::ml::ROW_SAMPLE, train_labels”送入train函数，这两种方式都是可行的 const std::string save_file{ &quot;E:/GitCode/NN_Test/data/decision_tree_model.xml&quot; }; // .xml, .yaml, .jsons dtree-&gt;save(save_file); 3、逻辑回归 &nbsp;逻辑回归(logistic regression)的实现，即cv::ml::LogisticRegression类，类的声明在include/opencv2/ml.hpp文件中，实现在modules/ml/src/lr.cpp文件中,它既支持两分类，也支持多分类，其中： (2)、setLearningRate/getLearningRate函数用来设置获取学习率； (3)、setIterations/getIterations函数用来设置/获取迭代次数； (4)、setRegularization函数用来设置采用哪种正则化方法，目前支持两种L1 norm和L2 norm，正则化方法主要用来防止过拟合； (5)、setTrainMethod函数用来设置采用哪种训练方法，目前支持两种Batch和Mini-Batch，使用Mini-Batch方法时，将MiniBatchSize设置一个正整数。 (6)、setMiniBatchSize函数用来设置在Mini-Batch梯度下降训练方法中每一个step采集的训练样本数，getMiniBatchSize函数用来获取每一个step采集的训练样本数； (7)、setTermCriteria/getTermCriteria函数用来设置/获取终止训练的条件，包括迭代次数和期望的精度; (8)、get_learnt_thetas函数用来获取训练参数； (9)、train函数(使用基类StatModel中的)进行训练； (10)、predict函数用于预测； (11)、save函数(使用基类Algorithm中的)保存已训练好的model，支持xml,yaml,json格式; (12)、load函数用来load已训练好的model； &nbsp; cv::Ptr&lt;cv::ml::LogisticRegression&gt; lr = cv::ml::LogisticRegression::create(); lr-&gt;setLearningRate(0.00001); lr-&gt;setIterations(100); lr-&gt;setRegularization(cv::ml::LogisticRegression::REG_DISABLE); lr-&gt;setTrainMethod(cv::ml::LogisticRegression::MINI_BATCH); lr-&gt;setMiniBatchSize(1); lr-&gt;train(data, cv::ml::ROW_SAMPLE, labels); const std::string save_file{ &quot;E:/GitCode/NN_Test/data/logistic_regression_model.xml&quot; }; // .xml, .yaml, .jsons lr-&gt;save(save_file); cv::Ptr&lt;cv::ml::LogisticRegression&gt; lr = cv::ml::LogisticRegression::load(model_file); lr-&gt;predict(data, result); 转自：https://blog.csdn.net/fengbingchun/article/details/78221693 4、SVM 支持向量机(Support Vector Machines)的实现，即cv::ml::SVM类，它既支持两分类，也支持多分类，还支持回归等，OpenCV中SVM的实现源自libsvm库。 (3)、setType/getType函数：设置/获取SVM公式类型，包括C_SVC、NU_SVC、ONE_CLASS、EPS_SVR、NU_SVR，用于指定分类、回归等，默认为C_SVC； C_SVC:C-支持向量分类。 n级分类（n≥2），允许类别的不完美分离与罚分乘数C的异常值。 NU_SVC:ν-支持向量分类。 n级分类，可能存在不完美的分离。 使用参数ν（在0..1范围内，值越大，决策边界越平滑）代替C. ONE_CLASS:分布估计（一类SVM）。 所有训练数据都来自同一个类，SVM构建一个边界，将类与特征空间的其余部分分开。 EPS_SVR:ε-支持向量回归。 来自训练集和拟合超平面的特征向量之间的距离必须小于p。 对于异常值，使用罚分乘数C. NU_SVR:ν-支持向量回归。 用ν代替p。&nbsp; (4)、setGamma/getGamma函数：设置/获取核函数的γ参数，默认值为1； (5)、setCoef0/getCoef0函数：设置/获取核函数的coef0参数，默认值为0； (6)、setDegree/getDegree函数：设置/获取多项式核函数的degreee参数（阶数），默认值为0； (7)、setC/getC函数：设置/获取SVM优化问题的C参数，默认值为0； (8)、setNu/getNu函数：设置/获取SVM优化问题的υ参数，默认值为0； (9)、setP/getP函数：设置/获取SVM优化问题的ε参数，默认值为0； (10)、setClassWeights/getClassWeights函数：应用在C_SVC,设置/获取weights，默认值是空cv::Mat； (11)、setTermCriteria/getTermCriteria函数：设置/获取SVM训练时迭代终止条件，默认值是cv::TermCriteria(cv::TermCriteria::MAX_ITER + TermCriteria::EPS,1000, FLT_EPSILON)； (12)、setKernel/getKernelType函数：设置/获取SVM核函数类型，包括CUSTOM、LINEAR、POLY、RBF、SIGMOID、CHI2、INTER，默认值为RBF； (13)、setCustomKernel函数：初始化CUSTOM核函数； (14)、trainAuto函数：用最优参数训练SVM；会相当耗时。 virtual bool cv::ml::SVM::trainAuto ( const Ptr&lt; TrainData &gt; &amp; data, int kFold = 10, ParamGrid Cgrid = getDefaultGrid(C), ParamGrid gammaGrid = getDefaultGrid(GAMMA), ParamGrid pGrid = getDefaultGrid(P), ParamGrid nuGrid = getDefaultGrid(NU), ParamGrid coeffGrid = getDefaultGrid(COEF), ParamGrid degreeGrid = getDefaultGrid(DEGREE), bool balanced = false ) (15)、getSupportVectors/getUncompressedSupportVectors函数：获取所有的支持向量； (16)、getDecisionFunction函数：决策函数； (17)、getDefaultGrid/getDefaultGridPtr函数：生成SVM参数网格； static Ptr&lt;ParamGrid&gt; cv::ml::SVM::getDefaultGridPtr(int param_id) &nbsp;param_id:&nbsp;&nbsp;C&nbsp;,&nbsp;&nbsp;&nbsp;GAMMA&nbsp;,&nbsp;&nbsp;&nbsp;P&nbsp;,&nbsp;&nbsp;&nbsp;NU,&nbsp;&nbsp;&nbsp;&nbsp;COEF&nbsp;,&nbsp;&nbsp;&nbsp;DEGREE&nbsp; (19)、train/predict函数：用于训练/预测，均使用基类StatModel中的。(18)、save/load函数：保存/载入已训练好的model，支持xml,yaml,json格式； cv::Ptr&lt;cv::ml::SVM&gt; svm = cv::ml::SVM::create(); svm-&gt;setType(cv::ml::SVM::C_SVC); svm-&gt;setKernel(cv::ml::SVM::LINEAR); svm-&gt;setTermCriteria(cv::TermCriteria(cv::TermCriteria::MAX_ITER, 100, 1e-6)); CHECK(svm-&gt;train(trainingDataMat, cv::ml::ROW_SAMPLE, labelsMat)); const std::string save_file{ &quot;E:/GitCode/NN_Test/data/svm_model.xml&quot; }; // .xml, .yaml, .jsons svm-&gt;save(save_file); 5、SVMSGD 随机梯度下降SVM分类器。 参数w更新公式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; ：是步骤t 中决策函数的权重向量 w γ(t) :是迭代t时,模型参数的步长,每次迭代步长减小公式：&nbsp; Qi ：是总数为i个样本的SVM的目标函数，该样本在算法的每个步骤上随机选择。 ASGD是平均随机梯度下降SVM分类器。 ASGD分类器通过公式对算法的每个步骤的权重向量进行平均： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 分类器具有以下参数： setMarginType()/getMarginType()：边界类型分为软边界和硬边界，SOFT_MARGIN，HARD_MARGIN setSvmsgdType()/getSvmsgdType():SGD和ASGD SGD&nbsp; Stochastic Gradient Descent. ASGD&nbsp; Average Stochastic Gradient Descent. 边界正则化参数（λ）：正则化参数负责每一步的权重减少以及对异常值的限制强度（参数越小，异常值不容易被忽略）。 SGD模型的推荐值为0.0001，ASGD模型的推荐值为0.00001。 初始步长（γ0）：初始步长参数是步长γ（t）的初始值。 逐步降低功率（c）：是γ（t）的功率参数，通过上述公式减小。 SGD模型的推荐值为1，ASGD模型的推荐值为0.75。 终止标准：终止标准可以是TermCriteria :: COUNT，TermCriteria :: EPS或TermCriteria :: COUNT + TermCriteria :: EPS。 // Create empty object cv::Ptr&lt;SVMSGD&gt; svmsgd = SVMSGD::create(); // Train the Stochastic Gradient Descent SVM svmsgd-&gt;train(trainData); // Predict labels for the new samples svmsgd-&gt;predict(samples, responses); 6、ANN_MLP 人工神经网络——多层感知器。与许多其他模型毫升构造和训练,在模型这些步骤是分开的。首先,创建一个与指定的网络拓扑结构使用非默认的构造函数或方法ANN_MLP::创建。所有的权重都设置为0。然后,网络训练使用一组输入和输出向量。训练过程可以重复不止一次,也就是说,权重可以基于新的训练数据，进行调整。 1、setLayerSizes()：整数向量，指定每层中神经元的数量，包括输入和输出层。 第一个元素指定输入图层中的元素数。 最后一个元素 - 输出图层中的元素数。 默认值为空Mat。 virtual void cv::ml::ANN_MLP::setLayerSizes(InputArray _layer_sizes) 2、setActivationFunction()：设置激活函数 virtual void cv::ml::ANN_MLP::setActivationFunction(int type,double param1 = 0,double param2 = 0 ) type： param1： activation function的第一个参数 α. Default value is 0. param2： activation function的第二个参数β. Default value is 0. 3、setTrainMethod()函数 virtual void cv::ml::ANN_MLP::setTrainMethod(int method,double param1 = 0,double param2 = 0 ) method：backprop:反向传播；rprop:弹性反向传播；anneal :模拟退火算法； Ptr&lt;cv&gt; ann = cv::ml::ANN_MLP::create(); ann-&gt;setLayerSizes(Mat_&lt;int&gt;(1,5) &lt;&lt; 5,2,2,2,5) ann-&gt;setTrainMethod(BACKPROP); ann-&gt;setActivationFunction(RELU); ann-&gt;train(....) &nbsp;" />
<meta property="og:description" content="&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 本文不涉原理，只介绍如何使用！ ml模块为opencv的机器学习(machine learning，ml)代码库，包含各种机器学习算法： 0, class CvStatModel ; class CvMLData; struct CvParamGrid; 1，Normal Bayes Classifier（贝叶斯分类）； 2，K-Nearest Neighbour Classifier（K-邻近算法）； 3，SVM,support vector machine(支持向量机)； 4，Expectation - Maximization （EM算法）； 5，Decision Tree（决策树）； 6，Random Trees Classifier（随机森林算法）； 7， Boosted tree classifier （Boost树算法）； 9，Stochastic Gradient Descent&nbsp;SVM&nbsp;classifier； 10，ANN,Artificial Neural Networks(人工神经网络)； ml模块中有一个cv::ml::TrainData类，用于生成训练数据 create函数，从内存数组创建训练数据。： static Ptr&lt;TrainData&gt; cv::ml::TrainData::create (InputArray samples, int layout, InputArray responses, InputArray varIdx = noArray(), InputArray sampleIdx = noArray(), InputArray sampleWeights = noArray(), InputArray varType = noArray() ) 参数 sample：样本矩阵样本。它应该具有CV_32F类型，原始数据一般是uchar型，所以要进行类型转化。 layout：有两种取值 ROW_SAMPLE&nbsp;&nbsp;&nbsp;一行为一个样本 COL_SAMPLE&nbsp;一列为一个样本 response：响应矩阵。如果响应是标量，则将它们存储为单行或单列。 矩阵应该具有类型CV_32F或CV_32S（在前一种情况下，响应在默认情况下被视为有序;在后一种情况下 - 作为分类） varIdx：vector，指定用于训练的变量。 它可以是包含基于0的变量索引的整数向量（CV_32S）或包含活动变量掩码的字节向量（CV_8U）。 sampleIdx：向量，指定用于训练的样本。 它可以是包含基于0的样本索引的整数向量（CV_32S）或包含训练样本掩码的字节向量（CV_8U）。 sampleWeights：权重向量，每个样本都有权重。 它应该具有CV_32F类型。 1、KNN &nbsp; &nbsp; &nbsp; OpenCV 3.4.3中给出了K-最近邻(KNN)算法的实现，即cv::ml::Knearest类，此类的声明在include/opecv2/ml.hpp文件中，实现在modules/ml/src/knearest.cpp文件中。Knearest类：继承自cv::ml::StateModel，而StateModel又继承自cv::Algorithm。 static Ptr&lt;KNearest&gt; cv::ml::KNearest::create () create函数：为静态成员函数，new一个KNearestImpl，用来创建一个KNearest对象，如下： cv::Ptr&lt;cv::ml::KNearest&gt; knn = cv::ml::KNearest::create(); setDefaultK/getDefaultK函数：在预测时，设置/获取的K值； setIsClassifier/getIsClassifier函数：设置/获取应用KNN是进行分类还是回归； setEmax/getEmax函数：在使用KDTree算法时，设置/获取Emax参数值； setAlgorithmType/getAlgorithmType函数：设置/获取KNN算法类型，目前支持两种：brute_force和KDTree； findNearest函数：根据输入预测分类/回归结果 virtual float cv::ml::KNearest::findNearest ( InputArray samples, int k, OutputArray results, OutputArray neighborResponses = noArray(), OutputArray dist = noArray() ) const 参数说明： samples:为 样本数*特征数 的浮点二维矩阵； k：为寻找最近点的个数； results:预测结果； neighborResponse:为样本数*k的二维矩阵,代表每个样本的K个近邻的输出值； dist：为样本数*k的二维矩阵，代表每个样本的K个近邻的距离。 实例：opencv提供了一张手写数字图片给我们， 图片大小为1000*2000,有0-9的10个数字，每5行为一个数字，总共50行，共有5000个手写数字。我们首先要做的，就是把这5000个手写数字，一个个截取出来，每个数字块大小为20*20。直接将每个小图块进行序列化，因此最终得到一个5000*400的特征矩阵。样本数为5000，维度为400维。取其中前3000个样本进行训练。 注意：截取的时候，是按列截取。不然取前3000个样本进行训练就会出现后几个数字训练不到。 #include &quot;stdafx.h&quot; #include &quot;opencv2\opencv.hpp&quot; #include &lt;iostream&gt; using namespace std; using namespace cv; using namespace cv::ml; int main() { Mat img = imread(&quot;E:/opencv/opencv/sources/samples/data/digits.png&quot;); Mat gray; cvtColor(img, gray, CV_BGR2GRAY); int b = 20; int m = gray.rows / b; //原图为1000*2000 int n = gray.cols / b; //裁剪为5000个20*20的小图块 Mat data,labels; //特征矩阵 for (int i = 0; i &lt; n; i++) { int offsetCol = i*b; //列上的偏移量 for (int j = 0; j &lt; m; j++) { int offsetRow = j*b; //行上的偏移量 //截取20*20的小块 Mat tmp; gray(Range(offsetRow, offsetRow + b), Range(offsetCol, offsetCol + b)).copyTo(tmp); data.push_back(tmp.reshape(0,1)); //序列化后放入特征矩阵 labels.push_back((int)j / 5); //对应的标注 } } data.convertTo(data, CV_32F); //uchar型转换为cv_32f int samplesNum = data.rows; int trainNum = 3000; Mat trainData, trainLabels; trainData = data(Range(0, trainNum), Range::all()); //前3000个样本为训练数据 trainLabels = labels(Range(0, trainNum), Range::all()); //使用KNN算法 int K = 5; Ptr&lt;TrainData&gt; tData = TrainData::create(trainData, ROW_SAMPLE, trainLabels);//降训练数据封装成一个TrainData对象，送入train函数 Ptr&lt;KNearest&gt; model = KNearest::create(); model-&gt;setDefaultK(K); model-&gt;setIsClassifier(true); model-&gt;train(tData); //预测分类 double train_hr = 0, test_hr = 0; Mat response; // compute prediction error on train and test data for (int i = 0; i &lt; samplesNum; i++) { Mat sample = data.row(i); float r = model-&gt;predict(sample); //对所有行进行预测 //预测结果与原结果相比，相等为1，不等为0 r = std::abs(r - labels.at&lt;int&gt;(i)) &lt;= FLT_EPSILON ? 1.f : 0.f; if (i &lt; trainNum) train_hr += r; //累积正确数 else test_hr += r; } test_hr /= samplesNum - trainNum; train_hr = trainNum &gt; 0 ? train_hr / trainNum : 1.; printf(&quot;accuracy: train = %.1f%%, test = %.1f%%\n&quot;, train_hr*100., test_hr*100.); waitKey(0); return 0; } 代码来自：https://www.cnblogs.com/denny402/p/5033898.html 2、DTree &nbsp;create函数用来创建一个DTrees对象； static Ptr&lt;DTrees&gt; cv::ml::DTrees::create() (1)、setMaxCategories/getMaxCategories函数：设置/获取最大的类别数，默认值为10； (2)、setMaxDepth/getMaxDepth函数：设置/获取树的最大深度，默认值为INT_MAX； (3)、setMinSampleCount/getMinSampleCount函数：设置/获取最小训练样本数，如果节点中的样本数小于此参数，则不会拆分该节点。默认值为10； (4)、setCVFolds/getCVFolds函数：设置/获取CVFolds值，如果此值大于1，算法使用K-fold交叉验证程序修剪构建的决策树，其中K等于CVFolds。 默认值为10。 (5)、setTruncatePrunedTree/getTruncatedTree函数：设置/获取是否进行剪枝后移除操作，默认值为true； (6)、setRegressionAccuracy/getRegressionAccuracy函数：设置/获取回归时用于终止的标准，默认值为0.01； (7)、setPriors/getPriors函数：设置/获取先验概率数值，用于调整决策树的偏好，默认值为空的Mat； (8)、getRoots函数：获取根节点索引； (9)、getNodes函数：获取所有节点索引； (10)、getSplits函数：获取所有拆分索引； (11)、getSubsets函数：获取分类拆分的所有bitsets； (12)、load函数：load已序列化的model文件。 static Ptr&lt;DTrees&gt; cv::ml::DTrees::load (const String &amp; filepath,const String &amp; nodeName = String() ) 使用方法与KNN基本一致： cv::Ptr&lt;cv::ml::DTrees&gt; dtree = cv::ml::DTrees::create(); dtree-&gt;setMaxCategories(10);//设置分类数为10,0-9 dtree-&gt;setMaxDepth(10); dtree-&gt;setMinSampleCount(10); dtree-&gt;setCVFolds(0); dtree-&gt;setUseSurrogates(false); dtree-&gt;setUse1SERule(false); dtree-&gt;setTruncatePrunedTree(false); dtree-&gt;setRegressionAccuracy(0); dtree-&gt;setPriors(cv::Mat()); dtree-&gt;train(train_data, cv::ml::ROW_SAMPLE, train_labels); //在这里我们没有使用Ptr&lt;TrainData&gt;创建训练对象，而是直接将“train_data, cv::ml::ROW_SAMPLE, train_labels”送入train函数，这两种方式都是可行的 const std::string save_file{ &quot;E:/GitCode/NN_Test/data/decision_tree_model.xml&quot; }; // .xml, .yaml, .jsons dtree-&gt;save(save_file); 3、逻辑回归 &nbsp;逻辑回归(logistic regression)的实现，即cv::ml::LogisticRegression类，类的声明在include/opencv2/ml.hpp文件中，实现在modules/ml/src/lr.cpp文件中,它既支持两分类，也支持多分类，其中： (2)、setLearningRate/getLearningRate函数用来设置获取学习率； (3)、setIterations/getIterations函数用来设置/获取迭代次数； (4)、setRegularization函数用来设置采用哪种正则化方法，目前支持两种L1 norm和L2 norm，正则化方法主要用来防止过拟合； (5)、setTrainMethod函数用来设置采用哪种训练方法，目前支持两种Batch和Mini-Batch，使用Mini-Batch方法时，将MiniBatchSize设置一个正整数。 (6)、setMiniBatchSize函数用来设置在Mini-Batch梯度下降训练方法中每一个step采集的训练样本数，getMiniBatchSize函数用来获取每一个step采集的训练样本数； (7)、setTermCriteria/getTermCriteria函数用来设置/获取终止训练的条件，包括迭代次数和期望的精度; (8)、get_learnt_thetas函数用来获取训练参数； (9)、train函数(使用基类StatModel中的)进行训练； (10)、predict函数用于预测； (11)、save函数(使用基类Algorithm中的)保存已训练好的model，支持xml,yaml,json格式; (12)、load函数用来load已训练好的model； &nbsp; cv::Ptr&lt;cv::ml::LogisticRegression&gt; lr = cv::ml::LogisticRegression::create(); lr-&gt;setLearningRate(0.00001); lr-&gt;setIterations(100); lr-&gt;setRegularization(cv::ml::LogisticRegression::REG_DISABLE); lr-&gt;setTrainMethod(cv::ml::LogisticRegression::MINI_BATCH); lr-&gt;setMiniBatchSize(1); lr-&gt;train(data, cv::ml::ROW_SAMPLE, labels); const std::string save_file{ &quot;E:/GitCode/NN_Test/data/logistic_regression_model.xml&quot; }; // .xml, .yaml, .jsons lr-&gt;save(save_file); cv::Ptr&lt;cv::ml::LogisticRegression&gt; lr = cv::ml::LogisticRegression::load(model_file); lr-&gt;predict(data, result); 转自：https://blog.csdn.net/fengbingchun/article/details/78221693 4、SVM 支持向量机(Support Vector Machines)的实现，即cv::ml::SVM类，它既支持两分类，也支持多分类，还支持回归等，OpenCV中SVM的实现源自libsvm库。 (3)、setType/getType函数：设置/获取SVM公式类型，包括C_SVC、NU_SVC、ONE_CLASS、EPS_SVR、NU_SVR，用于指定分类、回归等，默认为C_SVC； C_SVC:C-支持向量分类。 n级分类（n≥2），允许类别的不完美分离与罚分乘数C的异常值。 NU_SVC:ν-支持向量分类。 n级分类，可能存在不完美的分离。 使用参数ν（在0..1范围内，值越大，决策边界越平滑）代替C. ONE_CLASS:分布估计（一类SVM）。 所有训练数据都来自同一个类，SVM构建一个边界，将类与特征空间的其余部分分开。 EPS_SVR:ε-支持向量回归。 来自训练集和拟合超平面的特征向量之间的距离必须小于p。 对于异常值，使用罚分乘数C. NU_SVR:ν-支持向量回归。 用ν代替p。&nbsp; (4)、setGamma/getGamma函数：设置/获取核函数的γ参数，默认值为1； (5)、setCoef0/getCoef0函数：设置/获取核函数的coef0参数，默认值为0； (6)、setDegree/getDegree函数：设置/获取多项式核函数的degreee参数（阶数），默认值为0； (7)、setC/getC函数：设置/获取SVM优化问题的C参数，默认值为0； (8)、setNu/getNu函数：设置/获取SVM优化问题的υ参数，默认值为0； (9)、setP/getP函数：设置/获取SVM优化问题的ε参数，默认值为0； (10)、setClassWeights/getClassWeights函数：应用在C_SVC,设置/获取weights，默认值是空cv::Mat； (11)、setTermCriteria/getTermCriteria函数：设置/获取SVM训练时迭代终止条件，默认值是cv::TermCriteria(cv::TermCriteria::MAX_ITER + TermCriteria::EPS,1000, FLT_EPSILON)； (12)、setKernel/getKernelType函数：设置/获取SVM核函数类型，包括CUSTOM、LINEAR、POLY、RBF、SIGMOID、CHI2、INTER，默认值为RBF； (13)、setCustomKernel函数：初始化CUSTOM核函数； (14)、trainAuto函数：用最优参数训练SVM；会相当耗时。 virtual bool cv::ml::SVM::trainAuto ( const Ptr&lt; TrainData &gt; &amp; data, int kFold = 10, ParamGrid Cgrid = getDefaultGrid(C), ParamGrid gammaGrid = getDefaultGrid(GAMMA), ParamGrid pGrid = getDefaultGrid(P), ParamGrid nuGrid = getDefaultGrid(NU), ParamGrid coeffGrid = getDefaultGrid(COEF), ParamGrid degreeGrid = getDefaultGrid(DEGREE), bool balanced = false ) (15)、getSupportVectors/getUncompressedSupportVectors函数：获取所有的支持向量； (16)、getDecisionFunction函数：决策函数； (17)、getDefaultGrid/getDefaultGridPtr函数：生成SVM参数网格； static Ptr&lt;ParamGrid&gt; cv::ml::SVM::getDefaultGridPtr(int param_id) &nbsp;param_id:&nbsp;&nbsp;C&nbsp;,&nbsp;&nbsp;&nbsp;GAMMA&nbsp;,&nbsp;&nbsp;&nbsp;P&nbsp;,&nbsp;&nbsp;&nbsp;NU,&nbsp;&nbsp;&nbsp;&nbsp;COEF&nbsp;,&nbsp;&nbsp;&nbsp;DEGREE&nbsp; (19)、train/predict函数：用于训练/预测，均使用基类StatModel中的。(18)、save/load函数：保存/载入已训练好的model，支持xml,yaml,json格式； cv::Ptr&lt;cv::ml::SVM&gt; svm = cv::ml::SVM::create(); svm-&gt;setType(cv::ml::SVM::C_SVC); svm-&gt;setKernel(cv::ml::SVM::LINEAR); svm-&gt;setTermCriteria(cv::TermCriteria(cv::TermCriteria::MAX_ITER, 100, 1e-6)); CHECK(svm-&gt;train(trainingDataMat, cv::ml::ROW_SAMPLE, labelsMat)); const std::string save_file{ &quot;E:/GitCode/NN_Test/data/svm_model.xml&quot; }; // .xml, .yaml, .jsons svm-&gt;save(save_file); 5、SVMSGD 随机梯度下降SVM分类器。 参数w更新公式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; ：是步骤t 中决策函数的权重向量 w γ(t) :是迭代t时,模型参数的步长,每次迭代步长减小公式：&nbsp; Qi ：是总数为i个样本的SVM的目标函数，该样本在算法的每个步骤上随机选择。 ASGD是平均随机梯度下降SVM分类器。 ASGD分类器通过公式对算法的每个步骤的权重向量进行平均： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 分类器具有以下参数： setMarginType()/getMarginType()：边界类型分为软边界和硬边界，SOFT_MARGIN，HARD_MARGIN setSvmsgdType()/getSvmsgdType():SGD和ASGD SGD&nbsp; Stochastic Gradient Descent. ASGD&nbsp; Average Stochastic Gradient Descent. 边界正则化参数（λ）：正则化参数负责每一步的权重减少以及对异常值的限制强度（参数越小，异常值不容易被忽略）。 SGD模型的推荐值为0.0001，ASGD模型的推荐值为0.00001。 初始步长（γ0）：初始步长参数是步长γ（t）的初始值。 逐步降低功率（c）：是γ（t）的功率参数，通过上述公式减小。 SGD模型的推荐值为1，ASGD模型的推荐值为0.75。 终止标准：终止标准可以是TermCriteria :: COUNT，TermCriteria :: EPS或TermCriteria :: COUNT + TermCriteria :: EPS。 // Create empty object cv::Ptr&lt;SVMSGD&gt; svmsgd = SVMSGD::create(); // Train the Stochastic Gradient Descent SVM svmsgd-&gt;train(trainData); // Predict labels for the new samples svmsgd-&gt;predict(samples, responses); 6、ANN_MLP 人工神经网络——多层感知器。与许多其他模型毫升构造和训练,在模型这些步骤是分开的。首先,创建一个与指定的网络拓扑结构使用非默认的构造函数或方法ANN_MLP::创建。所有的权重都设置为0。然后,网络训练使用一组输入和输出向量。训练过程可以重复不止一次,也就是说,权重可以基于新的训练数据，进行调整。 1、setLayerSizes()：整数向量，指定每层中神经元的数量，包括输入和输出层。 第一个元素指定输入图层中的元素数。 最后一个元素 - 输出图层中的元素数。 默认值为空Mat。 virtual void cv::ml::ANN_MLP::setLayerSizes(InputArray _layer_sizes) 2、setActivationFunction()：设置激活函数 virtual void cv::ml::ANN_MLP::setActivationFunction(int type,double param1 = 0,double param2 = 0 ) type： param1： activation function的第一个参数 α. Default value is 0. param2： activation function的第二个参数β. Default value is 0. 3、setTrainMethod()函数 virtual void cv::ml::ANN_MLP::setTrainMethod(int method,double param1 = 0,double param2 = 0 ) method：backprop:反向传播；rprop:弹性反向传播；anneal :模拟退火算法； Ptr&lt;cv&gt; ann = cv::ml::ANN_MLP::create(); ann-&gt;setLayerSizes(Mat_&lt;int&gt;(1,5) &lt;&lt; 5,2,2,2,5) ann-&gt;setTrainMethod(BACKPROP); ann-&gt;setActivationFunction(RELU); ann-&gt;train(....) &nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/04/16/728245.html" />
<meta property="og:url" content="https://mlh.app/2019/04/16/728245.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-16T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 本文不涉原理，只介绍如何使用！ ml模块为opencv的机器学习(machine learning，ml)代码库，包含各种机器学习算法： 0, class CvStatModel ; class CvMLData; struct CvParamGrid; 1，Normal Bayes Classifier（贝叶斯分类）； 2，K-Nearest Neighbour Classifier（K-邻近算法）； 3，SVM,support vector machine(支持向量机)； 4，Expectation - Maximization （EM算法）； 5，Decision Tree（决策树）； 6，Random Trees Classifier（随机森林算法）； 7， Boosted tree classifier （Boost树算法）； 9，Stochastic Gradient Descent&nbsp;SVM&nbsp;classifier； 10，ANN,Artificial Neural Networks(人工神经网络)； ml模块中有一个cv::ml::TrainData类，用于生成训练数据 create函数，从内存数组创建训练数据。： static Ptr&lt;TrainData&gt; cv::ml::TrainData::create (InputArray samples, int layout, InputArray responses, InputArray varIdx = noArray(), InputArray sampleIdx = noArray(), InputArray sampleWeights = noArray(), InputArray varType = noArray() ) 参数 sample：样本矩阵样本。它应该具有CV_32F类型，原始数据一般是uchar型，所以要进行类型转化。 layout：有两种取值 ROW_SAMPLE&nbsp;&nbsp;&nbsp;一行为一个样本 COL_SAMPLE&nbsp;一列为一个样本 response：响应矩阵。如果响应是标量，则将它们存储为单行或单列。 矩阵应该具有类型CV_32F或CV_32S（在前一种情况下，响应在默认情况下被视为有序;在后一种情况下 - 作为分类） varIdx：vector，指定用于训练的变量。 它可以是包含基于0的变量索引的整数向量（CV_32S）或包含活动变量掩码的字节向量（CV_8U）。 sampleIdx：向量，指定用于训练的样本。 它可以是包含基于0的样本索引的整数向量（CV_32S）或包含训练样本掩码的字节向量（CV_8U）。 sampleWeights：权重向量，每个样本都有权重。 它应该具有CV_32F类型。 1、KNN &nbsp; &nbsp; &nbsp; OpenCV 3.4.3中给出了K-最近邻(KNN)算法的实现，即cv::ml::Knearest类，此类的声明在include/opecv2/ml.hpp文件中，实现在modules/ml/src/knearest.cpp文件中。Knearest类：继承自cv::ml::StateModel，而StateModel又继承自cv::Algorithm。 static Ptr&lt;KNearest&gt; cv::ml::KNearest::create () create函数：为静态成员函数，new一个KNearestImpl，用来创建一个KNearest对象，如下： cv::Ptr&lt;cv::ml::KNearest&gt; knn = cv::ml::KNearest::create(); setDefaultK/getDefaultK函数：在预测时，设置/获取的K值； setIsClassifier/getIsClassifier函数：设置/获取应用KNN是进行分类还是回归； setEmax/getEmax函数：在使用KDTree算法时，设置/获取Emax参数值； setAlgorithmType/getAlgorithmType函数：设置/获取KNN算法类型，目前支持两种：brute_force和KDTree； findNearest函数：根据输入预测分类/回归结果 virtual float cv::ml::KNearest::findNearest ( InputArray samples, int k, OutputArray results, OutputArray neighborResponses = noArray(), OutputArray dist = noArray() ) const 参数说明： samples:为 样本数*特征数 的浮点二维矩阵； k：为寻找最近点的个数； results:预测结果； neighborResponse:为样本数*k的二维矩阵,代表每个样本的K个近邻的输出值； dist：为样本数*k的二维矩阵，代表每个样本的K个近邻的距离。 实例：opencv提供了一张手写数字图片给我们， 图片大小为1000*2000,有0-9的10个数字，每5行为一个数字，总共50行，共有5000个手写数字。我们首先要做的，就是把这5000个手写数字，一个个截取出来，每个数字块大小为20*20。直接将每个小图块进行序列化，因此最终得到一个5000*400的特征矩阵。样本数为5000，维度为400维。取其中前3000个样本进行训练。 注意：截取的时候，是按列截取。不然取前3000个样本进行训练就会出现后几个数字训练不到。 #include &quot;stdafx.h&quot; #include &quot;opencv2\\opencv.hpp&quot; #include &lt;iostream&gt; using namespace std; using namespace cv; using namespace cv::ml; int main() { Mat img = imread(&quot;E:/opencv/opencv/sources/samples/data/digits.png&quot;); Mat gray; cvtColor(img, gray, CV_BGR2GRAY); int b = 20; int m = gray.rows / b; //原图为1000*2000 int n = gray.cols / b; //裁剪为5000个20*20的小图块 Mat data,labels; //特征矩阵 for (int i = 0; i &lt; n; i++) { int offsetCol = i*b; //列上的偏移量 for (int j = 0; j &lt; m; j++) { int offsetRow = j*b; //行上的偏移量 //截取20*20的小块 Mat tmp; gray(Range(offsetRow, offsetRow + b), Range(offsetCol, offsetCol + b)).copyTo(tmp); data.push_back(tmp.reshape(0,1)); //序列化后放入特征矩阵 labels.push_back((int)j / 5); //对应的标注 } } data.convertTo(data, CV_32F); //uchar型转换为cv_32f int samplesNum = data.rows; int trainNum = 3000; Mat trainData, trainLabels; trainData = data(Range(0, trainNum), Range::all()); //前3000个样本为训练数据 trainLabels = labels(Range(0, trainNum), Range::all()); //使用KNN算法 int K = 5; Ptr&lt;TrainData&gt; tData = TrainData::create(trainData, ROW_SAMPLE, trainLabels);//降训练数据封装成一个TrainData对象，送入train函数 Ptr&lt;KNearest&gt; model = KNearest::create(); model-&gt;setDefaultK(K); model-&gt;setIsClassifier(true); model-&gt;train(tData); //预测分类 double train_hr = 0, test_hr = 0; Mat response; // compute prediction error on train and test data for (int i = 0; i &lt; samplesNum; i++) { Mat sample = data.row(i); float r = model-&gt;predict(sample); //对所有行进行预测 //预测结果与原结果相比，相等为1，不等为0 r = std::abs(r - labels.at&lt;int&gt;(i)) &lt;= FLT_EPSILON ? 1.f : 0.f; if (i &lt; trainNum) train_hr += r; //累积正确数 else test_hr += r; } test_hr /= samplesNum - trainNum; train_hr = trainNum &gt; 0 ? train_hr / trainNum : 1.; printf(&quot;accuracy: train = %.1f%%, test = %.1f%%\\n&quot;, train_hr*100., test_hr*100.); waitKey(0); return 0; } 代码来自：https://www.cnblogs.com/denny402/p/5033898.html 2、DTree &nbsp;create函数用来创建一个DTrees对象； static Ptr&lt;DTrees&gt; cv::ml::DTrees::create() (1)、setMaxCategories/getMaxCategories函数：设置/获取最大的类别数，默认值为10； (2)、setMaxDepth/getMaxDepth函数：设置/获取树的最大深度，默认值为INT_MAX； (3)、setMinSampleCount/getMinSampleCount函数：设置/获取最小训练样本数，如果节点中的样本数小于此参数，则不会拆分该节点。默认值为10； (4)、setCVFolds/getCVFolds函数：设置/获取CVFolds值，如果此值大于1，算法使用K-fold交叉验证程序修剪构建的决策树，其中K等于CVFolds。 默认值为10。 (5)、setTruncatePrunedTree/getTruncatedTree函数：设置/获取是否进行剪枝后移除操作，默认值为true； (6)、setRegressionAccuracy/getRegressionAccuracy函数：设置/获取回归时用于终止的标准，默认值为0.01； (7)、setPriors/getPriors函数：设置/获取先验概率数值，用于调整决策树的偏好，默认值为空的Mat； (8)、getRoots函数：获取根节点索引； (9)、getNodes函数：获取所有节点索引； (10)、getSplits函数：获取所有拆分索引； (11)、getSubsets函数：获取分类拆分的所有bitsets； (12)、load函数：load已序列化的model文件。 static Ptr&lt;DTrees&gt; cv::ml::DTrees::load (const String &amp; filepath,const String &amp; nodeName = String() ) 使用方法与KNN基本一致： cv::Ptr&lt;cv::ml::DTrees&gt; dtree = cv::ml::DTrees::create(); dtree-&gt;setMaxCategories(10);//设置分类数为10,0-9 dtree-&gt;setMaxDepth(10); dtree-&gt;setMinSampleCount(10); dtree-&gt;setCVFolds(0); dtree-&gt;setUseSurrogates(false); dtree-&gt;setUse1SERule(false); dtree-&gt;setTruncatePrunedTree(false); dtree-&gt;setRegressionAccuracy(0); dtree-&gt;setPriors(cv::Mat()); dtree-&gt;train(train_data, cv::ml::ROW_SAMPLE, train_labels); //在这里我们没有使用Ptr&lt;TrainData&gt;创建训练对象，而是直接将“train_data, cv::ml::ROW_SAMPLE, train_labels”送入train函数，这两种方式都是可行的 const std::string save_file{ &quot;E:/GitCode/NN_Test/data/decision_tree_model.xml&quot; }; // .xml, .yaml, .jsons dtree-&gt;save(save_file); 3、逻辑回归 &nbsp;逻辑回归(logistic regression)的实现，即cv::ml::LogisticRegression类，类的声明在include/opencv2/ml.hpp文件中，实现在modules/ml/src/lr.cpp文件中,它既支持两分类，也支持多分类，其中： (2)、setLearningRate/getLearningRate函数用来设置获取学习率； (3)、setIterations/getIterations函数用来设置/获取迭代次数； (4)、setRegularization函数用来设置采用哪种正则化方法，目前支持两种L1 norm和L2 norm，正则化方法主要用来防止过拟合； (5)、setTrainMethod函数用来设置采用哪种训练方法，目前支持两种Batch和Mini-Batch，使用Mini-Batch方法时，将MiniBatchSize设置一个正整数。 (6)、setMiniBatchSize函数用来设置在Mini-Batch梯度下降训练方法中每一个step采集的训练样本数，getMiniBatchSize函数用来获取每一个step采集的训练样本数； (7)、setTermCriteria/getTermCriteria函数用来设置/获取终止训练的条件，包括迭代次数和期望的精度; (8)、get_learnt_thetas函数用来获取训练参数； (9)、train函数(使用基类StatModel中的)进行训练； (10)、predict函数用于预测； (11)、save函数(使用基类Algorithm中的)保存已训练好的model，支持xml,yaml,json格式; (12)、load函数用来load已训练好的model； &nbsp; cv::Ptr&lt;cv::ml::LogisticRegression&gt; lr = cv::ml::LogisticRegression::create(); lr-&gt;setLearningRate(0.00001); lr-&gt;setIterations(100); lr-&gt;setRegularization(cv::ml::LogisticRegression::REG_DISABLE); lr-&gt;setTrainMethod(cv::ml::LogisticRegression::MINI_BATCH); lr-&gt;setMiniBatchSize(1); lr-&gt;train(data, cv::ml::ROW_SAMPLE, labels); const std::string save_file{ &quot;E:/GitCode/NN_Test/data/logistic_regression_model.xml&quot; }; // .xml, .yaml, .jsons lr-&gt;save(save_file); cv::Ptr&lt;cv::ml::LogisticRegression&gt; lr = cv::ml::LogisticRegression::load(model_file); lr-&gt;predict(data, result); 转自：https://blog.csdn.net/fengbingchun/article/details/78221693 4、SVM 支持向量机(Support Vector Machines)的实现，即cv::ml::SVM类，它既支持两分类，也支持多分类，还支持回归等，OpenCV中SVM的实现源自libsvm库。 (3)、setType/getType函数：设置/获取SVM公式类型，包括C_SVC、NU_SVC、ONE_CLASS、EPS_SVR、NU_SVR，用于指定分类、回归等，默认为C_SVC； C_SVC:C-支持向量分类。 n级分类（n≥2），允许类别的不完美分离与罚分乘数C的异常值。 NU_SVC:ν-支持向量分类。 n级分类，可能存在不完美的分离。 使用参数ν（在0..1范围内，值越大，决策边界越平滑）代替C. ONE_CLASS:分布估计（一类SVM）。 所有训练数据都来自同一个类，SVM构建一个边界，将类与特征空间的其余部分分开。 EPS_SVR:ε-支持向量回归。 来自训练集和拟合超平面的特征向量之间的距离必须小于p。 对于异常值，使用罚分乘数C. NU_SVR:ν-支持向量回归。 用ν代替p。&nbsp; (4)、setGamma/getGamma函数：设置/获取核函数的γ参数，默认值为1； (5)、setCoef0/getCoef0函数：设置/获取核函数的coef0参数，默认值为0； (6)、setDegree/getDegree函数：设置/获取多项式核函数的degreee参数（阶数），默认值为0； (7)、setC/getC函数：设置/获取SVM优化问题的C参数，默认值为0； (8)、setNu/getNu函数：设置/获取SVM优化问题的υ参数，默认值为0； (9)、setP/getP函数：设置/获取SVM优化问题的ε参数，默认值为0； (10)、setClassWeights/getClassWeights函数：应用在C_SVC,设置/获取weights，默认值是空cv::Mat； (11)、setTermCriteria/getTermCriteria函数：设置/获取SVM训练时迭代终止条件，默认值是cv::TermCriteria(cv::TermCriteria::MAX_ITER + TermCriteria::EPS,1000, FLT_EPSILON)； (12)、setKernel/getKernelType函数：设置/获取SVM核函数类型，包括CUSTOM、LINEAR、POLY、RBF、SIGMOID、CHI2、INTER，默认值为RBF； (13)、setCustomKernel函数：初始化CUSTOM核函数； (14)、trainAuto函数：用最优参数训练SVM；会相当耗时。 virtual bool cv::ml::SVM::trainAuto ( const Ptr&lt; TrainData &gt; &amp; data, int kFold = 10, ParamGrid Cgrid = getDefaultGrid(C), ParamGrid gammaGrid = getDefaultGrid(GAMMA), ParamGrid pGrid = getDefaultGrid(P), ParamGrid nuGrid = getDefaultGrid(NU), ParamGrid coeffGrid = getDefaultGrid(COEF), ParamGrid degreeGrid = getDefaultGrid(DEGREE), bool balanced = false ) (15)、getSupportVectors/getUncompressedSupportVectors函数：获取所有的支持向量； (16)、getDecisionFunction函数：决策函数； (17)、getDefaultGrid/getDefaultGridPtr函数：生成SVM参数网格； static Ptr&lt;ParamGrid&gt; cv::ml::SVM::getDefaultGridPtr(int param_id) &nbsp;param_id:&nbsp;&nbsp;C&nbsp;,&nbsp;&nbsp;&nbsp;GAMMA&nbsp;,&nbsp;&nbsp;&nbsp;P&nbsp;,&nbsp;&nbsp;&nbsp;NU,&nbsp;&nbsp;&nbsp;&nbsp;COEF&nbsp;,&nbsp;&nbsp;&nbsp;DEGREE&nbsp; (19)、train/predict函数：用于训练/预测，均使用基类StatModel中的。(18)、save/load函数：保存/载入已训练好的model，支持xml,yaml,json格式； cv::Ptr&lt;cv::ml::SVM&gt; svm = cv::ml::SVM::create(); svm-&gt;setType(cv::ml::SVM::C_SVC); svm-&gt;setKernel(cv::ml::SVM::LINEAR); svm-&gt;setTermCriteria(cv::TermCriteria(cv::TermCriteria::MAX_ITER, 100, 1e-6)); CHECK(svm-&gt;train(trainingDataMat, cv::ml::ROW_SAMPLE, labelsMat)); const std::string save_file{ &quot;E:/GitCode/NN_Test/data/svm_model.xml&quot; }; // .xml, .yaml, .jsons svm-&gt;save(save_file); 5、SVMSGD 随机梯度下降SVM分类器。 参数w更新公式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; ：是步骤t 中决策函数的权重向量 w γ(t) :是迭代t时,模型参数的步长,每次迭代步长减小公式：&nbsp; Qi ：是总数为i个样本的SVM的目标函数，该样本在算法的每个步骤上随机选择。 ASGD是平均随机梯度下降SVM分类器。 ASGD分类器通过公式对算法的每个步骤的权重向量进行平均： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 分类器具有以下参数： setMarginType()/getMarginType()：边界类型分为软边界和硬边界，SOFT_MARGIN，HARD_MARGIN setSvmsgdType()/getSvmsgdType():SGD和ASGD SGD&nbsp; Stochastic Gradient Descent. ASGD&nbsp; Average Stochastic Gradient Descent. 边界正则化参数（λ）：正则化参数负责每一步的权重减少以及对异常值的限制强度（参数越小，异常值不容易被忽略）。 SGD模型的推荐值为0.0001，ASGD模型的推荐值为0.00001。 初始步长（γ0）：初始步长参数是步长γ（t）的初始值。 逐步降低功率（c）：是γ（t）的功率参数，通过上述公式减小。 SGD模型的推荐值为1，ASGD模型的推荐值为0.75。 终止标准：终止标准可以是TermCriteria :: COUNT，TermCriteria :: EPS或TermCriteria :: COUNT + TermCriteria :: EPS。 // Create empty object cv::Ptr&lt;SVMSGD&gt; svmsgd = SVMSGD::create(); // Train the Stochastic Gradient Descent SVM svmsgd-&gt;train(trainData); // Predict labels for the new samples svmsgd-&gt;predict(samples, responses); 6、ANN_MLP 人工神经网络——多层感知器。与许多其他模型毫升构造和训练,在模型这些步骤是分开的。首先,创建一个与指定的网络拓扑结构使用非默认的构造函数或方法ANN_MLP::创建。所有的权重都设置为0。然后,网络训练使用一组输入和输出向量。训练过程可以重复不止一次,也就是说,权重可以基于新的训练数据，进行调整。 1、setLayerSizes()：整数向量，指定每层中神经元的数量，包括输入和输出层。 第一个元素指定输入图层中的元素数。 最后一个元素 - 输出图层中的元素数。 默认值为空Mat。 virtual void cv::ml::ANN_MLP::setLayerSizes(InputArray _layer_sizes) 2、setActivationFunction()：设置激活函数 virtual void cv::ml::ANN_MLP::setActivationFunction(int type,double param1 = 0,double param2 = 0 ) type： param1： activation function的第一个参数 α. Default value is 0. param2： activation function的第二个参数β. Default value is 0. 3、setTrainMethod()函数 virtual void cv::ml::ANN_MLP::setTrainMethod(int method,double param1 = 0,double param2 = 0 ) method：backprop:反向传播；rprop:弹性反向传播；anneal :模拟退火算法； Ptr&lt;cv&gt; ann = cv::ml::ANN_MLP::create(); ann-&gt;setLayerSizes(Mat_&lt;int&gt;(1,5) &lt;&lt; 5,2,2,2,5) ann-&gt;setTrainMethod(BACKPROP); ann-&gt;setActivationFunction(RELU); ann-&gt;train(....) &nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/04/16/728245.html","headline":"opencv机器学习ml模块简介","dateModified":"2019-04-16T00:00:00+08:00","datePublished":"2019-04-16T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/04/16/728245.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>opencv机器学习ml模块简介</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h3>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 本文不涉原理，只介绍如何使用！</h3> 
  <p>ml模块为opencv的机器学习(machine learning，ml)代码库，包含各种机器学习算法：</p> 
  <p>0, class CvStatModel ; class CvMLData; struct CvParamGrid;</p> 
  <p>1，Normal Bayes Classifier（贝叶斯分类）；</p> 
  <p>2，K-Nearest Neighbour Classifier（K-邻近算法）；</p> 
  <p>3，SVM,support vector machine(支持向量机)；</p> 
  <p>4，Expectation - Maximization （EM算法）；</p> 
  <p>5，Decision Tree（决策树）；</p> 
  <p>6，Random Trees Classifier（随机森林算法）；</p> 
  <p>7， Boosted tree classifier （Boost树算法）；</p> 
  <p>9，Stochastic Gradient Descent&nbsp;<a href="https://docs.opencv.org/3.4.3/d1/d2d/classcv_1_1ml_1_1SVM.html" rel="nofollow">SVM</a>&nbsp;classifier；</p> 
  <p>10，ANN,Artificial Neural Networks(人工神经网络)；</p> 
  <h3>ml模块中有一个<a href="https://docs.opencv.org/3.4.3/dc/d32/classcv_1_1ml_1_1TrainData.html" rel="nofollow">cv::ml::TrainData</a>类，用于生成训练数据</h3> 
  <p>create函数，从内存数组创建训练数据。：</p> 
  <pre class="has">
<code class="language-cpp">static Ptr&lt;TrainData&gt; cv::ml::TrainData::create	(InputArray samples,
int layout,
InputArray 	responses,
InputArray 	varIdx = noArray(),
InputArray 	sampleIdx = noArray(),
InputArray 	sampleWeights = noArray(),
InputArray 	varType = noArray() 
)		</code></pre> 
  <p>参数<br> sample：样本矩阵样本。它应该具有<strong>CV_32F类型，原始数据一般是uchar型，所以要进行类型转化</strong>。<br> layout：有两种取值</p> 
  <ol>
   <li>ROW_SAMPLE&nbsp;&nbsp;&nbsp;一行为一个样本</li> 
   <li>COL_SAMPLE&nbsp;一列为一个样本</li> 
  </ol>
  <p>response：响应矩阵。如果响应是标量，则将它们存储为单行或单列。 矩阵应该具有类型CV_32F或CV_32S（在前一种情况下，响应在默认情况下被视为有序;在后一种情况下 - 作为分类）<br> varIdx：vector，指定用于训练的<strong>变量</strong>。 它可以是包含基于0的变量索引的整数向量（CV_32S）或包含活动变量掩码的字节向量（CV_8U）。<br> sampleIdx：向量，指定用于训练的<strong>样本</strong>。 它可以是包含基于0的样本索引的整数向量（CV_32S）或包含训练样本掩码的字节向量（CV_8U）。<br> sampleWeights：权重向量，每个样本都有权重。 它应该具有CV_32F类型。</p> 
  <h1>1、KNN</h1> 
  <p>&nbsp; &nbsp; &nbsp; OpenCV 3.4.3中给出了K-最近邻(KNN)算法的实现，即cv::ml::Knearest类，此类的声明在include/opecv2/ml.hpp文件中，实现在modules/ml/src/knearest.cpp文件中。Knearest类：继承自cv::ml::StateModel，而StateModel又继承自cv::Algorithm。</p> 
  <pre class="has">
<code class="language-cpp">static Ptr&lt;KNearest&gt; cv::ml::KNearest::create ()	</code></pre> 
  <p>create函数：为静态成员函数，new一个KNearestImpl，用来创建一个KNearest对象，如下：</p> 
  <pre class="has">
<code class="language-cpp">cv::Ptr&lt;cv::ml::KNearest&gt; knn = cv::ml::KNearest::create();</code></pre> 
  <p>setDefaultK/getDefaultK函数：在预测时，设置/获取的K值；</p> 
  <p>setIsClassifier/getIsClassifier函数：设置/获取应用KNN是进行分类还是回归；</p> 
  <p>setEmax/getEmax函数：在使用KDTree算法时，设置/获取Emax参数值；</p> 
  <p>setAlgorithmType/getAlgorithmType函数：设置/获取KNN算法类型，目前支持两种：brute_force和KDTree；</p> 
  <p>findNearest函数：根据输入预测分类/回归结果</p> 
  <pre class="has">
<code class="language-cpp">virtual float cv::ml::KNearest::findNearest	(
InputArray 	samples,
int k,
OutputArray 	results,
OutputArray 	neighborResponses = noArray(),
OutputArray 	dist = noArray() 
)		const</code></pre> 
  <p>参数说明：</p> 
  <ol>
   <li>samples:为 样本数*特征数 的浮点二维矩阵；</li> 
   <li>k：为寻找最近点的个数；</li> 
   <li>results:预测结果；</li> 
   <li>neighborResponse:为样本数*k的二维矩阵,代表每个样本的K个近邻的输出值；</li> 
   <li>dist：为样本数*k的二维矩阵，代表每个样本的K个近邻的距离。</li> 
  </ol>
  <p>实例：opencv提供了一张手写数字图片给我们，</p> 
  <p><img alt="" class="has" height="372" src="https://images2015.cnblogs.com/blog/140867/201512/140867-20151209190543371-695937703.png" width="808"></p> 
  <p>图片大小为1000*2000,有0-9的10个数字，每5行为一个数字，总共50行，共有5000个手写数字。我们首先要做的，就是把这5000个手写数字，一个个截取出来，每个数字块大小为20*20。直接将每个小图块进行序列化，因此最终得到一个5000*400的特征矩阵。样本数为5000，维度为400维。取其中前3000个样本进行训练。</p> 
  <p><span style="color:#f33b45;"><strong>注意：截取的时候，是按列截取。不然取前3000个样本进行训练就会出现后几个数字训练不到。</strong></span></p> 
  <pre class="has">
<code class="language-cpp">#include "stdafx.h"
#include "opencv2\opencv.hpp"
#include &lt;iostream&gt;
using namespace std;
using namespace cv;
using namespace cv::ml;

int main()
{
    Mat img = imread("E:/opencv/opencv/sources/samples/data/digits.png");
    Mat gray;
    cvtColor(img, gray, CV_BGR2GRAY);
    int b = 20;
    int m = gray.rows / b;   //原图为1000*2000
    int n = gray.cols / b;   //裁剪为5000个20*20的小图块
    Mat data,labels;   //特征矩阵
    for (int i = 0; i &lt; n; i++)
    {
        int offsetCol = i*b; //列上的偏移量
        for (int j = 0; j &lt; m; j++)
        {
            int offsetRow = j*b;  //行上的偏移量
            //截取20*20的小块
            Mat tmp;
            gray(Range(offsetRow, offsetRow + b), Range(offsetCol, offsetCol + b)).copyTo(tmp);
            data.push_back(tmp.reshape(0,1));  //序列化后放入特征矩阵
            labels.push_back((int)j / 5);  //对应的标注
        }

    }
    data.convertTo(data, CV_32F); //uchar型转换为cv_32f
    int samplesNum = data.rows;
    int trainNum = 3000;
    Mat trainData, trainLabels;
    trainData = data(Range(0, trainNum), Range::all());   //前3000个样本为训练数据
    trainLabels = labels(Range(0, trainNum), Range::all());

    //使用KNN算法
    int K = 5;
    Ptr&lt;TrainData&gt; tData = TrainData::create(trainData, ROW_SAMPLE, trainLabels);//降训练数据封装成一个TrainData对象，送入train函数
    Ptr&lt;KNearest&gt; model = KNearest::create();
    model-&gt;setDefaultK(K);
    model-&gt;setIsClassifier(true);
    model-&gt;train(tData);

    //预测分类
    double train_hr = 0, test_hr = 0;
    Mat response;
    // compute prediction error on train and test data
    for (int i = 0; i &lt; samplesNum; i++)
    {
        Mat sample = data.row(i);
        float r = model-&gt;predict(sample);   //对所有行进行预测
        //预测结果与原结果相比，相等为1，不等为0
        r = std::abs(r - labels.at&lt;int&gt;(i)) &lt;= FLT_EPSILON ? 1.f : 0.f;          

        if (i &lt; trainNum)
            train_hr += r;  //累积正确数
        else
            test_hr += r;
    }

    test_hr /= samplesNum - trainNum;
    train_hr = trainNum &gt; 0 ? train_hr / trainNum : 1.;

    printf("accuracy: train = %.1f%%, test = %.1f%%\n",
        train_hr*100., test_hr*100.);
    waitKey(0);
    return 0;
}</code></pre> 
  <p>代码来自：<a href="https://www.cnblogs.com/denny402/p/5033898.html" rel="nofollow">https://www.cnblogs.com/denny402/p/5033898.html</a></p> 
  <h1>2、DTree</h1> 
  <p>&nbsp;create函数用来创建一个DTrees对象；</p> 
  <pre class="has">
<code class="language-cpp">static Ptr&lt;DTrees&gt; cv::ml::DTrees::create()</code></pre> 
  <p>(1)、setMaxCategories/getMaxCategories函数：设置/获取最大的类别数，默认值为10；</p> 
  <p>(2)、setMaxDepth/getMaxDepth函数：设置/获取树的最大深度，默认值为INT_MAX；</p> 
  <p>(3)、setMinSampleCount/getMinSampleCount函数：设置/获取最小训练样本数，如果节点中的样本数小于此参数，则不会拆分该节点。默认值为10；</p> 
  <p>(4)、setCVFolds/getCVFolds函数：设置/获取CVFolds值，如果此值大于1，算法使用K-fold交叉验证程序修剪构建的决策树，其中K等于CVFolds。 默认值为10。</p> 
  <p>(5)、setTruncatePrunedTree/getTruncatedTree函数：设置/获取是否进行剪枝后移除操作，默认值为true；</p> 
  <p>(6)、setRegressionAccuracy/getRegressionAccuracy函数：设置/获取回归时用于终止的标准，默认值为0.01；</p> 
  <p>(7)、setPriors/getPriors函数：设置/获取先验概率数值，用于调整决策树的偏好，默认值为空的Mat；</p> 
  <p>(8)、getRoots函数：获取根节点索引；</p> 
  <p>(9)、getNodes函数：获取所有节点索引；</p> 
  <p>(10)、getSplits函数：获取所有拆分索引；</p> 
  <p>(11)、getSubsets函数：获取分类拆分的所有bitsets；</p> 
  <p>(12)、load函数：load已序列化的model文件。</p> 
  <pre class="has">
<code class="language-cpp">static Ptr&lt;DTrees&gt; cv::ml::DTrees::load	(const String &amp; filepath,const String &amp; nodeName = String() )	</code></pre> 
  <p>使用方法与KNN基本一致：</p> 
  <pre class="has">
<code class="language-cpp">cv::Ptr&lt;cv::ml::DTrees&gt; dtree = cv::ml::DTrees::create();
	dtree-&gt;setMaxCategories(10);//设置分类数为10,0-9
	dtree-&gt;setMaxDepth(10);
	dtree-&gt;setMinSampleCount(10);
	dtree-&gt;setCVFolds(0);
	dtree-&gt;setUseSurrogates(false);
	dtree-&gt;setUse1SERule(false);
	dtree-&gt;setTruncatePrunedTree(false);
	dtree-&gt;setRegressionAccuracy(0);
	dtree-&gt;setPriors(cv::Mat()); 
	dtree-&gt;train(train_data, cv::ml::ROW_SAMPLE, train_labels);
//在这里我们没有使用Ptr&lt;TrainData&gt;创建训练对象，而是直接将“train_data, cv::ml::ROW_SAMPLE, train_labels”送入train函数，这两种方式都是可行的

const std::string save_file{ "E:/GitCode/NN_Test/data/decision_tree_model.xml" }; // .xml, .yaml, .jsons
	dtree-&gt;save(save_file);</code></pre> 
  <h1>3、逻辑回归</h1> 
  <p>&nbsp;逻辑回归(logistic regression)的实现，即cv::ml::LogisticRegression类，类的声明在include/opencv2/ml.hpp文件中，实现在modules/ml/src/lr.cpp文件中,它<span style="color:#f33b45;"><strong>既支持两分类，也支持多分类</strong></span>，其中：</p> 
  <p>(2)、setLearningRate/getLearningRate函数用来设置获取学习率；</p> 
  <p>(3)、setIterations/getIterations函数用来设置/获取迭代次数；</p> 
  <p>(4)、setRegularization函数用来设置采用哪种正则化方法，目前支持两种L1 norm和L2 norm，正则化方法主要用来防止过拟合；</p> 
  <p>(5)、setTrainMethod函数用来设置采用哪种训练方法，目前支持两种Batch和Mini-Batch，使用Mini-Batch方法时，将MiniBatchSize设置一个正整数。</p> 
  <p>(6)、setMiniBatchSize函数用来设置在Mini-Batch梯度下降训练方法中每一个step采集的训练样本数，getMiniBatchSize函数用来获取每一个step采集的训练样本数；</p> 
  <p>(7)、setTermCriteria/getTermCriteria函数用来设置/获取终止训练的条件，包括迭代次数和期望的精度;</p> 
  <p>(8)、get_learnt_thetas函数用来获取训练参数；</p> 
  <p>(9)、train函数(使用基类StatModel中的)进行训练；</p> 
  <p>(10)、predict函数用于预测；</p> 
  <p>(11)、save函数(使用基类Algorithm中的)保存已训练好的model，支持xml,yaml,json格式;</p> 
  <p>(12)、load函数用来load已训练好的model；<br> &nbsp;</p> 
  <pre class="has">
<code class="language-cpp">cv::Ptr&lt;cv::ml::LogisticRegression&gt; lr = cv::ml::LogisticRegression::create();
	lr-&gt;setLearningRate(0.00001);
	lr-&gt;setIterations(100);
	lr-&gt;setRegularization(cv::ml::LogisticRegression::REG_DISABLE);
	lr-&gt;setTrainMethod(cv::ml::LogisticRegression::MINI_BATCH);
	lr-&gt;setMiniBatchSize(1);
 
	lr-&gt;train(data, cv::ml::ROW_SAMPLE, labels);
    const std::string save_file{ "E:/GitCode/NN_Test/data/logistic_regression_model.xml" }; // .xml, .yaml, .jsons
	lr-&gt;save(save_file);

cv::Ptr&lt;cv::ml::LogisticRegression&gt; lr = cv::ml::LogisticRegression::load(model_file);
     lr-&gt;predict(data, result);</code></pre> 
  <p>转自：<a href="https://blog.csdn.net/fengbingchun/article/details/78221693" rel="nofollow">https://blog.csdn.net/fengbingchun/article/details/78221693</a></p> 
  <h1>4、SVM</h1> 
  <p>支持向量机(Support Vector Machines)的实现，即cv::ml::SVM类，它既支持两分类，也支持多分类，还支持回归等，OpenCV中SVM的实现源自libsvm库。</p> 
  <p>(3)、setType/getType函数：设置/获取SVM公式类型，包括C_SVC、NU_SVC、ONE_CLASS、EPS_SVR、NU_SVR，用于指定分类、回归等，默认为C_SVC；</p> 
  <ol>
   <li>C_SVC:C-支持向量分类。 n级分类（n≥2），允许类别的不完美分离与罚分乘数C的异常值。</li> 
   <li>NU_SVC:ν-支持向量分类。 n级分类，可能存在不完美的分离。 使用参数ν（在0..1范围内，值越大，决策边界越平滑）代替C.</li> 
   <li>ONE_CLASS:分布估计（一类SVM）。 所有训练数据都来自同一个类，SVM构建一个边界，将类与特征空间的其余部分分开。</li> 
   <li>EPS_SVR:ε-支持向量回归。 来自训练集和拟合超平面的特征向量之间的距离必须小于p。 对于异常值，使用罚分乘数C.</li> 
   <li>NU_SVR:ν-支持向量回归。 用ν代替p。&nbsp;</li> 
  </ol>
  <p>(4)、setGamma/getGamma函数：设置/获取<strong>核函数</strong>的γ参数，默认值为1；</p> 
  <p>(5)、setCoef0/getCoef0函数：设置/获取核函数的coef0参数，默认值为0；</p> 
  <p>(6)、setDegree/getDegree函数：设置/获取多项式核函数的degreee参数（阶数），默认值为0；</p> 
  <p>(7)、setC/getC函数：设置/获取SVM优化问题的C参数，默认值为0；</p> 
  <p>(8)、setNu/getNu函数：设置/获取SVM优化问题的υ参数，默认值为0；</p> 
  <p>(9)、setP/getP函数：设置/获取SVM优化问题的ε参数，默认值为0；</p> 
  <p>(10)、setClassWeights/getClassWeights函数：应用在C_SVC,设置/获取weights，默认值是空cv::Mat；</p> 
  <p>(11)、setTermCriteria/getTermCriteria函数：设置/获取SVM训练时迭代终止条件，默认值是cv::TermCriteria(cv::TermCriteria::MAX_ITER + TermCriteria::EPS,1000, FLT_EPSILON)；</p> 
  <p>(12)、setKernel/getKernelType函数：设置/获取SVM核函数类型，包括CUSTOM、LINEAR、POLY、RBF、SIGMOID、CHI2、INTER，默认值为RBF；</p> 
  <p><img alt="" class="has" height="268" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190416145321984.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwODE1MjM3,size_16,color_FFFFFF,t_70" width="954"></p> 
  <p>(13)、setCustomKernel函数：初始化CUSTOM核函数；</p> 
  <p>(14)、trainAuto函数：用最优参数训练SVM；会相当耗时。</p> 
  <pre class="has">
<code class="language-cpp">virtual bool cv::ml::SVM::trainAuto	(	const Ptr&lt; TrainData &gt; &amp; 	data,
int kFold = 10,
ParamGrid 	Cgrid = getDefaultGrid(C),
ParamGrid 	gammaGrid = getDefaultGrid(GAMMA),
ParamGrid 	pGrid = getDefaultGrid(P),
ParamGrid 	nuGrid = getDefaultGrid(NU),
ParamGrid 	coeffGrid = getDefaultGrid(COEF),
ParamGrid 	degreeGrid = getDefaultGrid(DEGREE),
bool 	balanced = false 
)	</code></pre> 
  <p>(15)、getSupportVectors/getUncompressedSupportVectors函数：获取所有的支持向量；</p> 
  <p>(16)、getDecisionFunction函数：决策函数；</p> 
  <p>(17)、getDefaultGrid/getDefaultGridPtr函数：生成SVM参数网格；</p> 
  <pre class="has">
<code class="language-cpp">static Ptr&lt;ParamGrid&gt; cv::ml::SVM::getDefaultGridPtr(int param_id)	</code></pre> 
  <p>&nbsp;param_id:&nbsp;&nbsp;<a href="https://docs.opencv.org/3.4.3/d1/d2d/classcv_1_1ml_1_1SVM.html#a32d2e8d21aaa4f58cdf9c27c102becf3a8eafc49ef685613b37e1b96351fd2bd1" rel="nofollow">C</a>&nbsp;,&nbsp;&nbsp;&nbsp;<a href="https://docs.opencv.org/3.4.3/d1/d2d/classcv_1_1ml_1_1SVM.html#a32d2e8d21aaa4f58cdf9c27c102becf3a9b81805a0cd06dc59c354b0ad6fc9e9a" rel="nofollow">GAMMA</a>&nbsp;,&nbsp;&nbsp;&nbsp;<a href="https://docs.opencv.org/3.4.3/d1/d2d/classcv_1_1ml_1_1SVM.html#a32d2e8d21aaa4f58cdf9c27c102becf3ae14aa4668daf05a4ea6918b10806acd5" rel="nofollow">P</a>&nbsp;,&nbsp;&nbsp;&nbsp;<a href="https://docs.opencv.org/3.4.3/d1/d2d/classcv_1_1ml_1_1SVM.html#a32d2e8d21aaa4f58cdf9c27c102becf3ae0c1409f55f0158101fcc5e07f095605" rel="nofollow">NU</a>,&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://docs.opencv.org/3.4.3/d1/d2d/classcv_1_1ml_1_1SVM.html#a32d2e8d21aaa4f58cdf9c27c102becf3ae7112825b482d70cf5f04bc571f86e57" rel="nofollow">COEF</a>&nbsp;,&nbsp;&nbsp;&nbsp;<a href="https://docs.opencv.org/3.4.3/d1/d2d/classcv_1_1ml_1_1SVM.html#a32d2e8d21aaa4f58cdf9c27c102becf3a61a897bf6519f4be834ce379a1543869" rel="nofollow">DEGREE</a>&nbsp;<br> (19)、train/predict函数：用于训练/预测，均使用基类StatModel中的。(18)、save/load函数：保存/载入已训练好的model，支持xml,yaml,json格式；</p> 
  <pre class="has">
<code class="language-cpp">cv::Ptr&lt;cv::ml::SVM&gt; svm = cv::ml::SVM::create();
	svm-&gt;setType(cv::ml::SVM::C_SVC);
	svm-&gt;setKernel(cv::ml::SVM::LINEAR);
	svm-&gt;setTermCriteria(cv::TermCriteria(cv::TermCriteria::MAX_ITER, 100, 1e-6));
 
	CHECK(svm-&gt;train(trainingDataMat, cv::ml::ROW_SAMPLE, labelsMat));
 
	const std::string save_file{ "E:/GitCode/NN_Test/data/svm_model.xml" }; // .xml, .yaml, .jsons
	svm-&gt;save(save_file);
</code></pre> 
  <h1>5、SVMSGD 随机梯度下降SVM分类器。</h1> 
  <p><span style="color:#86ca5e;">参数w更新公式为：</span></p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img alt="" class="has" height="55" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190416151228411.png" width="217"></p> 
  <p><img alt="w_{t}" class="mathcode" src="https://private.codecogs.com/gif.latex?w_%7Bt%7D">：是步骤t 中决策函数的权重向量 w</p> 
  <p>γ(t) :是迭代t时,模型参数的步长,每次迭代步长减小公式：&nbsp;<img alt="" class="has" height="28" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190416151457563.png" width="163"></p> 
  <p>Qi ：是总数为i个样本的SVM的目标函数，该样本在算法的每个步骤上随机选择。</p> 
  <p><span style="color:#86ca5e;">ASGD是平均随机梯度下降SVM分类器。 ASGD分类器通过公式对算法的每个步骤的权重向量进行平均：</span></p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img alt="" class="has" height="28" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190416151726702.png" width="191"></p> 
  <h3>分类器具有以下参数：</h3> 
  <p>setMarginType()/getMarginType()：边界类型分为软边界和硬边界，SOFT_MARGIN，HARD_MARGIN</p> 
  <p>setSvmsgdType()/getSvmsgdType():SGD和ASGD</p> 
  <table>
   <tbody>
    <tr>
     <td>SGD&nbsp;</td> 
     <td> <p>Stochastic Gradient Descent.</p> </td> 
    </tr>
    <tr>
     <td><a id="ab028695cc8ec1491888d8d03f80bc8c2abb41d794ce113aa2123f45120e8354af"></a>ASGD&nbsp;</td> 
     <td> <p>Average Stochastic Gradient Descent.</p> </td> 
    </tr>
   </tbody>
  </table>
  <p>边界正则化参数（λ）：正则化参数负责每一步的权重减少以及对异常值的限制强度（参数越小，异常值不容易被忽略）。 SGD模型的推荐值为0.0001，ASGD模型的推荐值为0.00001。</p> 
  <p>初始步长（γ0）：初始步长参数是步长γ（t）的初始值。</p> 
  <p>逐步降低功率（c）：是γ（t）的功率参数，通过上述公式减小。 SGD模型的推荐值为1，ASGD模型的推荐值为0.75。</p> 
  <p>终止标准：终止标准可以是TermCriteria :: COUNT，TermCriteria :: EPS或TermCriteria :: COUNT + TermCriteria :: EPS。</p> 
  <pre class="has">
<code class="language-cpp">// Create empty object
cv::Ptr&lt;SVMSGD&gt; svmsgd = SVMSGD::create();
// Train the Stochastic Gradient Descent SVM
svmsgd-&gt;train(trainData);
// Predict labels for the new samples
svmsgd-&gt;predict(samples, responses);</code></pre> 
  <h1>6、ANN_MLP</h1> 
  <p>人工神经网络——多层感知器。与许多其他模型毫升构造和训练,在模型这些步骤是分开的。首先,创建一个与指定的网络拓扑结构使用非默认的构造函数或方法ANN_MLP::创建。所有的权重都设置为0。然后,网络训练使用一组输入和输出向量。训练过程可以重复不止一次,也就是说,权重可以基于新的训练数据，进行调整。</p> 
  <p><span style="color:#e579b6;">1、setLayerSizes()：</span>整数向量，指定每层中神经元的数量，包括输入和输出层。 第一个元素指定输入图层中的元素数。 最后一个元素 - 输出图层中的元素数。 默认值为空Mat。</p> 
  <pre class="has">
<code class="language-cpp">virtual void cv::ml::ANN_MLP::setLayerSizes(InputArray _layer_sizes)	
</code></pre> 
  <p><span style="color:#e579b6;">2、setActivationFunction()：</span>设置激活函数</p> 
  <pre class="has">
<code class="language-cpp">virtual void cv::ml::ANN_MLP::setActivationFunction(int type,double param1 = 0,double param2 = 0 )	</code></pre> 
  <p>type：</p> 
  <p><img alt="" class="has" height="266" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190416153424209.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwODE1MjM3,size_16,color_FFFFFF,t_70" width="946"></p> 
  <p>param1： activation function的第一个参数 α. Default value is 0.<br> param2： activation function的第二个参数β. Default value is 0.</p> 
  <p><span style="color:#e579b6;">3、setTrainMethod()函数</span></p> 
  <pre class="has">
<code class="language-cpp">virtual void cv::ml::ANN_MLP::setTrainMethod(int method,double 	param1 = 0,double param2 = 0 )</code></pre> 
  <p>method：backprop:反向传播；rprop:弹性反向传播；anneal :模拟退火算法；</p> 
  <p><img alt="" class="has" height="88" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190416153750846.png" width="453"></p> 
  <pre class="has">
<code class="language-cpp">Ptr&lt;cv&gt; ann = cv::ml::ANN_MLP::create();
ann-&gt;setLayerSizes(Mat_&lt;int&gt;(1,5) &lt;&lt; 5,2,2,2,5)
ann-&gt;setTrainMethod(BACKPROP);
ann-&gt;setActivationFunction(RELU);

ann-&gt;train(....)
</code></pre> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
