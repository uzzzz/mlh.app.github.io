<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Sqoop导入mysql所有表到HDFS | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Sqoop导入mysql所有表到HDFS" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="1、sqoop-import-all-tables导入多表 [root@node1 sqoop-1.4.7]# bin/sqoop-import-all-tables --connect jdbc:mysql://node1:3306/esdb --username root --password 123456 --as-textfile --warehouse-dir /user/root Warning: /opt/sqoop-1.4.7/bin/../../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. 18/05/24 14:58:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 18/05/24 14:58:06 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 18/05/24 14:58:06 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 18/05/24 14:58:06 INFO tool.CodeGenTool: Beginning code generation 18/05/24 14:58:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1 18/05/24 14:58:06 ERROR manager.SqlManager: Error reading from database: java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@1817d444 is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries. java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@1817d444 is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries. &nbsp; &nbsp; at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:931) &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.checkForOutstandingStreamingData(MysqlIO.java:2518) &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1748) &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:1961) &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2537) &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2466) &nbsp; &nbsp; at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1383) &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.getMaxBytesPerChar(ConnectionImpl.java:2939) &nbsp; &nbsp; at com.mysql.jdbc.Field.getMaxBytesPerCharacter(Field.java:576) &nbsp; &nbsp; at com.mysql.jdbc.ResultSetMetaData.getPrecision(ResultSetMetaData.java:440) &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:305) &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260) &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246) &nbsp; &nbsp; at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327) &nbsp; &nbsp; at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1872) &nbsp; &nbsp; at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1671) &nbsp; &nbsp; at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106) &nbsp; &nbsp; at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501) &nbsp; &nbsp; at org.apache.sqoop.tool.ImportAllTablesTool.run(ImportAllTablesTool.java:110) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.run(Sqoop.java:147) &nbsp; &nbsp; at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.main(Sqoop.java:252) 18/05/24 14:58:06 ERROR tool.ImportAllTablesTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter [root@node1 sqoop-1.4.7]# 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 由于之前上传到Sqoop lib下的MySQL驱动程序有些低，更新到mysql-connector-java-5.1.32-bin.jar即可 [root@node1 ~]# ls /opt/sqoop-1.4.7/lib |grep mysql mysql-connector-java-5.1.32-bin.jar [root@node1 ~]# 1 2 3 2、再次执行 [root@node1 sqoop-1.4.7]# bin/sqoop-import-all-tables --connect jdbc:mysql://node1:3306/esdb --username root --password 123456 --as-textfile --warehouse-dir /user/root Warning: /opt/sqoop-1.4.7/bin/../../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. 18/05/24 15:03:33 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 18/05/24 15:03:33 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 18/05/24 15:03:34 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 18/05/24 15:03:34 INFO tool.CodeGenTool: Beginning code generation 18/05/24 15:03:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1 18/05/24 15:03:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1 18/05/24 15:03:34 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-2.7.5 Note: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/files.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details. 18/05/24 15:03:39 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/files.jar 18/05/24 15:03:39 WARN manager.MySQLManager: It looks like you are importing from mysql. 18/05/24 15:03:39 WARN manager.MySQLManager: This transfer can be faster! Use the --direct 18/05/24 15:03:39 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path. 18/05/24 15:03:39 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql) 18/05/24 15:03:39 INFO mapreduce.ImportJobBase: Beginning import of files 18/05/24 15:03:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 18/05/24 15:03:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 18/05/24 15:03:40 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2 18/05/24 15:03:43 INFO db.DBInputFormat: Using read commited transaction isolation 18/05/24 15:03:43 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `files` 18/05/24 15:03:43 INFO db.IntegerSplitter: Split size: 810; Num splits: 4 from: 1 to: 3244 18/05/24 15:03:43 INFO mapreduce.JobSubmitter: number of splits:4 18/05/24 15:03:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1526097883376_0013 18/05/24 15:03:44 INFO impl.YarnClientImpl: Submitted application application_1526097883376_0013 18/05/24 15:03:44 INFO mapreduce.Job: The url to track the job: http://bigdata03-test:8088/proxy/application_1526097883376_0013/ 18/05/24 15:03:44 INFO mapreduce.Job: Running job: job_1526097883376_0013 18/05/24 15:03:51 INFO mapreduce.Job: Job job_1526097883376_0013 running in uber mode : false 18/05/24 15:03:51 INFO mapreduce.Job: &nbsp;map 0% reduce 0% 18/05/24 15:03:57 INFO mapreduce.Job: &nbsp;map 25% reduce 0% 18/05/24 15:03:58 INFO mapreduce.Job: &nbsp;map 75% reduce 0% 18/05/24 15:03:59 INFO mapreduce.Job: &nbsp;map 100% reduce 0% 18/05/24 15:03:59 INFO mapreduce.Job: Job job_1526097883376_0013 completed successfully 18/05/24 15:03:59 INFO mapreduce.Job: Counters: 30 &nbsp; &nbsp; File System Counters &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes read=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes written=570260 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of write operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes read=412 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes written=3799556 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of read operations=16 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of write operations=8 &nbsp; &nbsp; Job Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Launched map tasks=4 &nbsp; &nbsp; &nbsp; &nbsp; Other local map tasks=4 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all maps in occupied slots (ms)=18756 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all reduces in occupied slots (ms)=0 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all map tasks (ms)=18756 &nbsp; &nbsp; &nbsp; &nbsp; Total vcore-milliseconds taken by all map tasks=18756 &nbsp; &nbsp; &nbsp; &nbsp; Total megabyte-milliseconds taken by all map tasks=19206144 &nbsp; &nbsp; Map-Reduce Framework &nbsp; &nbsp; &nbsp; &nbsp; Map input records=3244 &nbsp; &nbsp; &nbsp; &nbsp; Map output records=3244 &nbsp; &nbsp; &nbsp; &nbsp; Input split bytes=412 &nbsp; &nbsp; &nbsp; &nbsp; Spilled Records=0 &nbsp; &nbsp; &nbsp; &nbsp; Failed Shuffles=0 &nbsp; &nbsp; &nbsp; &nbsp; Merged Map outputs=0 &nbsp; &nbsp; &nbsp; &nbsp; GC time elapsed (ms)=279 &nbsp; &nbsp; &nbsp; &nbsp; CPU time spent (ms)=7090 &nbsp; &nbsp; &nbsp; &nbsp; Physical memory (bytes) snapshot=727486464 &nbsp; &nbsp; &nbsp; &nbsp; Virtual memory (bytes) snapshot=8496140288 &nbsp; &nbsp; &nbsp; &nbsp; Total committed heap usage (bytes)=443023360 &nbsp; &nbsp; File Input Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Read=0 &nbsp; &nbsp; File Output Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Written=3799556 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Transferred 3.6235 MB in 19.1194 seconds (194.0698 KB/sec) 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Retrieved 3244 records. 18/05/24 15:03:59 INFO tool.CodeGenTool: Beginning code generation 18/05/24 15:03:59 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `logs` AS t LIMIT 1 18/05/24 15:03:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-2.7.5 Note: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/logs.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details. 18/05/24 15:03:59 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/logs.jar 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Beginning import of logs 18/05/24 15:03:59 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2 18/05/24 15:04:01 INFO db.DBInputFormat: Using read commited transaction isolation 18/05/24 15:04:01 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `logs` 18/05/24 15:04:01 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 1 18/05/24 15:04:01 INFO mapreduce.JobSubmitter: number of splits:1 18/05/24 15:04:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1526097883376_0014 18/05/24 15:04:01 INFO impl.YarnClientImpl: Submitted application application_1526097883376_0014 18/05/24 15:04:01 INFO mapreduce.Job: The url to track the job: http://bigdata03-test:8088/proxy/application_1526097883376_0014/ 18/05/24 15:04:01 INFO mapreduce.Job: Running job: job_1526097883376_0014 18/05/24 15:04:10 INFO mapreduce.Job: Job job_1526097883376_0014 running in uber mode : false 18/05/24 15:04:10 INFO mapreduce.Job: &nbsp;map 0% reduce 0% 18/05/24 15:04:17 INFO mapreduce.Job: &nbsp;map 100% reduce 0% 18/05/24 15:04:17 INFO mapreduce.Job: Job job_1526097883376_0014 completed successfully 18/05/24 15:04:17 INFO mapreduce.Job: Counters: 30 &nbsp; &nbsp; File System Counters &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes read=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes written=142495 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of write operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes read=99 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes written=47 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of read operations=4 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of write operations=2 &nbsp; &nbsp; Job Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Launched map tasks=1 &nbsp; &nbsp; &nbsp; &nbsp; Other local map tasks=1 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all maps in occupied slots (ms)=4490 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all reduces in occupied slots (ms)=0 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all map tasks (ms)=4490 &nbsp; &nbsp; &nbsp; &nbsp; Total vcore-milliseconds taken by all map tasks=4490 &nbsp; &nbsp; &nbsp; &nbsp; Total megabyte-milliseconds taken by all map tasks=4597760 &nbsp; &nbsp; Map-Reduce Framework &nbsp; &nbsp; &nbsp; &nbsp; Map input records=1 &nbsp; &nbsp; &nbsp; &nbsp; Map output records=1 &nbsp; &nbsp; &nbsp; &nbsp; Input split bytes=99 &nbsp; &nbsp; &nbsp; &nbsp; Spilled Records=0 &nbsp; &nbsp; &nbsp; &nbsp; Failed Shuffles=0 &nbsp; &nbsp; &nbsp; &nbsp; Merged Map outputs=0 &nbsp; &nbsp; &nbsp; &nbsp; GC time elapsed (ms)=72 &nbsp; &nbsp; &nbsp; &nbsp; CPU time spent (ms)=1560 &nbsp; &nbsp; &nbsp; &nbsp; Physical memory (bytes) snapshot=179109888 &nbsp; &nbsp; &nbsp; &nbsp; Virtual memory (bytes) snapshot=2121519104 &nbsp; &nbsp; &nbsp; &nbsp; Total committed heap usage (bytes)=108003328 &nbsp; &nbsp; File Input Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Read=0 &nbsp; &nbsp; File Output Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Written=47 18/05/24 15:04:17 INFO mapreduce.ImportJobBase: Transferred 47 bytes in 17.8489 seconds (2.6332 bytes/sec) 18/05/24 15:04:17 INFO mapreduce.ImportJobBase: Retrieved 1 records. [root@node1 sqoop-1.4.7]#&nbsp; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 一共导入了2张表 [root@node1 ~]# hdfs dfs -ls /user/root Found 5 items drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:03 /user/root/files drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-04-23 14:05 /user/root/input drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:04 /user/root/logs drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-22 13:36 /user/root/users [root@node1 ~]# hdfs dfs -ls /user/root/files Found 5 items -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:03 /user/root/files/_SUCCESS -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 895593 2018-05-24 15:03 /user/root/files/part-m-00000 -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 912033 2018-05-24 15:03 /user/root/files/part-m-00001 -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp;1109032 2018-05-24 15:03 /user/root/files/part-m-00002 -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 882898 2018-05-24 15:03 /user/root/files/part-m-00003 [root@node1 ~]# ---------------------&nbsp; 作者：程裕强&nbsp; 来源：CSDN&nbsp; 原文：https://blog.csdn.net/chengyuqiang/article/details/80434840&nbsp; 版权声明：本文为博主原创文章，转载请附上博文链接！" />
<meta property="og:description" content="1、sqoop-import-all-tables导入多表 [root@node1 sqoop-1.4.7]# bin/sqoop-import-all-tables --connect jdbc:mysql://node1:3306/esdb --username root --password 123456 --as-textfile --warehouse-dir /user/root Warning: /opt/sqoop-1.4.7/bin/../../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. 18/05/24 14:58:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 18/05/24 14:58:06 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 18/05/24 14:58:06 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 18/05/24 14:58:06 INFO tool.CodeGenTool: Beginning code generation 18/05/24 14:58:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1 18/05/24 14:58:06 ERROR manager.SqlManager: Error reading from database: java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@1817d444 is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries. java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@1817d444 is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries. &nbsp; &nbsp; at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:931) &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.checkForOutstandingStreamingData(MysqlIO.java:2518) &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1748) &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:1961) &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2537) &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2466) &nbsp; &nbsp; at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1383) &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.getMaxBytesPerChar(ConnectionImpl.java:2939) &nbsp; &nbsp; at com.mysql.jdbc.Field.getMaxBytesPerCharacter(Field.java:576) &nbsp; &nbsp; at com.mysql.jdbc.ResultSetMetaData.getPrecision(ResultSetMetaData.java:440) &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:305) &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260) &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246) &nbsp; &nbsp; at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327) &nbsp; &nbsp; at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1872) &nbsp; &nbsp; at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1671) &nbsp; &nbsp; at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106) &nbsp; &nbsp; at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501) &nbsp; &nbsp; at org.apache.sqoop.tool.ImportAllTablesTool.run(ImportAllTablesTool.java:110) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.run(Sqoop.java:147) &nbsp; &nbsp; at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.main(Sqoop.java:252) 18/05/24 14:58:06 ERROR tool.ImportAllTablesTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter [root@node1 sqoop-1.4.7]# 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 由于之前上传到Sqoop lib下的MySQL驱动程序有些低，更新到mysql-connector-java-5.1.32-bin.jar即可 [root@node1 ~]# ls /opt/sqoop-1.4.7/lib |grep mysql mysql-connector-java-5.1.32-bin.jar [root@node1 ~]# 1 2 3 2、再次执行 [root@node1 sqoop-1.4.7]# bin/sqoop-import-all-tables --connect jdbc:mysql://node1:3306/esdb --username root --password 123456 --as-textfile --warehouse-dir /user/root Warning: /opt/sqoop-1.4.7/bin/../../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. 18/05/24 15:03:33 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 18/05/24 15:03:33 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 18/05/24 15:03:34 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 18/05/24 15:03:34 INFO tool.CodeGenTool: Beginning code generation 18/05/24 15:03:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1 18/05/24 15:03:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1 18/05/24 15:03:34 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-2.7.5 Note: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/files.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details. 18/05/24 15:03:39 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/files.jar 18/05/24 15:03:39 WARN manager.MySQLManager: It looks like you are importing from mysql. 18/05/24 15:03:39 WARN manager.MySQLManager: This transfer can be faster! Use the --direct 18/05/24 15:03:39 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path. 18/05/24 15:03:39 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql) 18/05/24 15:03:39 INFO mapreduce.ImportJobBase: Beginning import of files 18/05/24 15:03:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 18/05/24 15:03:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 18/05/24 15:03:40 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2 18/05/24 15:03:43 INFO db.DBInputFormat: Using read commited transaction isolation 18/05/24 15:03:43 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `files` 18/05/24 15:03:43 INFO db.IntegerSplitter: Split size: 810; Num splits: 4 from: 1 to: 3244 18/05/24 15:03:43 INFO mapreduce.JobSubmitter: number of splits:4 18/05/24 15:03:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1526097883376_0013 18/05/24 15:03:44 INFO impl.YarnClientImpl: Submitted application application_1526097883376_0013 18/05/24 15:03:44 INFO mapreduce.Job: The url to track the job: http://bigdata03-test:8088/proxy/application_1526097883376_0013/ 18/05/24 15:03:44 INFO mapreduce.Job: Running job: job_1526097883376_0013 18/05/24 15:03:51 INFO mapreduce.Job: Job job_1526097883376_0013 running in uber mode : false 18/05/24 15:03:51 INFO mapreduce.Job: &nbsp;map 0% reduce 0% 18/05/24 15:03:57 INFO mapreduce.Job: &nbsp;map 25% reduce 0% 18/05/24 15:03:58 INFO mapreduce.Job: &nbsp;map 75% reduce 0% 18/05/24 15:03:59 INFO mapreduce.Job: &nbsp;map 100% reduce 0% 18/05/24 15:03:59 INFO mapreduce.Job: Job job_1526097883376_0013 completed successfully 18/05/24 15:03:59 INFO mapreduce.Job: Counters: 30 &nbsp; &nbsp; File System Counters &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes read=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes written=570260 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of write operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes read=412 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes written=3799556 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of read operations=16 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of write operations=8 &nbsp; &nbsp; Job Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Launched map tasks=4 &nbsp; &nbsp; &nbsp; &nbsp; Other local map tasks=4 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all maps in occupied slots (ms)=18756 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all reduces in occupied slots (ms)=0 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all map tasks (ms)=18756 &nbsp; &nbsp; &nbsp; &nbsp; Total vcore-milliseconds taken by all map tasks=18756 &nbsp; &nbsp; &nbsp; &nbsp; Total megabyte-milliseconds taken by all map tasks=19206144 &nbsp; &nbsp; Map-Reduce Framework &nbsp; &nbsp; &nbsp; &nbsp; Map input records=3244 &nbsp; &nbsp; &nbsp; &nbsp; Map output records=3244 &nbsp; &nbsp; &nbsp; &nbsp; Input split bytes=412 &nbsp; &nbsp; &nbsp; &nbsp; Spilled Records=0 &nbsp; &nbsp; &nbsp; &nbsp; Failed Shuffles=0 &nbsp; &nbsp; &nbsp; &nbsp; Merged Map outputs=0 &nbsp; &nbsp; &nbsp; &nbsp; GC time elapsed (ms)=279 &nbsp; &nbsp; &nbsp; &nbsp; CPU time spent (ms)=7090 &nbsp; &nbsp; &nbsp; &nbsp; Physical memory (bytes) snapshot=727486464 &nbsp; &nbsp; &nbsp; &nbsp; Virtual memory (bytes) snapshot=8496140288 &nbsp; &nbsp; &nbsp; &nbsp; Total committed heap usage (bytes)=443023360 &nbsp; &nbsp; File Input Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Read=0 &nbsp; &nbsp; File Output Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Written=3799556 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Transferred 3.6235 MB in 19.1194 seconds (194.0698 KB/sec) 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Retrieved 3244 records. 18/05/24 15:03:59 INFO tool.CodeGenTool: Beginning code generation 18/05/24 15:03:59 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `logs` AS t LIMIT 1 18/05/24 15:03:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-2.7.5 Note: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/logs.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details. 18/05/24 15:03:59 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/logs.jar 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Beginning import of logs 18/05/24 15:03:59 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2 18/05/24 15:04:01 INFO db.DBInputFormat: Using read commited transaction isolation 18/05/24 15:04:01 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `logs` 18/05/24 15:04:01 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 1 18/05/24 15:04:01 INFO mapreduce.JobSubmitter: number of splits:1 18/05/24 15:04:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1526097883376_0014 18/05/24 15:04:01 INFO impl.YarnClientImpl: Submitted application application_1526097883376_0014 18/05/24 15:04:01 INFO mapreduce.Job: The url to track the job: http://bigdata03-test:8088/proxy/application_1526097883376_0014/ 18/05/24 15:04:01 INFO mapreduce.Job: Running job: job_1526097883376_0014 18/05/24 15:04:10 INFO mapreduce.Job: Job job_1526097883376_0014 running in uber mode : false 18/05/24 15:04:10 INFO mapreduce.Job: &nbsp;map 0% reduce 0% 18/05/24 15:04:17 INFO mapreduce.Job: &nbsp;map 100% reduce 0% 18/05/24 15:04:17 INFO mapreduce.Job: Job job_1526097883376_0014 completed successfully 18/05/24 15:04:17 INFO mapreduce.Job: Counters: 30 &nbsp; &nbsp; File System Counters &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes read=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes written=142495 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of write operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes read=99 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes written=47 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of read operations=4 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of write operations=2 &nbsp; &nbsp; Job Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Launched map tasks=1 &nbsp; &nbsp; &nbsp; &nbsp; Other local map tasks=1 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all maps in occupied slots (ms)=4490 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all reduces in occupied slots (ms)=0 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all map tasks (ms)=4490 &nbsp; &nbsp; &nbsp; &nbsp; Total vcore-milliseconds taken by all map tasks=4490 &nbsp; &nbsp; &nbsp; &nbsp; Total megabyte-milliseconds taken by all map tasks=4597760 &nbsp; &nbsp; Map-Reduce Framework &nbsp; &nbsp; &nbsp; &nbsp; Map input records=1 &nbsp; &nbsp; &nbsp; &nbsp; Map output records=1 &nbsp; &nbsp; &nbsp; &nbsp; Input split bytes=99 &nbsp; &nbsp; &nbsp; &nbsp; Spilled Records=0 &nbsp; &nbsp; &nbsp; &nbsp; Failed Shuffles=0 &nbsp; &nbsp; &nbsp; &nbsp; Merged Map outputs=0 &nbsp; &nbsp; &nbsp; &nbsp; GC time elapsed (ms)=72 &nbsp; &nbsp; &nbsp; &nbsp; CPU time spent (ms)=1560 &nbsp; &nbsp; &nbsp; &nbsp; Physical memory (bytes) snapshot=179109888 &nbsp; &nbsp; &nbsp; &nbsp; Virtual memory (bytes) snapshot=2121519104 &nbsp; &nbsp; &nbsp; &nbsp; Total committed heap usage (bytes)=108003328 &nbsp; &nbsp; File Input Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Read=0 &nbsp; &nbsp; File Output Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Written=47 18/05/24 15:04:17 INFO mapreduce.ImportJobBase: Transferred 47 bytes in 17.8489 seconds (2.6332 bytes/sec) 18/05/24 15:04:17 INFO mapreduce.ImportJobBase: Retrieved 1 records. [root@node1 sqoop-1.4.7]#&nbsp; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 一共导入了2张表 [root@node1 ~]# hdfs dfs -ls /user/root Found 5 items drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:03 /user/root/files drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-04-23 14:05 /user/root/input drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:04 /user/root/logs drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-22 13:36 /user/root/users [root@node1 ~]# hdfs dfs -ls /user/root/files Found 5 items -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:03 /user/root/files/_SUCCESS -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 895593 2018-05-24 15:03 /user/root/files/part-m-00000 -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 912033 2018-05-24 15:03 /user/root/files/part-m-00001 -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp;1109032 2018-05-24 15:03 /user/root/files/part-m-00002 -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 882898 2018-05-24 15:03 /user/root/files/part-m-00003 [root@node1 ~]# ---------------------&nbsp; 作者：程裕强&nbsp; 来源：CSDN&nbsp; 原文：https://blog.csdn.net/chengyuqiang/article/details/80434840&nbsp; 版权声明：本文为博主原创文章，转载请附上博文链接！" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-08T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"1、sqoop-import-all-tables导入多表 [root@node1 sqoop-1.4.7]# bin/sqoop-import-all-tables --connect jdbc:mysql://node1:3306/esdb --username root --password 123456 --as-textfile --warehouse-dir /user/root Warning: /opt/sqoop-1.4.7/bin/../../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. 18/05/24 14:58:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 18/05/24 14:58:06 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 18/05/24 14:58:06 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 18/05/24 14:58:06 INFO tool.CodeGenTool: Beginning code generation 18/05/24 14:58:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1 18/05/24 14:58:06 ERROR manager.SqlManager: Error reading from database: java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@1817d444 is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries. java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@1817d444 is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries. &nbsp; &nbsp; at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:931) &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.checkForOutstandingStreamingData(MysqlIO.java:2518) &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1748) &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:1961) &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2537) &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2466) &nbsp; &nbsp; at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1383) &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.getMaxBytesPerChar(ConnectionImpl.java:2939) &nbsp; &nbsp; at com.mysql.jdbc.Field.getMaxBytesPerCharacter(Field.java:576) &nbsp; &nbsp; at com.mysql.jdbc.ResultSetMetaData.getPrecision(ResultSetMetaData.java:440) &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:305) &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260) &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246) &nbsp; &nbsp; at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327) &nbsp; &nbsp; at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1872) &nbsp; &nbsp; at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1671) &nbsp; &nbsp; at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106) &nbsp; &nbsp; at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501) &nbsp; &nbsp; at org.apache.sqoop.tool.ImportAllTablesTool.run(ImportAllTablesTool.java:110) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.run(Sqoop.java:147) &nbsp; &nbsp; at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) &nbsp; &nbsp; at org.apache.sqoop.Sqoop.main(Sqoop.java:252) 18/05/24 14:58:06 ERROR tool.ImportAllTablesTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter [root@node1 sqoop-1.4.7]# 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 由于之前上传到Sqoop lib下的MySQL驱动程序有些低，更新到mysql-connector-java-5.1.32-bin.jar即可 [root@node1 ~]# ls /opt/sqoop-1.4.7/lib |grep mysql mysql-connector-java-5.1.32-bin.jar [root@node1 ~]# 1 2 3 2、再次执行 [root@node1 sqoop-1.4.7]# bin/sqoop-import-all-tables --connect jdbc:mysql://node1:3306/esdb --username root --password 123456 --as-textfile --warehouse-dir /user/root Warning: /opt/sqoop-1.4.7/bin/../../hbase does not exist! HBase imports will fail. Please set $HBASE_HOME to the root of your HBase installation. 18/05/24 15:03:33 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7 18/05/24 15:03:33 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 18/05/24 15:03:34 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 18/05/24 15:03:34 INFO tool.CodeGenTool: Beginning code generation 18/05/24 15:03:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1 18/05/24 15:03:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1 18/05/24 15:03:34 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-2.7.5 Note: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/files.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details. 18/05/24 15:03:39 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/files.jar 18/05/24 15:03:39 WARN manager.MySQLManager: It looks like you are importing from mysql. 18/05/24 15:03:39 WARN manager.MySQLManager: This transfer can be faster! Use the --direct 18/05/24 15:03:39 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path. 18/05/24 15:03:39 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql) 18/05/24 15:03:39 INFO mapreduce.ImportJobBase: Beginning import of files 18/05/24 15:03:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar 18/05/24 15:03:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps 18/05/24 15:03:40 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2 18/05/24 15:03:43 INFO db.DBInputFormat: Using read commited transaction isolation 18/05/24 15:03:43 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `files` 18/05/24 15:03:43 INFO db.IntegerSplitter: Split size: 810; Num splits: 4 from: 1 to: 3244 18/05/24 15:03:43 INFO mapreduce.JobSubmitter: number of splits:4 18/05/24 15:03:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1526097883376_0013 18/05/24 15:03:44 INFO impl.YarnClientImpl: Submitted application application_1526097883376_0013 18/05/24 15:03:44 INFO mapreduce.Job: The url to track the job: http://bigdata03-test:8088/proxy/application_1526097883376_0013/ 18/05/24 15:03:44 INFO mapreduce.Job: Running job: job_1526097883376_0013 18/05/24 15:03:51 INFO mapreduce.Job: Job job_1526097883376_0013 running in uber mode : false 18/05/24 15:03:51 INFO mapreduce.Job: &nbsp;map 0% reduce 0% 18/05/24 15:03:57 INFO mapreduce.Job: &nbsp;map 25% reduce 0% 18/05/24 15:03:58 INFO mapreduce.Job: &nbsp;map 75% reduce 0% 18/05/24 15:03:59 INFO mapreduce.Job: &nbsp;map 100% reduce 0% 18/05/24 15:03:59 INFO mapreduce.Job: Job job_1526097883376_0013 completed successfully 18/05/24 15:03:59 INFO mapreduce.Job: Counters: 30 &nbsp; &nbsp; File System Counters &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes read=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes written=570260 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of write operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes read=412 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes written=3799556 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of read operations=16 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of write operations=8 &nbsp; &nbsp; Job Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Launched map tasks=4 &nbsp; &nbsp; &nbsp; &nbsp; Other local map tasks=4 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all maps in occupied slots (ms)=18756 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all reduces in occupied slots (ms)=0 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all map tasks (ms)=18756 &nbsp; &nbsp; &nbsp; &nbsp; Total vcore-milliseconds taken by all map tasks=18756 &nbsp; &nbsp; &nbsp; &nbsp; Total megabyte-milliseconds taken by all map tasks=19206144 &nbsp; &nbsp; Map-Reduce Framework &nbsp; &nbsp; &nbsp; &nbsp; Map input records=3244 &nbsp; &nbsp; &nbsp; &nbsp; Map output records=3244 &nbsp; &nbsp; &nbsp; &nbsp; Input split bytes=412 &nbsp; &nbsp; &nbsp; &nbsp; Spilled Records=0 &nbsp; &nbsp; &nbsp; &nbsp; Failed Shuffles=0 &nbsp; &nbsp; &nbsp; &nbsp; Merged Map outputs=0 &nbsp; &nbsp; &nbsp; &nbsp; GC time elapsed (ms)=279 &nbsp; &nbsp; &nbsp; &nbsp; CPU time spent (ms)=7090 &nbsp; &nbsp; &nbsp; &nbsp; Physical memory (bytes) snapshot=727486464 &nbsp; &nbsp; &nbsp; &nbsp; Virtual memory (bytes) snapshot=8496140288 &nbsp; &nbsp; &nbsp; &nbsp; Total committed heap usage (bytes)=443023360 &nbsp; &nbsp; File Input Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Read=0 &nbsp; &nbsp; File Output Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Written=3799556 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Transferred 3.6235 MB in 19.1194 seconds (194.0698 KB/sec) 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Retrieved 3244 records. 18/05/24 15:03:59 INFO tool.CodeGenTool: Beginning code generation 18/05/24 15:03:59 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `logs` AS t LIMIT 1 18/05/24 15:03:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-2.7.5 Note: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/logs.java uses or overrides a deprecated API. Note: Recompile with -Xlint:deprecation for details. 18/05/24 15:03:59 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/logs.jar 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Beginning import of logs 18/05/24 15:03:59 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2 18/05/24 15:04:01 INFO db.DBInputFormat: Using read commited transaction isolation 18/05/24 15:04:01 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `logs` 18/05/24 15:04:01 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 1 18/05/24 15:04:01 INFO mapreduce.JobSubmitter: number of splits:1 18/05/24 15:04:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1526097883376_0014 18/05/24 15:04:01 INFO impl.YarnClientImpl: Submitted application application_1526097883376_0014 18/05/24 15:04:01 INFO mapreduce.Job: The url to track the job: http://bigdata03-test:8088/proxy/application_1526097883376_0014/ 18/05/24 15:04:01 INFO mapreduce.Job: Running job: job_1526097883376_0014 18/05/24 15:04:10 INFO mapreduce.Job: Job job_1526097883376_0014 running in uber mode : false 18/05/24 15:04:10 INFO mapreduce.Job: &nbsp;map 0% reduce 0% 18/05/24 15:04:17 INFO mapreduce.Job: &nbsp;map 100% reduce 0% 18/05/24 15:04:17 INFO mapreduce.Job: Job job_1526097883376_0014 completed successfully 18/05/24 15:04:17 INFO mapreduce.Job: Counters: 30 &nbsp; &nbsp; File System Counters &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes read=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes written=142495 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of write operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes read=99 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes written=47 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of read operations=4 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of large read operations=0 &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of write operations=2 &nbsp; &nbsp; Job Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Launched map tasks=1 &nbsp; &nbsp; &nbsp; &nbsp; Other local map tasks=1 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all maps in occupied slots (ms)=4490 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all reduces in occupied slots (ms)=0 &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all map tasks (ms)=4490 &nbsp; &nbsp; &nbsp; &nbsp; Total vcore-milliseconds taken by all map tasks=4490 &nbsp; &nbsp; &nbsp; &nbsp; Total megabyte-milliseconds taken by all map tasks=4597760 &nbsp; &nbsp; Map-Reduce Framework &nbsp; &nbsp; &nbsp; &nbsp; Map input records=1 &nbsp; &nbsp; &nbsp; &nbsp; Map output records=1 &nbsp; &nbsp; &nbsp; &nbsp; Input split bytes=99 &nbsp; &nbsp; &nbsp; &nbsp; Spilled Records=0 &nbsp; &nbsp; &nbsp; &nbsp; Failed Shuffles=0 &nbsp; &nbsp; &nbsp; &nbsp; Merged Map outputs=0 &nbsp; &nbsp; &nbsp; &nbsp; GC time elapsed (ms)=72 &nbsp; &nbsp; &nbsp; &nbsp; CPU time spent (ms)=1560 &nbsp; &nbsp; &nbsp; &nbsp; Physical memory (bytes) snapshot=179109888 &nbsp; &nbsp; &nbsp; &nbsp; Virtual memory (bytes) snapshot=2121519104 &nbsp; &nbsp; &nbsp; &nbsp; Total committed heap usage (bytes)=108003328 &nbsp; &nbsp; File Input Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Read=0 &nbsp; &nbsp; File Output Format Counters&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Bytes Written=47 18/05/24 15:04:17 INFO mapreduce.ImportJobBase: Transferred 47 bytes in 17.8489 seconds (2.6332 bytes/sec) 18/05/24 15:04:17 INFO mapreduce.ImportJobBase: Retrieved 1 records. [root@node1 sqoop-1.4.7]#&nbsp; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 一共导入了2张表 [root@node1 ~]# hdfs dfs -ls /user/root Found 5 items drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:03 /user/root/files drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-04-23 14:05 /user/root/input drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:04 /user/root/logs drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-22 13:36 /user/root/users [root@node1 ~]# hdfs dfs -ls /user/root/files Found 5 items -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:03 /user/root/files/_SUCCESS -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 895593 2018-05-24 15:03 /user/root/files/part-m-00000 -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 912033 2018-05-24 15:03 /user/root/files/part-m-00001 -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp;1109032 2018-05-24 15:03 /user/root/files/part-m-00002 -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 882898 2018-05-24 15:03 /user/root/files/part-m-00003 [root@node1 ~]# ---------------------&nbsp; 作者：程裕强&nbsp; 来源：CSDN&nbsp; 原文：https://blog.csdn.net/chengyuqiang/article/details/80434840&nbsp; 版权声明：本文为博主原创文章，转载请附上博文链接！","@type":"BlogPosting","url":"/2019/04/08/728262.html","headline":"Sqoop导入mysql所有表到HDFS","dateModified":"2019-04-08T00:00:00+08:00","datePublished":"2019-04-08T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/04/08/728262.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Sqoop导入mysql所有表到HDFS</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>1、sqoop-import-all-tables导入多表<br> [root@node1 sqoop-1.4.7]# bin/sqoop-import-all-tables --connect jdbc:mysql://node1:3306/esdb --username root --password 123456 --as-textfile --warehouse-dir /user/root<br> Warning: /opt/sqoop-1.4.7/bin/../../hbase does not exist! HBase imports will fail.<br> Please set $HBASE_HOME to the root of your HBase installation.<br> 18/05/24 14:58:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7<br> 18/05/24 14:58:06 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.<br> 18/05/24 14:58:06 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.<br> 18/05/24 14:58:06 INFO tool.CodeGenTool: Beginning code generation<br> 18/05/24 14:58:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1<br> 18/05/24 14:58:06 ERROR manager.SqlManager: Error reading from database: java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@1817d444 is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries.<br> java.sql.SQLException: Streaming result set com.mysql.jdbc.RowDataDynamic@1817d444 is still active. No statements may be issued when any streaming result sets are open and in use on a given connection. Ensure that you have called .close() on any active streaming result sets before attempting more queries.<br> &nbsp; &nbsp; at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:931)<br> &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.checkForOutstandingStreamingData(MysqlIO.java:2518)<br> &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1748)<br> &nbsp; &nbsp; at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:1961)<br> &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2537)<br> &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2466)<br> &nbsp; &nbsp; at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1383)<br> &nbsp; &nbsp; at com.mysql.jdbc.ConnectionImpl.getMaxBytesPerChar(ConnectionImpl.java:2939)<br> &nbsp; &nbsp; at com.mysql.jdbc.Field.getMaxBytesPerCharacter(Field.java:576)<br> &nbsp; &nbsp; at com.mysql.jdbc.ResultSetMetaData.getPrecision(ResultSetMetaData.java:440)<br> &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:305)<br> &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260)<br> &nbsp; &nbsp; at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246)<br> &nbsp; &nbsp; at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327)<br> &nbsp; &nbsp; at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1872)<br> &nbsp; &nbsp; at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1671)<br> &nbsp; &nbsp; at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:106)<br> &nbsp; &nbsp; at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:501)<br> &nbsp; &nbsp; at org.apache.sqoop.tool.ImportAllTablesTool.run(ImportAllTablesTool.java:110)<br> &nbsp; &nbsp; at org.apache.sqoop.Sqoop.run(Sqoop.java:147)<br> &nbsp; &nbsp; at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)<br> &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)<br> &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)<br> &nbsp; &nbsp; at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)<br> &nbsp; &nbsp; at org.apache.sqoop.Sqoop.main(Sqoop.java:252)<br> 18/05/24 14:58:06 ERROR tool.ImportAllTablesTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter<br> [root@node1 sqoop-1.4.7]#<br> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br> 10<br> 11<br> 12<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 由于之前上传到Sqoop lib下的MySQL驱动程序有些低，更新到mysql-connector-java-5.1.32-bin.jar即可</p> 
  <p>[root@node1 ~]# ls /opt/sqoop-1.4.7/lib |grep mysql<br> mysql-connector-java-5.1.32-bin.jar<br> [root@node1 ~]#<br> 1<br> 2<br> 3<br> 2、再次执行<br> [root@node1 sqoop-1.4.7]# bin/sqoop-import-all-tables --connect jdbc:mysql://node1:3306/esdb --username root --password 123456 --as-textfile --warehouse-dir /user/root<br> Warning: /opt/sqoop-1.4.7/bin/../../hbase does not exist! HBase imports will fail.<br> Please set $HBASE_HOME to the root of your HBase installation.<br> 18/05/24 15:03:33 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7<br> 18/05/24 15:03:33 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.<br> 18/05/24 15:03:34 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.<br> 18/05/24 15:03:34 INFO tool.CodeGenTool: Beginning code generation<br> 18/05/24 15:03:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1<br> 18/05/24 15:03:34 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `files` AS t LIMIT 1<br> 18/05/24 15:03:34 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-2.7.5<br> Note: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/files.java uses or overrides a deprecated API.<br> Note: Recompile with -Xlint:deprecation for details.<br> 18/05/24 15:03:39 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/files.jar<br> 18/05/24 15:03:39 WARN manager.MySQLManager: It looks like you are importing from mysql.<br> 18/05/24 15:03:39 WARN manager.MySQLManager: This transfer can be faster! Use the --direct<br> 18/05/24 15:03:39 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.<br> 18/05/24 15:03:39 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)<br> 18/05/24 15:03:39 INFO mapreduce.ImportJobBase: Beginning import of files<br> 18/05/24 15:03:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar<br> 18/05/24 15:03:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps<br> 18/05/24 15:03:40 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2<br> 18/05/24 15:03:43 INFO db.DBInputFormat: Using read commited transaction isolation<br> 18/05/24 15:03:43 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `files`<br> 18/05/24 15:03:43 INFO db.IntegerSplitter: Split size: 810; Num splits: 4 from: 1 to: 3244<br> 18/05/24 15:03:43 INFO mapreduce.JobSubmitter: number of splits:4<br> 18/05/24 15:03:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1526097883376_0013<br> 18/05/24 15:03:44 INFO impl.YarnClientImpl: Submitted application application_1526097883376_0013<br> 18/05/24 15:03:44 INFO mapreduce.Job: The url to track the job: http://bigdata03-test:8088/proxy/application_1526097883376_0013/<br> 18/05/24 15:03:44 INFO mapreduce.Job: Running job: job_1526097883376_0013<br> 18/05/24 15:03:51 INFO mapreduce.Job: Job job_1526097883376_0013 running in uber mode : false<br> 18/05/24 15:03:51 INFO mapreduce.Job: &nbsp;map 0% reduce 0%<br> 18/05/24 15:03:57 INFO mapreduce.Job: &nbsp;map 25% reduce 0%<br> 18/05/24 15:03:58 INFO mapreduce.Job: &nbsp;map 75% reduce 0%<br> 18/05/24 15:03:59 INFO mapreduce.Job: &nbsp;map 100% reduce 0%<br> 18/05/24 15:03:59 INFO mapreduce.Job: Job job_1526097883376_0013 completed successfully<br> 18/05/24 15:03:59 INFO mapreduce.Job: Counters: 30<br> &nbsp; &nbsp; File System Counters<br> &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes read=0<br> &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes written=570260<br> &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of read operations=0<br> &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of large read operations=0<br> &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of write operations=0<br> &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes read=412<br> &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes written=3799556<br> &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of read operations=16<br> &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of large read operations=0<br> &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of write operations=8<br> &nbsp; &nbsp; Job Counters&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; Launched map tasks=4<br> &nbsp; &nbsp; &nbsp; &nbsp; Other local map tasks=4<br> &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all maps in occupied slots (ms)=18756<br> &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all reduces in occupied slots (ms)=0<br> &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all map tasks (ms)=18756<br> &nbsp; &nbsp; &nbsp; &nbsp; Total vcore-milliseconds taken by all map tasks=18756<br> &nbsp; &nbsp; &nbsp; &nbsp; Total megabyte-milliseconds taken by all map tasks=19206144<br> &nbsp; &nbsp; Map-Reduce Framework<br> &nbsp; &nbsp; &nbsp; &nbsp; Map input records=3244<br> &nbsp; &nbsp; &nbsp; &nbsp; Map output records=3244<br> &nbsp; &nbsp; &nbsp; &nbsp; Input split bytes=412<br> &nbsp; &nbsp; &nbsp; &nbsp; Spilled Records=0<br> &nbsp; &nbsp; &nbsp; &nbsp; Failed Shuffles=0<br> &nbsp; &nbsp; &nbsp; &nbsp; Merged Map outputs=0<br> &nbsp; &nbsp; &nbsp; &nbsp; GC time elapsed (ms)=279<br> &nbsp; &nbsp; &nbsp; &nbsp; CPU time spent (ms)=7090<br> &nbsp; &nbsp; &nbsp; &nbsp; Physical memory (bytes) snapshot=727486464<br> &nbsp; &nbsp; &nbsp; &nbsp; Virtual memory (bytes) snapshot=8496140288<br> &nbsp; &nbsp; &nbsp; &nbsp; Total committed heap usage (bytes)=443023360<br> &nbsp; &nbsp; File Input Format Counters&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; Bytes Read=0<br> &nbsp; &nbsp; File Output Format Counters&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; Bytes Written=3799556<br> 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Transferred 3.6235 MB in 19.1194 seconds (194.0698 KB/sec)<br> 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Retrieved 3244 records.<br> 18/05/24 15:03:59 INFO tool.CodeGenTool: Beginning code generation<br> 18/05/24 15:03:59 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `logs` AS t LIMIT 1<br> 18/05/24 15:03:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/hadoop-2.7.5<br> Note: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/logs.java uses or overrides a deprecated API.<br> Note: Recompile with -Xlint:deprecation for details.<br> 18/05/24 15:03:59 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/6a64974699f84d472b1e4438782a3423/logs.jar<br> 18/05/24 15:03:59 INFO mapreduce.ImportJobBase: Beginning import of logs<br> 18/05/24 15:03:59 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm2<br> 18/05/24 15:04:01 INFO db.DBInputFormat: Using read commited transaction isolation<br> 18/05/24 15:04:01 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `logs`<br> 18/05/24 15:04:01 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 1<br> 18/05/24 15:04:01 INFO mapreduce.JobSubmitter: number of splits:1<br> 18/05/24 15:04:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1526097883376_0014<br> 18/05/24 15:04:01 INFO impl.YarnClientImpl: Submitted application application_1526097883376_0014<br> 18/05/24 15:04:01 INFO mapreduce.Job: The url to track the job: http://bigdata03-test:8088/proxy/application_1526097883376_0014/<br> 18/05/24 15:04:01 INFO mapreduce.Job: Running job: job_1526097883376_0014<br> 18/05/24 15:04:10 INFO mapreduce.Job: Job job_1526097883376_0014 running in uber mode : false<br> 18/05/24 15:04:10 INFO mapreduce.Job: &nbsp;map 0% reduce 0%<br> 18/05/24 15:04:17 INFO mapreduce.Job: &nbsp;map 100% reduce 0%<br> 18/05/24 15:04:17 INFO mapreduce.Job: Job job_1526097883376_0014 completed successfully<br> 18/05/24 15:04:17 INFO mapreduce.Job: Counters: 30<br> &nbsp; &nbsp; File System Counters<br> &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes read=0<br> &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of bytes written=142495<br> &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of read operations=0<br> &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of large read operations=0<br> &nbsp; &nbsp; &nbsp; &nbsp; FILE: Number of write operations=0<br> &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes read=99<br> &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of bytes written=47<br> &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of read operations=4<br> &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of large read operations=0<br> &nbsp; &nbsp; &nbsp; &nbsp; HDFS: Number of write operations=2<br> &nbsp; &nbsp; Job Counters&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; Launched map tasks=1<br> &nbsp; &nbsp; &nbsp; &nbsp; Other local map tasks=1<br> &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all maps in occupied slots (ms)=4490<br> &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all reduces in occupied slots (ms)=0<br> &nbsp; &nbsp; &nbsp; &nbsp; Total time spent by all map tasks (ms)=4490<br> &nbsp; &nbsp; &nbsp; &nbsp; Total vcore-milliseconds taken by all map tasks=4490<br> &nbsp; &nbsp; &nbsp; &nbsp; Total megabyte-milliseconds taken by all map tasks=4597760<br> &nbsp; &nbsp; Map-Reduce Framework<br> &nbsp; &nbsp; &nbsp; &nbsp; Map input records=1<br> &nbsp; &nbsp; &nbsp; &nbsp; Map output records=1<br> &nbsp; &nbsp; &nbsp; &nbsp; Input split bytes=99<br> &nbsp; &nbsp; &nbsp; &nbsp; Spilled Records=0<br> &nbsp; &nbsp; &nbsp; &nbsp; Failed Shuffles=0<br> &nbsp; &nbsp; &nbsp; &nbsp; Merged Map outputs=0<br> &nbsp; &nbsp; &nbsp; &nbsp; GC time elapsed (ms)=72<br> &nbsp; &nbsp; &nbsp; &nbsp; CPU time spent (ms)=1560<br> &nbsp; &nbsp; &nbsp; &nbsp; Physical memory (bytes) snapshot=179109888<br> &nbsp; &nbsp; &nbsp; &nbsp; Virtual memory (bytes) snapshot=2121519104<br> &nbsp; &nbsp; &nbsp; &nbsp; Total committed heap usage (bytes)=108003328<br> &nbsp; &nbsp; File Input Format Counters&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; Bytes Read=0<br> &nbsp; &nbsp; File Output Format Counters&nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; Bytes Written=47<br> 18/05/24 15:04:17 INFO mapreduce.ImportJobBase: Transferred 47 bytes in 17.8489 seconds (2.6332 bytes/sec)<br> 18/05/24 15:04:17 INFO mapreduce.ImportJobBase: Retrieved 1 records.<br> [root@node1 sqoop-1.4.7]#&nbsp;<br> 1<br> 2<br> 3<br> 4<br> 5<br> 6<br> 7<br> 8<br> 9<br> 10<br> 11<br> 12<br> 13<br> 14<br> 15<br> 16<br> 17<br> 18<br> 19<br> 20<br> 21<br> 22<br> 23<br> 24<br> 25<br> 26<br> 27<br> 28<br> 29<br> 30<br> 31<br> 32<br> 33<br> 34<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br> 100<br> 101<br> 102<br> 103<br> 104<br> 105<br> 106<br> 107<br> 108<br> 109<br> 110<br> 111<br> 112<br> 113<br> 114<br> 115<br> 116<br> 117<br> 118<br> 119<br> 120<br> 121<br> 122<br> 123<br> 124<br> 125<br> 126<br> 127<br> 128<br> 129<br> 130<br> 131<br> 132<br> 一共导入了2张表</p> 
  <p>[root@node1 ~]# hdfs dfs -ls /user/root<br> Found 5 items<br> drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:03 /user/root/files<br> drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-04-23 14:05 /user/root/input<br> drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:04 /user/root/logs<br> drwxr-xr-x &nbsp; - root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-22 13:36 /user/root/users<br> [root@node1 ~]# hdfs dfs -ls /user/root/files<br> Found 5 items<br> -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0 2018-05-24 15:03 /user/root/files/_SUCCESS<br> -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 895593 2018-05-24 15:03 /user/root/files/part-m-00000<br> -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 912033 2018-05-24 15:03 /user/root/files/part-m-00001<br> -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp;1109032 2018-05-24 15:03 /user/root/files/part-m-00002<br> -rw-r--r-- &nbsp; 3 root supergroup &nbsp; &nbsp; 882898 2018-05-24 15:03 /user/root/files/part-m-00003<br> [root@node1 ~]#<br> ---------------------&nbsp;<br> 作者：程裕强&nbsp;<br> 来源：CSDN&nbsp;<br> 原文：https://blog.csdn.net/chengyuqiang/article/details/80434840&nbsp;<br> 版权声明：本文为博主原创文章，转载请附上博文链接！</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
