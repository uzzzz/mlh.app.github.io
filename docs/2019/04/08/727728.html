<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Fast RCNN译文转发 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Fast RCNN译文转发" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Fast R-CNN Ross Girshick Microsoft Research rbg@microsoft.com 摘要 本文提出了一种快速的基于区域的卷积网络方法（fast R-CNN）用于目标检测。Fast R-CNN建立在以前使用的深卷积网络有效地分类目标的成果上。相比于之前的成果，Fast R-CNN采用了多项创新提高训练和测试速度来提高检测精度。Fast R-CNN训练非常深的VGG16网络比R-CNN快9倍，测试时间快213倍，并在PASCAL VOC上得到更高的精度。与SPPnet相比，fast R-CNN训练VGG16网络比他快3倍，测试速度快10倍，并且更准确。Fast R-CNN的Python和C ++（使用Caffe）实现以MIT开源许可证发布在：https://github.com/rbgirshick/fast-rcnn。 简介 最近，深度卷积网络1 2已经显著提高了图像分类1和目标检测3 4的准确性。与图像分类相比，目标检测是一个更具挑战性的任务，需要更复杂的方法来解决。由于这种复杂性，当前的方法（例如，3 5 4 6）采用多级流水线的方式训练模型，既慢且精度不高。 复杂性的产生是因为检测需要目标的精确定位，这就导致两个主要的难点。首先，必须处理大量候选目标位置（通常称为“提案”）。 第二，这些候选框仅提供粗略定位，其必须被精细化以实现精确定位。 这些问题的解决方案经常会影响速度，准确性或简单性。 在本文中，我们简化了最先进的基于卷积网络的目标检测器的训练过程3 5。我们提出一个单阶段训练算法，联合学习候选框分类和修正他们的空间位置。 所得到的方法用来训练非常深的检测网络（例如VGG16） 比R-CNN快9倍，比SPPnet快3倍。在运行时，检测网络在PASCAL VOC 2012数据集上实现最高准确度，其中mAP为66％（R-CNN为62％），每张图像处理时间为0.3秒，不包括候选框的生成（所有的时间都是使用一个超频到875MHz的Nvidia K40 GPU测试的）。 R-CNN与SPPnet 基于区域的卷积网络方法（RCNN）通过使用深度卷积网络来分类目标候选框，获得了很高的目标检测精度。然而，R-CNN具有显着的缺点： **训练过程是多级流水线。**R-CNN首先使用目标候选框对卷积神经网络使用log损失进行微调。然后，它将卷积神经网络得到的特征送入SVM。 这些SVM作为目标检测器，替代通过微调学习的softmax分类器。 在第三个训练阶段，学习检测框回归。 训练在时间和空间上是的开销很大。对于SVM和检测框回归训练，从每个图像中的每个目标候选框提取特征，并写入磁盘。对于非常深的网络，如VGG16，这个过程在单个GPU上需要2.5天（VOC07 trainval上的5k个图像）。这些特征需要数百GB的存储空间。 目标检测速度很慢。在测试时，从每个测试图像中的每个目标候选框提取特征。用VGG16网络检测目标每个图像需要47秒（在GPU上）。 R-CNN很慢是因为它为每个目标候选框进行卷积神经网络正向传递，而不共享计算。SPPnet5通过共享计算加速R-CNN。SPPnet5计算整个输入图像的卷积特征图，然后使用从共享特征图提取的特征向量来对每个候选框进行分类。通过最大池化将候选框内的特征图转化为固定大小的输出（例如，6X6）来提取针对候选框的特征。多个输出被池化，然后连接成空间金字塔池7。SPPnet在测试时将R-CNN加速10到100倍。由于更快的候选框特征提取训练时间也减少3倍。 SPP网络也有显著的缺点。像R-CNN一样，训练过程是一个多级流水线，涉及提取特征，使用log损失对网络进行微调，训练SVM分类器，最后拟合检测框回归。特征也写入磁盘。但与R-CNN不同，在5中提出的微调算法不能更新在空间金字塔池之前的卷积层。不出所料，这种限制（固定的卷积层）限制了深层网络的精度。 贡献 我们提出一种新的训练算法，修正R-CNN和SPPnet的缺点，同时提高其速度和准确性。因为它能比较快地进行训练和测试，我们称之为Fast R-CNN。Fast RCNN方法有以下几个优点： 比R-CNN和SPPnet具有更高的目标检测精度（mAP）。 训练是使用多任务损失的单阶段训练。 训练可以更新所有网络层参数。 不需要磁盘空间缓存特征。 Fast R-CNN使用Python和C++(Caffe8)语言编写，以MIT开源许可证发布在：https://github.com/rbgirshick/fast-rcnn。 Fast R-CNN架构与训练 Fast R-CNN的架构如下图（图1）所示： 图1. Fast R-CNN架构。输入图像和多个感兴趣区域（RoI）被输入到全卷积网络中。每个RoI被池化到固定大小的特征图中，然后通过全连接层（FC）映射到特征向量。网络对于每个RoI具有两个输出向量：Softmax概率和每类检测框回归偏移量。该架构是使用多任务丢失端到端训练的。 Fast R-CNN网络将整个图像和一组候选框作为输入。网络首先使用几个卷积层（conv）和最大池化层来处理整个图像，以产生卷积特征图。然后，对于每个候选框，RoI池化层从特征图中提取固定长度的特征向量。每个特征向量被送入一系列全连接（fc）层中，其最终分支成两个同级输出层 ：一个输出KK个类别加上1个背景类别的Softmax概率估计，另一个为KK个类别的每一个类别输出四个实数值。每组4个值表示KK个类别的一个类别的检测框位置的修正。 RoI池化层 RoI池化层使用最大池化将任何有效的RoI内的特征转换成具有H×WH×W（例如，7×77×7）的固定空间范围的小特征图，其中HH和WW是层的超参数，独立于任何特定的RoI。在本文中，RoI是卷积特征图中的一个矩形窗口。 每个RoI由指定其左上角(r,c)(r,c)及其高度和宽度(h,w)(h,w)的四元组(r,c,h,w)(r,c,h,w)定义。 RoI最大池化通过将大小为h×wh×w的RoI窗口分割成H×WH×W个网格，子窗口大小约为h/H×w/Wh/H×w/W，然后对每个子窗口执行最大池化，并将输出合并到相应的输出网格单元中。同标准的最大池化一样，池化操作独立应用于每个特征图通道。RoI层只是SPPnets 5中使用的空间金字塔池层的特殊情况，其只有一个金字塔层。 我们使用5中给出的池化子窗口计算方法。 从预训练网络初始化 我们实验了三个预训练的ImageNet9网络，每个网络有五个最大池化层和五到十三个卷积层（网络详细信息，请参见实验配置）。当预训练网络初始化fast R-CNN网络时，其经历三个变换。 首先，最后的最大池化层由RoI池层代替，其将H和W设置为与网络的第一个全连接层兼容的配置（例如，对于VGG16，H=W=7H=W=7）。 然后，网络的最后一格全连接层和Softmax（其被训练用于1000类ImageNet分类）被替换为前面描述的两个同级层（全连接层和K+1K+1个类别的Softmax以及类别特定的检测框回归）。 最后，网络被修改为采用两个数据输入：图像的列表和这些图像中的RoI的列表。 微调 用反向传播训练所有网络权重是Fast R-CNN的重要能力。首先，让我们阐明为什么SPPnet无法更新低于空间金字塔池化层的权重。 根本原因是当每个训练样本（即RoI）来自不同的图像时，通过SPP层的反向传播是非常低效的，这正是训练R-CNN和SPPnet网络的方法。低效的部分是因为每个RoI可能具有非常大的感受野，通常跨越整个输入图像。由于正向传播必须处理整个感受野，训练输入很大（通常是整个图像）。 我们提出了一种更有效的训练方法，利用训练期间的特征共享。在Fast RCNN网络训练中，随机梯度下降（SGD）的小批量是被分层采样的，首先采样NN个图像，然后从每个图像采样R/NR/N个 RoI。关键的是，来自同一图像的RoI在向前和向后传播中共享计算和内存。减小NN，就减少了小批量的计算。例如，当N=2N=2和R=128R=128时，得到的训练方案比从128幅不同的图采样一个RoI（即R-CNN和SPPnet的策略）快64倍。 这个策略的一个令人担心的问题是它可能导致训练收敛变慢，因为来自相同图像的RoI是相关的。这个问题似乎在实际情况下并不存在，当N=2N=2和R=128R=128时，我们使用比R-CNN更少的SGD迭代就获得了良好的结果。 除了分层采样，Fast R-CNN使用了一个精细的训练过程，在微调阶段联合优化Softmax分类器和检测框回归，而不是分别在三个独立的阶段训练softmax分类器，SVM和回归器3 [^ 11]。 下面将详细描述该过程（损失，小批量采样策略，通过RoI池化层的反向传播和SGD超参数）。 **多任务损失。**Fast R-CNN网络具有两个同级输出层。 第一个输出在K+1K+1个类别上的离散概率分布（每个RoI），p=(p0,…,pK)p=(p0,…,pK)。 通常，通过全连接层的K+1K+1个输出上的Softmax来计算pp。第二个输出层输出检测框回归偏移，tk=(tkx,tky,tkw,tkh)tk=(txk,tyk,twk,thk)，对于由k索引的K个类别中的每一个。 我们使用3中给出的tktk的参数化，其中tktk指定相对于候选框的尺度不变转换和对数空间高度/宽度移位。 每个训练的RoI用类真值uu和检测框回归目标真值vv标记。我们对每个标记的RoI使用多任务损失LL以联合训练分类和检测框回归：L(p,u,tu,v)=Lcls(p,u)+λ[u≥1]Lloc(tu,v)(1)(1)L(p,u,tu,v)=Lcls(p,u)+λ[u≥1]Lloc(tu,v) 其中Lcls(p,u)=−logpuLcls(p,u)=−log⁡pu， 是类真值uu的log损失。 对于类真值uu，第二个损失LlocLloc是定义在检测框回归目标真值元组u,v=(vx,vy,vw,vh)u,v=(vx,vy,vw,vh)和预测元组tu=(tux,tuy,tuw,tuh)tu=(txu,tyu,twu,thu)上的损失。 Iverson括号指示函数[u≥1][u≥1]当u≥1u≥1的时候为值1，否则为0。按照惯例，背景类标记为u=0u=0。对于背景RoI，没有检测框真值的概念，因此LlocLloc被忽略。对于检测框回归，我们使用损失Lloc(tu,v)=∑i∈{x,y,w,h}smoothL1(tui−vi)(2)(2)Lloc(tu,v)=∑i∈{x,y,w,h}smoothL1(tiu−vi)其中：smoothL1(x)={0.5x2|x|−0.5if|x|&lt;1otherwise(3)(3)smoothL1(x)={0.5x2if|x|&lt;1|x|−0.5otherwise是鲁棒的L1L1损失，对于异常值比在R-CNN和SPPnet中使用的L2L2损失更不敏感。当回归目标无界时，具有L2L2损失的训练可能需要仔细调整学习速率，以防止爆炸梯度。公式(3)(3)消除了这种灵敏度。 公式(1)(1)中的超参数λλ控制两个任务损失之间的平衡。我们将回归目标真值vivi归一化为具有零均值和单位方差。所有实验都使用λ=1λ=1。 我们注意到10使用相关损失来训练一个类别无关的目标候选网络。 与我们的方法不同的是10倡导一个分离定位和分类的双网络系统。OverFeat4，R-CNN3和SPPnet5也训练分类器和检测框定位器，但是这些方法使用逐级训练，这对于Fast RCNN来说不是最好的选择。 小批量采样。在微调期间，每个SGD的小批量由N=2N=2个图像构成，均匀地随机选择（如通常的做法，我们实际上迭代数据集的排列）。 我们使用大小为R=128R=128的小批量，从每个图像采样64个RoI。 如在3中，我们从候选框中获取25％的RoI，这些候选框与检测框真值的IoU至少为0.5。 这些RoI只包括用前景对象类标记的样本，即u≥1u≥1。 剩余的RoI从候选框中采样，该候选框与检测框真值的最大IoU在区间[0.1,0.5)[0.1,0.5)上5。 这些是背景样本，并用u=0u=0标记。0.1的阈值下限似乎充当难负样本重训练的启发式算法11。 在训练期间，图像以概率0.5水平翻转。不使用其他数据增强。 通过RoI池化层的反向传播。反向传播通过RoI池化层。为了清楚起见，我们假设每个小批量(N=1N=1)只有一个图像，扩展到N&gt;1N&gt;1是显而易见的，因为前向传播独立地处理所有图像。 令xi∈ℝxi∈R是到RoI池化层的第ii个激活输入，并且令yrjyrj是来自第rr个RoI层的第jj个输出。RoI池化层计算yrj=xi∗(r,j)yrj=xi∗(r,j)，其中xi∗(r,j)=argmaxi′∈(r,j)xi′xi∗(r,j)=argmaxi′∈R(r,j)xi′。(r,j)R(r,j)是输出单元yrjyrj最大池化的子窗口中的输入的索引集合。单个xixi可以被分配给几个不同的输出yrjyrj。 RoI池化层反向传播函数通过遵循argmax switches来计算关于每个输入变量xixi的损失函数的偏导数：∂L∂xi=∑r∑j[i=i∗(r,j)]∂L∂yrj(4)(4)∂L∂xi=∑r∑j[i=i∗(r,j)]∂L∂yrj换句话说，对于每个小批量RoI rr和对于每个池化输出单元yrjyrj，如果ii是yrjyrj通过最大池化选择的argmax，则将这个偏导数∂L∂yrj∂L∂yrj积累下来。在反向传播中，偏导数∂L∂yrj∂L∂yrj已经由RoI池化层顶部的层的反向传播函数计算。 SGD超参数。用于Softmax分类和检测框回归的全连接层的权重分别使用具有方差0.01和0.001的零均值高斯分布初始化。偏置初始化为0。所有层的权重学习率为1倍的全局学习率，偏置为2倍的全局学习率，全局学习率为0.001。 当对VOC07或VOC12 trainval训练时，我们运行SGD进行30k次小批量迭代，然后将学习率降低到0.0001，再训练10k次迭代。当我们训练更大的数据集，我们运行SGD更多的迭代，如下文所述。 使用0.9的动量和0.0005的参数衰减（权重和偏置）。 尺度不变性 我们探索两种实现尺度不变对象检测的方法：（1）通过“brute force”学习和（2）通过使用图像金字塔。 这些策略遵循5中的两种方法。 在“brute force”方法中，在训练和测试期间以预定义的像素大小处理每个图像。网络必须直接从训练数据学习尺度不变性目标检测。 相反，多尺度方法通过图像金字塔向网络提供近似尺度不变性。 在测试时，图像金字塔用于大致缩放-规范化每个候选框。 在多尺度训练期间，我们在每次图像采样时随机采样金字塔尺度，遵循5，作为数据增强的形式。由于GPU内存限制，我们只对较小的网络进行多尺度训练。 Fast R-CNN检测 一旦Fast R-CNN网络被微调完毕，检测相当于运行前向传播（假设候选框是预先计算的）。网络将图像（或图像金字塔，编码为图像列表）和待计算概率的RR个候选框的列表作为输入。在测试的时候，RR通常在2000左右，虽然我们将考虑将它变大（约45k）的情况。当使用图像金字塔时，每个RoI被缩放，使其最接近5中的22422242个像素。 对于每个测试的RoI rr，正向传播输出类别后验概率分布pp和相对于rr的预测的检测框框偏移集合（KK个类别中的每一个获得其自己的精细检测框预测）。我们使用估计的概率Pr(class=k|r)≜pkPr(class=k|r)≜pk为每个对象类别kk分配rr的检测置信度。然后，我们使用R-CNN算法的设置和对每个类别独立执行非最大抑制3。 使用截断的SVD来进行更快的检测 对于整体图像分类，与卷积层相比，计算全连接层花费的时间较小。相反，为了检测，要处理的RoI的数量很大，并且接近一半的正向传递时间用于计算全连接层（参见图2）。大的全连接层容易通过用截短的SVD压缩来加速1213。 在这种技术中，层的u×vu×v权重矩阵WW通过SVD被近似分解为：W≈UΣtVT(5)(5)W≈UΣtVT在这种分解中，UU是一个u×tu×t的矩阵，包括WW的前tt个左奇异向量，ΣtΣt是t×tt×t对角矩阵，其包含WW的前tt个奇异值，并且VV是v×tv×t矩阵，包括WW的前tt个右奇异向量。截断SVD将参数计数从uvuv减少到t(u+v)t(u+v)个，如果tt远小于min(u,v)min(u,v)，则SVD可能是重要的。 为了压缩网络，对应于WW的单个全连接层由两个全连接层替代，在它们之间没有非线性。这些层中的第一层使用权重矩阵ΣtVTΣtVT（没有偏置），并且第二层使用UU（其中原始偏差与WW相关联）。当RoI的数量大时，这种简单的压缩方法给出良好的加速。 主要结果 三个主要结果支持本文的贡献： VOC07，2010和2012的最高的mAP。 相比R-CNN，SPPnet，快速训练和测试。 在VGG16中微调卷积层改善了mAP。 实验配置 我们的实验使用了三个经过预训练的ImageNet网络模型，这些模型可以在线获得(https://github.com/BVLC/caffe/wiki/Model-Zoo)。第一个是来自R-CNN3的CaffeNet（实质上是AlexNet1）。 我们将这个CaffeNet称为模型S，即小模型。第二网络是来自14的VGG_CNN_M_1024，其具有与S相同的深度，但是更宽。 我们把这个网络模型称为M，即中等模型。最后一个网络是来自15的非常深的VGG16模型。由于这个模型是最大的，我们称之为L。在本节中，所有实验都使用单尺度训练和测试（s=600s=600，详见尺度不变性：暴力或精细？）。 VOC 2010和2012数据集结果 表2. VOC 2010测试检测平均精度（％）。 BabyLearning使用基于16的网络。 所有其他方法使用VGG16。训练集：12：VOC12 trainval，Prop.：专有数据集，12+seg：12具有分段注释，07++12：VOC07 trainval，VOC07测试和VOC12 trainval的联合。 表3. VOC 2012测试检测平均精度（％）。 BabyLearning和NUS_NIN_c2000使用基于16的网络。 所有其他方法使用VGG16。训练设置：见表2，Unk.：未知。 如上表（表2，表3）所示，在这些数据集上，我们比较Fast R-CNN（简称FRCN）和公共排行榜中comp4（外部数据）上的主流方法（http://host.robots.ox.ac.uk:8080/leaderboard ，访问时间是2015.4.18）。对于NUS_NIN_c2000和BabyLearning方法，目前没有其架构的确切信息，它们是Network-in-Network的变体16。所有其他方法从相同的预训练VGG16网络初始化。 Fast R-CNN在VOC12上获得最高结果，mAP为65.7％（加上额外数据为68.4％）。它也比其他方法快两个数量级，这些方法都基于比较“慢”的R-CNN网络。在VOC10上，SegDeepM 6获得了比Fast R-CNN更高的mAP（67.2％对比66.1％）。SegDeepM使用VOC12 trainval训练集训练并添加了分割的标注，它被设计为通过使用马尔可夫随机场推理R-CNN检测和来自O2PO2P17的语义分割方法的分割来提高R-CNN精度。Fast R-CNN可以替换SegDeepM中使用的R-CNN，这可以导致更好的结果。当使用放大的07++12训练集（见表2标题）时，Fast R-CNN的mAP增加到68.8％，超过SegDeepM。 VOC 2007数据集上的结果 在VOC07数据集上，我们比较Fast R-CNN与R-CNN和SPPnet的mAP。 所有方法从相同的预训练VGG16网络开始，并使用边界框回归。 VGG16 SPPnet结果由5的作者提供。SPPnet在训练和测试期间使用五个尺度。Fast R-CNN对SPPnet的改进说明，即使Fast R-CNN使用单个尺度训练和测试，卷积层微调在mAP中提供了大的改进（从63.1％到66.9％）。R-CNN的mAP为66.0％。 作为次要点，SPPnet在PASCAL中没有使用被标记为“困难”的样本进行训练。 除去这些样本，Fast R-CNN 的mAP为68.1％。 所有其他实验都使用被标记为“困难”的样本。 训练和测试时间 表4. Fast RCNN，R-CNN和SPPnet中相同模型之间的运行时间比较。Fast R-CNN使用单尺度模式。SPPnet使用5中指定的五个尺度，由5的作者提供在Nvidia K40 GPU上的测量时间。 快速的训练和测试是我们的第二个主要成果。表4比较了Fast RCNN，R-CNN和SPPnet之间的训练时间（小时），测试速率（每秒图像数）和VOC07上的mAP。对于VGG16，没有截断SVD的Fast R-CNN处理图像比R-CNN快146倍，有截断SVD的R-CNN快213倍。训练时间减少9倍，从84小时减少到9.5小时。与SPPnet相比，没有截断SVD的Fast RCNN训练VGG16网络比SPPnet快2.7倍（9.5小时对25.5小时），测试时间快7倍，有截断SVD的Fast RCNN比的SPPnet快10倍。 Fast R-CNN还不需要数百GB的磁盘存储，因为它不缓存特征。 截断SVD。截断的SVD可以将检测时间减少30％以上，同时在mAP中只有很小（0.3个百分点）的下降，并且无需在模型压缩后执行额外的微调。 图2. 截断SVD之前和之后VGG16的时间分布。在SVD之前，完全连接的层fc6和fc7需要45％的时间。 图2示出了如何使用来自VGG16的fc6层中的25088×409625088×4096矩阵的顶部1024个奇异值和来自fc7层的4096×40964096×4096矩阵的顶部256个奇异值减少运行时间，而在mAP中几乎没有损失。如果在压缩之后再次微调，则可以在mAP中具有更小的下降的情况下进一步加速。 微调哪些层？ 对于在SPPnet论文5中考虑的不太深的网络，仅微调全连接层似乎足以获得良好的精度。我们假设这个结果不适用于非常深的网络。为了验证微调卷积层对于VGG16的重要性，我们使用Fast R-CNN微调，但冻结十三个卷积层，以便只有全连接层学习。这种消融模拟单尺度SPPnet训练，将mAP从66.9％降低到61.4％（表5）。这个实验验证了我们的假设：通过RoI池化层的训练对于非常深的网是重要的。 表5. 限制哪些层对VGG16进行微调产生的影响。微调≥≥fc6模拟单尺度SPPnet训练算法5。 SPPnet L是使用五个尺度，以显著（7倍）的速度成本获得的结果。 这是否意味着所有卷积层应该微调？没有。在较小的网络（S和M）中，我们发现conv1（第一个卷积层）是通用的和任务独立的（一个众所周知的事实1）。允许conv1学习或不学习，对mAP没有很有意义的影响。对于VGG16，我们发现只需要更新conv3_1及以上（13个卷积层中的9个）的层。这种观察是实用的：（1）从conv2_1更新使训练变慢1.3倍（12.5小时对比9.5小时）和（2）从conv1_1更新GPU内存不够用。当从conv2_1学习时mAP仅为增加0.3个点（表5，最后一列）。 所有Fast R-CNN在本文中结果都使用VGG16微调层conv3_1及以上的层，所有实验用模型S和M微调层conv2及以上的层。 设计评估 我们通过实验来了解Fast RCNN与R-CNN和SPPnet的比较，以及评估设计决策。按照最佳实践，我们在PASCAL VOC07数据集上进行了这些实验。 多任务训练有用吗？ 多任务训练是方便的，因为它避免管理顺序训练任务的流水线。但它也有可能改善结果，因为任务通过共享的表示（ConvNet）18相互影响。多任务训练能提高Fast R-CNN中的目标检测精度吗？ 为了测试这个问题，我们训练仅使用公式(1)(1)中的分类损失LclsLcls（即设置λ=0λ=0）的基准网络。这些基线是表6中每组的第一列。请注意，这些模型没有检测框回归。接下来（每组的第二列），是我们采用多任务损失（公式(1)(1)，λ=1λ=1）训练的网络，但是我们在测试时禁用检测框回归。这隔离了网络的分类准确性，并允许与基准网络的apple to apple的比较。 在所有三个网络中，我们观察到多任务训练相对于单独的分类训练提高了纯分类精度。改进范围从+0.8到+1.1 个mAP点，显示了多任务学习的一致的积极效果。 最后，我们采用基线模型（仅使用分类损失进行训练），加上检测回归层，并使用LlocLloc训练它们，同时保持所有其他网络参数冻结。每组中的第三列显示了这种逐级训练方案的结果：mAP相对于第一列改进，但逐级训练表现不如多任务训练（每组第四列）。 表6. 多任务训练（每组第四列）改进了分段训练（每组第三列）的mAP。 尺度不变性：暴力或精细？ 我们比较两个策略实现尺度不变物体检测：暴力学习（单尺度）和图像金字塔（多尺度）。在任一情况下，我们将图像的尺度ss定义为其最短边的长度。 所有单尺度实验使用s=600s=600像素，对于一些图像，ss可以小于600，因为我们保持横纵比缩放图像，并限制其最长边为1000像素。选择这些值使得VGG16在微调期间不至于GPU内存不足。较小的模型占用显存更少，所以可受益于较大的ss值。然而，每个模型的优化不是我们的主要的关注点。我们注意到PASCAL图像是384×473像素的，因此单尺度设置通常以1.6倍的倍数上采样图像。因此，RoI池化层的平均有效步进为约10像素。 在多尺度设置中，我们使用5中指定的相同的五个尺度（s∈{480,576,688,864,1200}s∈{480,576,688,864,1200}）以方便与SPPnet进行比较。但是，我们以2000像素为上限，以避免GPU内存不足。 表7显示了当使用一个或五个尺度进行训练和测试时的模型S和M的结果。也许在5中最令人惊讶的结果是单尺度检测几乎与多尺度检测一样好。我们的研究结果能证明他们的结果：深度卷积网络擅长直接学习尺度不变性。多尺度方法消耗大量的计算时间仅带来了很小的mAP增加（表7）。在VGG16（模型L）的情况下，我们受限于实施细节仅能使用单个尺度。然而，它得到了66.9％的mAP，略高于R-CNN的66.0％19，尽管R-CNN在每个候选区域被缩放为规范大小，在意义上使用了“无限”尺度。 由于单尺度处理提供速度和精度之间的最佳折衷，特别是对于非常深的模型，本小节以外的所有实验使用单尺度训练和测试，s=600s=600像素。 表7. 多尺度与单尺度。SPPnet ZF（类似于模型S）的结果来自5。 具有单尺度的较大网络提供最佳的速度/精度平衡。（L在我们的实现中不能使用多尺度，因为GPU内存限制。） 我们需要更过训练数据吗？ 当提供更多的训练数据时，好的目标检测器应该会得到改善。 Zhu等人20发现DPM11mAP在只有几百到千个训练样本的时候就饱和了。在这里我们增加VOC07 trainval训练集与VOC12 trainval训练集，大约增加到三倍的图像，数量达到16.5k，以评估Fast R-CNN。扩大训练集提高了VOC07测试的mAP，从66.9％到70.0％（表1）。 当对这个数据集进行训练时，我们使用60k次小批量迭代而不是40k。 表1. VOC 2007测试检测平均精度（％）。 所有方法都使用VGG16。 训练集：07：VOC07 trainval，07 \diff：07没有“困难”的样本，07 + 12：07和VOC12训练的联合。 SPPnet结果由5的作者提供。 我们对VOC10和2012进行类似的实验，我们用VOC07 trainval，test和VOC12 trainval构造了21.5k图像的数据集。当训练这个数据集时，我们使用100k次SGD迭代和每40k次迭代（而不是每30k次）降低学习率10倍。对于VOC10和2012，mAP分别从66.1％提高到68.8％和从65.7％提高到68.4％。 SVM分类是否优于Softmax？ Fast R-CNN在微调期间使用softmax分类器学习，而不是如在R-CNN和SPPnet中训练线性SVM。为了理解这种选择的影响，我们在Fast R-CNN中实施了具有难负采样重训练的SVM训练。我们使用与R-CNN中相同的训练算法和超参数。 表8. 用Softmax的Fast R-CNN对比用SVM的Fast RCNN（VOC07 mAP）。 对于所有三个网络，Softmax略优于SVM，mAP分别提高了0.1和0.8个点。这种效应很小，但是它表明与先前的多级训练方法相比，“一次性”微调是足够的。我们注意到，Softmax，不像SVM那样，在分类RoI时引入类之间的竞争。 更多的候选区域更好吗？ 存在（广义地）两种类型的目标检测器：使用候选区域的稀疏集合（例如，选择性搜索21）和使用密集集合（例如DPM11）。分类稀疏提议是级联的一种类型22，其中提议机制首先拒绝大量候选者，让分类器来评估留下的小集合。当应用于DPM检测时，该级联提高了检测精度21。我们发现提案分类器级联也提高了Fast R-CNN的精度。 使用选择性搜索的质量模式，我们扫描每个图像1k到10k个候选框，每次重新训练和重新测试模型M.如果候选框纯粹扮演计算的角色，增加每个图像的候选框数量不应该损害mAP。 图3. 各种候选区域方案的VOC07测试mAP和AR。 我们发现mAP上升，然后随着候选区域计数增加而略微下降（图3，实线蓝线）。这个实验表明，用更多的候选区域没有帮助，甚至稍微有点伤害准确性。 如果不实际运行实验，这个结果很难预测。用于测量候选区域质量的最先进的技术是平均召回率(AR)23。当对每个图像使用固定数量的候选区域时，AR与使用R-CNN的几种候选区域方法良好地相关。图3示出了AR（实线红线）与mAP不相关，因为每个图像的候选区域数量是变化的。AR必须小心使用，由于更多的候选区域更高的AR并不意味着mAP会增加。幸运的是，使用模型M的训练和测试需要不到2.5小时。因此，Fast R-CNN能够高效地，直接地评估目标候选区域mAP，这优于代理度量。 我们还调查Fast R-CNN当使用密集生成框（在缩放，位置和宽高比上），大约45k个框/图像。这个密集集足够丰富，当每个选择性搜索框被其最近（IoU）密集框替换时，mAP只降低1个点（到57.7％，图3，蓝色三角形）。 密集框的统计数据与选择性搜索框的统计数据不同。从2k个选择性搜索框开始，我们在添加1000×{2,4,6,8,10,32,45}1000×{2,4,6,8,10,32,45}的随机样本密集框时测试mAP。对于每个实验，我们重新训练和重新测试模型M。当添加这些密集框时，mAP比添加更多选择性搜索框时下降得更强，最终达到53.0％。 我们还训练和测试Fast R-CNN只使用密集框（45k/图像）。此设置的mAP为52.9％（蓝色菱形）。最后，我们检查是否需要使用难样本重训练的SVM来处理密集框分布。 SVM做得更糟：49.3％（蓝色圆圈）。 MS COCO初步结果 我们将fast R-CNN（使用VGG16）应用于MS COCO数据集24，以建立初步基线。我们对80k图像训练集进行了240k次迭代训练，并使用评估服务器对“test-dev”集进行评估。 PASCAL标准下的mAP为35.9％;。新的COCO标准下的AP（也平均）为19.7％。 结论 本文提出Fast R-CNN，一个对R-CNN和SPPnet干净，快速的更新。 除了报告目前的检测结果之外，我们还提供了详细的实验，希望提供新的见解。 特别值得注意的是，稀疏目标候选区域似乎提高了检测器的质量。 过去探索这个问题过于昂贵（在时间上），但Fast R-CNN使其变得可能。当然，可能存在允许密集盒执行以及稀疏候选框的尚未发现的技术。这样的方法如果被开发，可以帮助进一步加速目标检测。 致谢：感谢Kaiming He，Larry Zitnick和Piotr Dollár的帮助和鼓励。 参考文献： A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012. ↩ ↩2 ↩3 ↩4 Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comp., 1989. ↩ R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. ↩ ↩2 ↩3 ↩4 ↩5 ↩6 ↩7 ↩8 ↩9 P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In ICLR, 2014. ↩ ↩2 ↩3 K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. ↩ ↩2 ↩3 ↩4 ↩5 ↩6 ↩7 ↩8 ↩9 ↩10 ↩11 ↩12 ↩13 ↩14 ↩15 ↩16 ↩17 ↩18 ↩19 ↩20 ↩21 Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler. segDeepM: Exploiting segmentation and context in deep neural networks for object detection. In CVPR, 2015. ↩ ↩2 S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR, 2006. ↩ Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proc. of the ACM International Conf. on Multimedia, 2014. ↩ J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009. ↩ D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014. ↩ ↩2 P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. TPAMI, 2010. ↩ ↩2 ↩3 E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In NIPS, 2014. ↩ J. Xue, J. Li, and Y. Gong. Restructuring of deep neural network acoustic models with singular value decomposition. In Interspeech, 2013. ↩ K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep into convolutional nets. In BMVC, 2014. ↩ K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. ↩ M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, 2014. ↩ ↩2 ↩3 J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Semantic segmentation with second-order pooling. In ECCV, 2012. ↩ R. Caruana. Multitask learning. Machine learning, 28(1), 1997. ↩ R. Girshick, J. Donahue, T. Darrell, and J. Malik. Region-based convolutional networks for accurate object detection and segmentation. TPAMI, 2015. ↩ X. Zhu, C. Vondrick, D. Ramanan, and C. Fowlkes. Do we need more training data or better models for object detection? In BMVC, 2012. ↩ J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. IJCV, 2013. ↩ ↩2 P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In CVPR, 2001. ↩ J. H. Hosang, R. Benenson, P. Dollár, and B. Schiele. What makes for effective detection proposals? arXiv preprint arXiv:1502.05082, 2015. ↩ T. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: common objects in context. arXiv e-prints, arXiv:1405.0312 [cs.CV], 2014. ↩ original translation link" />
<meta property="og:description" content="Fast R-CNN Ross Girshick Microsoft Research rbg@microsoft.com 摘要 本文提出了一种快速的基于区域的卷积网络方法（fast R-CNN）用于目标检测。Fast R-CNN建立在以前使用的深卷积网络有效地分类目标的成果上。相比于之前的成果，Fast R-CNN采用了多项创新提高训练和测试速度来提高检测精度。Fast R-CNN训练非常深的VGG16网络比R-CNN快9倍，测试时间快213倍，并在PASCAL VOC上得到更高的精度。与SPPnet相比，fast R-CNN训练VGG16网络比他快3倍，测试速度快10倍，并且更准确。Fast R-CNN的Python和C ++（使用Caffe）实现以MIT开源许可证发布在：https://github.com/rbgirshick/fast-rcnn。 简介 最近，深度卷积网络1 2已经显著提高了图像分类1和目标检测3 4的准确性。与图像分类相比，目标检测是一个更具挑战性的任务，需要更复杂的方法来解决。由于这种复杂性，当前的方法（例如，3 5 4 6）采用多级流水线的方式训练模型，既慢且精度不高。 复杂性的产生是因为检测需要目标的精确定位，这就导致两个主要的难点。首先，必须处理大量候选目标位置（通常称为“提案”）。 第二，这些候选框仅提供粗略定位，其必须被精细化以实现精确定位。 这些问题的解决方案经常会影响速度，准确性或简单性。 在本文中，我们简化了最先进的基于卷积网络的目标检测器的训练过程3 5。我们提出一个单阶段训练算法，联合学习候选框分类和修正他们的空间位置。 所得到的方法用来训练非常深的检测网络（例如VGG16） 比R-CNN快9倍，比SPPnet快3倍。在运行时，检测网络在PASCAL VOC 2012数据集上实现最高准确度，其中mAP为66％（R-CNN为62％），每张图像处理时间为0.3秒，不包括候选框的生成（所有的时间都是使用一个超频到875MHz的Nvidia K40 GPU测试的）。 R-CNN与SPPnet 基于区域的卷积网络方法（RCNN）通过使用深度卷积网络来分类目标候选框，获得了很高的目标检测精度。然而，R-CNN具有显着的缺点： **训练过程是多级流水线。**R-CNN首先使用目标候选框对卷积神经网络使用log损失进行微调。然后，它将卷积神经网络得到的特征送入SVM。 这些SVM作为目标检测器，替代通过微调学习的softmax分类器。 在第三个训练阶段，学习检测框回归。 训练在时间和空间上是的开销很大。对于SVM和检测框回归训练，从每个图像中的每个目标候选框提取特征，并写入磁盘。对于非常深的网络，如VGG16，这个过程在单个GPU上需要2.5天（VOC07 trainval上的5k个图像）。这些特征需要数百GB的存储空间。 目标检测速度很慢。在测试时，从每个测试图像中的每个目标候选框提取特征。用VGG16网络检测目标每个图像需要47秒（在GPU上）。 R-CNN很慢是因为它为每个目标候选框进行卷积神经网络正向传递，而不共享计算。SPPnet5通过共享计算加速R-CNN。SPPnet5计算整个输入图像的卷积特征图，然后使用从共享特征图提取的特征向量来对每个候选框进行分类。通过最大池化将候选框内的特征图转化为固定大小的输出（例如，6X6）来提取针对候选框的特征。多个输出被池化，然后连接成空间金字塔池7。SPPnet在测试时将R-CNN加速10到100倍。由于更快的候选框特征提取训练时间也减少3倍。 SPP网络也有显著的缺点。像R-CNN一样，训练过程是一个多级流水线，涉及提取特征，使用log损失对网络进行微调，训练SVM分类器，最后拟合检测框回归。特征也写入磁盘。但与R-CNN不同，在5中提出的微调算法不能更新在空间金字塔池之前的卷积层。不出所料，这种限制（固定的卷积层）限制了深层网络的精度。 贡献 我们提出一种新的训练算法，修正R-CNN和SPPnet的缺点，同时提高其速度和准确性。因为它能比较快地进行训练和测试，我们称之为Fast R-CNN。Fast RCNN方法有以下几个优点： 比R-CNN和SPPnet具有更高的目标检测精度（mAP）。 训练是使用多任务损失的单阶段训练。 训练可以更新所有网络层参数。 不需要磁盘空间缓存特征。 Fast R-CNN使用Python和C++(Caffe8)语言编写，以MIT开源许可证发布在：https://github.com/rbgirshick/fast-rcnn。 Fast R-CNN架构与训练 Fast R-CNN的架构如下图（图1）所示： 图1. Fast R-CNN架构。输入图像和多个感兴趣区域（RoI）被输入到全卷积网络中。每个RoI被池化到固定大小的特征图中，然后通过全连接层（FC）映射到特征向量。网络对于每个RoI具有两个输出向量：Softmax概率和每类检测框回归偏移量。该架构是使用多任务丢失端到端训练的。 Fast R-CNN网络将整个图像和一组候选框作为输入。网络首先使用几个卷积层（conv）和最大池化层来处理整个图像，以产生卷积特征图。然后，对于每个候选框，RoI池化层从特征图中提取固定长度的特征向量。每个特征向量被送入一系列全连接（fc）层中，其最终分支成两个同级输出层 ：一个输出KK个类别加上1个背景类别的Softmax概率估计，另一个为KK个类别的每一个类别输出四个实数值。每组4个值表示KK个类别的一个类别的检测框位置的修正。 RoI池化层 RoI池化层使用最大池化将任何有效的RoI内的特征转换成具有H×WH×W（例如，7×77×7）的固定空间范围的小特征图，其中HH和WW是层的超参数，独立于任何特定的RoI。在本文中，RoI是卷积特征图中的一个矩形窗口。 每个RoI由指定其左上角(r,c)(r,c)及其高度和宽度(h,w)(h,w)的四元组(r,c,h,w)(r,c,h,w)定义。 RoI最大池化通过将大小为h×wh×w的RoI窗口分割成H×WH×W个网格，子窗口大小约为h/H×w/Wh/H×w/W，然后对每个子窗口执行最大池化，并将输出合并到相应的输出网格单元中。同标准的最大池化一样，池化操作独立应用于每个特征图通道。RoI层只是SPPnets 5中使用的空间金字塔池层的特殊情况，其只有一个金字塔层。 我们使用5中给出的池化子窗口计算方法。 从预训练网络初始化 我们实验了三个预训练的ImageNet9网络，每个网络有五个最大池化层和五到十三个卷积层（网络详细信息，请参见实验配置）。当预训练网络初始化fast R-CNN网络时，其经历三个变换。 首先，最后的最大池化层由RoI池层代替，其将H和W设置为与网络的第一个全连接层兼容的配置（例如，对于VGG16，H=W=7H=W=7）。 然后，网络的最后一格全连接层和Softmax（其被训练用于1000类ImageNet分类）被替换为前面描述的两个同级层（全连接层和K+1K+1个类别的Softmax以及类别特定的检测框回归）。 最后，网络被修改为采用两个数据输入：图像的列表和这些图像中的RoI的列表。 微调 用反向传播训练所有网络权重是Fast R-CNN的重要能力。首先，让我们阐明为什么SPPnet无法更新低于空间金字塔池化层的权重。 根本原因是当每个训练样本（即RoI）来自不同的图像时，通过SPP层的反向传播是非常低效的，这正是训练R-CNN和SPPnet网络的方法。低效的部分是因为每个RoI可能具有非常大的感受野，通常跨越整个输入图像。由于正向传播必须处理整个感受野，训练输入很大（通常是整个图像）。 我们提出了一种更有效的训练方法，利用训练期间的特征共享。在Fast RCNN网络训练中，随机梯度下降（SGD）的小批量是被分层采样的，首先采样NN个图像，然后从每个图像采样R/NR/N个 RoI。关键的是，来自同一图像的RoI在向前和向后传播中共享计算和内存。减小NN，就减少了小批量的计算。例如，当N=2N=2和R=128R=128时，得到的训练方案比从128幅不同的图采样一个RoI（即R-CNN和SPPnet的策略）快64倍。 这个策略的一个令人担心的问题是它可能导致训练收敛变慢，因为来自相同图像的RoI是相关的。这个问题似乎在实际情况下并不存在，当N=2N=2和R=128R=128时，我们使用比R-CNN更少的SGD迭代就获得了良好的结果。 除了分层采样，Fast R-CNN使用了一个精细的训练过程，在微调阶段联合优化Softmax分类器和检测框回归，而不是分别在三个独立的阶段训练softmax分类器，SVM和回归器3 [^ 11]。 下面将详细描述该过程（损失，小批量采样策略，通过RoI池化层的反向传播和SGD超参数）。 **多任务损失。**Fast R-CNN网络具有两个同级输出层。 第一个输出在K+1K+1个类别上的离散概率分布（每个RoI），p=(p0,…,pK)p=(p0,…,pK)。 通常，通过全连接层的K+1K+1个输出上的Softmax来计算pp。第二个输出层输出检测框回归偏移，tk=(tkx,tky,tkw,tkh)tk=(txk,tyk,twk,thk)，对于由k索引的K个类别中的每一个。 我们使用3中给出的tktk的参数化，其中tktk指定相对于候选框的尺度不变转换和对数空间高度/宽度移位。 每个训练的RoI用类真值uu和检测框回归目标真值vv标记。我们对每个标记的RoI使用多任务损失LL以联合训练分类和检测框回归：L(p,u,tu,v)=Lcls(p,u)+λ[u≥1]Lloc(tu,v)(1)(1)L(p,u,tu,v)=Lcls(p,u)+λ[u≥1]Lloc(tu,v) 其中Lcls(p,u)=−logpuLcls(p,u)=−log⁡pu， 是类真值uu的log损失。 对于类真值uu，第二个损失LlocLloc是定义在检测框回归目标真值元组u,v=(vx,vy,vw,vh)u,v=(vx,vy,vw,vh)和预测元组tu=(tux,tuy,tuw,tuh)tu=(txu,tyu,twu,thu)上的损失。 Iverson括号指示函数[u≥1][u≥1]当u≥1u≥1的时候为值1，否则为0。按照惯例，背景类标记为u=0u=0。对于背景RoI，没有检测框真值的概念，因此LlocLloc被忽略。对于检测框回归，我们使用损失Lloc(tu,v)=∑i∈{x,y,w,h}smoothL1(tui−vi)(2)(2)Lloc(tu,v)=∑i∈{x,y,w,h}smoothL1(tiu−vi)其中：smoothL1(x)={0.5x2|x|−0.5if|x|&lt;1otherwise(3)(3)smoothL1(x)={0.5x2if|x|&lt;1|x|−0.5otherwise是鲁棒的L1L1损失，对于异常值比在R-CNN和SPPnet中使用的L2L2损失更不敏感。当回归目标无界时，具有L2L2损失的训练可能需要仔细调整学习速率，以防止爆炸梯度。公式(3)(3)消除了这种灵敏度。 公式(1)(1)中的超参数λλ控制两个任务损失之间的平衡。我们将回归目标真值vivi归一化为具有零均值和单位方差。所有实验都使用λ=1λ=1。 我们注意到10使用相关损失来训练一个类别无关的目标候选网络。 与我们的方法不同的是10倡导一个分离定位和分类的双网络系统。OverFeat4，R-CNN3和SPPnet5也训练分类器和检测框定位器，但是这些方法使用逐级训练，这对于Fast RCNN来说不是最好的选择。 小批量采样。在微调期间，每个SGD的小批量由N=2N=2个图像构成，均匀地随机选择（如通常的做法，我们实际上迭代数据集的排列）。 我们使用大小为R=128R=128的小批量，从每个图像采样64个RoI。 如在3中，我们从候选框中获取25％的RoI，这些候选框与检测框真值的IoU至少为0.5。 这些RoI只包括用前景对象类标记的样本，即u≥1u≥1。 剩余的RoI从候选框中采样，该候选框与检测框真值的最大IoU在区间[0.1,0.5)[0.1,0.5)上5。 这些是背景样本，并用u=0u=0标记。0.1的阈值下限似乎充当难负样本重训练的启发式算法11。 在训练期间，图像以概率0.5水平翻转。不使用其他数据增强。 通过RoI池化层的反向传播。反向传播通过RoI池化层。为了清楚起见，我们假设每个小批量(N=1N=1)只有一个图像，扩展到N&gt;1N&gt;1是显而易见的，因为前向传播独立地处理所有图像。 令xi∈ℝxi∈R是到RoI池化层的第ii个激活输入，并且令yrjyrj是来自第rr个RoI层的第jj个输出。RoI池化层计算yrj=xi∗(r,j)yrj=xi∗(r,j)，其中xi∗(r,j)=argmaxi′∈(r,j)xi′xi∗(r,j)=argmaxi′∈R(r,j)xi′。(r,j)R(r,j)是输出单元yrjyrj最大池化的子窗口中的输入的索引集合。单个xixi可以被分配给几个不同的输出yrjyrj。 RoI池化层反向传播函数通过遵循argmax switches来计算关于每个输入变量xixi的损失函数的偏导数：∂L∂xi=∑r∑j[i=i∗(r,j)]∂L∂yrj(4)(4)∂L∂xi=∑r∑j[i=i∗(r,j)]∂L∂yrj换句话说，对于每个小批量RoI rr和对于每个池化输出单元yrjyrj，如果ii是yrjyrj通过最大池化选择的argmax，则将这个偏导数∂L∂yrj∂L∂yrj积累下来。在反向传播中，偏导数∂L∂yrj∂L∂yrj已经由RoI池化层顶部的层的反向传播函数计算。 SGD超参数。用于Softmax分类和检测框回归的全连接层的权重分别使用具有方差0.01和0.001的零均值高斯分布初始化。偏置初始化为0。所有层的权重学习率为1倍的全局学习率，偏置为2倍的全局学习率，全局学习率为0.001。 当对VOC07或VOC12 trainval训练时，我们运行SGD进行30k次小批量迭代，然后将学习率降低到0.0001，再训练10k次迭代。当我们训练更大的数据集，我们运行SGD更多的迭代，如下文所述。 使用0.9的动量和0.0005的参数衰减（权重和偏置）。 尺度不变性 我们探索两种实现尺度不变对象检测的方法：（1）通过“brute force”学习和（2）通过使用图像金字塔。 这些策略遵循5中的两种方法。 在“brute force”方法中，在训练和测试期间以预定义的像素大小处理每个图像。网络必须直接从训练数据学习尺度不变性目标检测。 相反，多尺度方法通过图像金字塔向网络提供近似尺度不变性。 在测试时，图像金字塔用于大致缩放-规范化每个候选框。 在多尺度训练期间，我们在每次图像采样时随机采样金字塔尺度，遵循5，作为数据增强的形式。由于GPU内存限制，我们只对较小的网络进行多尺度训练。 Fast R-CNN检测 一旦Fast R-CNN网络被微调完毕，检测相当于运行前向传播（假设候选框是预先计算的）。网络将图像（或图像金字塔，编码为图像列表）和待计算概率的RR个候选框的列表作为输入。在测试的时候，RR通常在2000左右，虽然我们将考虑将它变大（约45k）的情况。当使用图像金字塔时，每个RoI被缩放，使其最接近5中的22422242个像素。 对于每个测试的RoI rr，正向传播输出类别后验概率分布pp和相对于rr的预测的检测框框偏移集合（KK个类别中的每一个获得其自己的精细检测框预测）。我们使用估计的概率Pr(class=k|r)≜pkPr(class=k|r)≜pk为每个对象类别kk分配rr的检测置信度。然后，我们使用R-CNN算法的设置和对每个类别独立执行非最大抑制3。 使用截断的SVD来进行更快的检测 对于整体图像分类，与卷积层相比，计算全连接层花费的时间较小。相反，为了检测，要处理的RoI的数量很大，并且接近一半的正向传递时间用于计算全连接层（参见图2）。大的全连接层容易通过用截短的SVD压缩来加速1213。 在这种技术中，层的u×vu×v权重矩阵WW通过SVD被近似分解为：W≈UΣtVT(5)(5)W≈UΣtVT在这种分解中，UU是一个u×tu×t的矩阵，包括WW的前tt个左奇异向量，ΣtΣt是t×tt×t对角矩阵，其包含WW的前tt个奇异值，并且VV是v×tv×t矩阵，包括WW的前tt个右奇异向量。截断SVD将参数计数从uvuv减少到t(u+v)t(u+v)个，如果tt远小于min(u,v)min(u,v)，则SVD可能是重要的。 为了压缩网络，对应于WW的单个全连接层由两个全连接层替代，在它们之间没有非线性。这些层中的第一层使用权重矩阵ΣtVTΣtVT（没有偏置），并且第二层使用UU（其中原始偏差与WW相关联）。当RoI的数量大时，这种简单的压缩方法给出良好的加速。 主要结果 三个主要结果支持本文的贡献： VOC07，2010和2012的最高的mAP。 相比R-CNN，SPPnet，快速训练和测试。 在VGG16中微调卷积层改善了mAP。 实验配置 我们的实验使用了三个经过预训练的ImageNet网络模型，这些模型可以在线获得(https://github.com/BVLC/caffe/wiki/Model-Zoo)。第一个是来自R-CNN3的CaffeNet（实质上是AlexNet1）。 我们将这个CaffeNet称为模型S，即小模型。第二网络是来自14的VGG_CNN_M_1024，其具有与S相同的深度，但是更宽。 我们把这个网络模型称为M，即中等模型。最后一个网络是来自15的非常深的VGG16模型。由于这个模型是最大的，我们称之为L。在本节中，所有实验都使用单尺度训练和测试（s=600s=600，详见尺度不变性：暴力或精细？）。 VOC 2010和2012数据集结果 表2. VOC 2010测试检测平均精度（％）。 BabyLearning使用基于16的网络。 所有其他方法使用VGG16。训练集：12：VOC12 trainval，Prop.：专有数据集，12+seg：12具有分段注释，07++12：VOC07 trainval，VOC07测试和VOC12 trainval的联合。 表3. VOC 2012测试检测平均精度（％）。 BabyLearning和NUS_NIN_c2000使用基于16的网络。 所有其他方法使用VGG16。训练设置：见表2，Unk.：未知。 如上表（表2，表3）所示，在这些数据集上，我们比较Fast R-CNN（简称FRCN）和公共排行榜中comp4（外部数据）上的主流方法（http://host.robots.ox.ac.uk:8080/leaderboard ，访问时间是2015.4.18）。对于NUS_NIN_c2000和BabyLearning方法，目前没有其架构的确切信息，它们是Network-in-Network的变体16。所有其他方法从相同的预训练VGG16网络初始化。 Fast R-CNN在VOC12上获得最高结果，mAP为65.7％（加上额外数据为68.4％）。它也比其他方法快两个数量级，这些方法都基于比较“慢”的R-CNN网络。在VOC10上，SegDeepM 6获得了比Fast R-CNN更高的mAP（67.2％对比66.1％）。SegDeepM使用VOC12 trainval训练集训练并添加了分割的标注，它被设计为通过使用马尔可夫随机场推理R-CNN检测和来自O2PO2P17的语义分割方法的分割来提高R-CNN精度。Fast R-CNN可以替换SegDeepM中使用的R-CNN，这可以导致更好的结果。当使用放大的07++12训练集（见表2标题）时，Fast R-CNN的mAP增加到68.8％，超过SegDeepM。 VOC 2007数据集上的结果 在VOC07数据集上，我们比较Fast R-CNN与R-CNN和SPPnet的mAP。 所有方法从相同的预训练VGG16网络开始，并使用边界框回归。 VGG16 SPPnet结果由5的作者提供。SPPnet在训练和测试期间使用五个尺度。Fast R-CNN对SPPnet的改进说明，即使Fast R-CNN使用单个尺度训练和测试，卷积层微调在mAP中提供了大的改进（从63.1％到66.9％）。R-CNN的mAP为66.0％。 作为次要点，SPPnet在PASCAL中没有使用被标记为“困难”的样本进行训练。 除去这些样本，Fast R-CNN 的mAP为68.1％。 所有其他实验都使用被标记为“困难”的样本。 训练和测试时间 表4. Fast RCNN，R-CNN和SPPnet中相同模型之间的运行时间比较。Fast R-CNN使用单尺度模式。SPPnet使用5中指定的五个尺度，由5的作者提供在Nvidia K40 GPU上的测量时间。 快速的训练和测试是我们的第二个主要成果。表4比较了Fast RCNN，R-CNN和SPPnet之间的训练时间（小时），测试速率（每秒图像数）和VOC07上的mAP。对于VGG16，没有截断SVD的Fast R-CNN处理图像比R-CNN快146倍，有截断SVD的R-CNN快213倍。训练时间减少9倍，从84小时减少到9.5小时。与SPPnet相比，没有截断SVD的Fast RCNN训练VGG16网络比SPPnet快2.7倍（9.5小时对25.5小时），测试时间快7倍，有截断SVD的Fast RCNN比的SPPnet快10倍。 Fast R-CNN还不需要数百GB的磁盘存储，因为它不缓存特征。 截断SVD。截断的SVD可以将检测时间减少30％以上，同时在mAP中只有很小（0.3个百分点）的下降，并且无需在模型压缩后执行额外的微调。 图2. 截断SVD之前和之后VGG16的时间分布。在SVD之前，完全连接的层fc6和fc7需要45％的时间。 图2示出了如何使用来自VGG16的fc6层中的25088×409625088×4096矩阵的顶部1024个奇异值和来自fc7层的4096×40964096×4096矩阵的顶部256个奇异值减少运行时间，而在mAP中几乎没有损失。如果在压缩之后再次微调，则可以在mAP中具有更小的下降的情况下进一步加速。 微调哪些层？ 对于在SPPnet论文5中考虑的不太深的网络，仅微调全连接层似乎足以获得良好的精度。我们假设这个结果不适用于非常深的网络。为了验证微调卷积层对于VGG16的重要性，我们使用Fast R-CNN微调，但冻结十三个卷积层，以便只有全连接层学习。这种消融模拟单尺度SPPnet训练，将mAP从66.9％降低到61.4％（表5）。这个实验验证了我们的假设：通过RoI池化层的训练对于非常深的网是重要的。 表5. 限制哪些层对VGG16进行微调产生的影响。微调≥≥fc6模拟单尺度SPPnet训练算法5。 SPPnet L是使用五个尺度，以显著（7倍）的速度成本获得的结果。 这是否意味着所有卷积层应该微调？没有。在较小的网络（S和M）中，我们发现conv1（第一个卷积层）是通用的和任务独立的（一个众所周知的事实1）。允许conv1学习或不学习，对mAP没有很有意义的影响。对于VGG16，我们发现只需要更新conv3_1及以上（13个卷积层中的9个）的层。这种观察是实用的：（1）从conv2_1更新使训练变慢1.3倍（12.5小时对比9.5小时）和（2）从conv1_1更新GPU内存不够用。当从conv2_1学习时mAP仅为增加0.3个点（表5，最后一列）。 所有Fast R-CNN在本文中结果都使用VGG16微调层conv3_1及以上的层，所有实验用模型S和M微调层conv2及以上的层。 设计评估 我们通过实验来了解Fast RCNN与R-CNN和SPPnet的比较，以及评估设计决策。按照最佳实践，我们在PASCAL VOC07数据集上进行了这些实验。 多任务训练有用吗？ 多任务训练是方便的，因为它避免管理顺序训练任务的流水线。但它也有可能改善结果，因为任务通过共享的表示（ConvNet）18相互影响。多任务训练能提高Fast R-CNN中的目标检测精度吗？ 为了测试这个问题，我们训练仅使用公式(1)(1)中的分类损失LclsLcls（即设置λ=0λ=0）的基准网络。这些基线是表6中每组的第一列。请注意，这些模型没有检测框回归。接下来（每组的第二列），是我们采用多任务损失（公式(1)(1)，λ=1λ=1）训练的网络，但是我们在测试时禁用检测框回归。这隔离了网络的分类准确性，并允许与基准网络的apple to apple的比较。 在所有三个网络中，我们观察到多任务训练相对于单独的分类训练提高了纯分类精度。改进范围从+0.8到+1.1 个mAP点，显示了多任务学习的一致的积极效果。 最后，我们采用基线模型（仅使用分类损失进行训练），加上检测回归层，并使用LlocLloc训练它们，同时保持所有其他网络参数冻结。每组中的第三列显示了这种逐级训练方案的结果：mAP相对于第一列改进，但逐级训练表现不如多任务训练（每组第四列）。 表6. 多任务训练（每组第四列）改进了分段训练（每组第三列）的mAP。 尺度不变性：暴力或精细？ 我们比较两个策略实现尺度不变物体检测：暴力学习（单尺度）和图像金字塔（多尺度）。在任一情况下，我们将图像的尺度ss定义为其最短边的长度。 所有单尺度实验使用s=600s=600像素，对于一些图像，ss可以小于600，因为我们保持横纵比缩放图像，并限制其最长边为1000像素。选择这些值使得VGG16在微调期间不至于GPU内存不足。较小的模型占用显存更少，所以可受益于较大的ss值。然而，每个模型的优化不是我们的主要的关注点。我们注意到PASCAL图像是384×473像素的，因此单尺度设置通常以1.6倍的倍数上采样图像。因此，RoI池化层的平均有效步进为约10像素。 在多尺度设置中，我们使用5中指定的相同的五个尺度（s∈{480,576,688,864,1200}s∈{480,576,688,864,1200}）以方便与SPPnet进行比较。但是，我们以2000像素为上限，以避免GPU内存不足。 表7显示了当使用一个或五个尺度进行训练和测试时的模型S和M的结果。也许在5中最令人惊讶的结果是单尺度检测几乎与多尺度检测一样好。我们的研究结果能证明他们的结果：深度卷积网络擅长直接学习尺度不变性。多尺度方法消耗大量的计算时间仅带来了很小的mAP增加（表7）。在VGG16（模型L）的情况下，我们受限于实施细节仅能使用单个尺度。然而，它得到了66.9％的mAP，略高于R-CNN的66.0％19，尽管R-CNN在每个候选区域被缩放为规范大小，在意义上使用了“无限”尺度。 由于单尺度处理提供速度和精度之间的最佳折衷，特别是对于非常深的模型，本小节以外的所有实验使用单尺度训练和测试，s=600s=600像素。 表7. 多尺度与单尺度。SPPnet ZF（类似于模型S）的结果来自5。 具有单尺度的较大网络提供最佳的速度/精度平衡。（L在我们的实现中不能使用多尺度，因为GPU内存限制。） 我们需要更过训练数据吗？ 当提供更多的训练数据时，好的目标检测器应该会得到改善。 Zhu等人20发现DPM11mAP在只有几百到千个训练样本的时候就饱和了。在这里我们增加VOC07 trainval训练集与VOC12 trainval训练集，大约增加到三倍的图像，数量达到16.5k，以评估Fast R-CNN。扩大训练集提高了VOC07测试的mAP，从66.9％到70.0％（表1）。 当对这个数据集进行训练时，我们使用60k次小批量迭代而不是40k。 表1. VOC 2007测试检测平均精度（％）。 所有方法都使用VGG16。 训练集：07：VOC07 trainval，07 \diff：07没有“困难”的样本，07 + 12：07和VOC12训练的联合。 SPPnet结果由5的作者提供。 我们对VOC10和2012进行类似的实验，我们用VOC07 trainval，test和VOC12 trainval构造了21.5k图像的数据集。当训练这个数据集时，我们使用100k次SGD迭代和每40k次迭代（而不是每30k次）降低学习率10倍。对于VOC10和2012，mAP分别从66.1％提高到68.8％和从65.7％提高到68.4％。 SVM分类是否优于Softmax？ Fast R-CNN在微调期间使用softmax分类器学习，而不是如在R-CNN和SPPnet中训练线性SVM。为了理解这种选择的影响，我们在Fast R-CNN中实施了具有难负采样重训练的SVM训练。我们使用与R-CNN中相同的训练算法和超参数。 表8. 用Softmax的Fast R-CNN对比用SVM的Fast RCNN（VOC07 mAP）。 对于所有三个网络，Softmax略优于SVM，mAP分别提高了0.1和0.8个点。这种效应很小，但是它表明与先前的多级训练方法相比，“一次性”微调是足够的。我们注意到，Softmax，不像SVM那样，在分类RoI时引入类之间的竞争。 更多的候选区域更好吗？ 存在（广义地）两种类型的目标检测器：使用候选区域的稀疏集合（例如，选择性搜索21）和使用密集集合（例如DPM11）。分类稀疏提议是级联的一种类型22，其中提议机制首先拒绝大量候选者，让分类器来评估留下的小集合。当应用于DPM检测时，该级联提高了检测精度21。我们发现提案分类器级联也提高了Fast R-CNN的精度。 使用选择性搜索的质量模式，我们扫描每个图像1k到10k个候选框，每次重新训练和重新测试模型M.如果候选框纯粹扮演计算的角色，增加每个图像的候选框数量不应该损害mAP。 图3. 各种候选区域方案的VOC07测试mAP和AR。 我们发现mAP上升，然后随着候选区域计数增加而略微下降（图3，实线蓝线）。这个实验表明，用更多的候选区域没有帮助，甚至稍微有点伤害准确性。 如果不实际运行实验，这个结果很难预测。用于测量候选区域质量的最先进的技术是平均召回率(AR)23。当对每个图像使用固定数量的候选区域时，AR与使用R-CNN的几种候选区域方法良好地相关。图3示出了AR（实线红线）与mAP不相关，因为每个图像的候选区域数量是变化的。AR必须小心使用，由于更多的候选区域更高的AR并不意味着mAP会增加。幸运的是，使用模型M的训练和测试需要不到2.5小时。因此，Fast R-CNN能够高效地，直接地评估目标候选区域mAP，这优于代理度量。 我们还调查Fast R-CNN当使用密集生成框（在缩放，位置和宽高比上），大约45k个框/图像。这个密集集足够丰富，当每个选择性搜索框被其最近（IoU）密集框替换时，mAP只降低1个点（到57.7％，图3，蓝色三角形）。 密集框的统计数据与选择性搜索框的统计数据不同。从2k个选择性搜索框开始，我们在添加1000×{2,4,6,8,10,32,45}1000×{2,4,6,8,10,32,45}的随机样本密集框时测试mAP。对于每个实验，我们重新训练和重新测试模型M。当添加这些密集框时，mAP比添加更多选择性搜索框时下降得更强，最终达到53.0％。 我们还训练和测试Fast R-CNN只使用密集框（45k/图像）。此设置的mAP为52.9％（蓝色菱形）。最后，我们检查是否需要使用难样本重训练的SVM来处理密集框分布。 SVM做得更糟：49.3％（蓝色圆圈）。 MS COCO初步结果 我们将fast R-CNN（使用VGG16）应用于MS COCO数据集24，以建立初步基线。我们对80k图像训练集进行了240k次迭代训练，并使用评估服务器对“test-dev”集进行评估。 PASCAL标准下的mAP为35.9％;。新的COCO标准下的AP（也平均）为19.7％。 结论 本文提出Fast R-CNN，一个对R-CNN和SPPnet干净，快速的更新。 除了报告目前的检测结果之外，我们还提供了详细的实验，希望提供新的见解。 特别值得注意的是，稀疏目标候选区域似乎提高了检测器的质量。 过去探索这个问题过于昂贵（在时间上），但Fast R-CNN使其变得可能。当然，可能存在允许密集盒执行以及稀疏候选框的尚未发现的技术。这样的方法如果被开发，可以帮助进一步加速目标检测。 致谢：感谢Kaiming He，Larry Zitnick和Piotr Dollár的帮助和鼓励。 参考文献： A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012. ↩ ↩2 ↩3 ↩4 Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comp., 1989. ↩ R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. ↩ ↩2 ↩3 ↩4 ↩5 ↩6 ↩7 ↩8 ↩9 P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In ICLR, 2014. ↩ ↩2 ↩3 K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. ↩ ↩2 ↩3 ↩4 ↩5 ↩6 ↩7 ↩8 ↩9 ↩10 ↩11 ↩12 ↩13 ↩14 ↩15 ↩16 ↩17 ↩18 ↩19 ↩20 ↩21 Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler. segDeepM: Exploiting segmentation and context in deep neural networks for object detection. In CVPR, 2015. ↩ ↩2 S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR, 2006. ↩ Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proc. of the ACM International Conf. on Multimedia, 2014. ↩ J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009. ↩ D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014. ↩ ↩2 P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. TPAMI, 2010. ↩ ↩2 ↩3 E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In NIPS, 2014. ↩ J. Xue, J. Li, and Y. Gong. Restructuring of deep neural network acoustic models with singular value decomposition. In Interspeech, 2013. ↩ K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep into convolutional nets. In BMVC, 2014. ↩ K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. ↩ M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, 2014. ↩ ↩2 ↩3 J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Semantic segmentation with second-order pooling. In ECCV, 2012. ↩ R. Caruana. Multitask learning. Machine learning, 28(1), 1997. ↩ R. Girshick, J. Donahue, T. Darrell, and J. Malik. Region-based convolutional networks for accurate object detection and segmentation. TPAMI, 2015. ↩ X. Zhu, C. Vondrick, D. Ramanan, and C. Fowlkes. Do we need more training data or better models for object detection? In BMVC, 2012. ↩ J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. IJCV, 2013. ↩ ↩2 P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In CVPR, 2001. ↩ J. H. Hosang, R. Benenson, P. Dollár, and B. Schiele. What makes for effective detection proposals? arXiv preprint arXiv:1502.05082, 2015. ↩ T. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: common objects in context. arXiv e-prints, arXiv:1405.0312 [cs.CV], 2014. ↩ original translation link" />
<link rel="canonical" href="https://mlh.app/2019/04/08/727728.html" />
<meta property="og:url" content="https://mlh.app/2019/04/08/727728.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-08T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Fast R-CNN Ross Girshick Microsoft Research rbg@microsoft.com 摘要 本文提出了一种快速的基于区域的卷积网络方法（fast R-CNN）用于目标检测。Fast R-CNN建立在以前使用的深卷积网络有效地分类目标的成果上。相比于之前的成果，Fast R-CNN采用了多项创新提高训练和测试速度来提高检测精度。Fast R-CNN训练非常深的VGG16网络比R-CNN快9倍，测试时间快213倍，并在PASCAL VOC上得到更高的精度。与SPPnet相比，fast R-CNN训练VGG16网络比他快3倍，测试速度快10倍，并且更准确。Fast R-CNN的Python和C ++（使用Caffe）实现以MIT开源许可证发布在：https://github.com/rbgirshick/fast-rcnn。 简介 最近，深度卷积网络1 2已经显著提高了图像分类1和目标检测3 4的准确性。与图像分类相比，目标检测是一个更具挑战性的任务，需要更复杂的方法来解决。由于这种复杂性，当前的方法（例如，3 5 4 6）采用多级流水线的方式训练模型，既慢且精度不高。 复杂性的产生是因为检测需要目标的精确定位，这就导致两个主要的难点。首先，必须处理大量候选目标位置（通常称为“提案”）。 第二，这些候选框仅提供粗略定位，其必须被精细化以实现精确定位。 这些问题的解决方案经常会影响速度，准确性或简单性。 在本文中，我们简化了最先进的基于卷积网络的目标检测器的训练过程3 5。我们提出一个单阶段训练算法，联合学习候选框分类和修正他们的空间位置。 所得到的方法用来训练非常深的检测网络（例如VGG16） 比R-CNN快9倍，比SPPnet快3倍。在运行时，检测网络在PASCAL VOC 2012数据集上实现最高准确度，其中mAP为66％（R-CNN为62％），每张图像处理时间为0.3秒，不包括候选框的生成（所有的时间都是使用一个超频到875MHz的Nvidia K40 GPU测试的）。 R-CNN与SPPnet 基于区域的卷积网络方法（RCNN）通过使用深度卷积网络来分类目标候选框，获得了很高的目标检测精度。然而，R-CNN具有显着的缺点： **训练过程是多级流水线。**R-CNN首先使用目标候选框对卷积神经网络使用log损失进行微调。然后，它将卷积神经网络得到的特征送入SVM。 这些SVM作为目标检测器，替代通过微调学习的softmax分类器。 在第三个训练阶段，学习检测框回归。 训练在时间和空间上是的开销很大。对于SVM和检测框回归训练，从每个图像中的每个目标候选框提取特征，并写入磁盘。对于非常深的网络，如VGG16，这个过程在单个GPU上需要2.5天（VOC07 trainval上的5k个图像）。这些特征需要数百GB的存储空间。 目标检测速度很慢。在测试时，从每个测试图像中的每个目标候选框提取特征。用VGG16网络检测目标每个图像需要47秒（在GPU上）。 R-CNN很慢是因为它为每个目标候选框进行卷积神经网络正向传递，而不共享计算。SPPnet5通过共享计算加速R-CNN。SPPnet5计算整个输入图像的卷积特征图，然后使用从共享特征图提取的特征向量来对每个候选框进行分类。通过最大池化将候选框内的特征图转化为固定大小的输出（例如，6X6）来提取针对候选框的特征。多个输出被池化，然后连接成空间金字塔池7。SPPnet在测试时将R-CNN加速10到100倍。由于更快的候选框特征提取训练时间也减少3倍。 SPP网络也有显著的缺点。像R-CNN一样，训练过程是一个多级流水线，涉及提取特征，使用log损失对网络进行微调，训练SVM分类器，最后拟合检测框回归。特征也写入磁盘。但与R-CNN不同，在5中提出的微调算法不能更新在空间金字塔池之前的卷积层。不出所料，这种限制（固定的卷积层）限制了深层网络的精度。 贡献 我们提出一种新的训练算法，修正R-CNN和SPPnet的缺点，同时提高其速度和准确性。因为它能比较快地进行训练和测试，我们称之为Fast R-CNN。Fast RCNN方法有以下几个优点： 比R-CNN和SPPnet具有更高的目标检测精度（mAP）。 训练是使用多任务损失的单阶段训练。 训练可以更新所有网络层参数。 不需要磁盘空间缓存特征。 Fast R-CNN使用Python和C++(Caffe8)语言编写，以MIT开源许可证发布在：https://github.com/rbgirshick/fast-rcnn。 Fast R-CNN架构与训练 Fast R-CNN的架构如下图（图1）所示： 图1. Fast R-CNN架构。输入图像和多个感兴趣区域（RoI）被输入到全卷积网络中。每个RoI被池化到固定大小的特征图中，然后通过全连接层（FC）映射到特征向量。网络对于每个RoI具有两个输出向量：Softmax概率和每类检测框回归偏移量。该架构是使用多任务丢失端到端训练的。 Fast R-CNN网络将整个图像和一组候选框作为输入。网络首先使用几个卷积层（conv）和最大池化层来处理整个图像，以产生卷积特征图。然后，对于每个候选框，RoI池化层从特征图中提取固定长度的特征向量。每个特征向量被送入一系列全连接（fc）层中，其最终分支成两个同级输出层 ：一个输出KK个类别加上1个背景类别的Softmax概率估计，另一个为KK个类别的每一个类别输出四个实数值。每组4个值表示KK个类别的一个类别的检测框位置的修正。 RoI池化层 RoI池化层使用最大池化将任何有效的RoI内的特征转换成具有H×WH×W（例如，7×77×7）的固定空间范围的小特征图，其中HH和WW是层的超参数，独立于任何特定的RoI。在本文中，RoI是卷积特征图中的一个矩形窗口。 每个RoI由指定其左上角(r,c)(r,c)及其高度和宽度(h,w)(h,w)的四元组(r,c,h,w)(r,c,h,w)定义。 RoI最大池化通过将大小为h×wh×w的RoI窗口分割成H×WH×W个网格，子窗口大小约为h/H×w/Wh/H×w/W，然后对每个子窗口执行最大池化，并将输出合并到相应的输出网格单元中。同标准的最大池化一样，池化操作独立应用于每个特征图通道。RoI层只是SPPnets 5中使用的空间金字塔池层的特殊情况，其只有一个金字塔层。 我们使用5中给出的池化子窗口计算方法。 从预训练网络初始化 我们实验了三个预训练的ImageNet9网络，每个网络有五个最大池化层和五到十三个卷积层（网络详细信息，请参见实验配置）。当预训练网络初始化fast R-CNN网络时，其经历三个变换。 首先，最后的最大池化层由RoI池层代替，其将H和W设置为与网络的第一个全连接层兼容的配置（例如，对于VGG16，H=W=7H=W=7）。 然后，网络的最后一格全连接层和Softmax（其被训练用于1000类ImageNet分类）被替换为前面描述的两个同级层（全连接层和K+1K+1个类别的Softmax以及类别特定的检测框回归）。 最后，网络被修改为采用两个数据输入：图像的列表和这些图像中的RoI的列表。 微调 用反向传播训练所有网络权重是Fast R-CNN的重要能力。首先，让我们阐明为什么SPPnet无法更新低于空间金字塔池化层的权重。 根本原因是当每个训练样本（即RoI）来自不同的图像时，通过SPP层的反向传播是非常低效的，这正是训练R-CNN和SPPnet网络的方法。低效的部分是因为每个RoI可能具有非常大的感受野，通常跨越整个输入图像。由于正向传播必须处理整个感受野，训练输入很大（通常是整个图像）。 我们提出了一种更有效的训练方法，利用训练期间的特征共享。在Fast RCNN网络训练中，随机梯度下降（SGD）的小批量是被分层采样的，首先采样NN个图像，然后从每个图像采样R/NR/N个 RoI。关键的是，来自同一图像的RoI在向前和向后传播中共享计算和内存。减小NN，就减少了小批量的计算。例如，当N=2N=2和R=128R=128时，得到的训练方案比从128幅不同的图采样一个RoI（即R-CNN和SPPnet的策略）快64倍。 这个策略的一个令人担心的问题是它可能导致训练收敛变慢，因为来自相同图像的RoI是相关的。这个问题似乎在实际情况下并不存在，当N=2N=2和R=128R=128时，我们使用比R-CNN更少的SGD迭代就获得了良好的结果。 除了分层采样，Fast R-CNN使用了一个精细的训练过程，在微调阶段联合优化Softmax分类器和检测框回归，而不是分别在三个独立的阶段训练softmax分类器，SVM和回归器3 [^ 11]。 下面将详细描述该过程（损失，小批量采样策略，通过RoI池化层的反向传播和SGD超参数）。 **多任务损失。**Fast R-CNN网络具有两个同级输出层。 第一个输出在K+1K+1个类别上的离散概率分布（每个RoI），p=(p0,…,pK)p=(p0,…,pK)。 通常，通过全连接层的K+1K+1个输出上的Softmax来计算pp。第二个输出层输出检测框回归偏移，tk=(tkx,tky,tkw,tkh)tk=(txk,tyk,twk,thk)，对于由k索引的K个类别中的每一个。 我们使用3中给出的tktk的参数化，其中tktk指定相对于候选框的尺度不变转换和对数空间高度/宽度移位。 每个训练的RoI用类真值uu和检测框回归目标真值vv标记。我们对每个标记的RoI使用多任务损失LL以联合训练分类和检测框回归：L(p,u,tu,v)=Lcls(p,u)+λ[u≥1]Lloc(tu,v)(1)(1)L(p,u,tu,v)=Lcls(p,u)+λ[u≥1]Lloc(tu,v) 其中Lcls(p,u)=−logpuLcls(p,u)=−log⁡pu， 是类真值uu的log损失。 对于类真值uu，第二个损失LlocLloc是定义在检测框回归目标真值元组u,v=(vx,vy,vw,vh)u,v=(vx,vy,vw,vh)和预测元组tu=(tux,tuy,tuw,tuh)tu=(txu,tyu,twu,thu)上的损失。 Iverson括号指示函数[u≥1][u≥1]当u≥1u≥1的时候为值1，否则为0。按照惯例，背景类标记为u=0u=0。对于背景RoI，没有检测框真值的概念，因此LlocLloc被忽略。对于检测框回归，我们使用损失Lloc(tu,v)=∑i∈{x,y,w,h}smoothL1(tui−vi)(2)(2)Lloc(tu,v)=∑i∈{x,y,w,h}smoothL1(tiu−vi)其中：smoothL1(x)={0.5x2|x|−0.5if|x|&lt;1otherwise(3)(3)smoothL1(x)={0.5x2if|x|&lt;1|x|−0.5otherwise是鲁棒的L1L1损失，对于异常值比在R-CNN和SPPnet中使用的L2L2损失更不敏感。当回归目标无界时，具有L2L2损失的训练可能需要仔细调整学习速率，以防止爆炸梯度。公式(3)(3)消除了这种灵敏度。 公式(1)(1)中的超参数λλ控制两个任务损失之间的平衡。我们将回归目标真值vivi归一化为具有零均值和单位方差。所有实验都使用λ=1λ=1。 我们注意到10使用相关损失来训练一个类别无关的目标候选网络。 与我们的方法不同的是10倡导一个分离定位和分类的双网络系统。OverFeat4，R-CNN3和SPPnet5也训练分类器和检测框定位器，但是这些方法使用逐级训练，这对于Fast RCNN来说不是最好的选择。 小批量采样。在微调期间，每个SGD的小批量由N=2N=2个图像构成，均匀地随机选择（如通常的做法，我们实际上迭代数据集的排列）。 我们使用大小为R=128R=128的小批量，从每个图像采样64个RoI。 如在3中，我们从候选框中获取25％的RoI，这些候选框与检测框真值的IoU至少为0.5。 这些RoI只包括用前景对象类标记的样本，即u≥1u≥1。 剩余的RoI从候选框中采样，该候选框与检测框真值的最大IoU在区间[0.1,0.5)[0.1,0.5)上5。 这些是背景样本，并用u=0u=0标记。0.1的阈值下限似乎充当难负样本重训练的启发式算法11。 在训练期间，图像以概率0.5水平翻转。不使用其他数据增强。 通过RoI池化层的反向传播。反向传播通过RoI池化层。为了清楚起见，我们假设每个小批量(N=1N=1)只有一个图像，扩展到N&gt;1N&gt;1是显而易见的，因为前向传播独立地处理所有图像。 令xi∈ℝxi∈R是到RoI池化层的第ii个激活输入，并且令yrjyrj是来自第rr个RoI层的第jj个输出。RoI池化层计算yrj=xi∗(r,j)yrj=xi∗(r,j)，其中xi∗(r,j)=argmaxi′∈(r,j)xi′xi∗(r,j)=argmaxi′∈R(r,j)xi′。(r,j)R(r,j)是输出单元yrjyrj最大池化的子窗口中的输入的索引集合。单个xixi可以被分配给几个不同的输出yrjyrj。 RoI池化层反向传播函数通过遵循argmax switches来计算关于每个输入变量xixi的损失函数的偏导数：∂L∂xi=∑r∑j[i=i∗(r,j)]∂L∂yrj(4)(4)∂L∂xi=∑r∑j[i=i∗(r,j)]∂L∂yrj换句话说，对于每个小批量RoI rr和对于每个池化输出单元yrjyrj，如果ii是yrjyrj通过最大池化选择的argmax，则将这个偏导数∂L∂yrj∂L∂yrj积累下来。在反向传播中，偏导数∂L∂yrj∂L∂yrj已经由RoI池化层顶部的层的反向传播函数计算。 SGD超参数。用于Softmax分类和检测框回归的全连接层的权重分别使用具有方差0.01和0.001的零均值高斯分布初始化。偏置初始化为0。所有层的权重学习率为1倍的全局学习率，偏置为2倍的全局学习率，全局学习率为0.001。 当对VOC07或VOC12 trainval训练时，我们运行SGD进行30k次小批量迭代，然后将学习率降低到0.0001，再训练10k次迭代。当我们训练更大的数据集，我们运行SGD更多的迭代，如下文所述。 使用0.9的动量和0.0005的参数衰减（权重和偏置）。 尺度不变性 我们探索两种实现尺度不变对象检测的方法：（1）通过“brute force”学习和（2）通过使用图像金字塔。 这些策略遵循5中的两种方法。 在“brute force”方法中，在训练和测试期间以预定义的像素大小处理每个图像。网络必须直接从训练数据学习尺度不变性目标检测。 相反，多尺度方法通过图像金字塔向网络提供近似尺度不变性。 在测试时，图像金字塔用于大致缩放-规范化每个候选框。 在多尺度训练期间，我们在每次图像采样时随机采样金字塔尺度，遵循5，作为数据增强的形式。由于GPU内存限制，我们只对较小的网络进行多尺度训练。 Fast R-CNN检测 一旦Fast R-CNN网络被微调完毕，检测相当于运行前向传播（假设候选框是预先计算的）。网络将图像（或图像金字塔，编码为图像列表）和待计算概率的RR个候选框的列表作为输入。在测试的时候，RR通常在2000左右，虽然我们将考虑将它变大（约45k）的情况。当使用图像金字塔时，每个RoI被缩放，使其最接近5中的22422242个像素。 对于每个测试的RoI rr，正向传播输出类别后验概率分布pp和相对于rr的预测的检测框框偏移集合（KK个类别中的每一个获得其自己的精细检测框预测）。我们使用估计的概率Pr(class=k|r)≜pkPr(class=k|r)≜pk为每个对象类别kk分配rr的检测置信度。然后，我们使用R-CNN算法的设置和对每个类别独立执行非最大抑制3。 使用截断的SVD来进行更快的检测 对于整体图像分类，与卷积层相比，计算全连接层花费的时间较小。相反，为了检测，要处理的RoI的数量很大，并且接近一半的正向传递时间用于计算全连接层（参见图2）。大的全连接层容易通过用截短的SVD压缩来加速1213。 在这种技术中，层的u×vu×v权重矩阵WW通过SVD被近似分解为：W≈UΣtVT(5)(5)W≈UΣtVT在这种分解中，UU是一个u×tu×t的矩阵，包括WW的前tt个左奇异向量，ΣtΣt是t×tt×t对角矩阵，其包含WW的前tt个奇异值，并且VV是v×tv×t矩阵，包括WW的前tt个右奇异向量。截断SVD将参数计数从uvuv减少到t(u+v)t(u+v)个，如果tt远小于min(u,v)min(u,v)，则SVD可能是重要的。 为了压缩网络，对应于WW的单个全连接层由两个全连接层替代，在它们之间没有非线性。这些层中的第一层使用权重矩阵ΣtVTΣtVT（没有偏置），并且第二层使用UU（其中原始偏差与WW相关联）。当RoI的数量大时，这种简单的压缩方法给出良好的加速。 主要结果 三个主要结果支持本文的贡献： VOC07，2010和2012的最高的mAP。 相比R-CNN，SPPnet，快速训练和测试。 在VGG16中微调卷积层改善了mAP。 实验配置 我们的实验使用了三个经过预训练的ImageNet网络模型，这些模型可以在线获得(https://github.com/BVLC/caffe/wiki/Model-Zoo)。第一个是来自R-CNN3的CaffeNet（实质上是AlexNet1）。 我们将这个CaffeNet称为模型S，即小模型。第二网络是来自14的VGG_CNN_M_1024，其具有与S相同的深度，但是更宽。 我们把这个网络模型称为M，即中等模型。最后一个网络是来自15的非常深的VGG16模型。由于这个模型是最大的，我们称之为L。在本节中，所有实验都使用单尺度训练和测试（s=600s=600，详见尺度不变性：暴力或精细？）。 VOC 2010和2012数据集结果 表2. VOC 2010测试检测平均精度（％）。 BabyLearning使用基于16的网络。 所有其他方法使用VGG16。训练集：12：VOC12 trainval，Prop.：专有数据集，12+seg：12具有分段注释，07++12：VOC07 trainval，VOC07测试和VOC12 trainval的联合。 表3. VOC 2012测试检测平均精度（％）。 BabyLearning和NUS_NIN_c2000使用基于16的网络。 所有其他方法使用VGG16。训练设置：见表2，Unk.：未知。 如上表（表2，表3）所示，在这些数据集上，我们比较Fast R-CNN（简称FRCN）和公共排行榜中comp4（外部数据）上的主流方法（http://host.robots.ox.ac.uk:8080/leaderboard ，访问时间是2015.4.18）。对于NUS_NIN_c2000和BabyLearning方法，目前没有其架构的确切信息，它们是Network-in-Network的变体16。所有其他方法从相同的预训练VGG16网络初始化。 Fast R-CNN在VOC12上获得最高结果，mAP为65.7％（加上额外数据为68.4％）。它也比其他方法快两个数量级，这些方法都基于比较“慢”的R-CNN网络。在VOC10上，SegDeepM 6获得了比Fast R-CNN更高的mAP（67.2％对比66.1％）。SegDeepM使用VOC12 trainval训练集训练并添加了分割的标注，它被设计为通过使用马尔可夫随机场推理R-CNN检测和来自O2PO2P17的语义分割方法的分割来提高R-CNN精度。Fast R-CNN可以替换SegDeepM中使用的R-CNN，这可以导致更好的结果。当使用放大的07++12训练集（见表2标题）时，Fast R-CNN的mAP增加到68.8％，超过SegDeepM。 VOC 2007数据集上的结果 在VOC07数据集上，我们比较Fast R-CNN与R-CNN和SPPnet的mAP。 所有方法从相同的预训练VGG16网络开始，并使用边界框回归。 VGG16 SPPnet结果由5的作者提供。SPPnet在训练和测试期间使用五个尺度。Fast R-CNN对SPPnet的改进说明，即使Fast R-CNN使用单个尺度训练和测试，卷积层微调在mAP中提供了大的改进（从63.1％到66.9％）。R-CNN的mAP为66.0％。 作为次要点，SPPnet在PASCAL中没有使用被标记为“困难”的样本进行训练。 除去这些样本，Fast R-CNN 的mAP为68.1％。 所有其他实验都使用被标记为“困难”的样本。 训练和测试时间 表4. Fast RCNN，R-CNN和SPPnet中相同模型之间的运行时间比较。Fast R-CNN使用单尺度模式。SPPnet使用5中指定的五个尺度，由5的作者提供在Nvidia K40 GPU上的测量时间。 快速的训练和测试是我们的第二个主要成果。表4比较了Fast RCNN，R-CNN和SPPnet之间的训练时间（小时），测试速率（每秒图像数）和VOC07上的mAP。对于VGG16，没有截断SVD的Fast R-CNN处理图像比R-CNN快146倍，有截断SVD的R-CNN快213倍。训练时间减少9倍，从84小时减少到9.5小时。与SPPnet相比，没有截断SVD的Fast RCNN训练VGG16网络比SPPnet快2.7倍（9.5小时对25.5小时），测试时间快7倍，有截断SVD的Fast RCNN比的SPPnet快10倍。 Fast R-CNN还不需要数百GB的磁盘存储，因为它不缓存特征。 截断SVD。截断的SVD可以将检测时间减少30％以上，同时在mAP中只有很小（0.3个百分点）的下降，并且无需在模型压缩后执行额外的微调。 图2. 截断SVD之前和之后VGG16的时间分布。在SVD之前，完全连接的层fc6和fc7需要45％的时间。 图2示出了如何使用来自VGG16的fc6层中的25088×409625088×4096矩阵的顶部1024个奇异值和来自fc7层的4096×40964096×4096矩阵的顶部256个奇异值减少运行时间，而在mAP中几乎没有损失。如果在压缩之后再次微调，则可以在mAP中具有更小的下降的情况下进一步加速。 微调哪些层？ 对于在SPPnet论文5中考虑的不太深的网络，仅微调全连接层似乎足以获得良好的精度。我们假设这个结果不适用于非常深的网络。为了验证微调卷积层对于VGG16的重要性，我们使用Fast R-CNN微调，但冻结十三个卷积层，以便只有全连接层学习。这种消融模拟单尺度SPPnet训练，将mAP从66.9％降低到61.4％（表5）。这个实验验证了我们的假设：通过RoI池化层的训练对于非常深的网是重要的。 表5. 限制哪些层对VGG16进行微调产生的影响。微调≥≥fc6模拟单尺度SPPnet训练算法5。 SPPnet L是使用五个尺度，以显著（7倍）的速度成本获得的结果。 这是否意味着所有卷积层应该微调？没有。在较小的网络（S和M）中，我们发现conv1（第一个卷积层）是通用的和任务独立的（一个众所周知的事实1）。允许conv1学习或不学习，对mAP没有很有意义的影响。对于VGG16，我们发现只需要更新conv3_1及以上（13个卷积层中的9个）的层。这种观察是实用的：（1）从conv2_1更新使训练变慢1.3倍（12.5小时对比9.5小时）和（2）从conv1_1更新GPU内存不够用。当从conv2_1学习时mAP仅为增加0.3个点（表5，最后一列）。 所有Fast R-CNN在本文中结果都使用VGG16微调层conv3_1及以上的层，所有实验用模型S和M微调层conv2及以上的层。 设计评估 我们通过实验来了解Fast RCNN与R-CNN和SPPnet的比较，以及评估设计决策。按照最佳实践，我们在PASCAL VOC07数据集上进行了这些实验。 多任务训练有用吗？ 多任务训练是方便的，因为它避免管理顺序训练任务的流水线。但它也有可能改善结果，因为任务通过共享的表示（ConvNet）18相互影响。多任务训练能提高Fast R-CNN中的目标检测精度吗？ 为了测试这个问题，我们训练仅使用公式(1)(1)中的分类损失LclsLcls（即设置λ=0λ=0）的基准网络。这些基线是表6中每组的第一列。请注意，这些模型没有检测框回归。接下来（每组的第二列），是我们采用多任务损失（公式(1)(1)，λ=1λ=1）训练的网络，但是我们在测试时禁用检测框回归。这隔离了网络的分类准确性，并允许与基准网络的apple to apple的比较。 在所有三个网络中，我们观察到多任务训练相对于单独的分类训练提高了纯分类精度。改进范围从+0.8到+1.1 个mAP点，显示了多任务学习的一致的积极效果。 最后，我们采用基线模型（仅使用分类损失进行训练），加上检测回归层，并使用LlocLloc训练它们，同时保持所有其他网络参数冻结。每组中的第三列显示了这种逐级训练方案的结果：mAP相对于第一列改进，但逐级训练表现不如多任务训练（每组第四列）。 表6. 多任务训练（每组第四列）改进了分段训练（每组第三列）的mAP。 尺度不变性：暴力或精细？ 我们比较两个策略实现尺度不变物体检测：暴力学习（单尺度）和图像金字塔（多尺度）。在任一情况下，我们将图像的尺度ss定义为其最短边的长度。 所有单尺度实验使用s=600s=600像素，对于一些图像，ss可以小于600，因为我们保持横纵比缩放图像，并限制其最长边为1000像素。选择这些值使得VGG16在微调期间不至于GPU内存不足。较小的模型占用显存更少，所以可受益于较大的ss值。然而，每个模型的优化不是我们的主要的关注点。我们注意到PASCAL图像是384×473像素的，因此单尺度设置通常以1.6倍的倍数上采样图像。因此，RoI池化层的平均有效步进为约10像素。 在多尺度设置中，我们使用5中指定的相同的五个尺度（s∈{480,576,688,864,1200}s∈{480,576,688,864,1200}）以方便与SPPnet进行比较。但是，我们以2000像素为上限，以避免GPU内存不足。 表7显示了当使用一个或五个尺度进行训练和测试时的模型S和M的结果。也许在5中最令人惊讶的结果是单尺度检测几乎与多尺度检测一样好。我们的研究结果能证明他们的结果：深度卷积网络擅长直接学习尺度不变性。多尺度方法消耗大量的计算时间仅带来了很小的mAP增加（表7）。在VGG16（模型L）的情况下，我们受限于实施细节仅能使用单个尺度。然而，它得到了66.9％的mAP，略高于R-CNN的66.0％19，尽管R-CNN在每个候选区域被缩放为规范大小，在意义上使用了“无限”尺度。 由于单尺度处理提供速度和精度之间的最佳折衷，特别是对于非常深的模型，本小节以外的所有实验使用单尺度训练和测试，s=600s=600像素。 表7. 多尺度与单尺度。SPPnet ZF（类似于模型S）的结果来自5。 具有单尺度的较大网络提供最佳的速度/精度平衡。（L在我们的实现中不能使用多尺度，因为GPU内存限制。） 我们需要更过训练数据吗？ 当提供更多的训练数据时，好的目标检测器应该会得到改善。 Zhu等人20发现DPM11mAP在只有几百到千个训练样本的时候就饱和了。在这里我们增加VOC07 trainval训练集与VOC12 trainval训练集，大约增加到三倍的图像，数量达到16.5k，以评估Fast R-CNN。扩大训练集提高了VOC07测试的mAP，从66.9％到70.0％（表1）。 当对这个数据集进行训练时，我们使用60k次小批量迭代而不是40k。 表1. VOC 2007测试检测平均精度（％）。 所有方法都使用VGG16。 训练集：07：VOC07 trainval，07 \\diff：07没有“困难”的样本，07 + 12：07和VOC12训练的联合。 SPPnet结果由5的作者提供。 我们对VOC10和2012进行类似的实验，我们用VOC07 trainval，test和VOC12 trainval构造了21.5k图像的数据集。当训练这个数据集时，我们使用100k次SGD迭代和每40k次迭代（而不是每30k次）降低学习率10倍。对于VOC10和2012，mAP分别从66.1％提高到68.8％和从65.7％提高到68.4％。 SVM分类是否优于Softmax？ Fast R-CNN在微调期间使用softmax分类器学习，而不是如在R-CNN和SPPnet中训练线性SVM。为了理解这种选择的影响，我们在Fast R-CNN中实施了具有难负采样重训练的SVM训练。我们使用与R-CNN中相同的训练算法和超参数。 表8. 用Softmax的Fast R-CNN对比用SVM的Fast RCNN（VOC07 mAP）。 对于所有三个网络，Softmax略优于SVM，mAP分别提高了0.1和0.8个点。这种效应很小，但是它表明与先前的多级训练方法相比，“一次性”微调是足够的。我们注意到，Softmax，不像SVM那样，在分类RoI时引入类之间的竞争。 更多的候选区域更好吗？ 存在（广义地）两种类型的目标检测器：使用候选区域的稀疏集合（例如，选择性搜索21）和使用密集集合（例如DPM11）。分类稀疏提议是级联的一种类型22，其中提议机制首先拒绝大量候选者，让分类器来评估留下的小集合。当应用于DPM检测时，该级联提高了检测精度21。我们发现提案分类器级联也提高了Fast R-CNN的精度。 使用选择性搜索的质量模式，我们扫描每个图像1k到10k个候选框，每次重新训练和重新测试模型M.如果候选框纯粹扮演计算的角色，增加每个图像的候选框数量不应该损害mAP。 图3. 各种候选区域方案的VOC07测试mAP和AR。 我们发现mAP上升，然后随着候选区域计数增加而略微下降（图3，实线蓝线）。这个实验表明，用更多的候选区域没有帮助，甚至稍微有点伤害准确性。 如果不实际运行实验，这个结果很难预测。用于测量候选区域质量的最先进的技术是平均召回率(AR)23。当对每个图像使用固定数量的候选区域时，AR与使用R-CNN的几种候选区域方法良好地相关。图3示出了AR（实线红线）与mAP不相关，因为每个图像的候选区域数量是变化的。AR必须小心使用，由于更多的候选区域更高的AR并不意味着mAP会增加。幸运的是，使用模型M的训练和测试需要不到2.5小时。因此，Fast R-CNN能够高效地，直接地评估目标候选区域mAP，这优于代理度量。 我们还调查Fast R-CNN当使用密集生成框（在缩放，位置和宽高比上），大约45k个框/图像。这个密集集足够丰富，当每个选择性搜索框被其最近（IoU）密集框替换时，mAP只降低1个点（到57.7％，图3，蓝色三角形）。 密集框的统计数据与选择性搜索框的统计数据不同。从2k个选择性搜索框开始，我们在添加1000×{2,4,6,8,10,32,45}1000×{2,4,6,8,10,32,45}的随机样本密集框时测试mAP。对于每个实验，我们重新训练和重新测试模型M。当添加这些密集框时，mAP比添加更多选择性搜索框时下降得更强，最终达到53.0％。 我们还训练和测试Fast R-CNN只使用密集框（45k/图像）。此设置的mAP为52.9％（蓝色菱形）。最后，我们检查是否需要使用难样本重训练的SVM来处理密集框分布。 SVM做得更糟：49.3％（蓝色圆圈）。 MS COCO初步结果 我们将fast R-CNN（使用VGG16）应用于MS COCO数据集24，以建立初步基线。我们对80k图像训练集进行了240k次迭代训练，并使用评估服务器对“test-dev”集进行评估。 PASCAL标准下的mAP为35.9％;。新的COCO标准下的AP（也平均）为19.7％。 结论 本文提出Fast R-CNN，一个对R-CNN和SPPnet干净，快速的更新。 除了报告目前的检测结果之外，我们还提供了详细的实验，希望提供新的见解。 特别值得注意的是，稀疏目标候选区域似乎提高了检测器的质量。 过去探索这个问题过于昂贵（在时间上），但Fast R-CNN使其变得可能。当然，可能存在允许密集盒执行以及稀疏候选框的尚未发现的技术。这样的方法如果被开发，可以帮助进一步加速目标检测。 致谢：感谢Kaiming He，Larry Zitnick和Piotr Dollár的帮助和鼓励。 参考文献： A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012. ↩ ↩2 ↩3 ↩4 Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comp., 1989. ↩ R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. ↩ ↩2 ↩3 ↩4 ↩5 ↩6 ↩7 ↩8 ↩9 P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In ICLR, 2014. ↩ ↩2 ↩3 K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. ↩ ↩2 ↩3 ↩4 ↩5 ↩6 ↩7 ↩8 ↩9 ↩10 ↩11 ↩12 ↩13 ↩14 ↩15 ↩16 ↩17 ↩18 ↩19 ↩20 ↩21 Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler. segDeepM: Exploiting segmentation and context in deep neural networks for object detection. In CVPR, 2015. ↩ ↩2 S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR, 2006. ↩ Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proc. of the ACM International Conf. on Multimedia, 2014. ↩ J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009. ↩ D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014. ↩ ↩2 P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. TPAMI, 2010. ↩ ↩2 ↩3 E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In NIPS, 2014. ↩ J. Xue, J. Li, and Y. Gong. Restructuring of deep neural network acoustic models with singular value decomposition. In Interspeech, 2013. ↩ K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep into convolutional nets. In BMVC, 2014. ↩ K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. ↩ M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, 2014. ↩ ↩2 ↩3 J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Semantic segmentation with second-order pooling. In ECCV, 2012. ↩ R. Caruana. Multitask learning. Machine learning, 28(1), 1997. ↩ R. Girshick, J. Donahue, T. Darrell, and J. Malik. Region-based convolutional networks for accurate object detection and segmentation. TPAMI, 2015. ↩ X. Zhu, C. Vondrick, D. Ramanan, and C. Fowlkes. Do we need more training data or better models for object detection? In BMVC, 2012. ↩ J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. IJCV, 2013. ↩ ↩2 P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In CVPR, 2001. ↩ J. H. Hosang, R. Benenson, P. Dollár, and B. Schiele. What makes for effective detection proposals? arXiv preprint arXiv:1502.05082, 2015. ↩ T. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: common objects in context. arXiv e-prints, arXiv:1405.0312 [cs.CV], 2014. ↩ original translation link","@type":"BlogPosting","url":"https://mlh.app/2019/04/08/727728.html","headline":"Fast RCNN译文转发","dateModified":"2019-04-08T00:00:00+08:00","datePublished":"2019-04-08T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/04/08/727728.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Fast RCNN译文转发</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div id="article_content" class="article_content clearfix csdn-tracking-statistics"> 
   <div id="content_views" class="markdown_views prism-atom-one-dark"> 
    <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; --> 
    <svg xmlns="http://www.w3.org/2000/svg">
     <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block"></path>
    </svg> 
    <h1 id="fast-r-cnn"><a></a>Fast R-CNN</h1> 
   </div>
  </div>
  <p>Ross Girshick <br> Microsoft Research <br> rbg@microsoft.com</p> 
  <h2 id="摘要"><a></a>摘要</h2> 
  <p>本文提出了一种快速的基于区域的卷积网络方法（fast R-CNN）用于目标检测。Fast R-CNN建立在以前使用的深卷积网络有效地分类目标的成果上。相比于之前的成果，Fast R-CNN采用了多项创新提高训练和测试速度来提高检测精度。Fast R-CNN训练非常深的VGG16网络比R-CNN快9倍，测试时间快213倍，并在PASCAL VOC上得到更高的精度。与SPPnet相比，fast R-CNN训练VGG16网络比他快3倍，测试速度快10倍，并且更准确。Fast R-CNN的Python和C ++（使用Caffe）实现以MIT开源许可证发布在：<a href="https://github.com/rbgirshick/fast-rcnn" rel="nofollow" target="_blank">https://github.com/rbgirshick/fast-rcnn</a>。</p> 
  <h2 id="简介"><a></a>简介</h2> 
  <p>最近，深度卷积网络<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:14" rel="nofollow" target="_blank">1</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:16" rel="nofollow" target="_blank">2</a>已经显著提高了图像分类<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:14" rel="nofollow" target="_blank">1</a>和目标检测<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:9" rel="nofollow" target="_blank">3</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:19" rel="nofollow" target="_blank">4</a>的准确性。与图像分类相比，目标检测是一个更具挑战性的任务，需要更复杂的方法来解决。由于这种复杂性，当前的方法（例如，<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:9" rel="nofollow" target="_blank">3</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:19" rel="nofollow" target="_blank">4</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:25" rel="nofollow" target="_blank">6</a>）采用多级流水线的方式训练模型，既慢且精度不高。</p> 
  <p>复杂性的产生是因为检测需要目标的精确定位，这就导致两个主要的难点。首先，必须处理大量候选目标位置（通常称为“提案”）。 第二，这些候选框仅提供粗略定位，其必须被精细化以实现精确定位。 这些问题的解决方案经常会影响速度，准确性或简单性。</p> 
  <p>在本文中，我们简化了最先进的基于卷积网络的目标检测器的训练过程<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:9" rel="nofollow" target="_blank">3</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>。我们提出一个单阶段训练算法，联合学习候选框分类和修正他们的空间位置。</p> 
  <p>所得到的方法用来训练非常深的检测网络（例如VGG16） 比R-CNN快9倍，比SPPnet快3倍。在运行时，检测网络在PASCAL VOC 2012数据集上实现最高准确度，其中mAP为66％（R-CNN为62％），每张图像处理时间为0.3秒，不包括候选框的生成（所有的时间都是使用一个超频到875MHz的Nvidia K40 GPU测试的）。</p> 
  <h3 id="r-cnn与sppnet"><a></a>R-CNN与SPPnet</h3> 
  <p>基于区域的卷积网络方法（RCNN）通过使用深度卷积网络来分类目标候选框，获得了很高的目标检测精度。然而，R-CNN具有显着的缺点：</p> 
  <ol> 
   <li>**训练过程是多级流水线。**R-CNN首先使用目标候选框对卷积神经网络使用log损失进行微调。然后，它将卷积神经网络得到的特征送入SVM。 这些SVM作为目标检测器，替代通过微调学习的softmax分类器。 在第三个训练阶段，学习检测框回归。</li> 
   <li><strong>训练在时间和空间上是的开销很大。</strong>对于SVM和检测框回归训练，从每个图像中的每个目标候选框提取特征，并写入磁盘。对于非常深的网络，如VGG16，这个过程在单个GPU上需要2.5天（VOC07 trainval上的5k个图像）。这些特征需要数百GB的存储空间。</li> 
   <li><strong>目标检测速度很慢。</strong>在测试时，从每个测试图像中的每个目标候选框提取特征。用VGG16网络检测目标每个图像需要47秒（在GPU上）。</li> 
  </ol> 
  <p>R-CNN很慢是因为它为每个目标候选框进行卷积神经网络正向传递，而不共享计算。SPPnet<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>通过共享计算加速R-CNN。SPPnet<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>计算整个输入图像的卷积特征图，然后使用从共享特征图提取的特征向量来对每个候选框进行分类。通过最大池化将候选框内的特征图转化为固定大小的输出（例如，6X6）来提取针对候选框的特征。多个输出被池化，然后连接成空间金字塔池<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:15" rel="nofollow" target="_blank">7</a>。SPPnet在测试时将R-CNN加速10到100倍。由于更快的候选框特征提取训练时间也减少3倍。</p> 
  <p>SPP网络也有显著的缺点。像R-CNN一样，训练过程是一个多级流水线，涉及提取特征，使用log损失对网络进行微调，训练SVM分类器，最后拟合检测框回归。特征也写入磁盘。但与R-CNN不同，在<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>中提出的微调算法不能更新在空间金字塔池之前的卷积层。不出所料，这种限制（固定的卷积层）限制了深层网络的精度。</p> 
  <h3 id="贡献"><a></a>贡献</h3> 
  <p>我们提出一种新的训练算法，修正R-CNN和SPPnet的缺点，同时提高其速度和准确性。因为它能比较快地进行训练和测试，我们称之为Fast R-CNN。Fast RCNN方法有以下几个优点：</p> 
  <ol> 
   <li>比R-CNN和SPPnet具有更高的目标检测精度（mAP）。</li> 
   <li>训练是使用多任务损失的单阶段训练。</li> 
   <li>训练可以更新所有网络层参数。</li> 
   <li>不需要磁盘空间缓存特征。</li> 
  </ol> 
  <p>Fast R-CNN使用Python和C++(Caffe<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:13" rel="nofollow" target="_blank">8</a>)语言编写，以MIT开源许可证发布在：<a href="https://github.com/rbgirshick/fast-rcnn" rel="nofollow" target="_blank">https://github.com/rbgirshick/fast-rcnn</a>。</p> 
  <h2 id="fast-r-cnn架构与训练"><a></a>Fast R-CNN架构与训练</h2> 
  <p>Fast R-CNN的架构如下图（图1）所示：</p> 
  <p><img src="https://alvinzhu.xyz/assets/2017-10-10-fast-r-cnn/figure1.png" alt="Figure1" title=""></p> 
  <p>图1. Fast R-CNN架构。输入图像和多个感兴趣区域（RoI）被输入到全卷积网络中。每个RoI被池化到固定大小的特征图中，然后通过全连接层（FC）映射到特征向量。网络对于每个RoI具有两个输出向量：Softmax概率和每类检测框回归偏移量。该架构是使用多任务丢失端到端训练的。</p> 
  <p>Fast R-CNN网络将整个图像和一组候选框作为输入。网络首先使用几个卷积层（conv）和最大池化层来处理整个图像，以产生卷积特征图。然后，对于每个候选框，RoI池化层从特征图中提取固定长度的特征向量。每个特征向量被送入一系列全连接（fc）层中，其最终分支成两个同级输出层 ：一个输出KK个类别加上1个背景类别的Softmax概率估计，另一个为KK个类别的每一个类别输出四个实数值。每组4个值表示KK个类别的一个类别的检测框位置的修正。</p> 
  <h3 id="roi池化层"><a></a>RoI池化层</h3> 
  <p>RoI池化层使用最大池化将任何有效的RoI内的特征转换成具有H×WH×W（例如，7×77×7）的固定空间范围的小特征图，其中HH和WW是层的超参数，独立于任何特定的RoI。在本文中，RoI是卷积特征图中的一个矩形窗口。 每个RoI由指定其左上角(r,c)(r,c)及其高度和宽度(h,w)(h,w)的四元组(r,c,h,w)(r,c,h,w)定义。</p> 
  <p>RoI最大池化通过将大小为h×wh×w的RoI窗口分割成H×WH×W个网格，子窗口大小约为h/H×w/Wh/H×w/W，然后对每个子窗口执行最大池化，并将输出合并到相应的输出网格单元中。同标准的最大池化一样，池化操作独立应用于每个特征图通道。RoI层只是SPPnets <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>中使用的空间金字塔池层的特殊情况，其只有一个金字塔层。 我们使用<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>中给出的池化子窗口计算方法。</p> 
  <h3 id="从预训练网络初始化"><a></a>从预训练网络初始化</h3> 
  <p>我们实验了三个预训练的ImageNet<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:4" rel="nofollow" target="_blank">9</a>网络，每个网络有五个最大池化层和五到十三个卷积层（网络详细信息，请参见<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#实验配置" rel="nofollow" target="_blank">实验配置</a>）。当预训练网络初始化fast R-CNN网络时，其经历三个变换。</p> 
  <p>首先，最后的最大池化层由RoI池层代替，其将H和W设置为与网络的第一个全连接层兼容的配置（例如，对于VGG16，H=W=7H=W=7）。</p> 
  <p>然后，网络的最后一格全连接层和Softmax（其被训练用于1000类ImageNet分类）被替换为前面描述的两个同级层（全连接层和K+1K+1个类别的Softmax以及类别特定的检测框回归）。</p> 
  <p>最后，网络被修改为采用两个数据输入：图像的列表和这些图像中的RoI的列表。</p> 
  <h3 id="微调"><a></a>微调</h3> 
  <p>用反向传播训练所有网络权重是Fast R-CNN的重要能力。首先，让我们阐明为什么SPPnet无法更新低于空间金字塔池化层的权重。</p> 
  <p>根本原因是当每个训练样本（即RoI）来自不同的图像时，通过SPP层的反向传播是非常低效的，这正是训练R-CNN和SPPnet网络的方法。低效的部分是因为每个RoI可能具有非常大的感受野，通常跨越整个输入图像。由于正向传播必须处理整个感受野，训练输入很大（通常是整个图像）。</p> 
  <p>我们提出了一种更有效的训练方法，利用训练期间的特征共享。在Fast RCNN网络训练中，随机梯度下降（SGD）的小批量是被分层采样的，首先采样NN个图像，然后从每个图像采样R/NR/N个 RoI。关键的是，来自同一图像的RoI在向前和向后传播中共享计算和内存。减小NN，就减少了小批量的计算。例如，当N=2N=2和R=128R=128时，得到的训练方案比从128幅不同的图采样一个RoI（即R-CNN和SPPnet的策略）快64倍。</p> 
  <p>这个策略的一个令人担心的问题是它可能导致训练收敛变慢，因为来自相同图像的RoI是相关的。这个问题似乎在实际情况下并不存在，当N=2N=2和R=128R=128时，我们使用比R-CNN更少的SGD迭代就获得了良好的结果。</p> 
  <p>除了分层采样，Fast R-CNN使用了一个精细的训练过程，在微调阶段联合优化Softmax分类器和检测框回归，而不是分别在三个独立的阶段训练softmax分类器，SVM和回归器<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:9" rel="nofollow" target="_blank">3</a> [^ 11]。 下面将详细描述该过程（损失，小批量采样策略，通过RoI池化层的反向传播和SGD超参数）。</p> 
  <p>**多任务损失。**Fast R-CNN网络具有两个同级输出层。 第一个输出在K+1K+1个类别上的离散概率分布（每个RoI），p=(p0,…,pK)p=(p0,…,pK)。 通常，通过全连接层的K+1K+1个输出上的Softmax来计算pp。第二个输出层输出检测框回归偏移，tk=(tkx,tky,tkw,tkh)tk=(txk,tyk,twk,thk)，对于由k索引的K个类别中的每一个。 我们使用<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:9" rel="nofollow" target="_blank">3</a>中给出的tktk的参数化，其中tktk指定相对于候选框的尺度不变转换和对数空间高度/宽度移位。</p> 
  <p>每个训练的RoI用类真值uu和检测框回归目标真值vv标记。我们对每个标记的RoI使用多任务损失LL以联合训练分类和检测框回归：L(p,u,tu,v)=Lcls(p,u)+λ[u≥1]Lloc(tu,v)(1)(1)L(p,u,tu,v)=Lcls(p,u)+λ[u≥1]Lloc(tu,v)</p> 
  <p>其中Lcls(p,u)=−logpuLcls(p,u)=−log⁡pu， 是类真值uu的log损失。</p> 
  <p>对于类真值uu，第二个损失LlocLloc是定义在检测框回归目标真值元组u,v=(vx,vy,vw,vh)u,v=(vx,vy,vw,vh)和预测元组tu=(tux,tuy,tuw,tuh)tu=(txu,tyu,twu,thu)上的损失。 Iverson括号指示函数[u≥1][u≥1]当u≥1u≥1的时候为值1，否则为0。按照惯例，背景类标记为u=0u=0。对于背景RoI，没有检测框真值的概念，因此LlocLloc被忽略。对于检测框回归，我们使用损失Lloc(tu,v)=∑i∈{x,y,w,h}smoothL1(tui−vi)(2)(2)Lloc(tu,v)=∑i∈{x,y,w,h}smoothL1(tiu−vi)其中：smoothL1(x)={0.5x2|x|−0.5if|x|&lt;1otherwise(3)(3)smoothL1(x)={0.5x2if|x|&lt;1|x|−0.5otherwise是鲁棒的L1L1损失，对于异常值比在R-CNN和SPPnet中使用的L2L2损失更不敏感。当回归目标无界时，具有L2L2损失的训练可能需要仔细调整学习速率，以防止爆炸梯度。公式(3)(3)消除了这种灵敏度。</p> 
  <p>公式(1)(1)中的超参数λλ控制两个任务损失之间的平衡。我们将回归目标真值vivi归一化为具有零均值和单位方差。所有实验都使用λ=1λ=1。</p> 
  <p>我们注意到<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:6" rel="nofollow" target="_blank">10</a>使用相关损失来训练一个类别无关的目标候选网络。 与我们的方法不同的是<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:6" rel="nofollow" target="_blank">10</a>倡导一个分离定位和分类的双网络系统。OverFeat<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:19" rel="nofollow" target="_blank">4</a>，R-CNN<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:9" rel="nofollow" target="_blank">3</a>和SPPnet<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>也训练分类器和检测框定位器，但是这些方法使用逐级训练，这对于Fast RCNN来说不是最好的选择。</p> 
  <p><strong>小批量采样。</strong>在微调期间，每个SGD的小批量由N=2N=2个图像构成，均匀地随机选择（如通常的做法，我们实际上迭代数据集的排列）。 我们使用大小为R=128R=128的小批量，从每个图像采样64个RoI。 如在<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:9" rel="nofollow" target="_blank">3</a>中，我们从候选框中获取25％的RoI，这些候选框与检测框真值的IoU至少为0.5。 这些RoI只包括用前景对象类标记的样本，即u≥1u≥1。 剩余的RoI从候选框中采样，该候选框与检测框真值的最大IoU在区间[0.1,0.5)[0.1,0.5)上<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>。 这些是背景样本，并用u=0u=0标记。0.1的阈值下限似乎充当难负样本重训练的启发式算法<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:8" rel="nofollow" target="_blank">11</a>。 在训练期间，图像以概率0.5水平翻转。不使用其他数据增强。</p> 
  <p><strong>通过RoI池化层的反向传播。</strong>反向传播通过RoI池化层。为了清楚起见，我们假设每个小批量(N=1N=1)只有一个图像，扩展到N&gt;1N&gt;1是显而易见的，因为前向传播独立地处理所有图像。</p> 
  <p>令xi∈ℝxi∈R是到RoI池化层的第ii个激活输入，并且令yrjyrj是来自第rr个RoI层的第jj个输出。RoI池化层计算yrj=xi∗(r,j)yrj=xi∗(r,j)，其中xi∗(r,j)=argmaxi′∈(r,j)xi′xi∗(r,j)=argmaxi′∈R(r,j)xi′。(r,j)R(r,j)是输出单元yrjyrj最大池化的子窗口中的输入的索引集合。单个xixi可以被分配给几个不同的输出yrjyrj。</p> 
  <p>RoI池化层反向传播函数通过遵循argmax switches来计算关于每个输入变量xixi的损失函数的偏导数：∂L∂xi=∑r∑j[i=i∗(r,j)]∂L∂yrj(4)(4)∂L∂xi=∑r∑j[i=i∗(r,j)]∂L∂yrj换句话说，对于每个小批量RoI rr和对于每个池化输出单元yrjyrj，如果ii是yrjyrj通过最大池化选择的argmax，则将这个偏导数∂L∂yrj∂L∂yrj积累下来。在反向传播中，偏导数∂L∂yrj∂L∂yrj已经由RoI池化层顶部的层的反向传播函数计算。</p> 
  <p><strong>SGD超参数。</strong>用于Softmax分类和检测框回归的全连接层的权重分别使用具有方差0.01和0.001的零均值高斯分布初始化。偏置初始化为0。所有层的权重学习率为1倍的全局学习率，偏置为2倍的全局学习率，全局学习率为0.001。 当对VOC07或VOC12 trainval训练时，我们运行SGD进行30k次小批量迭代，然后将学习率降低到0.0001，再训练10k次迭代。当我们训练更大的数据集，我们运行SGD更多的迭代，如下文所述。 使用0.9的动量和0.0005的参数衰减（权重和偏置）。</p> 
  <h3 id="尺度不变性"><a></a>尺度不变性</h3> 
  <p>我们探索两种实现尺度不变对象检测的方法：（1）通过“brute force”学习和（2）通过使用图像金字塔。 这些策略遵循<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>中的两种方法。 在“brute force”方法中，在训练和测试期间以预定义的像素大小处理每个图像。网络必须直接从训练数据学习尺度不变性目标检测。</p> 
  <p>相反，多尺度方法通过图像金字塔向网络提供近似尺度不变性。 在测试时，图像金字塔用于大致缩放-规范化每个候选框。 在多尺度训练期间，我们在每次图像采样时随机采样金字塔尺度，遵循<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>，作为数据增强的形式。由于GPU内存限制，我们只对较小的网络进行多尺度训练。</p> 
  <h2 id="fast-r-cnn检测"><a></a>Fast R-CNN检测</h2> 
  <p>一旦Fast R-CNN网络被微调完毕，检测相当于运行前向传播（假设候选框是预先计算的）。网络将图像（或图像金字塔，编码为图像列表）和待计算概率的RR个候选框的列表作为输入。在测试的时候，RR通常在2000左右，虽然我们将考虑将它变大（约45k）的情况。当使用图像金字塔时，每个RoI被缩放，使其最接近<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>中的22422242个像素。</p> 
  <p>对于每个测试的RoI rr，正向传播输出类别后验概率分布pp和相对于rr的预测的检测框框偏移集合（KK个类别中的每一个获得其自己的精细检测框预测）。我们使用估计的概率Pr(class=k|r)≜pkPr(class=k|r)≜pk为每个对象类别kk分配rr的检测置信度。然后，我们使用R-CNN算法的设置和对每个类别独立执行非最大抑制<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:9" rel="nofollow" target="_blank">3</a>。</p> 
  <h3 id="使用截断的svd来进行更快的检测"><a></a>使用截断的SVD来进行更快的检测</h3> 
  <p>对于整体图像分类，与卷积层相比，计算全连接层花费的时间较小。相反，为了检测，要处理的RoI的数量很大，并且接近一半的正向传递时间用于计算全连接层（参见图2）。大的全连接层容易通过用截短的SVD压缩来加速<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:5" rel="nofollow" target="_blank">12</a><a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:23" rel="nofollow" target="_blank">13</a>。</p> 
  <p>在这种技术中，层的u×vu×v权重矩阵WW通过SVD被近似分解为：W≈UΣtVT(5)(5)W≈UΣtVT在这种分解中，UU是一个u×tu×t的矩阵，包括WW的前tt个左奇异向量，ΣtΣt是t×tt×t对角矩阵，其包含WW的前tt个奇异值，并且VV是v×tv×t矩阵，包括WW的前tt个右奇异向量。截断SVD将参数计数从uvuv减少到t(u+v)t(u+v)个，如果tt远小于min(u,v)min(u,v)，则SVD可能是重要的。 为了压缩网络，对应于WW的单个全连接层由两个全连接层替代，在它们之间没有非线性。这些层中的第一层使用权重矩阵ΣtVTΣtVT（没有偏置），并且第二层使用UU（其中原始偏差与WW相关联）。当RoI的数量大时，这种简单的压缩方法给出良好的加速。</p> 
  <h2 id="主要结果"><a></a>主要结果</h2> 
  <p>三个主要结果支持本文的贡献：</p> 
  <ol> 
   <li>VOC07，2010和2012的最高的mAP。</li> 
   <li>相比R-CNN，SPPnet，快速训练和测试。</li> 
   <li>在VGG16中微调卷积层改善了mAP。</li> 
  </ol> 
  <h3 id="实验配置"><a></a>实验配置</h3> 
  <p>我们的实验使用了三个经过预训练的ImageNet网络模型，这些模型可以在线获得(<a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" rel="nofollow" target="_blank">https://github.com/BVLC/caffe/wiki/Model-Zoo</a>)。第一个是来自R-CNN<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:9" rel="nofollow" target="_blank">3</a>的CaffeNet（实质上是AlexNet<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:14" rel="nofollow" target="_blank">1</a>）。 我们将这个CaffeNet称为模型<strong>S</strong>，即小模型。第二网络是来自<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:3" rel="nofollow" target="_blank">14</a>的VGG_CNN_M_1024，其具有与<strong>S</strong>相同的深度，但是更宽。 我们把这个网络模型称为<strong>M</strong>，即中等模型。最后一个网络是来自<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:20" rel="nofollow" target="_blank">15</a>的非常深的VGG16模型。由于这个模型是最大的，我们称之为<strong>L</strong>。在本节中，所有实验都使用单尺度训练和测试（s=600s=600，详见<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#尺度不变性：暴力或精细？" rel="nofollow" target="_blank">尺度不变性：暴力或精细？</a>）。</p> 
  <h3 id="voc-2010和2012数据集结果"><a></a>VOC 2010和2012数据集结果</h3> 
  <p><img src="https://alvinzhu.xyz/assets/2017-10-10-fast-r-cnn/table2.png" alt="Table 2" title=""></p> 
  <p>表2. VOC 2010测试检测平均精度（％）。 BabyLearning使用基于<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:17" rel="nofollow" target="_blank">16</a>的网络。 所有其他方法使用VGG16。训练集：12：VOC12 trainval，Prop.：专有数据集，12+seg：12具有分段注释，07++12：VOC07 trainval，VOC07测试和VOC12 trainval的联合。</p> 
  <p><img src="https://alvinzhu.xyz/assets/2017-10-10-fast-r-cnn/table3.png" alt="Table 3" title=""></p> 
  <p>表3. VOC 2012测试检测平均精度（％）。 BabyLearning和NUS_NIN_c2000使用基于<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:17" rel="nofollow" target="_blank">16</a>的网络。 所有其他方法使用VGG16。训练设置：见表2，Unk.：未知。</p> 
  <p>如上表（表2，表3）所示，在这些数据集上，我们比较Fast R-CNN（简称FRCN）和公共排行榜中comp4（外部数据）上的主流方法（<a href="http://host.robots.ox.ac.uk:8080/leaderboard" rel="nofollow" target="_blank">http://host.robots.ox.ac.uk:8080/leaderboard</a> ，访问时间是2015.4.18）。对于NUS_NIN_c2000和BabyLearning方法，目前没有其架构的确切信息，它们是Network-in-Network的变体<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:17" rel="nofollow" target="_blank">16</a>。所有其他方法从相同的预训练VGG16网络初始化。</p> 
  <p>Fast R-CNN在VOC12上获得最高结果，mAP为65.7％（加上额外数据为68.4％）。它也比其他方法快两个数量级，这些方法都基于比较“慢”的R-CNN网络。在VOC10上，SegDeepM <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:25" rel="nofollow" target="_blank">6</a>获得了比Fast R-CNN更高的mAP（67.2％对比66.1％）。SegDeepM使用VOC12 trainval训练集训练并添加了分割的标注，它被设计为通过使用马尔可夫随机场推理R-CNN检测和来自O2PO2P<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:1" rel="nofollow" target="_blank">17</a>的语义分割方法的分割来提高R-CNN精度。Fast R-CNN可以替换SegDeepM中使用的R-CNN，这可以导致更好的结果。当使用放大的07++12训练集（见表2标题）时，Fast R-CNN的mAP增加到68.8％，超过SegDeepM。</p> 
  <h3 id="voc-2007数据集上的结果"><a></a>VOC 2007数据集上的结果</h3> 
  <p>在VOC07数据集上，我们比较Fast R-CNN与R-CNN和SPPnet的mAP。 所有方法从相同的预训练VGG16网络开始，并使用边界框回归。 VGG16 SPPnet结果由<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>的作者提供。SPPnet在训练和测试期间使用五个尺度。Fast R-CNN对SPPnet的改进说明，即使Fast R-CNN使用单个尺度训练和测试，卷积层微调在mAP中提供了大的改进（从63.1％到66.9％）。R-CNN的mAP为66.0％。 作为次要点，SPPnet在PASCAL中没有使用被标记为“困难”的样本进行训练。 除去这些样本，Fast R-CNN 的mAP为68.1％。 所有其他实验都使用被标记为“困难”的样本。</p> 
  <h3 id="训练和测试时间"><a></a>训练和测试时间</h3> 
  <p><img src="https://alvinzhu.xyz/assets/2017-10-10-fast-r-cnn/table4.png" alt="Table 4" title=""></p> 
  <p>表4. Fast RCNN，R-CNN和SPPnet中相同模型之间的运行时间比较。Fast R-CNN使用单尺度模式。SPPnet使用<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>中指定的五个尺度，由<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>的作者提供在Nvidia K40 GPU上的测量时间。</p> 
  <p>快速的训练和测试是我们的第二个主要成果。表4比较了Fast RCNN，R-CNN和SPPnet之间的训练时间（小时），测试速率（每秒图像数）和VOC07上的mAP。对于VGG16，没有截断SVD的Fast R-CNN处理图像比R-CNN快146倍，有截断SVD的R-CNN快213倍。训练时间减少9倍，从84小时减少到9.5小时。与SPPnet相比，没有截断SVD的Fast RCNN训练VGG16网络比SPPnet快2.7倍（9.5小时对25.5小时），测试时间快7倍，有截断SVD的Fast RCNN比的SPPnet快10倍。 Fast R-CNN还不需要数百GB的磁盘存储，因为它不缓存特征。</p> 
  <p><strong>截断SVD。</strong>截断的SVD可以将检测时间减少30％以上，同时在mAP中只有很小（0.3个百分点）的下降，并且无需在模型压缩后执行额外的微调。</p> 
  <p><img src="https://alvinzhu.xyz/assets/2017-10-10-fast-r-cnn/figure2.png" alt="Figure 2" title=""></p> 
  <p>图2. 截断SVD之前和之后VGG16的时间分布。在SVD之前，完全连接的层fc6和fc7需要45％的时间。</p> 
  <p>图2示出了如何使用来自VGG16的fc6层中的25088×409625088×4096矩阵的顶部1024个奇异值和来自fc7层的4096×40964096×4096矩阵的顶部256个奇异值减少运行时间，而在mAP中几乎没有损失。如果在压缩之后再次微调，则可以在mAP中具有更小的下降的情况下进一步加速。</p> 
  <h3 id="微调哪些层"><a></a>微调哪些层？</h3> 
  <p>对于在SPPnet论文<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>中考虑的不太深的网络，仅微调全连接层似乎足以获得良好的精度。我们假设这个结果不适用于非常深的网络。为了验证微调卷积层对于VGG16的重要性，我们使用Fast R-CNN微调，但冻结十三个卷积层，以便只有全连接层学习。这种消融模拟单尺度SPPnet训练，将mAP从66.9％降低到61.4％（表5）。这个实验验证了我们的假设：通过RoI池化层的训练对于非常深的网是重要的。</p> 
  <p><img src="https://alvinzhu.xyz/assets/2017-10-10-fast-r-cnn/table5.png" alt="Table 5" title=""></p> 
  <p>表5. 限制哪些层对VGG16进行微调产生的影响。微调≥≥fc6模拟单尺度SPPnet训练算法<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>。 SPPnet L是使用五个尺度，以显著（7倍）的速度成本获得的结果。</p> 
  <p>这是否意味着所有卷积层应该微调？没有。在较小的网络（S和M）中，我们发现conv1（第一个卷积层）是通用的和任务独立的（一个众所周知的事实<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:14" rel="nofollow" target="_blank">1</a>）。允许conv1学习或不学习，对mAP没有很有意义的影响。对于VGG16，我们发现只需要更新conv3_1及以上（13个卷积层中的9个）的层。这种观察是实用的：（1）从conv2_1更新使训练变慢1.3倍（12.5小时对比9.5小时）和（2）从conv1_1更新GPU内存不够用。当从conv2_1学习时mAP仅为增加0.3个点（表5，最后一列）。 所有Fast R-CNN在本文中结果都使用VGG16微调层conv3_1及以上的层，所有实验用模型S和M微调层conv2及以上的层。</p> 
  <h2 id="设计评估"><a></a>设计评估</h2> 
  <p>我们通过实验来了解Fast RCNN与R-CNN和SPPnet的比较，以及评估设计决策。按照最佳实践，我们在PASCAL VOC07数据集上进行了这些实验。</p> 
  <h3 id="多任务训练有用吗"><a></a>多任务训练有用吗？</h3> 
  <p>多任务训练是方便的，因为它避免管理顺序训练任务的流水线。但它也有可能改善结果，因为任务通过共享的表示（ConvNet）<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:2" rel="nofollow" target="_blank">18</a>相互影响。多任务训练能提高Fast R-CNN中的目标检测精度吗？</p> 
  <p>为了测试这个问题，我们训练仅使用公式(1)(1)中的分类损失LclsLcls（即设置λ=0λ=0）的基准网络。这些基线是表6中每组的第一列。请注意，这些模型没有检测框回归。接下来（每组的第二列），是我们采用多任务损失（公式(1)(1)，λ=1λ=1）训练的网络，但是我们在测试时禁用检测框回归。这隔离了网络的分类准确性，并允许与基准网络的apple to apple的比较。</p> 
  <p>在所有三个网络中，我们观察到多任务训练相对于单独的分类训练提高了纯分类精度。改进范围从+0.8到+1.1 个mAP点，显示了多任务学习的一致的积极效果。</p> 
  <p>最后，我们采用基线模型（仅使用分类损失进行训练），加上检测回归层，并使用LlocLloc训练它们，同时保持所有其他网络参数冻结。每组中的第三列显示了这种逐级训练方案的结果：mAP相对于第一列改进，但逐级训练表现不如多任务训练（每组第四列）。</p> 
  <p><img src="https://alvinzhu.xyz/assets/2017-10-10-fast-r-cnn/table6.png" alt="Table 6" title=""></p> 
  <p>表6. 多任务训练（每组第四列）改进了分段训练（每组第三列）的mAP。</p> 
  <h3 id="尺度不变性暴力或精细"><a></a>尺度不变性：暴力或精细？</h3> 
  <p>我们比较两个策略实现尺度不变物体检测：暴力学习（单尺度）和图像金字塔（多尺度）。在任一情况下，我们将图像的尺度ss定义为其最短边的长度。</p> 
  <p>所有单尺度实验使用s=600s=600像素，对于一些图像，ss可以小于600，因为我们保持横纵比缩放图像，并限制其最长边为1000像素。选择这些值使得VGG16在微调期间不至于GPU内存不足。较小的模型占用显存更少，所以可受益于较大的ss值。然而，每个模型的优化不是我们的主要的关注点。我们注意到PASCAL图像是384×473像素的，因此单尺度设置通常以1.6倍的倍数上采样图像。因此，RoI池化层的平均有效步进为约10像素。</p> 
  <p>在多尺度设置中，我们使用<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>中指定的相同的五个尺度（s∈{480,576,688,864,1200}s∈{480,576,688,864,1200}）以方便与SPPnet进行比较。但是，我们以2000像素为上限，以避免GPU内存不足。</p> 
  <p>表7显示了当使用一个或五个尺度进行训练和测试时的模型S和M的结果。也许在<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>中最令人惊讶的结果是单尺度检测几乎与多尺度检测一样好。我们的研究结果能证明他们的结果：深度卷积网络擅长直接学习尺度不变性。多尺度方法消耗大量的计算时间仅带来了很小的mAP增加（表7）。在VGG16（模型L）的情况下，我们受限于实施细节仅能使用单个尺度。然而，它得到了66.9％的mAP，略高于R-CNN的66.0％<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:10" rel="nofollow" target="_blank">19</a>，尽管R-CNN在每个候选区域被缩放为规范大小，在意义上使用了“无限”尺度。</p> 
  <p>由于单尺度处理提供速度和精度之间的最佳折衷，特别是对于非常深的模型，本小节以外的所有实验使用单尺度训练和测试，s=600s=600像素。</p> 
  <p><img src="https://alvinzhu.xyz/assets/2017-10-10-fast-r-cnn/table7.png" alt="Table 7" title=""></p> 
  <p>表7. 多尺度与单尺度。SPPnet ZF（类似于模型S）的结果来自<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>。 具有单尺度的较大网络提供最佳的速度/精度平衡。（L在我们的实现中不能使用多尺度，因为GPU内存限制。）</p> 
  <h3 id="我们需要更过训练数据吗"><a></a>我们需要更过训练数据吗？</h3> 
  <p>当提供更多的训练数据时，好的目标检测器应该会得到改善。 Zhu等人<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:24" rel="nofollow" target="_blank">20</a>发现DPM<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:8" rel="nofollow" target="_blank">11</a>mAP在只有几百到千个训练样本的时候就饱和了。在这里我们增加VOC07 trainval训练集与VOC12 trainval训练集，大约增加到三倍的图像，数量达到16.5k，以评估Fast R-CNN。扩大训练集提高了VOC07测试的mAP，从66.9％到70.0％（表1）。 当对这个数据集进行训练时，我们使用60k次小批量迭代而不是40k。</p> 
  <p><img src="https://alvinzhu.xyz/assets/2017-10-10-fast-r-cnn/table1.png" alt="Table 1" title=""></p> 
  <p>表1. VOC 2007测试检测平均精度（％）。 所有方法都使用VGG16。 训练集：07：VOC07 trainval，07 \diff：07没有“困难”的样本，07 + 12：07和VOC12训练的联合。 SPPnet结果由<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:11" rel="nofollow" target="_blank">5</a>的作者提供。</p> 
  <p>我们对VOC10和2012进行类似的实验，我们用VOC07 trainval，test和VOC12 trainval构造了21.5k图像的数据集。当训练这个数据集时，我们使用100k次SGD迭代和每40k次迭代（而不是每30k次）降低学习率10倍。对于VOC10和2012，mAP分别从66.1％提高到68.8％和从65.7％提高到68.4％。</p> 
  <h3 id="svm分类是否优于softmax"><a></a>SVM分类是否优于Softmax？</h3> 
  <p>Fast R-CNN在微调期间使用softmax分类器学习，而不是如在R-CNN和SPPnet中训练线性SVM。为了理解这种选择的影响，我们在Fast R-CNN中实施了具有难负采样重训练的SVM训练。我们使用与R-CNN中相同的训练算法和超参数。</p> 
  <p><img src="https://alvinzhu.xyz/assets/2017-10-10-fast-r-cnn/table8.png" alt="Table 8" title=""></p> 
  <p>表8. 用Softmax的Fast R-CNN对比用SVM的Fast RCNN（VOC07 mAP）。</p> 
  <p>对于所有三个网络，Softmax略优于SVM，mAP分别提高了0.1和0.8个点。这种效应很小，但是它表明与先前的多级训练方法相比，“一次性”微调是足够的。我们注意到，Softmax，不像SVM那样，在分类RoI时引入类之间的竞争。</p> 
  <h3 id="更多的候选区域更好吗"><a></a>更多的候选区域更好吗？</h3> 
  <p>存在（广义地）两种类型的目标检测器：使用候选区域的稀疏集合（例如，选择性搜索<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:21" rel="nofollow" target="_blank">21</a>）和使用密集集合（例如DPM<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:8" rel="nofollow" target="_blank">11</a>）。分类稀疏提议是级联的一种类型<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:22" rel="nofollow" target="_blank">22</a>，其中提议机制首先拒绝大量候选者，让分类器来评估留下的小集合。当应用于DPM检测时，该级联提高了检测精度<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:21" rel="nofollow" target="_blank">21</a>。我们发现提案分类器级联也提高了Fast R-CNN的精度。</p> 
  <p>使用选择性搜索的质量模式，我们扫描每个图像1k到10k个候选框，每次重新训练和重新测试模型M.如果候选框纯粹扮演计算的角色，增加每个图像的候选框数量不应该损害mAP。</p> 
  <p><img src="https://alvinzhu.xyz/assets/2017-10-10-fast-r-cnn/figure3.png" alt="Figure 3" title=""></p> 
  <p>图3. 各种候选区域方案的VOC07测试mAP和AR。</p> 
  <p>我们发现mAP上升，然后随着候选区域计数增加而略微下降（图3，实线蓝线）。这个实验表明，用更多的候选区域没有帮助，甚至稍微有点伤害准确性。</p> 
  <p>如果不实际运行实验，这个结果很难预测。用于测量候选区域质量的最先进的技术是平均召回率(AR)<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:12" rel="nofollow" target="_blank">23</a>。当对每个图像使用固定数量的候选区域时，AR与使用R-CNN的几种候选区域方法良好地相关。图3示出了AR（实线红线）与mAP不相关，因为每个图像的候选区域数量是变化的。AR必须小心使用，由于更多的候选区域更高的AR并不意味着mAP会增加。幸运的是，使用模型M的训练和测试需要不到2.5小时。因此，Fast R-CNN能够高效地，直接地评估目标候选区域mAP，这优于代理度量。</p> 
  <p>我们还调查Fast R-CNN当使用密集生成框（在缩放，位置和宽高比上），大约45k个框/图像。这个密集集足够丰富，当每个选择性搜索框被其最近（IoU）密集框替换时，mAP只降低1个点（到57.7％，图3，蓝色三角形）。</p> 
  <p>密集框的统计数据与选择性搜索框的统计数据不同。从2k个选择性搜索框开始，我们在添加1000×{2,4,6,8,10,32,45}1000×{2,4,6,8,10,32,45}的随机样本密集框时测试mAP。对于每个实验，我们重新训练和重新测试模型M。当添加这些密集框时，mAP比添加更多选择性搜索框时下降得更强，最终达到53.0％。</p> 
  <p>我们还训练和测试Fast R-CNN只使用密集框（45k/图像）。此设置的mAP为52.9％（蓝色菱形）。最后，我们检查是否需要使用难样本重训练的SVM来处理密集框分布。 SVM做得更糟：49.3％（蓝色圆圈）。</p> 
  <h3 id="ms-coco初步结果"><a></a>MS COCO初步结果</h3> 
  <p>我们将fast R-CNN（使用VGG16）应用于MS COCO数据集<a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fn:18" rel="nofollow" target="_blank">24</a>，以建立初步基线。我们对80k图像训练集进行了240k次迭代训练，并使用评估服务器对“test-dev”集进行评估。 PASCAL标准下的mAP为35.9％;。新的COCO标准下的AP（也平均）为19.7％。</p> 
  <h2 id="结论"><a></a>结论</h2> 
  <p>本文提出Fast R-CNN，一个对R-CNN和SPPnet干净，快速的更新。 除了报告目前的检测结果之外，我们还提供了详细的实验，希望提供新的见解。 特别值得注意的是，稀疏目标候选区域似乎提高了检测器的质量。 过去探索这个问题过于昂贵（在时间上），但Fast R-CNN使其变得可能。当然，可能存在允许密集盒执行以及稀疏候选框的尚未发现的技术。这样的方法如果被开发，可以帮助进一步加速目标检测。</p> 
  <p><strong>致谢：</strong>感谢Kaiming He，Larry Zitnick和Piotr Dollár的帮助和鼓励。</p> 
  <p><strong>参考文献：</strong></p> 
  <ol> 
   <li>A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:14" rel="nofollow" target="_blank">↩</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:14:1" rel="nofollow" target="_blank">↩2</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:14:2" rel="nofollow" target="_blank">↩3</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:14:3" rel="nofollow" target="_blank">↩4</a></li> 
   <li>Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comp., 1989. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:16" rel="nofollow" target="_blank">↩</a></li> 
   <li>R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:9" rel="nofollow" target="_blank">↩</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:9:1" rel="nofollow" target="_blank">↩2</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:9:2" rel="nofollow" target="_blank">↩3</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:9:3" rel="nofollow" target="_blank">↩4</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:9:4" rel="nofollow" target="_blank">↩5</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:9:5" rel="nofollow" target="_blank">↩6</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:9:6" rel="nofollow" target="_blank">↩7</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:9:7" rel="nofollow" target="_blank">↩8</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:9:8" rel="nofollow" target="_blank">↩9</a></li> 
   <li>P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In ICLR, 2014. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:19" rel="nofollow" target="_blank">↩</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:19:1" rel="nofollow" target="_blank">↩2</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:19:2" rel="nofollow" target="_blank">↩3</a></li> 
   <li>K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11" rel="nofollow" target="_blank">↩</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:1" rel="nofollow" target="_blank">↩2</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:2" rel="nofollow" target="_blank">↩3</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:3" rel="nofollow" target="_blank">↩4</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:4" rel="nofollow" target="_blank">↩5</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:5" rel="nofollow" target="_blank">↩6</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:6" rel="nofollow" target="_blank">↩7</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:7" rel="nofollow" target="_blank">↩8</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:8" rel="nofollow" target="_blank">↩9</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:9" rel="nofollow" target="_blank">↩10</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:10" rel="nofollow" target="_blank">↩11</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:11" rel="nofollow" target="_blank">↩12</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:12" rel="nofollow" target="_blank">↩13</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:13" rel="nofollow" target="_blank">↩14</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:14" rel="nofollow" target="_blank">↩15</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:15" rel="nofollow" target="_blank">↩16</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:16" rel="nofollow" target="_blank">↩17</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:17" rel="nofollow" target="_blank">↩18</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:18" rel="nofollow" target="_blank">↩19</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:19" rel="nofollow" target="_blank">↩20</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:11:20" rel="nofollow" target="_blank">↩21</a></li> 
   <li>Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler. segDeepM: Exploiting segmentation and context in deep neural networks for object detection. In CVPR, 2015. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:25" rel="nofollow" target="_blank">↩</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:25:1" rel="nofollow" target="_blank">↩2</a></li> 
   <li>S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR, 2006. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:15" rel="nofollow" target="_blank">↩</a></li> 
   <li>Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proc. of the ACM International Conf. on Multimedia, 2014. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:13" rel="nofollow" target="_blank">↩</a></li> 
   <li>J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:4" rel="nofollow" target="_blank">↩</a></li> 
   <li>D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:6" rel="nofollow" target="_blank">↩</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:6:1" rel="nofollow" target="_blank">↩2</a></li> 
   <li>P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. TPAMI, 2010. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:8" rel="nofollow" target="_blank">↩</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:8:1" rel="nofollow" target="_blank">↩2</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:8:2" rel="nofollow" target="_blank">↩3</a></li> 
   <li>E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In NIPS, 2014. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:5" rel="nofollow" target="_blank">↩</a></li> 
   <li>J. Xue, J. Li, and Y. Gong. Restructuring of deep neural network acoustic models with singular value decomposition. In Interspeech, 2013. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:23" rel="nofollow" target="_blank">↩</a></li> 
   <li>K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep into convolutional nets. In BMVC, 2014. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:3" rel="nofollow" target="_blank">↩</a></li> 
   <li>K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:20" rel="nofollow" target="_blank">↩</a></li> 
   <li>M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, 2014. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:17" rel="nofollow" target="_blank">↩</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:17:1" rel="nofollow" target="_blank">↩2</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:17:2" rel="nofollow" target="_blank">↩3</a></li> 
   <li>J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Semantic segmentation with second-order pooling. In ECCV, 2012. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:1" rel="nofollow" target="_blank">↩</a></li> 
   <li>R. Caruana. Multitask learning. Machine learning, 28(1), 1997. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:2" rel="nofollow" target="_blank">↩</a></li> 
   <li>R. Girshick, J. Donahue, T. Darrell, and J. Malik. Region-based convolutional networks for accurate object detection and segmentation. TPAMI, 2015. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:10" rel="nofollow" target="_blank">↩</a></li> 
   <li>X. Zhu, C. Vondrick, D. Ramanan, and C. Fowlkes. Do we need more training data or better models for object detection? In BMVC, 2012. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:24" rel="nofollow" target="_blank">↩</a></li> 
   <li>J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. IJCV, 2013. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:21" rel="nofollow" target="_blank">↩</a> <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:21:1" rel="nofollow" target="_blank">↩2</a></li> 
   <li>P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. In CVPR, 2001. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:22" rel="nofollow" target="_blank">↩</a></li> 
   <li>J. H. Hosang, R. Benenson, P. Dollár, and B. Schiele. What makes for effective detection proposals? arXiv preprint arXiv:1502.05082, 2015. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:12" rel="nofollow" target="_blank">↩</a></li> 
   <li>T. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: common objects in context. arXiv e-prints, arXiv:1405.0312 [cs.CV], 2014. <a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/#fnref:18" rel="nofollow" target="_blank">↩</a></li> 
   <li><a href="https://alvinzhu.xyz/2017/10/10/fast-r-cnn/" rel="nofollow" target="_blank">original translation link</a></li> 
  </ol> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
