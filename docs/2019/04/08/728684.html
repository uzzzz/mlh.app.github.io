<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>一份从代码出发的强化学习Q-Learning入门教程，请笑纳！ | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="一份从代码出发的强化学习Q-Learning入门教程，请笑纳！" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="本文由机器之心编译（ID：almosthuman2014） 强化学习（RL） 强化学习是机器学习的一个重要领域，其中智能体通过对状态的感知、对行动的选择以及接受奖励和环境相连接。在每一步，智能体都要观察状态、选择并执行一个行动，这会改变它的状态并产生一个奖励。 马尔科夫决策过程（MDP） 我们将要解决「forest fire」的马尔科夫决策问题，这个在 python 的 MDP 工具箱（http://pymdptoolbox.readthedocs.io/en/latest/api/example.html）中是可以看到的。 森林由两种行动来管理：「等待」和「砍伐」。我们每年做出一个行动，首要目标是为野生动物维护一片古老的森林，次要目标是伐木赚钱。每年都会以 p 的概率发生森林火灾（森林正常生长的概率就是 1-p）。 我们将马尔科夫决策过程记为 (S, A, P, R, γ)，其中： S 是有限状态空间：按照树龄分为三类：0—20 年，21—40 年，大于 40 年 A 是有限行动空间：「等待」和「砍伐」 P 和 R 是转换矩阵和奖励矩阵，它们的闭合形式容易表达出来 γ是表示长期奖励和短期奖励之间的区别的折扣因子 策略π是在给定的当前状态下动作的平稳分布（马尔可夫性） 目标就是找到在不了解任何马尔科夫动态特性的情况下来寻找马尔科夫决策过程的最优策略π*。需要注意的是，如果我们具有这种知识，像最有价值迭代这种算法就可以找到最优策略。 def optimal_value_iteration(mdp, V0, num_iterations, epsilon=0.0001): &nbsp; &nbsp;V = np.zeros((num_iterations+1, mdp.S)) &nbsp; &nbsp;V[0][:] = np.ones(mdp.S)*V0 &nbsp; &nbsp;X = np.zeros((num_iterations+1, mdp.A, mdp.S)) &nbsp; &nbsp;star = np.zeros((num_iterations+1,mdp.S)) &nbsp; &nbsp;for k in range(num_iterations): &nbsp; &nbsp; &nbsp; &nbsp;for s in range(mdp.S): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for a in range(mdp.A): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;X[k+1][a][s] = mdp.R[a][s] + mdp.discount*np.sum(mdp.P[a][s].dot(V[k])) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;star[k+1][s] = (np.argmax(X[k+1,:,s])) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;V[k+1][s] = np.max(X[k+1,:,s]) &nbsp; &nbsp; &nbsp; &nbsp;if (np.max(V[k+1]-V[k])-np.min(V[k+1]-V[k])) &lt; epsilon: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;V[k+1:] = V[k+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;star[k+1:] = star[k+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;X[k+1:] = X[k+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;break &nbsp; &nbsp; &nbsp; &nbsp;else: pass &nbsp; &nbsp;return star, V, X &nbsp; &nbsp;&nbsp; 这里的最优策略是相当符合直觉的，就是等到森林达到最老的树龄再砍伐，因为我们把树龄最老的时候的砍伐奖励设置成了其成长过程中的 5 倍（r1=10,r2=50）。 Q-LEARNING Q-Learning 中策略（π）的质量函数，它将任何一个状态动作组合（s,a）和在观察状态 s 下通过选择行动 a 而得到的期望积累折扣未来奖励映射在一起。 Q-Leraning 被称为「没有模型」，这意味着它不会尝试为马尔科夫决策过程的动态特性建模，它直接估计每个状态下每个动作的 Q 值。然后可以通过选择每个状态具有最高 Q 值的动作来绘制策略。 如果智能体能够以无限多的次数访问状态—行动对，那么 Q-Learning 将会收敛到最优的 Q 函数 [1]。 同样，我们也不会深入讨论 Q-Learning 的细节。如果你对它不太熟悉，这里有 Siraj Raval 的解释视频。 下面我们将展示 Q-Learning 的 Python 实现。请注意，这里所拥的学习率（alpha）遵循 [3] 的结果，使用 w=0.8 的多项式。 这里用到的探索策略（ε-greedy）在后面会有细节介绍。 def q_learning(mdp, num_episodes, T_max, epsilon=0.01): &nbsp; &nbsp;Q = np.zeros((mdp.S, mdp.A)) &nbsp; &nbsp;episode_rewards = np.zeros(num_episodes) &nbsp; &nbsp;policy = np.ones(mdp.S) &nbsp; &nbsp;V = np.zeros((num_episodes, mdp.S)) &nbsp; &nbsp;N = np.zeros((mdp.S, mdp.A)) &nbsp; &nbsp;for i_episode in range(num_episodes): &nbsp; &nbsp; &nbsp; &nbsp;# epsilon greedy exploration &nbsp; &nbsp; &nbsp; &nbsp;greedy_probs = epsilon_greedy_exploration(Q, epsilon, mdp.A) &nbsp; &nbsp; &nbsp; &nbsp;state = np.random.choice(np.arange(mdp.S)) &nbsp; &nbsp; &nbsp; &nbsp;for t in range(T_max): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# epsilon greedy exploration &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;action_probs = greedy_probs(state) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;action = np.random.choice(np.arange(len(action_probs)), p=action_probs) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;next_state, reward = playtransition(mdp, state, action) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;episode_rewards[i_episode] += reward &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;N[state, action] += 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;alpha = 1/(t+1)**0.8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;best_next_action = np.argmax(Q[next_state]) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;td_target = reward + mdp.discount * Q[next_state][best_next_action] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;td_delta = td_target - Q[state][action] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Q[state][action] += alpha * td_delta &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;state = next_state &nbsp; &nbsp; &nbsp; &nbsp;V[i_episode,:] = Q.max(axis=1) &nbsp; &nbsp; &nbsp; &nbsp;policy = Q.argmax(axis=1) &nbsp; &nbsp;return V, policy, episode_rewards, N &nbsp; &nbsp; &nbsp; Exploration/Exploitation 权衡 连续学习算法会涉及到一个基本的选择： Exploitation: 在目前已经给定的信息下做出最佳选择 Exploration: 通过做出其他选择收集更多的信息 成功的平衡 exploration 和 exploitation 对智能体的学习性能有着重要的影响。太多的 exploration 会阻止智能体最大化短期奖励，因为所选的 exploration 行动可能导致来自环境的不良奖励。另一方面，在不完全的知识中 exploiting 会阻止智能体最大化长期奖励，因为所选的 exploiting 行动可能一直不是最优的。 ε-greedy exploration 策略 这是一个很朴素的 exploration 策略：在每一步都以概率 ε选择随机的动作。 这也许是最常用的、也是最简单的 exploration 策略。在很多实现中，ε都被设置为随时间衰减，但是在一些例子中，ε也被设置为定值。 def epsilon_greedy_exploration(Q, epsilon, num_actions): &nbsp; &nbsp;def policy_exp(state): &nbsp; &nbsp; &nbsp; &nbsp;probs = np.ones(num_actions, dtype=float) * epsilon / num_actions &nbsp; &nbsp; &nbsp; &nbsp;best_action = np.argmax(Q[state]) &nbsp; &nbsp; &nbsp; &nbsp;probs[best_action] += (1.0 - epsilon) &nbsp; &nbsp; &nbsp; &nbsp;return probs &nbsp; &nbsp;return policy_exp 「乐观面对不确定性」的 exploration 策略 这个概念首次在随机多臂赌博机（SMAB）环境中被首次提出，这是一个古老的决策过程，为了最大化机器给出的期望折扣奖励，赌徒在每一步都要决定摇动哪一个机器。 赌徒面临着一个 exploration/exploitation 权衡，使用具有最高平均奖励的机器或者探索其他表现并不是很好的机器以得到更高的奖励。 SMAB 和 Q-Learning 中的 exploration 问题很相似。 Exploitation: 在给定的状态下选择具有最高 Q 值的动作 Exploration: 探索更多的动作（选择没有被足够得访问或者从未被访问的动作） 「面对不确定性的乐观」（OFU）状态：无论什么时候，我们都对老虎机的输出结果是不确定的，所以我们预计环境是最好的，然后选择最好的老虎机。 OFU 背后的直觉是： 如果我们处于最好的处境：OFU 会选择最佳的老虎机（没有遗憾） 如果我么不在最好的处境中：不确定性会减少（最佳） 最著名的 OFU 算法之一是 UCB（置信区上界）[2]。我们按照下面的方法将它用在 Q-learning 中。 定义： Q(s, a): 状态 s 下采用动作 a 的 Q 值 N(t, s, a):在时间 t，动作 a 在状态 s 被选择的次数 智能体的目标是：Argmax {Q(s, a)/ a ∈ A}。代表在状态 s 下选择具有最大 Q 值的动作。但是时间 t 的实际 Q(s, a) 是未知的。 我们有：Q(s,a) = &lt;Q(t, s, a)&gt; + (Q(s,a) − &lt;Q(t, s, a)&gt;)，&lt;Q(t, s, a)&gt;是时间 t 估计的 Q 值。(Q(s,a) − &lt;Q(t, s, a)&gt;) 对应的是误差项，我们可以形成边界并使用 OFU。 Hoeffding 不等式是限制这个误差的一种方式，我们可以证明： 最优策略可以被写成： Argmax {Q+(t, s, a)/ a ∈ A} 其中β ≥ 0 调节 exploration。当β＝0 的时候，该策略仅仅利用过去的估计（遵循 leader 策略）。 这个范围是该领域最常用的。还有很多其他改进这个范围的工作（UCB-V、UCB、KL-UCB、Bayes-UCB、BESA 等）。 这里是我们对经典 UCB exploration 策略的实现，以及它在 Q-Learning 中使用的结果。 def UCB_exploration(Q, num_actions, beta=1): &nbsp; &nbsp;def UCB_exp(state, N, t): &nbsp; &nbsp; &nbsp; &nbsp;probs = np.zeros(num_actions, dtype=float) &nbsp; &nbsp; &nbsp; &nbsp;Q_ = Q[state,:]/max(Q[state,:]) + np.sqrt(beta*np.log(t+1)/(2*N[state])) &nbsp; &nbsp; &nbsp; &nbsp;best_action = Q_.argmax() &nbsp; &nbsp; &nbsp; &nbsp;probs[best_action] = 1 &nbsp; &nbsp; &nbsp; &nbsp;return probs &nbsp; &nbsp;return UCB_exp UCB exploration 似乎能够快速地达到很高的奖励，然而训练过程还是受到早期 exploration 的干扰，对于更复杂的马尔科夫决策过程而言，这是有优势的，因为智能体可以免于非最优的解决方案。 我们来更加仔细地比较这两种策略。 推荐阅读 顶级AI【数据】资源送给你！ 程序员给银行植入病毒，盗取 718 万，被判 10 年半！ 45张让你又笑又哭的趣图，有没有戳中你？ 有了GAN，可以自由的对人脸照片进行涂鸦编辑啦！ 一张大尺度美女图，竟然推进了图片算法的进步。。。 喜欢就点击“在看”吧！" />
<meta property="og:description" content="本文由机器之心编译（ID：almosthuman2014） 强化学习（RL） 强化学习是机器学习的一个重要领域，其中智能体通过对状态的感知、对行动的选择以及接受奖励和环境相连接。在每一步，智能体都要观察状态、选择并执行一个行动，这会改变它的状态并产生一个奖励。 马尔科夫决策过程（MDP） 我们将要解决「forest fire」的马尔科夫决策问题，这个在 python 的 MDP 工具箱（http://pymdptoolbox.readthedocs.io/en/latest/api/example.html）中是可以看到的。 森林由两种行动来管理：「等待」和「砍伐」。我们每年做出一个行动，首要目标是为野生动物维护一片古老的森林，次要目标是伐木赚钱。每年都会以 p 的概率发生森林火灾（森林正常生长的概率就是 1-p）。 我们将马尔科夫决策过程记为 (S, A, P, R, γ)，其中： S 是有限状态空间：按照树龄分为三类：0—20 年，21—40 年，大于 40 年 A 是有限行动空间：「等待」和「砍伐」 P 和 R 是转换矩阵和奖励矩阵，它们的闭合形式容易表达出来 γ是表示长期奖励和短期奖励之间的区别的折扣因子 策略π是在给定的当前状态下动作的平稳分布（马尔可夫性） 目标就是找到在不了解任何马尔科夫动态特性的情况下来寻找马尔科夫决策过程的最优策略π*。需要注意的是，如果我们具有这种知识，像最有价值迭代这种算法就可以找到最优策略。 def optimal_value_iteration(mdp, V0, num_iterations, epsilon=0.0001): &nbsp; &nbsp;V = np.zeros((num_iterations+1, mdp.S)) &nbsp; &nbsp;V[0][:] = np.ones(mdp.S)*V0 &nbsp; &nbsp;X = np.zeros((num_iterations+1, mdp.A, mdp.S)) &nbsp; &nbsp;star = np.zeros((num_iterations+1,mdp.S)) &nbsp; &nbsp;for k in range(num_iterations): &nbsp; &nbsp; &nbsp; &nbsp;for s in range(mdp.S): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for a in range(mdp.A): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;X[k+1][a][s] = mdp.R[a][s] + mdp.discount*np.sum(mdp.P[a][s].dot(V[k])) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;star[k+1][s] = (np.argmax(X[k+1,:,s])) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;V[k+1][s] = np.max(X[k+1,:,s]) &nbsp; &nbsp; &nbsp; &nbsp;if (np.max(V[k+1]-V[k])-np.min(V[k+1]-V[k])) &lt; epsilon: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;V[k+1:] = V[k+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;star[k+1:] = star[k+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;X[k+1:] = X[k+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;break &nbsp; &nbsp; &nbsp; &nbsp;else: pass &nbsp; &nbsp;return star, V, X &nbsp; &nbsp;&nbsp; 这里的最优策略是相当符合直觉的，就是等到森林达到最老的树龄再砍伐，因为我们把树龄最老的时候的砍伐奖励设置成了其成长过程中的 5 倍（r1=10,r2=50）。 Q-LEARNING Q-Learning 中策略（π）的质量函数，它将任何一个状态动作组合（s,a）和在观察状态 s 下通过选择行动 a 而得到的期望积累折扣未来奖励映射在一起。 Q-Leraning 被称为「没有模型」，这意味着它不会尝试为马尔科夫决策过程的动态特性建模，它直接估计每个状态下每个动作的 Q 值。然后可以通过选择每个状态具有最高 Q 值的动作来绘制策略。 如果智能体能够以无限多的次数访问状态—行动对，那么 Q-Learning 将会收敛到最优的 Q 函数 [1]。 同样，我们也不会深入讨论 Q-Learning 的细节。如果你对它不太熟悉，这里有 Siraj Raval 的解释视频。 下面我们将展示 Q-Learning 的 Python 实现。请注意，这里所拥的学习率（alpha）遵循 [3] 的结果，使用 w=0.8 的多项式。 这里用到的探索策略（ε-greedy）在后面会有细节介绍。 def q_learning(mdp, num_episodes, T_max, epsilon=0.01): &nbsp; &nbsp;Q = np.zeros((mdp.S, mdp.A)) &nbsp; &nbsp;episode_rewards = np.zeros(num_episodes) &nbsp; &nbsp;policy = np.ones(mdp.S) &nbsp; &nbsp;V = np.zeros((num_episodes, mdp.S)) &nbsp; &nbsp;N = np.zeros((mdp.S, mdp.A)) &nbsp; &nbsp;for i_episode in range(num_episodes): &nbsp; &nbsp; &nbsp; &nbsp;# epsilon greedy exploration &nbsp; &nbsp; &nbsp; &nbsp;greedy_probs = epsilon_greedy_exploration(Q, epsilon, mdp.A) &nbsp; &nbsp; &nbsp; &nbsp;state = np.random.choice(np.arange(mdp.S)) &nbsp; &nbsp; &nbsp; &nbsp;for t in range(T_max): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# epsilon greedy exploration &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;action_probs = greedy_probs(state) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;action = np.random.choice(np.arange(len(action_probs)), p=action_probs) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;next_state, reward = playtransition(mdp, state, action) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;episode_rewards[i_episode] += reward &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;N[state, action] += 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;alpha = 1/(t+1)**0.8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;best_next_action = np.argmax(Q[next_state]) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;td_target = reward + mdp.discount * Q[next_state][best_next_action] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;td_delta = td_target - Q[state][action] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Q[state][action] += alpha * td_delta &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;state = next_state &nbsp; &nbsp; &nbsp; &nbsp;V[i_episode,:] = Q.max(axis=1) &nbsp; &nbsp; &nbsp; &nbsp;policy = Q.argmax(axis=1) &nbsp; &nbsp;return V, policy, episode_rewards, N &nbsp; &nbsp; &nbsp; Exploration/Exploitation 权衡 连续学习算法会涉及到一个基本的选择： Exploitation: 在目前已经给定的信息下做出最佳选择 Exploration: 通过做出其他选择收集更多的信息 成功的平衡 exploration 和 exploitation 对智能体的学习性能有着重要的影响。太多的 exploration 会阻止智能体最大化短期奖励，因为所选的 exploration 行动可能导致来自环境的不良奖励。另一方面，在不完全的知识中 exploiting 会阻止智能体最大化长期奖励，因为所选的 exploiting 行动可能一直不是最优的。 ε-greedy exploration 策略 这是一个很朴素的 exploration 策略：在每一步都以概率 ε选择随机的动作。 这也许是最常用的、也是最简单的 exploration 策略。在很多实现中，ε都被设置为随时间衰减，但是在一些例子中，ε也被设置为定值。 def epsilon_greedy_exploration(Q, epsilon, num_actions): &nbsp; &nbsp;def policy_exp(state): &nbsp; &nbsp; &nbsp; &nbsp;probs = np.ones(num_actions, dtype=float) * epsilon / num_actions &nbsp; &nbsp; &nbsp; &nbsp;best_action = np.argmax(Q[state]) &nbsp; &nbsp; &nbsp; &nbsp;probs[best_action] += (1.0 - epsilon) &nbsp; &nbsp; &nbsp; &nbsp;return probs &nbsp; &nbsp;return policy_exp 「乐观面对不确定性」的 exploration 策略 这个概念首次在随机多臂赌博机（SMAB）环境中被首次提出，这是一个古老的决策过程，为了最大化机器给出的期望折扣奖励，赌徒在每一步都要决定摇动哪一个机器。 赌徒面临着一个 exploration/exploitation 权衡，使用具有最高平均奖励的机器或者探索其他表现并不是很好的机器以得到更高的奖励。 SMAB 和 Q-Learning 中的 exploration 问题很相似。 Exploitation: 在给定的状态下选择具有最高 Q 值的动作 Exploration: 探索更多的动作（选择没有被足够得访问或者从未被访问的动作） 「面对不确定性的乐观」（OFU）状态：无论什么时候，我们都对老虎机的输出结果是不确定的，所以我们预计环境是最好的，然后选择最好的老虎机。 OFU 背后的直觉是： 如果我们处于最好的处境：OFU 会选择最佳的老虎机（没有遗憾） 如果我么不在最好的处境中：不确定性会减少（最佳） 最著名的 OFU 算法之一是 UCB（置信区上界）[2]。我们按照下面的方法将它用在 Q-learning 中。 定义： Q(s, a): 状态 s 下采用动作 a 的 Q 值 N(t, s, a):在时间 t，动作 a 在状态 s 被选择的次数 智能体的目标是：Argmax {Q(s, a)/ a ∈ A}。代表在状态 s 下选择具有最大 Q 值的动作。但是时间 t 的实际 Q(s, a) 是未知的。 我们有：Q(s,a) = &lt;Q(t, s, a)&gt; + (Q(s,a) − &lt;Q(t, s, a)&gt;)，&lt;Q(t, s, a)&gt;是时间 t 估计的 Q 值。(Q(s,a) − &lt;Q(t, s, a)&gt;) 对应的是误差项，我们可以形成边界并使用 OFU。 Hoeffding 不等式是限制这个误差的一种方式，我们可以证明： 最优策略可以被写成： Argmax {Q+(t, s, a)/ a ∈ A} 其中β ≥ 0 调节 exploration。当β＝0 的时候，该策略仅仅利用过去的估计（遵循 leader 策略）。 这个范围是该领域最常用的。还有很多其他改进这个范围的工作（UCB-V、UCB、KL-UCB、Bayes-UCB、BESA 等）。 这里是我们对经典 UCB exploration 策略的实现，以及它在 Q-Learning 中使用的结果。 def UCB_exploration(Q, num_actions, beta=1): &nbsp; &nbsp;def UCB_exp(state, N, t): &nbsp; &nbsp; &nbsp; &nbsp;probs = np.zeros(num_actions, dtype=float) &nbsp; &nbsp; &nbsp; &nbsp;Q_ = Q[state,:]/max(Q[state,:]) + np.sqrt(beta*np.log(t+1)/(2*N[state])) &nbsp; &nbsp; &nbsp; &nbsp;best_action = Q_.argmax() &nbsp; &nbsp; &nbsp; &nbsp;probs[best_action] = 1 &nbsp; &nbsp; &nbsp; &nbsp;return probs &nbsp; &nbsp;return UCB_exp UCB exploration 似乎能够快速地达到很高的奖励，然而训练过程还是受到早期 exploration 的干扰，对于更复杂的马尔科夫决策过程而言，这是有优势的，因为智能体可以免于非最优的解决方案。 我们来更加仔细地比较这两种策略。 推荐阅读 顶级AI【数据】资源送给你！ 程序员给银行植入病毒，盗取 718 万，被判 10 年半！ 45张让你又笑又哭的趣图，有没有戳中你？ 有了GAN，可以自由的对人脸照片进行涂鸦编辑啦！ 一张大尺度美女图，竟然推进了图片算法的进步。。。 喜欢就点击“在看”吧！" />
<link rel="canonical" href="https://mlh.app/2019/04/08/728684.html" />
<meta property="og:url" content="https://mlh.app/2019/04/08/728684.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-08T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"本文由机器之心编译（ID：almosthuman2014） 强化学习（RL） 强化学习是机器学习的一个重要领域，其中智能体通过对状态的感知、对行动的选择以及接受奖励和环境相连接。在每一步，智能体都要观察状态、选择并执行一个行动，这会改变它的状态并产生一个奖励。 马尔科夫决策过程（MDP） 我们将要解决「forest fire」的马尔科夫决策问题，这个在 python 的 MDP 工具箱（http://pymdptoolbox.readthedocs.io/en/latest/api/example.html）中是可以看到的。 森林由两种行动来管理：「等待」和「砍伐」。我们每年做出一个行动，首要目标是为野生动物维护一片古老的森林，次要目标是伐木赚钱。每年都会以 p 的概率发生森林火灾（森林正常生长的概率就是 1-p）。 我们将马尔科夫决策过程记为 (S, A, P, R, γ)，其中： S 是有限状态空间：按照树龄分为三类：0—20 年，21—40 年，大于 40 年 A 是有限行动空间：「等待」和「砍伐」 P 和 R 是转换矩阵和奖励矩阵，它们的闭合形式容易表达出来 γ是表示长期奖励和短期奖励之间的区别的折扣因子 策略π是在给定的当前状态下动作的平稳分布（马尔可夫性） 目标就是找到在不了解任何马尔科夫动态特性的情况下来寻找马尔科夫决策过程的最优策略π*。需要注意的是，如果我们具有这种知识，像最有价值迭代这种算法就可以找到最优策略。 def optimal_value_iteration(mdp, V0, num_iterations, epsilon=0.0001): &nbsp; &nbsp;V = np.zeros((num_iterations+1, mdp.S)) &nbsp; &nbsp;V[0][:] = np.ones(mdp.S)*V0 &nbsp; &nbsp;X = np.zeros((num_iterations+1, mdp.A, mdp.S)) &nbsp; &nbsp;star = np.zeros((num_iterations+1,mdp.S)) &nbsp; &nbsp;for k in range(num_iterations): &nbsp; &nbsp; &nbsp; &nbsp;for s in range(mdp.S): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for a in range(mdp.A): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;X[k+1][a][s] = mdp.R[a][s] + mdp.discount*np.sum(mdp.P[a][s].dot(V[k])) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;star[k+1][s] = (np.argmax(X[k+1,:,s])) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;V[k+1][s] = np.max(X[k+1,:,s]) &nbsp; &nbsp; &nbsp; &nbsp;if (np.max(V[k+1]-V[k])-np.min(V[k+1]-V[k])) &lt; epsilon: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;V[k+1:] = V[k+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;star[k+1:] = star[k+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;X[k+1:] = X[k+1] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;break &nbsp; &nbsp; &nbsp; &nbsp;else: pass &nbsp; &nbsp;return star, V, X &nbsp; &nbsp;&nbsp; 这里的最优策略是相当符合直觉的，就是等到森林达到最老的树龄再砍伐，因为我们把树龄最老的时候的砍伐奖励设置成了其成长过程中的 5 倍（r1=10,r2=50）。 Q-LEARNING Q-Learning 中策略（π）的质量函数，它将任何一个状态动作组合（s,a）和在观察状态 s 下通过选择行动 a 而得到的期望积累折扣未来奖励映射在一起。 Q-Leraning 被称为「没有模型」，这意味着它不会尝试为马尔科夫决策过程的动态特性建模，它直接估计每个状态下每个动作的 Q 值。然后可以通过选择每个状态具有最高 Q 值的动作来绘制策略。 如果智能体能够以无限多的次数访问状态—行动对，那么 Q-Learning 将会收敛到最优的 Q 函数 [1]。 同样，我们也不会深入讨论 Q-Learning 的细节。如果你对它不太熟悉，这里有 Siraj Raval 的解释视频。 下面我们将展示 Q-Learning 的 Python 实现。请注意，这里所拥的学习率（alpha）遵循 [3] 的结果，使用 w=0.8 的多项式。 这里用到的探索策略（ε-greedy）在后面会有细节介绍。 def q_learning(mdp, num_episodes, T_max, epsilon=0.01): &nbsp; &nbsp;Q = np.zeros((mdp.S, mdp.A)) &nbsp; &nbsp;episode_rewards = np.zeros(num_episodes) &nbsp; &nbsp;policy = np.ones(mdp.S) &nbsp; &nbsp;V = np.zeros((num_episodes, mdp.S)) &nbsp; &nbsp;N = np.zeros((mdp.S, mdp.A)) &nbsp; &nbsp;for i_episode in range(num_episodes): &nbsp; &nbsp; &nbsp; &nbsp;# epsilon greedy exploration &nbsp; &nbsp; &nbsp; &nbsp;greedy_probs = epsilon_greedy_exploration(Q, epsilon, mdp.A) &nbsp; &nbsp; &nbsp; &nbsp;state = np.random.choice(np.arange(mdp.S)) &nbsp; &nbsp; &nbsp; &nbsp;for t in range(T_max): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# epsilon greedy exploration &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;action_probs = greedy_probs(state) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;action = np.random.choice(np.arange(len(action_probs)), p=action_probs) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;next_state, reward = playtransition(mdp, state, action) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;episode_rewards[i_episode] += reward &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;N[state, action] += 1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;alpha = 1/(t+1)**0.8 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;best_next_action = np.argmax(Q[next_state]) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;td_target = reward + mdp.discount * Q[next_state][best_next_action] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;td_delta = td_target - Q[state][action] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Q[state][action] += alpha * td_delta &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;state = next_state &nbsp; &nbsp; &nbsp; &nbsp;V[i_episode,:] = Q.max(axis=1) &nbsp; &nbsp; &nbsp; &nbsp;policy = Q.argmax(axis=1) &nbsp; &nbsp;return V, policy, episode_rewards, N &nbsp; &nbsp; &nbsp; Exploration/Exploitation 权衡 连续学习算法会涉及到一个基本的选择： Exploitation: 在目前已经给定的信息下做出最佳选择 Exploration: 通过做出其他选择收集更多的信息 成功的平衡 exploration 和 exploitation 对智能体的学习性能有着重要的影响。太多的 exploration 会阻止智能体最大化短期奖励，因为所选的 exploration 行动可能导致来自环境的不良奖励。另一方面，在不完全的知识中 exploiting 会阻止智能体最大化长期奖励，因为所选的 exploiting 行动可能一直不是最优的。 ε-greedy exploration 策略 这是一个很朴素的 exploration 策略：在每一步都以概率 ε选择随机的动作。 这也许是最常用的、也是最简单的 exploration 策略。在很多实现中，ε都被设置为随时间衰减，但是在一些例子中，ε也被设置为定值。 def epsilon_greedy_exploration(Q, epsilon, num_actions): &nbsp; &nbsp;def policy_exp(state): &nbsp; &nbsp; &nbsp; &nbsp;probs = np.ones(num_actions, dtype=float) * epsilon / num_actions &nbsp; &nbsp; &nbsp; &nbsp;best_action = np.argmax(Q[state]) &nbsp; &nbsp; &nbsp; &nbsp;probs[best_action] += (1.0 - epsilon) &nbsp; &nbsp; &nbsp; &nbsp;return probs &nbsp; &nbsp;return policy_exp 「乐观面对不确定性」的 exploration 策略 这个概念首次在随机多臂赌博机（SMAB）环境中被首次提出，这是一个古老的决策过程，为了最大化机器给出的期望折扣奖励，赌徒在每一步都要决定摇动哪一个机器。 赌徒面临着一个 exploration/exploitation 权衡，使用具有最高平均奖励的机器或者探索其他表现并不是很好的机器以得到更高的奖励。 SMAB 和 Q-Learning 中的 exploration 问题很相似。 Exploitation: 在给定的状态下选择具有最高 Q 值的动作 Exploration: 探索更多的动作（选择没有被足够得访问或者从未被访问的动作） 「面对不确定性的乐观」（OFU）状态：无论什么时候，我们都对老虎机的输出结果是不确定的，所以我们预计环境是最好的，然后选择最好的老虎机。 OFU 背后的直觉是： 如果我们处于最好的处境：OFU 会选择最佳的老虎机（没有遗憾） 如果我么不在最好的处境中：不确定性会减少（最佳） 最著名的 OFU 算法之一是 UCB（置信区上界）[2]。我们按照下面的方法将它用在 Q-learning 中。 定义： Q(s, a): 状态 s 下采用动作 a 的 Q 值 N(t, s, a):在时间 t，动作 a 在状态 s 被选择的次数 智能体的目标是：Argmax {Q(s, a)/ a ∈ A}。代表在状态 s 下选择具有最大 Q 值的动作。但是时间 t 的实际 Q(s, a) 是未知的。 我们有：Q(s,a) = &lt;Q(t, s, a)&gt; + (Q(s,a) − &lt;Q(t, s, a)&gt;)，&lt;Q(t, s, a)&gt;是时间 t 估计的 Q 值。(Q(s,a) − &lt;Q(t, s, a)&gt;) 对应的是误差项，我们可以形成边界并使用 OFU。 Hoeffding 不等式是限制这个误差的一种方式，我们可以证明： 最优策略可以被写成： Argmax {Q+(t, s, a)/ a ∈ A} 其中β ≥ 0 调节 exploration。当β＝0 的时候，该策略仅仅利用过去的估计（遵循 leader 策略）。 这个范围是该领域最常用的。还有很多其他改进这个范围的工作（UCB-V、UCB、KL-UCB、Bayes-UCB、BESA 等）。 这里是我们对经典 UCB exploration 策略的实现，以及它在 Q-Learning 中使用的结果。 def UCB_exploration(Q, num_actions, beta=1): &nbsp; &nbsp;def UCB_exp(state, N, t): &nbsp; &nbsp; &nbsp; &nbsp;probs = np.zeros(num_actions, dtype=float) &nbsp; &nbsp; &nbsp; &nbsp;Q_ = Q[state,:]/max(Q[state,:]) + np.sqrt(beta*np.log(t+1)/(2*N[state])) &nbsp; &nbsp; &nbsp; &nbsp;best_action = Q_.argmax() &nbsp; &nbsp; &nbsp; &nbsp;probs[best_action] = 1 &nbsp; &nbsp; &nbsp; &nbsp;return probs &nbsp; &nbsp;return UCB_exp UCB exploration 似乎能够快速地达到很高的奖励，然而训练过程还是受到早期 exploration 的干扰，对于更复杂的马尔科夫决策过程而言，这是有优势的，因为智能体可以免于非最优的解决方案。 我们来更加仔细地比较这两种策略。 推荐阅读 顶级AI【数据】资源送给你！ 程序员给银行植入病毒，盗取 718 万，被判 10 年半！ 45张让你又笑又哭的趣图，有没有戳中你？ 有了GAN，可以自由的对人脸照片进行涂鸦编辑啦！ 一张大尺度美女图，竟然推进了图片算法的进步。。。 喜欢就点击“在看”吧！","@type":"BlogPosting","url":"https://mlh.app/2019/04/08/728684.html","headline":"一份从代码出发的强化学习Q-Learning入门教程，请笑纳！","dateModified":"2019-04-08T00:00:00+08:00","datePublished":"2019-04-08T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/04/08/728684.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>一份从代码出发的强化学习Q-Learning入门教程，请笑纳！</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="rich_media_content" id="js_content"> 
   <p style="text-align:center;margin-left:16px;"><span style="color:rgb(127,127,127);font-size:14px;">本文由机器之心编译（ID：almosthuman2014）</span></p>
   <p style="text-align:center;margin-left:16px;"><strong style="text-align:center;"><span style="font-size:16px;"><strong style="line-height:25.6px;font-family:'微软雅黑';font-size:14px;"><span style="color:rgb(127,127,127);"><strong style="color:rgb(62,62,62);line-height:25.6px;"><span style="color:rgb(127,127,127);"><br></span></strong></span></strong></span></strong></p>
   <p style="text-align:center;margin-left:16px;"><strong style="text-align:center;"><span style="font-size:16px;">强化学习（RL）</span></strong></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><strong><span style="font-size:15px;color:rgb(0,82,255);">强化学习是机器学习的一个重要领域，其中智能体通过对状态的感知、对行动的选择以及接受奖励和环境相连接。在每一步，智能体都要观察状态、选择并执行一个行动，这会改变它的状态并产生一个奖励。</span></strong></p>
   <p><br></p>
   <p style="line-height:1.75em;text-align:center;margin-left:16px;"><strong><span style="font-size:16px;">马尔科夫决策过程（MDP）</span></strong></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">我们将要解决「forest fire」的马尔科夫决策问题，这个在 python 的 MDP 工具箱（http://pymdptoolbox.readthedocs.io/en/latest/api/example.html）中是可以看到的。</span></p>
   <p><br></p>
   <blockquote>
    <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;color:rgb(136,136,136);">森林由两种行动来管理：「等待」和「砍伐」。我们每年做出一个行动，首要目标是为野生动物维护一片古老的森林，次要目标是伐木赚钱。每年都会以 p 的概率发生森林火灾（森林正常生长的概率就是 1-p）。</span></p>
   </blockquote>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"></span></p>
   <p style="margin-left:16px;text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW8atOrHpP2ZJruUIawdCsq7hAKt3mRIo1Au5Ih3qiaA7OJ2ZSY6bLH7jxOKGyLLrk8j8lBQ3hRGf2w/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"></span></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">我们将马尔科夫决策过程记为 (S, A, P, R, γ)，其中：</span></p>
   <p><br></p>
   <ul class="list-paddingleft-2" style="list-style-type:disc;margin-left:16px;">
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">S 是</span><strong><span style="font-size:15px;color:rgb(0,0,0);">有限状态空间</span></strong><span style="font-size:15px;">：按照树龄分为三类：0—20 年，21—40 年，大于 40 年</span></p></li>
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">A 是</span><strong><span style="font-size:15px;color:rgb(0,0,0);">有限行动空间</span></strong><span style="font-size:15px;">：「等待」和「砍伐」</span></p></li>
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">P 和 R 是</span><strong><span style="font-size:15px;color:rgb(0,0,0);">转换矩阵和奖励矩阵</span></strong><span style="font-size:15px;">，它们的闭合形式容易表达出来</span></p></li>
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">γ是表示</span><strong><span style="font-size:15px;color:rgb(0,0,0);">长期奖励和短期奖励之间的区别的折扣因子</span></strong></p></li>
    <li><p style="line-height:1.75em;"><strong><span style="font-size:15px;color:rgb(0,0,0);">策略</span></strong><span style="font-size:15px;">π是在给定的当前状态下动作的平稳分布（马尔可夫性）</span></p></li>
   </ul>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><strong><span style="font-size:15px;color:rgb(0,82,255);">目标就是找到在不了解任何马尔科夫动态特性的情况下来寻找马尔科夫决策过程的最优策略π*。</span></strong><span style="font-size:15px;">需要注意的是，如果我们具有这种知识，像最有价值迭代这种算法就可以找到最优策略。</span></p>
   <p><br></p>
   <pre style="font-size:16px;color:rgb(62,62,62);line-height:inherit;"></pre>
   <p style="font-size:14px;color:rgb(169,183,198);line-height:18px;background:rgb(40,43,46);font-family:Consolas, Inconsolata, Courier, monospace;letter-spacing:0px;margin-left:16px;"><span style="font-size:12px;"><span class="hljs-function" style="color:rgb(248,35,117);line-height:inherit;"><span class="hljs-keyword">def</span> <span class="hljs-title" style="line-height:inherit;">optimal_value_iteration(mdp, V0, num_iterations, epsilon=<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">0.0001</span>)</span>:</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;V = np.zeros((num_iterations+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>, mdp.S))<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;V[<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">0</span>][:] = np.ones(mdp.S)*V0<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;X = np.zeros((num_iterations+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>, mdp.A, mdp.S))<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;star = np.zeros((num_iterations+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>,mdp.S))<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(248,35,117);line-height:inherit;">for</span> k <span class="hljs-keyword" style="color:rgb(248,35,117);line-height:inherit;">in</span> range(num_iterations):<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(248,35,117);line-height:inherit;">for</span> s <span class="hljs-keyword" style="color:rgb(248,35,117);line-height:inherit;">in</span> range(mdp.S):<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(248,35,117);line-height:inherit;">for</span> a <span class="hljs-keyword" style="color:rgb(248,35,117);line-height:inherit;">in</span> range(mdp.A):<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;X[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>][a][s] = mdp.R[a][s] + mdp.discount*np.sum(mdp.P[a][s].dot(V[k]))<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;star[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>][s] = (np.argmax(X[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>,:,s]))<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;V[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>][s] = np.max(X[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>,:,s])<br style="font-size:inherit;color:inherit;line-height:inherit;"><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(248,35,117);line-height:inherit;">if</span> (np.max(V[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>]-V[k])-np.min(V[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>]-V[k])) &lt; epsilon:<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;V[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>:] = V[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>]<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;star[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>:] = star[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>]<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;X[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>:] = X[k+<span class="hljs-number" style="color:rgb(174,135,250);line-height:inherit;">1</span>]<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(248,35,117);line-height:inherit;">break</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(248,35,117);line-height:inherit;">else</span>: <span class="hljs-keyword" style="color:rgb(248,35,117);line-height:inherit;">pass</span><br style="font-size:inherit;color:inherit;line-height:inherit;"><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(248,35,117);line-height:inherit;">return</span> star, V, X</span><br style="font-size:inherit;color:inherit;line-height:inherit;"></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"></span></p>
   <p style="margin-left:16px;text-align:center;">&nbsp; &nbsp;&nbsp;<img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8atOrHpP2ZJruUIawdCsq75D3cjta2mRY976oicN6AGibcMDoBHRwtuvSzDA05Biardl9KXprhOTvxg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"></span></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">这里的最优策略是相当符合直觉的，就是等到森林达到最老的树龄再砍伐，因为我们把树龄最老的时候的砍伐奖励设置成了其成长过程中的 5 倍（r1=10,r2=50）。</span></p>
   <p><br></p>
   <p style="text-align:center;margin-left:16px;"><span style="font-size:16px;"><strong>Q-LEARNING</strong></span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><strong><span style="font-size:15px;color:rgb(0,82,255);">Q-Learning 中策略（π）的质量函数，它将任何一个状态动作组合（s,a）和在观察状态 s 下通过选择行动 a 而得到的期望积累折扣未来奖励映射在一起。</span></strong></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">Q-Leraning 被称为「没有模型」，</span><strong><span style="font-size:15px;color:rgb(0,82,255);">这意味着它不会尝试为马尔科夫决策过程的动态特性建模，它直接估计每个状态下每个动作的 Q 值。然后可以通过选择每个状态具有最高 Q 值的动作来绘制策略。</span></strong></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">如果智能体能够以无限多的次数访问状态—行动对，那么 Q-Learning 将会收敛到最优的 Q 函数 [1]。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">同样，我们也不会深入讨论 Q-Learning 的细节。如果你对它不太熟悉，这里有 Siraj Raval 的解释视频。</span></p>
   <p style="line-height:1.75em;"><br></p>
   <p style="margin-left:16px;"><br></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">下面我们将展示 Q-Learning 的 Python 实现。请注意，这里所拥的学习率（alpha）遵循 [3] 的结果，使用 w=0.8 的多项式。</span></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"><br></span></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">这里用到的探索策略（ε-greedy）在后面会有细节介绍。</span></p>
   <p><br></p>
   <pre style="font-size:16px;color:rgb(62,62,62);line-height:inherit;"></pre>
   <p style="font-size:14px;color:rgb(169,183,198);line-height:18px;background:rgb(40,43,46);font-family:Consolas, Inconsolata, Courier, monospace;letter-spacing:0px;margin-left:16px;"><span class="hljs-function" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;"><span class="hljs-keyword">def</span> <span class="hljs-title" style="line-height:inherit;">q_learning(mdp, num_episodes, T_max, epsilon=<span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">0.01</span>)</span>:</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;Q = np.zeros((mdp.S, mdp.A))<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;episode_rewards = np.zeros(num_episodes)<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;policy = np.ones(mdp.S)<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;V = np.zeros((num_episodes, mdp.S))<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;N = np.zeros((mdp.S, mdp.A))<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;<span class="hljs-keyword" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;">for</span> i_episode <span class="hljs-keyword" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;">in</span> range(num_episodes): <br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-comment" style="font-size:inherit;color:rgb(128,128,128);line-height:inherit;"># epsilon greedy exploration</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;greedy_probs = epsilon_greedy_exploration(Q, epsilon, mdp.A)<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;state = np.random.choice(np.arange(mdp.S))<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;">for</span> t <span class="hljs-keyword" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;">in</span> range(T_max):<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-comment" style="font-size:inherit;color:rgb(128,128,128);line-height:inherit;"># epsilon greedy exploration</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;action_probs = greedy_probs(state)<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;action = np.random.choice(np.arange(len(action_probs)), p=action_probs)<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;next_state, reward = playtransition(mdp, state, action)<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;episode_rewards[i_episode] += reward<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;N[state, action] += <span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">1</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;alpha = <span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">1</span>/(t+<span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">1</span>)**<span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">0.8</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;best_next_action = np.argmax(Q[next_state]) &nbsp; &nbsp;<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;td_target = reward + mdp.discount * Q[next_state][best_next_action]<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;td_delta = td_target - Q[state][action]<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Q[state][action] += alpha * td_delta<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;state = next_state<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;V[i_episode,:] = Q.max(axis=<span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">1</span>)<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;policy = Q.argmax(axis=<span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">1</span>)<br style="font-size:inherit;color:inherit;line-height:inherit;"><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;<span class="hljs-keyword" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;">return</span> V, policy, episode_rewards, N<br style="font-size:inherit;color:inherit;line-height:inherit;"></p>
   <p><br></p>
   <p style="margin-left:16px;text-align:center;">&nbsp; &nbsp; &nbsp;<img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8atOrHpP2ZJruUIawdCsq7WMicNmWOFwaHCbDOQPp23Sho2LWeN2Fy3xgTK7xzxCe5d5BD2AnlPPQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;margin-left:16px;"><strong><span style="font-size:16px;">Exploration/Exploitation 权衡</span></strong></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">连续学习算法会涉及到一个基本的选择：</span></p>
   <p><br></p>
   <ul class="list-paddingleft-2" style="list-style-type:disc;margin-left:16px;">
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">Exploitation: 在目前已经给定的信息下做出最佳选择</span></p></li>
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">Exploration: 通过做出其他选择收集更多的信息</span></p></li>
   </ul>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">成功的平衡 exploration 和 exploitation 对智能体的学习性能有着重要的影响。</span><strong><span style="font-size:15px;color:rgb(0,82,255);">太多的 exploration 会阻止智能体最大化短期奖励，因为所选的 exploration 行动可能导致来自环境的不良奖励。另一方面，在不完全的知识中 exploiting 会阻止智能体最大化长期奖励，因为所选的 exploiting 行动可能一直不是最优的。</span></strong></p>
   <p><br></p>
   <p style="line-height:1.75em;text-align:center;margin-left:16px;"><strong><span style="font-size:16px;">ε-greedy exploration 策略</span></strong></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"><br></span></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">这是一个很朴素的 exploration 策略：</span><strong><span style="font-size:15px;color:rgb(0,82,255);">在每一步都以概率 ε选择随机的动作。</span></strong></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"><br></span></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">这也许是最常用的、也是最简单的 exploration 策略。在很多实现中，ε都被设置为随时间衰减，但是在一些例子中，ε也被设置为定值。</span></p>
   <p><br></p>
   <pre style="font-size:16px;color:rgb(62,62,62);line-height:inherit;"></pre>
   <p style="font-size:14px;color:rgb(169,183,198);line-height:18px;background:rgb(40,43,46);font-family:Consolas, Inconsolata, Courier, monospace;letter-spacing:0px;margin-left:16px;"><span class="hljs-function" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;"><span class="hljs-keyword">def</span> <span class="hljs-title" style="line-height:inherit;">epsilon_greedy_exploration(Q, epsilon, num_actions)</span>:</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;<span class="hljs-function" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;"><span class="hljs-keyword">def</span> <span class="hljs-title" style="line-height:inherit;">policy_exp(state)</span>:</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;probs = np.ones(num_actions, dtype=float) * epsilon / num_actions<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;best_action = np.argmax(Q[state])<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;probs[best_action] += (<span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">1.0</span> - epsilon)<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;">return</span> probs<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;<span class="hljs-keyword" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;">return</span> policy_exp<br style="font-size:inherit;color:inherit;line-height:inherit;"></p>
   <p><br></p>
   <p style="line-height:1.75em;text-align:center;margin-left:16px;"><span style="font-size:16px;"><strong>「乐观面对不确定性」的 exploration 策略</strong></span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">这个概念首次在随机多臂赌博机（SMAB）环境中被首次提出，这是一个古老的决策过程，为了最大化机器给出的期望折扣奖励，赌徒在每一步都要决定摇动哪一个机器。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">赌徒面临着一个 exploration/exploitation 权衡，使用具有最高平均奖励的机器或者探索其他表现并不是很好的机器以得到更高的奖励。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">SMAB 和 Q-Learning 中的 exploration 问题很相似。</span></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"><br></span></p>
   <ul class="list-paddingleft-2" style="list-style-type:disc;margin-left:16px;">
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">Exploitation: 在给定的状态下选择具有最高 Q 值的动作</span></p></li>
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">Exploration: 探索更多的动作（选择没有被足够得访问或者从未被访问的动作）</span></p></li>
   </ul>
   <p><br></p>
   <p style="margin-left:16px;text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8atOrHpP2ZJruUIawdCsq7tg5zeiajTx5BSgkAzyX3lD7C8Sia1CTl7ibEjXgcvSiaclkibRAFOnpib4QQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">「面对不确定性的乐观」（OFU）状态：无论什么时候，我们都对老虎机的输出结果是不确定的，所以我们预计环境是最好的，然后选择最好的老虎机。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">OFU 背后的直觉是：</span></p>
   <p><br></p>
   <ul class="list-paddingleft-2" style="list-style-type:disc;margin-left:16px;">
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">如果我们处于最好的处境：OFU 会选择最佳的老虎机（没有遗憾）</span></p></li>
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">如果我么不在最好的处境中：不确定性会减少（最佳）</span></p></li>
   </ul>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">最著名的 OFU 算法之一是 UCB（置信区上界）[2]。我们按照下面的方法将它用在 Q-learning 中。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">定义：</span></p>
   <p><br></p>
   <ul class="list-paddingleft-2" style="list-style-type:disc;margin-left:16px;">
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">Q(s, a): 状态 s 下采用动作 a 的 Q 值</span></p></li>
    <li><p style="line-height:1.75em;"><span style="font-size:15px;">N(t, s, a):在时间 t，动作 a 在状态 s 被选择的次数</span></p></li>
   </ul>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">智能体的目标是：Argmax {Q(s, a)/ a ∈ A}。代表在状态 s 下选择具有最大 Q 值的动作。但是时间 t 的实际 Q(s, a) 是未知的。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">我们有：Q(s,a) = &lt;Q(t, s, a)&gt; + (Q(s,a) − &lt;Q(t, s, a)&gt;)，&lt;Q(t, s, a)&gt;是时间 t 估计的 Q 值。(Q(s,a) − &lt;Q(t, s, a)&gt;) 对应的是误差项，我们可以形成边界并使用 OFU。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">Hoeffding 不等式是限制这个误差的一种方式，我们可以证明：</span></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"></span></p>
   <p style="margin-left:16px;text-align:center;"><img style="width:296px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8atOrHpP2ZJruUIawdCsq7LxhEfc3iclSWdzOgpMHtaiatkveW7YINjPqa256QQJzkV9AfknXWp5sg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">最优策略可以被写成：</span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">Argmax {Q+(t, s, a)/ a ∈ A}</span></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"></span></p>
   <p style="margin-left:16px;text-align:center;"><img style="width:323px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8atOrHpP2ZJruUIawdCsq7yZCN8S8QPGhicqXp0uK45hl18YWzZibGmdNvy1yZYVfjv8JFVk5cWmmA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">其中β ≥ 0 调节 exploration。当β＝0 的时候，该策略仅仅利用过去的估计（遵循 leader 策略）。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">这个范围是该领域最常用的。还有很多其他改进这个范围的工作（UCB-V、UCB、KL-UCB、Bayes-UCB、BESA 等）。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">这里是我们对经典 UCB exploration 策略的实现，以及它在 Q-Learning 中使用的结果。</span></p>
   <p><br></p>
   <pre style="font-size:16px;color:rgb(62,62,62);line-height:inherit;"></pre>
   <p style="font-size:14px;color:rgb(169,183,198);line-height:18px;background:rgb(40,43,46);font-family:Consolas, Inconsolata, Courier, monospace;letter-spacing:0px;margin-left:16px;"><span class="hljs-function" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;"><span class="hljs-keyword">def</span> <span class="hljs-title" style="line-height:inherit;">UCB_exploration(Q, num_actions, beta=<span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">1</span>)</span>:</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;<span class="hljs-function" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;"><span class="hljs-keyword">def</span> <span class="hljs-title" style="line-height:inherit;">UCB_exp(state, N, t)</span>:</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;probs = np.zeros(num_actions, dtype=float)<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;Q_ = Q[state,:]/max(Q[state,:]) + np.sqrt(beta*np.log(t+<span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">1</span>)/(<span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">2</span>*N[state]))<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;best_action = Q_.argmax()<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;probs[best_action] = <span class="hljs-number" style="font-size:inherit;color:rgb(174,135,250);line-height:inherit;">1</span><br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;">return</span> probs<br style="font-size:inherit;color:inherit;line-height:inherit;"> &nbsp; &nbsp;<span class="hljs-keyword" style="font-size:inherit;color:rgb(248,35,117);line-height:inherit;">return</span> UCB_exp<br style="font-size:inherit;color:inherit;line-height:inherit;"></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"></span></p>
   <p style="margin-left:16px;text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8atOrHpP2ZJruUIawdCsq7RHWicfZWUvFBSgBLKkDBq1UmTd44iby60TFyZZPetN8nYz5n5picVHa3g/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"></span><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">UCB exploration 似乎能够快速地达到很高的奖励，然而训练过程还是受到早期 exploration 的干扰，对于更复杂的马尔科夫决策过程而言，这是有优势的，因为智能体可以免于非最优的解决方案。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;">我们来更加仔细地比较这两种策略。</span></p>
   <p><br></p>
   <p style="margin-left:16px;text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8atOrHpP2ZJruUIawdCsq7zQtyCBgt8PkuU553WAkh8Ddn9RkfYOMXAia1WwvLiceVJAlpc3GOFH8Q/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"><br></span></p>
   <p style="line-height:1.75em;margin-left:16px;"><span style="font-size:15px;"></span></p>
   <p style="min-height:1em;letter-spacing:.544px;text-align:center;font-size:16px;font-family:'微软雅黑';color:rgb(63,63,63);"><span style="text-decoration:underline;color:rgb(0,0,0);"><strong>推荐阅读</strong></span></p>
   <p style="min-height:1em;letter-spacing:.544px;text-align:center;font-size:16px;font-family:'微软雅黑';color:rgb(63,63,63);"><span style="text-decoration:underline;color:rgb(0,0,0);font-size:12px;"><strong><a href="http://mp.weixin.qq.com/s?__biz=MzIxODM4MjA5MA==&amp;mid=2247488925&amp;idx=2&amp;sn=05a1b82132b5af409d1c7f9f056e431e&amp;chksm=97ea3ff8a09db6ee12b4030018cf1065c1f4bcc20ba752b43cb96f59118a180ee7183e34757d&amp;scene=21#wechat_redirect" rel="nofollow">顶级AI【数据】资源送给你！</a></strong></span></p>
   <p style="min-height:1em;letter-spacing:.544px;text-align:center;font-size:16px;font-family:'微软雅黑';color:rgb(63,63,63);"><span style="text-decoration:underline;color:rgb(0,0,0);font-size:12px;"><strong><a href="http://mp.weixin.qq.com/s?__biz=MzIxODM4MjA5MA==&amp;mid=2247488915&amp;idx=1&amp;sn=c486ffc4c0bbd3d5da6e71fb973a4539&amp;chksm=97ea3ff6a09db6e02ddc6398a89139434c4a1b532303f087a778bfc1836afdd3f10db66d343d&amp;scene=21#wechat_redirect" rel="nofollow">程序员给银行植入病毒，盗取 718 万，被判 10 年半！</a></strong></span></p>
   <p style="min-height:1em;letter-spacing:.544px;text-align:center;font-size:16px;font-family:'微软雅黑';color:rgb(63,63,63);"><strong><span style="text-decoration:underline;color:rgb(0,0,0);font-size:12px;"><a href="http://mp.weixin.qq.com/s?__biz=MzIxODM4MjA5MA==&amp;mid=2247488911&amp;idx=1&amp;sn=aa6f610d153eec5a610ce6ba9327b868&amp;chksm=97ea3feaa09db6fcfb3f936a6802bdf03192852f00216cc7f32b2c4e3cb5fb71f64ce457468d&amp;scene=21#wechat_redirect" rel="nofollow">45张让你又笑又哭的趣图，有没有戳中你？</a><br></span></strong></p>
   <p style="min-height:1em;letter-spacing:.544px;text-align:center;font-size:16px;font-family:'微软雅黑';color:rgb(63,63,63);"><strong><span style="text-decoration:underline;color:rgb(0,0,0);font-size:12px;"><a href="http://mp.weixin.qq.com/s?__biz=MzIxODM4MjA5MA==&amp;mid=2247488911&amp;idx=2&amp;sn=d148d016c7b974e54f8db54430392105&amp;chksm=97ea3feaa09db6fca780fd7fb5671cdfaaa25f46dd108e2754bf1265b8e400ee4e5ec59a5b0b&amp;scene=21#wechat_redirect" rel="nofollow">有了GAN，可以自由的对人脸照片进行涂鸦编辑啦！</a></span></strong></p>
   <p style="min-height:1em;letter-spacing:.544px;text-align:center;font-size:16px;font-family:'微软雅黑';color:rgb(63,63,63);"><strong><span style="text-decoration:underline;color:rgb(0,0,0);font-size:12px;"><a href="http://mp.weixin.qq.com/s?__biz=MzIxODM4MjA5MA==&amp;mid=2247488889&amp;idx=1&amp;sn=a140749ca6968d1d03ac56254c7ec80a&amp;chksm=97ea3f1ca09db60af40a020d09e0e7b0edb0db3f840f6cd13332012451b34ba3bcc1202cbddb&amp;scene=21#wechat_redirect" rel="nofollow">一张大尺度美女图，竟然推进了图片算法的进步。。。</a></span></strong></p>
   <p style="margin-left:16px;min-height:1em;letter-spacing:.544px;font-size:15px;text-align:center;color:rgb(62,62,62);"><img style="letter-spacing:.544px;border-width:0px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/KdayOo3PqHDIurOfIatYehahcqXDLC7d51EsSdVb7Ulu1bOWmfhtfDYxESJ9uwtcwibicicKicZem0uDo3cFnSRKow/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <span style="color:rgb(255,0,0);"><strong><span style="font-size:15px;">喜欢就点击“在看”吧！</span></strong></span>
  </div> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
