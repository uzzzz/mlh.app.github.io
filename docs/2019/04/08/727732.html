<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>达观杯文本智能处理（三）——Word2vec原理与实践 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="达观杯文本智能处理（三）——Word2vec原理与实践" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="达观杯文本智能处理（三）——Word2vec原理与实践 一、Word Embedding背景介绍 二、Word2vec 1.Word2vec理论 2.Word2vec原理 3.CBOW与Skip-Gram模式 4.训练注意事项 三.Word2vec实践 1. python实践 2.word2Vec参数详解 参考文献 一、Word Embedding背景介绍 在NLP（自然语言处理）里面，最细粒度的是词语，词语组成句子，句子再组成段落、篇章、文档。所以要处理 NLP 的问题，首先就要拿词语开刀。 举个简单例子，判断一个词语的情感，是积极还是消极。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的情感类别，我们要构建 f(x)-&gt;y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec ，就是词嵌入（ word embedding) 的一种。 也就是说，我们要怎么把一堆计算机不认识的词语表示成它认识的，从而继续为我们人类服务，达到人类的目的？这个途径就是词嵌入（word embedding）：把人类理解的词语表示成在数学空间里的“词语”，这个数学空间里的“词语”能够帮助计算机去理解我们人类的语义，也就是说，知道哪些词的意思是相近的，等等，这体现在数学空间里的距离和概率等的概念上。 以上我们理解了在NLP领域word embedding的理由和重要性。 二、Word2vec 1.Word2vec理论 word2vec也叫word embeddings，中文名“词向量”，作用就是将自然语言中的字词转为计算机可以理解的稠密向量（Dense Vector）。在word2vec出现之前，自然语言处理经常把字词转为离散的单独的符号，也就是One-Hot Encoder。 杭州 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0] 上海 [0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0] 宁波 [0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0] 北京 [0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0] 比如上面的这个例子，在语料库中，杭州、上海、宁波、北京各对应一个向量，向量中只有一个值为1，其余都为0。但是使用One-Hot Encoder有以下问题。一方面，城市编码是随机的，向量之间相互独立，看不出城市之间可能存在的关联关系。其次，向量维度的大小取决于语料库中字词的多少。如果将世界所有城市名称对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。 使用Vector Representations可以有效解决这个问题。Word2Vec可以将One-Hot Encoder转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词将被映射到向量空间中相近的位置。 如果将embed后的城市向量通过PCA降维后可视化展示出来，那就是这个样子。 我们可以发现，华盛顿和纽约聚集在一起，北京上海聚集在一起，且北京到上海的距离与华盛顿到纽约的距离相近。也就是说模型学习到了城市的地理位置，也学习到了城市地位的关系。 2.Word2vec原理 Wordvec的目标是：将一个词表示成一个向量 Word2vec中两个重要模型是：CBOW和Skip-gram模型 word2vec模型其实就是简单化的神经网络。 输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。我们要获取的dense vector其实就是Hidden Layer的输出单元。有的地方定为Input Layer和Hidden Layer之间的权重，其实说的是一回事。 3.CBOW与Skip-Gram模式 word2vec主要分为CBOW（Continuous Bag of Words）和Skip-Gram两种模式。 CBOW是从原始语句推测目标字词；而Skip-Gram正好相反，是从目标字词推测出原始语句。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。 对同样一个句子：Hangzhou is a nice city。我们要构造一个语境与目标词汇的映射关系，其实就是input与label的关系。 这里假设滑窗尺寸为1（滑窗尺寸……这个……不懂自己google吧-_-|||） CBOW可以制造的映射关系为：[Hangzhou,a]—&gt;is，[is,nice]—&gt;a，[a,city]—&gt;nice Skip-Gram可以制造的映射关系为(is,Hangzhou)，(is,a)，(a,is)， (a,nice)，(nice,a)，(nice,city) CBOW CBOW（Continuous Bag-of-Word Model）又称连续词袋模型，是一个三层神经网络。如下图所示，该模型的特点是输入已知上下文，输出对当前单词的预测。 其学习目标是最大化对数似然函数： 其中，w表示语料库C中任意一个词。 首先输入的是one-hot向量，第一层是一个全连接层，然后没有激活函数，输出层是一个softmax层，输出一个概率分布，表示词典中每个词出现的概率。 我们并不关心输出的内容，训练完成后第一个全连接层的参数就是word embedding。 Skip-gram Skip-gram只是逆转了CBOW的因果关系而已，即已知当前词语，预测上下文. 4.训练注意事项 在第一部分讲解完成后，我们会发现Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。 举个栗子，我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难。 下面主要介绍两种方法优化训练过程。 1.负采样（negative sampling） 负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。 2.层序softmax也是解决这个问题的一种方法。 三.Word2vec实践 1. python实践 ##读取数据，数据太大，仅选5000条测试 import pandas as pd import numpy as np train_data=pd.read_csv(&#39;datalab/14936/train_set.csv&#39;,nrows=5000) ##将训练集拆分成训练集和测试集 x=train_data.drop(&#39;class&#39;,axis=1) y=train_data[&#39;class&#39;] from sklearn.model_selection import train_test_split #x:要划分的样本集 y:要划分的样本结果 #test_size:测试集占比 random_state:随机数的种子 x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=123) ..................................................................................... ##word2vec处理##. import gensim import pickle vector_size = 100 ##辅助函数 def sentence2list(sentence): return sentence.strip().split() print(&quot;准备数据................ &quot;) sentences_train = list(x_train.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences_test = list(x_test.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences = sentences_train + sentences_test print(&quot;准备数据完成! &quot;) print(&quot;开始训练................ &quot;) model = gensim.models.Word2Vec(sentences=sentences,size=vector_size, window=5, min_count=5, workers=8, sg=1, iter=5) print(&quot;训练完成! &quot;) ##保存结果 wv = model.wv vocab_list = wv.index2word word_idx_dict = {} for idx, word in enumerate(vocab_list): word_idx_dict[word] = idx vectors_arr = wv.vectors vectors_arr = np.concatenate((np.zeros(100)[np.newaxis, :], vectors_arr), axis=0) # 第0位置的vector为&#39;unk&#39;的vector print(word_idx_dict) print(vectors_arr) f_wordidx = open(&#39;word_seg_word_idx_dict.pkl&#39;, &#39;wb&#39;) f_vectors = open(&#39;word_seg_vectors_arr.pkl&#39;, &#39;wb&#39;) pickle.dump(word_idx_dict, f_wordidx) pickle.dump(vectors_arr, f_vectors) f_wordidx.close() f_vectors.close() 结果：展示了词的字典及向量数组 2.word2Vec参数详解 在Python中,我们可以使用gensim库中的word2vec。 函数中的参数如下所示: sentences: 我们要分析的语料，可以是一个列表，或者从文件中遍历读出。 size: 词向量的维度，默认值是100。这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。如果是超大的语料，建议增大维度。 window：即词向量上下文最大距离，这个参数在我们的算法原理篇中标记为cc，window越大，则和某一词较远的词也会产生上下文关系。默认值为5。在实际使用中，可以根据实际的需求来动态调整这个window的大小。如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。 sg: 即我们的word2vec两个模型的选择了。如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。 hs: 即我们的word2vec两个解法的选择了，如果是0， 则是Negative Sampling，是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。 negative:即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在我们的算法原理篇中标记为neg。 cbow_mean: 仅用于CBOW在做投影的时候，为0，则算法中的xwxw为上下文的词向量之和，为1则为上下文的词向量的平均值。在我们的原理篇中，是按照词向量的平均值来描述的。个人比较喜欢用平均值来表示xwxw,默认值也是1,不推荐修改默认值。 min_count:需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。 iter: 随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。 alpha: 在随机梯度下降法中迭代的初始步长。算法原理篇中标记为ηη，默认是0.025。 min_alpha: 由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。这部分由于不是word2vec算法的核心内容，因此在原理篇我们没有提到。对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。 参考文献 1）CS224：https://www.bilibili.com/video/av41393758/?p=2 2）https://github.com/Heitao5200/DGB/blob/master/feature/feature_code/train_word2vec.py 3）https://blog.csdn.net/sinat_23133783/article/details/89136415" />
<meta property="og:description" content="达观杯文本智能处理（三）——Word2vec原理与实践 一、Word Embedding背景介绍 二、Word2vec 1.Word2vec理论 2.Word2vec原理 3.CBOW与Skip-Gram模式 4.训练注意事项 三.Word2vec实践 1. python实践 2.word2Vec参数详解 参考文献 一、Word Embedding背景介绍 在NLP（自然语言处理）里面，最细粒度的是词语，词语组成句子，句子再组成段落、篇章、文档。所以要处理 NLP 的问题，首先就要拿词语开刀。 举个简单例子，判断一个词语的情感，是积极还是消极。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的情感类别，我们要构建 f(x)-&gt;y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec ，就是词嵌入（ word embedding) 的一种。 也就是说，我们要怎么把一堆计算机不认识的词语表示成它认识的，从而继续为我们人类服务，达到人类的目的？这个途径就是词嵌入（word embedding）：把人类理解的词语表示成在数学空间里的“词语”，这个数学空间里的“词语”能够帮助计算机去理解我们人类的语义，也就是说，知道哪些词的意思是相近的，等等，这体现在数学空间里的距离和概率等的概念上。 以上我们理解了在NLP领域word embedding的理由和重要性。 二、Word2vec 1.Word2vec理论 word2vec也叫word embeddings，中文名“词向量”，作用就是将自然语言中的字词转为计算机可以理解的稠密向量（Dense Vector）。在word2vec出现之前，自然语言处理经常把字词转为离散的单独的符号，也就是One-Hot Encoder。 杭州 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0] 上海 [0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0] 宁波 [0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0] 北京 [0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0] 比如上面的这个例子，在语料库中，杭州、上海、宁波、北京各对应一个向量，向量中只有一个值为1，其余都为0。但是使用One-Hot Encoder有以下问题。一方面，城市编码是随机的，向量之间相互独立，看不出城市之间可能存在的关联关系。其次，向量维度的大小取决于语料库中字词的多少。如果将世界所有城市名称对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。 使用Vector Representations可以有效解决这个问题。Word2Vec可以将One-Hot Encoder转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词将被映射到向量空间中相近的位置。 如果将embed后的城市向量通过PCA降维后可视化展示出来，那就是这个样子。 我们可以发现，华盛顿和纽约聚集在一起，北京上海聚集在一起，且北京到上海的距离与华盛顿到纽约的距离相近。也就是说模型学习到了城市的地理位置，也学习到了城市地位的关系。 2.Word2vec原理 Wordvec的目标是：将一个词表示成一个向量 Word2vec中两个重要模型是：CBOW和Skip-gram模型 word2vec模型其实就是简单化的神经网络。 输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。我们要获取的dense vector其实就是Hidden Layer的输出单元。有的地方定为Input Layer和Hidden Layer之间的权重，其实说的是一回事。 3.CBOW与Skip-Gram模式 word2vec主要分为CBOW（Continuous Bag of Words）和Skip-Gram两种模式。 CBOW是从原始语句推测目标字词；而Skip-Gram正好相反，是从目标字词推测出原始语句。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。 对同样一个句子：Hangzhou is a nice city。我们要构造一个语境与目标词汇的映射关系，其实就是input与label的关系。 这里假设滑窗尺寸为1（滑窗尺寸……这个……不懂自己google吧-_-|||） CBOW可以制造的映射关系为：[Hangzhou,a]—&gt;is，[is,nice]—&gt;a，[a,city]—&gt;nice Skip-Gram可以制造的映射关系为(is,Hangzhou)，(is,a)，(a,is)， (a,nice)，(nice,a)，(nice,city) CBOW CBOW（Continuous Bag-of-Word Model）又称连续词袋模型，是一个三层神经网络。如下图所示，该模型的特点是输入已知上下文，输出对当前单词的预测。 其学习目标是最大化对数似然函数： 其中，w表示语料库C中任意一个词。 首先输入的是one-hot向量，第一层是一个全连接层，然后没有激活函数，输出层是一个softmax层，输出一个概率分布，表示词典中每个词出现的概率。 我们并不关心输出的内容，训练完成后第一个全连接层的参数就是word embedding。 Skip-gram Skip-gram只是逆转了CBOW的因果关系而已，即已知当前词语，预测上下文. 4.训练注意事项 在第一部分讲解完成后，我们会发现Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。 举个栗子，我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难。 下面主要介绍两种方法优化训练过程。 1.负采样（negative sampling） 负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。 2.层序softmax也是解决这个问题的一种方法。 三.Word2vec实践 1. python实践 ##读取数据，数据太大，仅选5000条测试 import pandas as pd import numpy as np train_data=pd.read_csv(&#39;datalab/14936/train_set.csv&#39;,nrows=5000) ##将训练集拆分成训练集和测试集 x=train_data.drop(&#39;class&#39;,axis=1) y=train_data[&#39;class&#39;] from sklearn.model_selection import train_test_split #x:要划分的样本集 y:要划分的样本结果 #test_size:测试集占比 random_state:随机数的种子 x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=123) ..................................................................................... ##word2vec处理##. import gensim import pickle vector_size = 100 ##辅助函数 def sentence2list(sentence): return sentence.strip().split() print(&quot;准备数据................ &quot;) sentences_train = list(x_train.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences_test = list(x_test.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences = sentences_train + sentences_test print(&quot;准备数据完成! &quot;) print(&quot;开始训练................ &quot;) model = gensim.models.Word2Vec(sentences=sentences,size=vector_size, window=5, min_count=5, workers=8, sg=1, iter=5) print(&quot;训练完成! &quot;) ##保存结果 wv = model.wv vocab_list = wv.index2word word_idx_dict = {} for idx, word in enumerate(vocab_list): word_idx_dict[word] = idx vectors_arr = wv.vectors vectors_arr = np.concatenate((np.zeros(100)[np.newaxis, :], vectors_arr), axis=0) # 第0位置的vector为&#39;unk&#39;的vector print(word_idx_dict) print(vectors_arr) f_wordidx = open(&#39;word_seg_word_idx_dict.pkl&#39;, &#39;wb&#39;) f_vectors = open(&#39;word_seg_vectors_arr.pkl&#39;, &#39;wb&#39;) pickle.dump(word_idx_dict, f_wordidx) pickle.dump(vectors_arr, f_vectors) f_wordidx.close() f_vectors.close() 结果：展示了词的字典及向量数组 2.word2Vec参数详解 在Python中,我们可以使用gensim库中的word2vec。 函数中的参数如下所示: sentences: 我们要分析的语料，可以是一个列表，或者从文件中遍历读出。 size: 词向量的维度，默认值是100。这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。如果是超大的语料，建议增大维度。 window：即词向量上下文最大距离，这个参数在我们的算法原理篇中标记为cc，window越大，则和某一词较远的词也会产生上下文关系。默认值为5。在实际使用中，可以根据实际的需求来动态调整这个window的大小。如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。 sg: 即我们的word2vec两个模型的选择了。如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。 hs: 即我们的word2vec两个解法的选择了，如果是0， 则是Negative Sampling，是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。 negative:即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在我们的算法原理篇中标记为neg。 cbow_mean: 仅用于CBOW在做投影的时候，为0，则算法中的xwxw为上下文的词向量之和，为1则为上下文的词向量的平均值。在我们的原理篇中，是按照词向量的平均值来描述的。个人比较喜欢用平均值来表示xwxw,默认值也是1,不推荐修改默认值。 min_count:需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。 iter: 随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。 alpha: 在随机梯度下降法中迭代的初始步长。算法原理篇中标记为ηη，默认是0.025。 min_alpha: 由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。这部分由于不是word2vec算法的核心内容，因此在原理篇我们没有提到。对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。 参考文献 1）CS224：https://www.bilibili.com/video/av41393758/?p=2 2）https://github.com/Heitao5200/DGB/blob/master/feature/feature_code/train_word2vec.py 3）https://blog.csdn.net/sinat_23133783/article/details/89136415" />
<link rel="canonical" href="https://mlh.app/2019/04/08/727732.html" />
<meta property="og:url" content="https://mlh.app/2019/04/08/727732.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-08T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"达观杯文本智能处理（三）——Word2vec原理与实践 一、Word Embedding背景介绍 二、Word2vec 1.Word2vec理论 2.Word2vec原理 3.CBOW与Skip-Gram模式 4.训练注意事项 三.Word2vec实践 1. python实践 2.word2Vec参数详解 参考文献 一、Word Embedding背景介绍 在NLP（自然语言处理）里面，最细粒度的是词语，词语组成句子，句子再组成段落、篇章、文档。所以要处理 NLP 的问题，首先就要拿词语开刀。 举个简单例子，判断一个词语的情感，是积极还是消极。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的情感类别，我们要构建 f(x)-&gt;y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec ，就是词嵌入（ word embedding) 的一种。 也就是说，我们要怎么把一堆计算机不认识的词语表示成它认识的，从而继续为我们人类服务，达到人类的目的？这个途径就是词嵌入（word embedding）：把人类理解的词语表示成在数学空间里的“词语”，这个数学空间里的“词语”能够帮助计算机去理解我们人类的语义，也就是说，知道哪些词的意思是相近的，等等，这体现在数学空间里的距离和概率等的概念上。 以上我们理解了在NLP领域word embedding的理由和重要性。 二、Word2vec 1.Word2vec理论 word2vec也叫word embeddings，中文名“词向量”，作用就是将自然语言中的字词转为计算机可以理解的稠密向量（Dense Vector）。在word2vec出现之前，自然语言处理经常把字词转为离散的单独的符号，也就是One-Hot Encoder。 杭州 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0] 上海 [0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0] 宁波 [0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0] 北京 [0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0] 比如上面的这个例子，在语料库中，杭州、上海、宁波、北京各对应一个向量，向量中只有一个值为1，其余都为0。但是使用One-Hot Encoder有以下问题。一方面，城市编码是随机的，向量之间相互独立，看不出城市之间可能存在的关联关系。其次，向量维度的大小取决于语料库中字词的多少。如果将世界所有城市名称对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。 使用Vector Representations可以有效解决这个问题。Word2Vec可以将One-Hot Encoder转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词将被映射到向量空间中相近的位置。 如果将embed后的城市向量通过PCA降维后可视化展示出来，那就是这个样子。 我们可以发现，华盛顿和纽约聚集在一起，北京上海聚集在一起，且北京到上海的距离与华盛顿到纽约的距离相近。也就是说模型学习到了城市的地理位置，也学习到了城市地位的关系。 2.Word2vec原理 Wordvec的目标是：将一个词表示成一个向量 Word2vec中两个重要模型是：CBOW和Skip-gram模型 word2vec模型其实就是简单化的神经网络。 输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。我们要获取的dense vector其实就是Hidden Layer的输出单元。有的地方定为Input Layer和Hidden Layer之间的权重，其实说的是一回事。 3.CBOW与Skip-Gram模式 word2vec主要分为CBOW（Continuous Bag of Words）和Skip-Gram两种模式。 CBOW是从原始语句推测目标字词；而Skip-Gram正好相反，是从目标字词推测出原始语句。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。 对同样一个句子：Hangzhou is a nice city。我们要构造一个语境与目标词汇的映射关系，其实就是input与label的关系。 这里假设滑窗尺寸为1（滑窗尺寸……这个……不懂自己google吧-_-|||） CBOW可以制造的映射关系为：[Hangzhou,a]—&gt;is，[is,nice]—&gt;a，[a,city]—&gt;nice Skip-Gram可以制造的映射关系为(is,Hangzhou)，(is,a)，(a,is)， (a,nice)，(nice,a)，(nice,city) CBOW CBOW（Continuous Bag-of-Word Model）又称连续词袋模型，是一个三层神经网络。如下图所示，该模型的特点是输入已知上下文，输出对当前单词的预测。 其学习目标是最大化对数似然函数： 其中，w表示语料库C中任意一个词。 首先输入的是one-hot向量，第一层是一个全连接层，然后没有激活函数，输出层是一个softmax层，输出一个概率分布，表示词典中每个词出现的概率。 我们并不关心输出的内容，训练完成后第一个全连接层的参数就是word embedding。 Skip-gram Skip-gram只是逆转了CBOW的因果关系而已，即已知当前词语，预测上下文. 4.训练注意事项 在第一部分讲解完成后，我们会发现Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。 举个栗子，我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难。 下面主要介绍两种方法优化训练过程。 1.负采样（negative sampling） 负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。 2.层序softmax也是解决这个问题的一种方法。 三.Word2vec实践 1. python实践 ##读取数据，数据太大，仅选5000条测试 import pandas as pd import numpy as np train_data=pd.read_csv(&#39;datalab/14936/train_set.csv&#39;,nrows=5000) ##将训练集拆分成训练集和测试集 x=train_data.drop(&#39;class&#39;,axis=1) y=train_data[&#39;class&#39;] from sklearn.model_selection import train_test_split #x:要划分的样本集 y:要划分的样本结果 #test_size:测试集占比 random_state:随机数的种子 x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=123) ..................................................................................... ##word2vec处理##. import gensim import pickle vector_size = 100 ##辅助函数 def sentence2list(sentence): return sentence.strip().split() print(&quot;准备数据................ &quot;) sentences_train = list(x_train.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences_test = list(x_test.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences = sentences_train + sentences_test print(&quot;准备数据完成! &quot;) print(&quot;开始训练................ &quot;) model = gensim.models.Word2Vec(sentences=sentences,size=vector_size, window=5, min_count=5, workers=8, sg=1, iter=5) print(&quot;训练完成! &quot;) ##保存结果 wv = model.wv vocab_list = wv.index2word word_idx_dict = {} for idx, word in enumerate(vocab_list): word_idx_dict[word] = idx vectors_arr = wv.vectors vectors_arr = np.concatenate((np.zeros(100)[np.newaxis, :], vectors_arr), axis=0) # 第0位置的vector为&#39;unk&#39;的vector print(word_idx_dict) print(vectors_arr) f_wordidx = open(&#39;word_seg_word_idx_dict.pkl&#39;, &#39;wb&#39;) f_vectors = open(&#39;word_seg_vectors_arr.pkl&#39;, &#39;wb&#39;) pickle.dump(word_idx_dict, f_wordidx) pickle.dump(vectors_arr, f_vectors) f_wordidx.close() f_vectors.close() 结果：展示了词的字典及向量数组 2.word2Vec参数详解 在Python中,我们可以使用gensim库中的word2vec。 函数中的参数如下所示: sentences: 我们要分析的语料，可以是一个列表，或者从文件中遍历读出。 size: 词向量的维度，默认值是100。这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。如果是超大的语料，建议增大维度。 window：即词向量上下文最大距离，这个参数在我们的算法原理篇中标记为cc，window越大，则和某一词较远的词也会产生上下文关系。默认值为5。在实际使用中，可以根据实际的需求来动态调整这个window的大小。如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。 sg: 即我们的word2vec两个模型的选择了。如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。 hs: 即我们的word2vec两个解法的选择了，如果是0， 则是Negative Sampling，是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。 negative:即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在我们的算法原理篇中标记为neg。 cbow_mean: 仅用于CBOW在做投影的时候，为0，则算法中的xwxw为上下文的词向量之和，为1则为上下文的词向量的平均值。在我们的原理篇中，是按照词向量的平均值来描述的。个人比较喜欢用平均值来表示xwxw,默认值也是1,不推荐修改默认值。 min_count:需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。 iter: 随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。 alpha: 在随机梯度下降法中迭代的初始步长。算法原理篇中标记为ηη，默认是0.025。 min_alpha: 由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。这部分由于不是word2vec算法的核心内容，因此在原理篇我们没有提到。对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。 参考文献 1）CS224：https://www.bilibili.com/video/av41393758/?p=2 2）https://github.com/Heitao5200/DGB/blob/master/feature/feature_code/train_word2vec.py 3）https://blog.csdn.net/sinat_23133783/article/details/89136415","@type":"BlogPosting","url":"https://mlh.app/2019/04/08/727732.html","headline":"达观杯文本智能处理（三）——Word2vec原理与实践","dateModified":"2019-04-08T00:00:00+08:00","datePublished":"2019-04-08T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/04/08/727732.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>达观杯文本智能处理（三）——Word2vec原理与实践</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p></p>
  <div class="toc">
   <h3>达观杯文本智能处理（三）——Word2vec原理与实践</h3>
   <ul>
    <li><a href="#Word_Embedding_1" rel="nofollow">一、Word Embedding背景介绍</a></li>
    <li><a href="#Word2vec_10" rel="nofollow">二、Word2vec</a></li>
    <ul>
     <li><a href="#1Word2vec_11" rel="nofollow">1.Word2vec理论</a></li>
     <li><a href="#2Word2vec_26" rel="nofollow">2.Word2vec原理</a></li>
     <li><a href="#3CBOWSkipGram_35" rel="nofollow">3.CBOW与Skip-Gram模式</a></li>
     <li><a href="#4_58" rel="nofollow">4.训练注意事项</a></li>
    </ul>
    <li><a href="#Word2vec_71" rel="nofollow">三.Word2vec实践</a></li>
    <ul>
     <li><a href="#1_python_72" rel="nofollow">1. python实践</a></li>
     <li><a href="#2word2Vec_133" rel="nofollow">2.word2Vec参数详解</a></li>
    </ul>
    <li><a href="#_159" rel="nofollow">参考文献</a></li>
   </ul>
  </div>
  <p></p> 
  <h1><a id="Word_Embedding_1"></a>一、Word Embedding背景介绍</h1> 
  <p>在NLP（自然语言处理）里面，最细粒度的是<strong>词语</strong>，词语组成句子，句子再组成段落、篇章、文档。所以要处理 NLP 的问题，首先就要拿词语开刀。</p> 
  <p>举个简单例子，判断一个词语的情感，是积极还是消极。用机器学习的思路，我们有一系列样本(x,y)，这里 x 是词语，y 是它们的情感类别，我们要构建 f(x)-&gt;y 的映射，但这里的数学模型 f（比如神经网络、SVM）只接受数值型输入，而 NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 <strong>Word2vec ，就是词嵌入（ word embedding)</strong> 的一种。</p> 
  <p>也就是说，我们要怎么把一堆计算机不认识的词语表示成它认识的，从而继续为我们人类服务，达到人类的目的？这个途径就是词嵌入（word embedding）：把人类理解的词语表示成在数学空间里的“词语”，这个数学空间里的“词语”能够帮助计算机去理解我们人类的语义，也就是说，知道哪些词的意思是相近的，等等，这体现在数学空间里的距离和概率等的概念上。</p> 
  <p>以上我们理解了在NLP领域word embedding的理由和重要性。</p> 
  <h1><a id="Word2vec_10"></a>二、Word2vec</h1> 
  <h2><a id="1Word2vec_11"></a>1.Word2vec理论</h2> 
  <p>word2vec也叫word embeddings，中文名“词向量”，作用就是将自然语言中的字词转为计算机可以理解的稠密向量（Dense Vector）。在word2vec出现之前，自然语言处理经常把字词转为离散的单独的符号，也就是One-Hot Encoder。</p> 
  <p>杭州 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0]<br> 上海 [0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0]<br> 宁波 [0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0]<br> 北京 [0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0]</p> 
  <p>比如上面的这个例子，在语料库中，杭州、上海、宁波、北京各对应一个向量，向量中只有一个值为1，其余都为0。但是使用One-Hot Encoder有以下问题。一方面，城市编码是随机的，向量之间相互独立，看不出城市之间可能存在的关联关系。其次，向量维度的大小取决于语料库中字词的多少。如果将世界所有城市名称对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。<br> 使用Vector Representations可以有效解决这个问题。Word2Vec可以将One-Hot Encoder转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词将被映射到向量空间中相近的位置。<br> 如果将embed后的城市向量通过PCA降维后可视化展示出来，那就是这个样子。</p> 
  <p><img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019040813075857.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlbW9uX3lx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>我们可以发现，华盛顿和纽约聚集在一起，北京上海聚集在一起，且北京到上海的距离与华盛顿到纽约的距离相近。也就是说模型学习到了城市的地理位置，也学习到了城市地位的关系。</p> 
  <h2><a id="2Word2vec_26"></a>2.Word2vec原理</h2> 
  <ul> 
   <li>Wordvec的目标是：将一个词表示成一个向量</li> 
   <li>Word2vec中两个重要模型是：CBOW和Skip-gram模型</li> 
   <li><strong>word2vec模型其实就是简单化的神经网络</strong>。</li> 
  </ul> 
  <p><img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190408131437336.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlbW9uX3lx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。我们要获取的dense vector其实就是Hidden Layer的输出单元。有的地方定为Input Layer和Hidden Layer之间的权重，其实说的是一回事。<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190408131457379." alt="在这里插入图片描述"></p> 
  <h2><a id="3CBOWSkipGram_35"></a>3.CBOW与Skip-Gram模式</h2> 
  <ul> 
   <li>word2vec主要分为CBOW（Continuous Bag of Words）和Skip-Gram两种模式。</li> 
   <li>CBOW是从原始语句推测目标字词；而Skip-Gram正好相反，是从目标字词推测出原始语句。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。</li> 
   <li>对同样一个句子：Hangzhou is a nice city。我们要构造一个语境与目标词汇的映射关系，其实就是input与label的关系。</li> 
   <li>这里假设滑窗尺寸为1（滑窗尺寸……这个……不懂自己google吧-_-|||）<br> CBOW可以制造的映射关系为：[Hangzhou,a]—&gt;is，[is,nice]—&gt;a，[a,city]—&gt;nice<br> Skip-Gram可以制造的映射关系为(is,Hangzhou)，(is,a)，(a,is)， (a,nice)，(nice,a)，(nice,city)</li> 
   <li><strong>CBOW</strong><br> CBOW（Continuous Bag-of-Word Model）又称连续词袋模型，是一个三层神经网络。如下图所示，该模型的特点是输入已知上下文，输出对当前单词的预测。<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190408131828297." alt="在这里插入图片描述"></li> 
  </ul> 
  <p>其学习目标是最大化对数似然函数：</p> 
  <p><img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190408131903345." alt="在这里插入图片描述"></p> 
  <p>其中，w表示语料库C中任意一个词。<br> 首先输入的是one-hot向量，第一层是一个全连接层，然后没有激活函数，输出层是一个softmax层，输出一个概率分布，表示词典中每个词出现的概率。<br> 我们并不关心输出的内容，训练完成后第一个全连接层的参数就是word embedding。</p> 
  <ul> 
   <li><strong>Skip-gram</strong><br> Skip-gram只是逆转了CBOW的因果关系而已，即已知当前词语，预测上下文.<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190408132004893.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlbW9uX3lx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li> 
  </ul> 
  <h2><a id="4_58"></a>4.训练注意事项</h2> 
  <p>在第一部分讲解完成后，我们会发现Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。</p> 
  <p>举个栗子，我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难。<br> 下面主要介绍两种方法优化训练过程。</p> 
  <p><strong>1.负采样（negative sampling）</strong><br> 负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。<br> <strong>2.层序softmax</strong>也是解决这个问题的一种方法。</p> 
  <hr> 
  <h1><a id="Word2vec_71"></a>三.Word2vec实践</h1> 
  <h2><a id="1_python_72"></a>1. python实践</h2> 
  <pre><code>##读取数据，数据太大，仅选5000条测试
import pandas as pd
import numpy as np
train_data=pd.read_csv('datalab/14936/train_set.csv',nrows=5000)


##将训练集拆分成训练集和测试集
x=train_data.drop('class',axis=1)
y=train_data['class']

from sklearn.model_selection import train_test_split
#x:要划分的样本集     y:要划分的样本结果
#test_size:测试集占比   random_state:随机数的种子
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=123)


.....................................................................................
##word2vec处理##.
import gensim
import pickle
vector_size = 100
##辅助函数
def sentence2list(sentence):
    return sentence.strip().split()

print("准备数据................ ")
sentences_train = list(x_train.loc[:, 'word_seg'].apply(sentence2list))
sentences_test = list(x_test.loc[:, 'word_seg'].apply(sentence2list))
sentences = sentences_train + sentences_test
print("准备数据完成! ")

print("开始训练................ ")
model = gensim.models.Word2Vec(sentences=sentences,size=vector_size, window=5, min_count=5, workers=8, sg=1, iter=5)
print("训练完成! ")

##保存结果
wv = model.wv
vocab_list = wv.index2word
word_idx_dict = {}
for idx, word in enumerate(vocab_list):
    word_idx_dict[word] = idx

vectors_arr = wv.vectors
vectors_arr = np.concatenate((np.zeros(100)[np.newaxis, :], vectors_arr), axis=0)  # 第0位置的vector为'unk'的vector

print(word_idx_dict)
print(vectors_arr)
f_wordidx = open('word_seg_word_idx_dict.pkl', 'wb')
f_vectors = open('word_seg_vectors_arr.pkl', 'wb')
pickle.dump(word_idx_dict, f_wordidx)
pickle.dump(vectors_arr, f_vectors)
f_wordidx.close()
f_vectors.close()
</code></pre> 
  <p>结果：展示了词的字典及向量数组<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190409131035393.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlbW9uX3lx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190409131216531.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlbW9uX3lx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="2word2Vec_133"></a>2.word2Vec参数详解</h2> 
  <p>在Python中,我们可以使用gensim库中的word2vec。<br> 函数中的参数如下所示:</p> 
  <ul> 
   <li> <p>sentences: 我们要分析的语料，可以是一个列表，或者从文件中遍历读出。</p> </li> 
   <li> <p>size: 词向量的维度，默认值是100。这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。如果是超大的语料，建议增大维度。</p> </li> 
   <li> <p>window：即词向量上下文最大距离，这个参数在我们的算法原理篇中标记为cc，window越大，则和某一词较远的词也会产生上下文关系。默认值为5。在实际使用中，可以根据实际的需求来动态调整这个window的大小。如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。</p> </li> 
   <li> <p>sg: 即我们的word2vec两个模型的选择了。如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。</p> </li> 
   <li> <p>hs: 即我们的word2vec两个解法的选择了，如果是0， 则是Negative Sampling，是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。</p> </li> 
   <li> <p>negative:即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。这个参数在我们的算法原理篇中标记为neg。</p> </li> 
   <li> <p>cbow_mean: 仅用于CBOW在做投影的时候，为0，则算法中的xwxw为上下文的词向量之和，为1则为上下文的词向量的平均值。在我们的原理篇中，是按照词向量的平均值来描述的。个人比较喜欢用平均值来表示xwxw,默认值也是1,不推荐修改默认值。</p> </li> 
   <li> <p>min_count:需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。</p> </li> 
   <li> <p>iter: 随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。</p> </li> 
   <li> <p>alpha: 在随机梯度下降法中迭代的初始步长。算法原理篇中标记为ηη，默认是0.025。</p> </li> 
   <li> <p>min_alpha: 由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。这部分由于不是word2vec算法的核心内容，因此在原理篇我们没有提到。对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。</p> </li> 
  </ul> 
  <h1><a id="_159"></a>参考文献</h1> 
  <p>1）CS224：<a href="https://www.bilibili.com/video/av41393758/?p=2" rel="nofollow">https://www.bilibili.com/video/av41393758/?p=2</a><br> 2）<a href="https://github.com/Heitao5200/DGB/blob/master/feature/feature_code/train_word2vec.py" rel="nofollow">https://github.com/Heitao5200/DGB/blob/master/feature/feature_code/train_word2vec.py</a><br> 3）<a href="https://blog.csdn.net/sinat_23133783/article/details/89136415" rel="nofollow">https://blog.csdn.net/sinat_23133783/article/details/89136415</a></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
