<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>MapReduce之词频统计 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="MapReduce之词频统计" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="MapReduce之词频统计 这次终于开始了这是的MapReduce的编码过程，记录以下 问题描述 编写MapReduce对一个文本中单词的使用频率进行统计 样例输入 hello world hello hadoop hello mapreduce hello spark hello school 输出结果 hadoop 1 hello 5 mapreduce 1 school 1 spark 1 world 1 问题思路 在这个问题中，将文本中的内容切割为每一个单词，然后将相同的单词聚集在一起，最后计算此书并输出 mapper阶段任务 在这个问题中，map阶段主要负责单词切割任务。 mapper阶段的工作原理 Mapper处理的数据是由InputFormat分解过的数据集，其中InputFormat的作用是将数据集切割成小数据集InputSplits,每一个InputSplit将由一个Mapper负责处理，此外，InputFormat中还提供了一个RecordReader的实现，并将一个InputSolit解析成&lt; key,value&gt;对提供给map函数。 mapper阶段源码如下 public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } reducer阶段任务 reduce阶段主要是接收map结算的单词分组，然后对单词进行统计。 reducer阶段工作原理 Mapper中传递的数据为的结果对&lt;key,value&gt;，会送到Reducer中进行合并，合并的过程中，有相同的key的键/值对则送到同一个Reducer上，并由reduce具体计算出单词所对应的频数。 reducer阶段源码如下 public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } 完整代码如下 在运行代码之前，我们需要在该项目之下建立一个input文件夹，在input文件夹下建立一个文本文件，存放用来处理的单词。 import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException; import java.util.StringTokenizer; public class WordCount { public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); String[] otherArgs = new String[]{&quot;input/file.txt&quot;, &quot;output&quot;}; //input/file.txt为本地文件 //程序在本地直接运行，毕竟每次都要打包并上传集群好麻烦，或许以后考虑写一个自动化的脚本 if (otherArgs.length != 2) { System.out.println(&quot;参数错误&quot;); System.exit(2); } Job job = Job.getInstance(conf, &quot;word count&quot;); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } } 写在最后 毕竟如上的输入数据比较少，大家可以动手查找自然语言处理的语料包添加到input文件中，感受以下大数据的魅力" />
<meta property="og:description" content="MapReduce之词频统计 这次终于开始了这是的MapReduce的编码过程，记录以下 问题描述 编写MapReduce对一个文本中单词的使用频率进行统计 样例输入 hello world hello hadoop hello mapreduce hello spark hello school 输出结果 hadoop 1 hello 5 mapreduce 1 school 1 spark 1 world 1 问题思路 在这个问题中，将文本中的内容切割为每一个单词，然后将相同的单词聚集在一起，最后计算此书并输出 mapper阶段任务 在这个问题中，map阶段主要负责单词切割任务。 mapper阶段的工作原理 Mapper处理的数据是由InputFormat分解过的数据集，其中InputFormat的作用是将数据集切割成小数据集InputSplits,每一个InputSplit将由一个Mapper负责处理，此外，InputFormat中还提供了一个RecordReader的实现，并将一个InputSolit解析成&lt; key,value&gt;对提供给map函数。 mapper阶段源码如下 public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } reducer阶段任务 reduce阶段主要是接收map结算的单词分组，然后对单词进行统计。 reducer阶段工作原理 Mapper中传递的数据为的结果对&lt;key,value&gt;，会送到Reducer中进行合并，合并的过程中，有相同的key的键/值对则送到同一个Reducer上，并由reduce具体计算出单词所对应的频数。 reducer阶段源码如下 public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } 完整代码如下 在运行代码之前，我们需要在该项目之下建立一个input文件夹，在input文件夹下建立一个文本文件，存放用来处理的单词。 import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException; import java.util.StringTokenizer; public class WordCount { public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); String[] otherArgs = new String[]{&quot;input/file.txt&quot;, &quot;output&quot;}; //input/file.txt为本地文件 //程序在本地直接运行，毕竟每次都要打包并上传集群好麻烦，或许以后考虑写一个自动化的脚本 if (otherArgs.length != 2) { System.out.println(&quot;参数错误&quot;); System.exit(2); } Job job = Job.getInstance(conf, &quot;word count&quot;); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } } 写在最后 毕竟如上的输入数据比较少，大家可以动手查找自然语言处理的语料包添加到input文件中，感受以下大数据的魅力" />
<link rel="canonical" href="https://mlh.app/2019/04/06/728709.html" />
<meta property="og:url" content="https://mlh.app/2019/04/06/728709.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-06T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"MapReduce之词频统计 这次终于开始了这是的MapReduce的编码过程，记录以下 问题描述 编写MapReduce对一个文本中单词的使用频率进行统计 样例输入 hello world hello hadoop hello mapreduce hello spark hello school 输出结果 hadoop 1 hello 5 mapreduce 1 school 1 spark 1 world 1 问题思路 在这个问题中，将文本中的内容切割为每一个单词，然后将相同的单词聚集在一起，最后计算此书并输出 mapper阶段任务 在这个问题中，map阶段主要负责单词切割任务。 mapper阶段的工作原理 Mapper处理的数据是由InputFormat分解过的数据集，其中InputFormat的作用是将数据集切割成小数据集InputSplits,每一个InputSplit将由一个Mapper负责处理，此外，InputFormat中还提供了一个RecordReader的实现，并将一个InputSolit解析成&lt; key,value&gt;对提供给map函数。 mapper阶段源码如下 public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } reducer阶段任务 reduce阶段主要是接收map结算的单词分组，然后对单词进行统计。 reducer阶段工作原理 Mapper中传递的数据为的结果对&lt;key,value&gt;，会送到Reducer中进行合并，合并的过程中，有相同的key的键/值对则送到同一个Reducer上，并由reduce具体计算出单词所对应的频数。 reducer阶段源码如下 public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } 完整代码如下 在运行代码之前，我们需要在该项目之下建立一个input文件夹，在input文件夹下建立一个文本文件，存放用来处理的单词。 import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException; import java.util.StringTokenizer; public class WordCount { public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); String[] otherArgs = new String[]{&quot;input/file.txt&quot;, &quot;output&quot;}; //input/file.txt为本地文件 //程序在本地直接运行，毕竟每次都要打包并上传集群好麻烦，或许以后考虑写一个自动化的脚本 if (otherArgs.length != 2) { System.out.println(&quot;参数错误&quot;); System.exit(2); } Job job = Job.getInstance(conf, &quot;word count&quot;); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } } 写在最后 毕竟如上的输入数据比较少，大家可以动手查找自然语言处理的语料包添加到input文件中，感受以下大数据的魅力","@type":"BlogPosting","url":"https://mlh.app/2019/04/06/728709.html","headline":"MapReduce之词频统计","dateModified":"2019-04-06T00:00:00+08:00","datePublished":"2019-04-06T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/04/06/728709.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>MapReduce之词频统计</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <h1><a id="MapReduce_0"></a>MapReduce之词频统计</h1> 
  <p>这次终于开始了这是的MapReduce的编码过程，记录以下</p> 
  <h2><a id="_2"></a>问题描述</h2> 
  <p>编写MapReduce对一个文本中单词的使用频率进行统计</p> 
  <h4><a id="_4"></a>样例输入</h4> 
  <p>hello world<br> hello hadoop<br> hello mapreduce<br> hello spark<br> hello school</p> 
  <h4><a id="_10"></a>输出结果</h4> 
  <p>hadoop 1<br> hello 5<br> mapreduce 1<br> school 1<br> spark 1<br> world 1</p> 
  <h2><a id="_17"></a>问题思路</h2> 
  <p>在这个问题中，将文本中的内容切割为每一个单词，然后将相同的单词聚集在一起，最后计算此书并输出</p> 
  <h3><a id="mapper_19"></a>mapper阶段任务</h3> 
  <p>在这个问题中，map阶段主要负责单词切割任务。</p> 
  <h4><a id="mapper_21"></a>mapper阶段的工作原理</h4> 
  <p>Mapper处理的数据是由InputFormat分解过的数据集，其中InputFormat的作用是将数据集切割成小数据集InputSplits,每一个InputSplit将由一个Mapper负责处理，此外，InputFormat中还提供了一个RecordReader的实现，并将一个InputSolit解析成&lt; key,value&gt;对提供给map函数。</p> 
  <h4><a id="mapper_23"></a>mapper阶段源码如下</h4> 
  <pre><code> public static class TokenizerMapper
            extends Mapper&lt;Object, Text, Text, IntWritable&gt; {

        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context
        ) throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }
</code></pre> 
  <h3><a id="reducer_41"></a>reducer阶段任务</h3> 
  <p>reduce阶段主要是接收map结算的单词分组，然后对单词进行统计。</p> 
  <h4><a id="reducer_43"></a>reducer阶段工作原理</h4> 
  <p>Mapper中传递的数据为的结果对&lt;key,value&gt;，会送到Reducer中进行合并，合并的过程中，有相同的key的键/值对则送到同一个Reducer上，并由reduce具体计算出单词所对应的频数。</p> 
  <h4><a id="reducer_45"></a>reducer阶段源码如下</h4> 
  <pre><code>public static class IntSumReducer
            extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                           Context context
        ) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }
</code></pre> 
  <h3><a id="_63"></a>完整代码如下</h3> 
  <p>在运行代码之前，我们需要在该项目之下建立一个input文件夹，在input文件夹下建立一个文本文件，存放用来处理的单词。</p> 
  <pre><code>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import java.io.IOException;
import java.util.StringTokenizer;

public class WordCount {

    public static class TokenizerMapper
            extends Mapper&lt;Object, Text, Text, IntWritable&gt; {

        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context
        ) throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class IntSumReducer
            extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                           Context context
        ) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        String[] otherArgs = new String[]{"input/file.txt", "output"};   //input/file.txt为本地文件
        //程序在本地直接运行，毕竟每次都要打包并上传集群好麻烦，或许以后考虑写一个自动化的脚本
        if (otherArgs.length != 2) {
            System.out.println("参数错误");
            System.exit(2);
        }
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
</code></pre> 
  <h3><a id="_133"></a>写在最后</h3> 
  <p>毕竟如上的输入数据比较少，大家可以动手查找自然语言处理的语料包添加到input文件中，感受以下大数据的魅力</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
