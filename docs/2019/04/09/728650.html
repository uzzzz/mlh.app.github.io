<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>在多台阿里云服务器上部署Hadoop分布式系统及WordCount实验 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="在多台阿里云服务器上部署Hadoop分布式系统及WordCount实验" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="一、实现master与slave之间无密码连接 分别在master及slave上生成rsa密钥： mkdir ~/.ssh cd ~/.ssh ssh-keygen -t -rsa 一路回车（选择默认设置），此时，ssh文件夹中生成了id_rsa.pub和id_rsa两个，然后使用scp命令将公钥（id_rsa.pub）分别拷到对方机器中 scp id_rsa.pub h1@对方机器IP：~/.ssh/authorized_keys 并设置权限： chmod 700 authoized_keys chomd 777 ~/.ssh 使用cat命令实现无密码访问自身（localhost）cat id_rsa.pub &gt;&gt; authorized_keys 二：关闭防火墙 sudo ufw disable 三：添加防火墙规则 因阿里云服务器原只支持22、80和４４３端口，所以需要到控制台中添加防火墙规则，使其支持９０００端口等（选择全部ＴＣＰ＋ＵDＰ）。 四：安装JDK 到官网下载JDK tar包，本次实验下载的JDK1.8.0版本,在h1用户下新建/usr/java文件夹，并将下载的压缩包解压到java文件夹中：tar -zvxf jdk-8u191-linux-x64.tar.gz然后打开etc/profile 进行环境变量配置： export JAVA_HOME=/usr/java/jdk1.8.0_191 export JRE_HOME=/usr/java/jdk1.8.0_191/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$JAVA_HOME:$PATH 保存退出，使用source 命令或者重启sudo shutdown -r now使配置生效。可以使用which java测试java是否配置成功。 五：修改hostname及hosts 修改/etc/hostname,master机器上改为master，slave1机器上改为slave1。 修改/etc/hosts,/etc/hosts中原来的内容全部删除，然后加入namenode和datanode的ＩＰ以及名字；master中的hosts中的masterＩＰ为master本机内网ＩＰ，其他slave节点ＩＰ为外网ＩＰ；相同的，slave中的hosts中的本机ＩＰ必为本机内网ＩＰ，其他节点为外网ＩＰ。 六：安装Hadoop 到官网下载Hadoop压缩包，本次下载的版本为2.9.1，在h1用户下新建文件夹/hadoop，并将下载好的压缩包解压缩到hadoop文件夹中tar -zvxf hadoop-2.9.1.tar.gz。 在hadoop目录下新建hdfs文件夹，并在hdfs文件夹中新建name、data、tmp三个文件夹。 进入hadoop目录下的hadoop/etc/hadoop文件夹中对hadoop配置文件进行配置。 1.使用vim打开core-site.xml文件进行如下配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 2.使用vim打开hadoop-env.sh文件，将JAVA_HOME配置为本机JAVA_HOME路径。 3.使用vim打开yarn-env.sh文件，将JAVA_HOME配置为本机JAVA_HOME路径。 4.使用vim打开hdfs-site.xml文件，进行如下配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/name&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/data&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 5.cp mapred-site.xml.template mapred-site.xml 复制其文件并命名为mapre-site.xml，使用vim打开进行配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 6.使用vim打开yarn-site.xml文件进行配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.sddress&lt;/name&gt; &lt;value&gt;master:18040&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:18030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:18088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:18025&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:18141&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 7.配置slaves文件，将其中的localhost改为slave1。 8.打开/etc/profile进行hadoop环境变量配置： export HADOOP_HOME=/home/h1/hadoop/hadoop-2.9.1 export PATH=&quot;$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH&quot; export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop 9.使用scp hadoop-2.9.1 h1@slaveIP:/home/h1/hadoop/命令将master上的hadoop配置文件拷到slave1上，并修改slave1上的/etc/profile文件，然后使用sudo shutdown -r now命令使两机重启。 10.在master机器上执行命令：hdfs namenode -format进行初始化 11.在master机器上执行start-all.sh启动hadoop，使用jps命令可以查看到相应进程。 12.在浏览器中输入http://masterIP:50070可以查看web UI。 七、Wordcount测试 1.在h1用户目录下新建hadoop_data文件夹。 2.在hadoop_data文件夹内新建file1.txt及file2.txt文件，并向其中写入要进行实验的文本： Verse 1: You should believe in me, and everything i choose to do You should believe that i’ll alaways come back to you Life is discovering the love that we create Life is a mystery we need to embrace In every way (We need to let go) You’ll see all our dreams will follow In every way (We need to let go) Chorus: People rise together when they believe in tomorrow Change today to forever, as life keeps moving People rise together when they believe in tomottow Change today to forever, as life keeps moving Verse 2: Open your mind and see, we have everything we need Dream a reality, fulfill its destiny In every way (We need to let go) You’ll see all our dream will follow In every way ~ 3.建立input文件夹3.hdfs dfs -mkdir /input将file1.txt及file2.txt文件上传到input文件夹中：hdfs dfs -put /home/h1/hadoop_data/*.txt /input/ 4..hadoop jar /home/h1/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar wordcount /input /output执行测试程序，并将结果输入到/output文件夹中 5.hadoop fs -cat /output/part-r-00000查看结果。 实验结果输出： (We 6 1: 2 2: 2 Change 4 Chorus: 2 Dream 2 In 8 Life 4 Open 2 People 4 Verse 4 You 4 You’ll 4 a 4 alaways 2 all 4 and 4 as 4 back 2 believe 8 choose 2 come 2 create 2 destiny 2 discovering 2 do 2 dream 2 dreams 2 embrace 2 every 8 everything 4 follow 4 forever, 4 fulfill 2 go) 6 have 2 i 2 i’ll 2 in 6 is 4 its 2 keeps 4 let 6 life 4 love 2 me, 2 mind 2 moving 4 mystery 2 need 10 our 4 reality, 2 rise 4 see 4 see, 2 should 4 that 4 the 2 they 4 to 16 today 4 together 4 tomorrow 2 tomottow 2 way 8 we 8 when 4 will 4 you 2 your 2" />
<meta property="og:description" content="一、实现master与slave之间无密码连接 分别在master及slave上生成rsa密钥： mkdir ~/.ssh cd ~/.ssh ssh-keygen -t -rsa 一路回车（选择默认设置），此时，ssh文件夹中生成了id_rsa.pub和id_rsa两个，然后使用scp命令将公钥（id_rsa.pub）分别拷到对方机器中 scp id_rsa.pub h1@对方机器IP：~/.ssh/authorized_keys 并设置权限： chmod 700 authoized_keys chomd 777 ~/.ssh 使用cat命令实现无密码访问自身（localhost）cat id_rsa.pub &gt;&gt; authorized_keys 二：关闭防火墙 sudo ufw disable 三：添加防火墙规则 因阿里云服务器原只支持22、80和４４３端口，所以需要到控制台中添加防火墙规则，使其支持９０００端口等（选择全部ＴＣＰ＋ＵDＰ）。 四：安装JDK 到官网下载JDK tar包，本次实验下载的JDK1.8.0版本,在h1用户下新建/usr/java文件夹，并将下载的压缩包解压到java文件夹中：tar -zvxf jdk-8u191-linux-x64.tar.gz然后打开etc/profile 进行环境变量配置： export JAVA_HOME=/usr/java/jdk1.8.0_191 export JRE_HOME=/usr/java/jdk1.8.0_191/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$JAVA_HOME:$PATH 保存退出，使用source 命令或者重启sudo shutdown -r now使配置生效。可以使用which java测试java是否配置成功。 五：修改hostname及hosts 修改/etc/hostname,master机器上改为master，slave1机器上改为slave1。 修改/etc/hosts,/etc/hosts中原来的内容全部删除，然后加入namenode和datanode的ＩＰ以及名字；master中的hosts中的masterＩＰ为master本机内网ＩＰ，其他slave节点ＩＰ为外网ＩＰ；相同的，slave中的hosts中的本机ＩＰ必为本机内网ＩＰ，其他节点为外网ＩＰ。 六：安装Hadoop 到官网下载Hadoop压缩包，本次下载的版本为2.9.1，在h1用户下新建文件夹/hadoop，并将下载好的压缩包解压缩到hadoop文件夹中tar -zvxf hadoop-2.9.1.tar.gz。 在hadoop目录下新建hdfs文件夹，并在hdfs文件夹中新建name、data、tmp三个文件夹。 进入hadoop目录下的hadoop/etc/hadoop文件夹中对hadoop配置文件进行配置。 1.使用vim打开core-site.xml文件进行如下配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 2.使用vim打开hadoop-env.sh文件，将JAVA_HOME配置为本机JAVA_HOME路径。 3.使用vim打开yarn-env.sh文件，将JAVA_HOME配置为本机JAVA_HOME路径。 4.使用vim打开hdfs-site.xml文件，进行如下配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/name&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/data&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 5.cp mapred-site.xml.template mapred-site.xml 复制其文件并命名为mapre-site.xml，使用vim打开进行配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 6.使用vim打开yarn-site.xml文件进行配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.sddress&lt;/name&gt; &lt;value&gt;master:18040&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:18030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:18088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:18025&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:18141&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 7.配置slaves文件，将其中的localhost改为slave1。 8.打开/etc/profile进行hadoop环境变量配置： export HADOOP_HOME=/home/h1/hadoop/hadoop-2.9.1 export PATH=&quot;$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH&quot; export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop 9.使用scp hadoop-2.9.1 h1@slaveIP:/home/h1/hadoop/命令将master上的hadoop配置文件拷到slave1上，并修改slave1上的/etc/profile文件，然后使用sudo shutdown -r now命令使两机重启。 10.在master机器上执行命令：hdfs namenode -format进行初始化 11.在master机器上执行start-all.sh启动hadoop，使用jps命令可以查看到相应进程。 12.在浏览器中输入http://masterIP:50070可以查看web UI。 七、Wordcount测试 1.在h1用户目录下新建hadoop_data文件夹。 2.在hadoop_data文件夹内新建file1.txt及file2.txt文件，并向其中写入要进行实验的文本： Verse 1: You should believe in me, and everything i choose to do You should believe that i’ll alaways come back to you Life is discovering the love that we create Life is a mystery we need to embrace In every way (We need to let go) You’ll see all our dreams will follow In every way (We need to let go) Chorus: People rise together when they believe in tomorrow Change today to forever, as life keeps moving People rise together when they believe in tomottow Change today to forever, as life keeps moving Verse 2: Open your mind and see, we have everything we need Dream a reality, fulfill its destiny In every way (We need to let go) You’ll see all our dream will follow In every way ~ 3.建立input文件夹3.hdfs dfs -mkdir /input将file1.txt及file2.txt文件上传到input文件夹中：hdfs dfs -put /home/h1/hadoop_data/*.txt /input/ 4..hadoop jar /home/h1/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar wordcount /input /output执行测试程序，并将结果输入到/output文件夹中 5.hadoop fs -cat /output/part-r-00000查看结果。 实验结果输出： (We 6 1: 2 2: 2 Change 4 Chorus: 2 Dream 2 In 8 Life 4 Open 2 People 4 Verse 4 You 4 You’ll 4 a 4 alaways 2 all 4 and 4 as 4 back 2 believe 8 choose 2 come 2 create 2 destiny 2 discovering 2 do 2 dream 2 dreams 2 embrace 2 every 8 everything 4 follow 4 forever, 4 fulfill 2 go) 6 have 2 i 2 i’ll 2 in 6 is 4 its 2 keeps 4 let 6 life 4 love 2 me, 2 mind 2 moving 4 mystery 2 need 10 our 4 reality, 2 rise 4 see 4 see, 2 should 4 that 4 the 2 they 4 to 16 today 4 together 4 tomorrow 2 tomottow 2 way 8 we 8 when 4 will 4 you 2 your 2" />
<link rel="canonical" href="https://mlh.app/2019/04/09/728650.html" />
<meta property="og:url" content="https://mlh.app/2019/04/09/728650.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-09T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"一、实现master与slave之间无密码连接 分别在master及slave上生成rsa密钥： mkdir ~/.ssh cd ~/.ssh ssh-keygen -t -rsa 一路回车（选择默认设置），此时，ssh文件夹中生成了id_rsa.pub和id_rsa两个，然后使用scp命令将公钥（id_rsa.pub）分别拷到对方机器中 scp id_rsa.pub h1@对方机器IP：~/.ssh/authorized_keys 并设置权限： chmod 700 authoized_keys chomd 777 ~/.ssh 使用cat命令实现无密码访问自身（localhost）cat id_rsa.pub &gt;&gt; authorized_keys 二：关闭防火墙 sudo ufw disable 三：添加防火墙规则 因阿里云服务器原只支持22、80和４４３端口，所以需要到控制台中添加防火墙规则，使其支持９０００端口等（选择全部ＴＣＰ＋ＵDＰ）。 四：安装JDK 到官网下载JDK tar包，本次实验下载的JDK1.8.0版本,在h1用户下新建/usr/java文件夹，并将下载的压缩包解压到java文件夹中：tar -zvxf jdk-8u191-linux-x64.tar.gz然后打开etc/profile 进行环境变量配置： export JAVA_HOME=/usr/java/jdk1.8.0_191 export JRE_HOME=/usr/java/jdk1.8.0_191/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$JAVA_HOME:$PATH 保存退出，使用source 命令或者重启sudo shutdown -r now使配置生效。可以使用which java测试java是否配置成功。 五：修改hostname及hosts 修改/etc/hostname,master机器上改为master，slave1机器上改为slave1。 修改/etc/hosts,/etc/hosts中原来的内容全部删除，然后加入namenode和datanode的ＩＰ以及名字；master中的hosts中的masterＩＰ为master本机内网ＩＰ，其他slave节点ＩＰ为外网ＩＰ；相同的，slave中的hosts中的本机ＩＰ必为本机内网ＩＰ，其他节点为外网ＩＰ。 六：安装Hadoop 到官网下载Hadoop压缩包，本次下载的版本为2.9.1，在h1用户下新建文件夹/hadoop，并将下载好的压缩包解压缩到hadoop文件夹中tar -zvxf hadoop-2.9.1.tar.gz。 在hadoop目录下新建hdfs文件夹，并在hdfs文件夹中新建name、data、tmp三个文件夹。 进入hadoop目录下的hadoop/etc/hadoop文件夹中对hadoop配置文件进行配置。 1.使用vim打开core-site.xml文件进行如下配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 2.使用vim打开hadoop-env.sh文件，将JAVA_HOME配置为本机JAVA_HOME路径。 3.使用vim打开yarn-env.sh文件，将JAVA_HOME配置为本机JAVA_HOME路径。 4.使用vim打开hdfs-site.xml文件，进行如下配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/name&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/data&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 5.cp mapred-site.xml.template mapred-site.xml 复制其文件并命名为mapre-site.xml，使用vim打开进行配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 6.使用vim打开yarn-site.xml文件进行配置： &lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.sddress&lt;/name&gt; &lt;value&gt;master:18040&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:18030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:18088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:18025&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:18141&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 7.配置slaves文件，将其中的localhost改为slave1。 8.打开/etc/profile进行hadoop环境变量配置： export HADOOP_HOME=/home/h1/hadoop/hadoop-2.9.1 export PATH=&quot;$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH&quot; export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop 9.使用scp hadoop-2.9.1 h1@slaveIP:/home/h1/hadoop/命令将master上的hadoop配置文件拷到slave1上，并修改slave1上的/etc/profile文件，然后使用sudo shutdown -r now命令使两机重启。 10.在master机器上执行命令：hdfs namenode -format进行初始化 11.在master机器上执行start-all.sh启动hadoop，使用jps命令可以查看到相应进程。 12.在浏览器中输入http://masterIP:50070可以查看web UI。 七、Wordcount测试 1.在h1用户目录下新建hadoop_data文件夹。 2.在hadoop_data文件夹内新建file1.txt及file2.txt文件，并向其中写入要进行实验的文本： Verse 1: You should believe in me, and everything i choose to do You should believe that i’ll alaways come back to you Life is discovering the love that we create Life is a mystery we need to embrace In every way (We need to let go) You’ll see all our dreams will follow In every way (We need to let go) Chorus: People rise together when they believe in tomorrow Change today to forever, as life keeps moving People rise together when they believe in tomottow Change today to forever, as life keeps moving Verse 2: Open your mind and see, we have everything we need Dream a reality, fulfill its destiny In every way (We need to let go) You’ll see all our dream will follow In every way ~ 3.建立input文件夹3.hdfs dfs -mkdir /input将file1.txt及file2.txt文件上传到input文件夹中：hdfs dfs -put /home/h1/hadoop_data/*.txt /input/ 4..hadoop jar /home/h1/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar wordcount /input /output执行测试程序，并将结果输入到/output文件夹中 5.hadoop fs -cat /output/part-r-00000查看结果。 实验结果输出： (We 6 1: 2 2: 2 Change 4 Chorus: 2 Dream 2 In 8 Life 4 Open 2 People 4 Verse 4 You 4 You’ll 4 a 4 alaways 2 all 4 and 4 as 4 back 2 believe 8 choose 2 come 2 create 2 destiny 2 discovering 2 do 2 dream 2 dreams 2 embrace 2 every 8 everything 4 follow 4 forever, 4 fulfill 2 go) 6 have 2 i 2 i’ll 2 in 6 is 4 its 2 keeps 4 let 6 life 4 love 2 me, 2 mind 2 moving 4 mystery 2 need 10 our 4 reality, 2 rise 4 see 4 see, 2 should 4 that 4 the 2 they 4 to 16 today 4 together 4 tomorrow 2 tomottow 2 way 8 we 8 when 4 will 4 you 2 your 2","@type":"BlogPosting","url":"https://mlh.app/2019/04/09/728650.html","headline":"在多台阿里云服务器上部署Hadoop分布式系统及WordCount实验","dateModified":"2019-04-09T00:00:00+08:00","datePublished":"2019-04-09T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/04/09/728650.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>在多台阿里云服务器上部署Hadoop分布式系统及WordCount实验</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p>一、实现master与slave之间无密码连接<br> 分别在master及slave上生成rsa密钥：</p> 
  <pre><code>mkdir ~/.ssh
cd ~/.ssh
ssh-keygen -t -rsa
</code></pre> 
  <p>一路回车（选择默认设置），此时，ssh文件夹中生成了id_rsa.pub和id_rsa两个，然后使用scp命令将公钥（id_rsa.pub）分别拷到对方机器中</p> 
  <pre><code>scp id_rsa.pub h1@对方机器IP：~/.ssh/authorized_keys
</code></pre> 
  <p>并设置权限：</p> 
  <pre><code>chmod 700 authoized_keys
chomd 777 ~/.ssh
</code></pre> 
  <p>使用cat命令实现无密码访问自身（localhost）<code>cat id_rsa.pub &gt;&gt; authorized_keys</code><br> 二：关闭防火墙</p> 
  <pre><code>sudo ufw disable
</code></pre> 
  <p>三：添加防火墙规则<br> 因阿里云服务器原只支持22、80和４４３端口，所以需要到控制台中添加防火墙规则，使其支持９０００端口等（选择全部ＴＣＰ＋ＵDＰ）。<br> 四：安装JDK<br> 到官网下载JDK tar包，本次实验下载的JDK1.8.0版本,在h1用户下新建/usr/java文件夹，并将下载的压缩包解压到java文件夹中：<code>tar -zvxf jdk-8u191-linux-x64.tar.gz</code>然后打开etc/profile 进行环境变量配置：</p> 
  <pre><code>export JAVA_HOME=/usr/java/jdk1.8.0_191
export JRE_HOME=/usr/java/jdk1.8.0_191/jre
export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH
export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$JAVA_HOME:$PATH
</code></pre> 
  <p>保存退出，使用source 命令或者重启<code>sudo shutdown -r now</code>使配置生效。可以使用which java测试java是否配置成功。<br> 五：修改hostname及hosts<br> 修改/etc/hostname,master机器上改为master，slave1机器上改为slave1。<br> 修改/etc/hosts,/etc/hosts中原来的内容全部删除，然后加入namenode和datanode的ＩＰ以及名字；master中的hosts中的masterＩＰ为master本机内网ＩＰ，其他slave节点ＩＰ为外网ＩＰ；相同的，slave中的hosts中的本机ＩＰ必为本机内网ＩＰ，其他节点为外网ＩＰ。<br> 六：安装Hadoop<br> 到官网下载Hadoop压缩包，本次下载的版本为2.9.1，在h1用户下新建文件夹/hadoop，并将下载好的压缩包解压缩到hadoop文件夹中tar -zvxf hadoop-2.9.1.tar.gz。<br> 在hadoop目录下新建hdfs文件夹，并在hdfs文件夹中新建name、data、tmp三个文件夹。<br> 进入hadoop目录下的hadoop/etc/hadoop文件夹中对hadoop配置文件进行配置。<br> 1.使用vim打开core-site.xml文件进行如下配置：</p> 
  <pre><code>&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
&lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/tmp&lt;/value&gt;
&lt;description&gt;A base for other temporary directories.&lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;io.file.buffer.size&lt;/name&gt;
&lt;value&gt;131072&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;fs.defaultFS&lt;/name&gt;
&lt;value&gt;hdfs://master:9000&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre> 
  <p>2.使用vim打开hadoop-env.sh文件，将JAVA_HOME配置为本机JAVA_HOME路径。<br> 3.使用vim打开yarn-env.sh文件，将JAVA_HOME配置为本机JAVA_HOME路径。<br> 4.使用vim打开hdfs-site.xml文件，进行如下配置：</p> 
  <pre><code>&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
&lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/name&lt;/value&gt;
&lt;final&gt;true&lt;/final&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
&lt;value&gt;file:/home/h1/hadoop/hadoop-2.9.1/hdfs/data&lt;/value&gt;
&lt;final&gt;true&lt;/final&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
&lt;value&gt;master:9001&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;dfs.permissions&lt;/name&gt;
&lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre> 
  <p>5.cp mapred-site.xml.template mapred-site.xml 复制其文件并命名为mapre-site.xml，使用vim打开进行配置：</p> 
  <pre><code>&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
&lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre> 
  <p>6.使用vim打开yarn-site.xml文件进行配置：</p> 
  <pre><code>&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.sddress&lt;/name&gt;
&lt;value&gt;master:18040&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
&lt;value&gt;master:18030&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
&lt;value&gt;master:18088&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
&lt;value&gt;master:18025&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
&lt;value&gt;master:18141&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.auxservices.mapreduce.shuffle.class&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
    &lt;value&gt;master:8032&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
    &lt;value&gt;master:8030&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
    &lt;value&gt;master:8031&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre> 
  <p>7.配置slaves文件，将其中的localhost改为slave1。<br> 8.打开/etc/profile进行hadoop环境变量配置：</p> 
  <pre><code>
export HADOOP_HOME=/home/h1/hadoop/hadoop-2.9.1
export PATH="$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH"
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
</code></pre> 
  <p>9.使用scp hadoop-2.9.1 h1@slaveIP:/home/h1/hadoop/命令将master上的hadoop配置文件拷到slave1上，并修改slave1上的/etc/profile文件，然后使用sudo shutdown -r now命令使两机重启。<br> 10.在master机器上执行命令：hdfs namenode -format进行初始化<br> 11.在master机器上执行start-all.sh启动hadoop，使用jps命令可以查看到相应进程。<br> 12.在浏览器中输入http://masterIP:50070可以查看web UI。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190409181309128.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjA0NzY5OQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 七、Wordcount测试<br> 1.在h1用户目录下新建hadoop_data文件夹。<br> 2.在hadoop_data文件夹内新建file1.txt及file2.txt文件，并向其中写入要进行实验的文本：</p> 
  <p>Verse 1:<br> You should believe in me, and everything i choose to do<br> You should believe that i’ll alaways come back to you<br> Life is discovering the love that we create<br> Life is a mystery we need to embrace<br> In every way<br> (We need to let go)<br> You’ll see all our dreams will follow<br> In every way<br> (We need to let go)<br> Chorus:<br> People rise together when they believe in tomorrow<br> Change today to forever, as life keeps moving<br> People rise together when they believe in tomottow<br> Change today to forever, as life keeps moving<br> Verse 2:<br> Open your mind and see, we have everything we need<br> Dream a reality, fulfill its destiny<br> In every way<br> (We need to let go)<br> You’ll see all our dream will follow<br> In every way<br> ~<br> 3.建立input文件夹<code>3.hdfs dfs -mkdir /input</code>将file1.txt及file2.txt文件上传到input文件夹中：<code>hdfs dfs -put /home/h1/hadoop_data/*.txt /input/</code><br> 4.<code>.hadoop jar /home/h1/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar wordcount /input /output</code>执行测试程序，并将结果输入到/output文件夹中<br> 5.<code>hadoop fs -cat /output/part-r-00000</code>查看结果。<br> 实验结果输出：<br> (We 6<br> 1: 2<br> 2: 2<br> Change 4<br> Chorus: 2<br> Dream 2<br> In 8<br> Life 4<br> Open 2<br> People 4<br> Verse 4<br> You 4<br> You’ll 4<br> a 4<br> alaways 2<br> all 4<br> and 4<br> as 4<br> back 2<br> believe 8<br> choose 2<br> come 2<br> create 2<br> destiny 2<br> discovering 2<br> do 2<br> dream 2<br> dreams 2<br> embrace 2<br> every 8<br> everything 4<br> follow 4<br> forever, 4<br> fulfill 2<br> go) 6<br> have 2<br> i 2<br> i’ll 2<br> in 6<br> is 4<br> its 2<br> keeps 4<br> let 6<br> life 4<br> love 2<br> me, 2<br> mind 2<br> moving 4<br> mystery 2<br> need 10<br> our 4<br> reality, 2<br> rise 4<br> see 4<br> see, 2<br> should 4<br> that 4<br> the 2<br> they 4<br> to 16<br> today 4<br> together 4<br> tomorrow 2<br> tomottow 2<br> way 8<br> we 8<br> when 4<br> will 4<br> you 2<br> your 2</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
