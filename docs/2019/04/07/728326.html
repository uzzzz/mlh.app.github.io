<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>hadoop之hadoop-2.6.0-cdh5.7.0源码编译支持压缩以及伪分布式部署 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="hadoop之hadoop-2.6.0-cdh5.7.0源码编译支持压缩以及伪分布式部署" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="摘要：本文详细记载hadoop-2.6.0-cdh5.7.0源码编译支持压缩以及伪分布式部署的详细步骤，可用于学习以及生产参考 1.需求与设计 1.1需求 直接使用的hadoop-2.6.0-cdh5.7.0.tar.gz包部署的hadoop集群不支持文件压缩，生产上是不可接受的，故需要将hadoop源码下载重新编译支持压缩 1.1概要设计 下载hadoop源码，使用maven编译，使其支持压缩。并成功进行伪分布式集群部署验证压缩功能。 2.环境需求以及部署规划 2.1 硬件环境 一台centos6.X虚拟机 2.2 软件环境： 组件名称 组件版本 百度网盘链接 vm vm10 链接：https://pan.baidu.com/s/1N5i8p8htXz9H_v__YNV1lA 提取码：yasn centos centos6.7 链接：https://pan.baidu.com/s/1Z_6AcQ_WnvKz1ga_VCSI9Q 提取码：a24x Hadoop Hadoop-2.6.0-cdh5.7.0-src.tar.gz 链接：https://pan.baidu.com/s/1uRMGIhLSL9QHT-Ee4F16jw 提取码：jb1d jdk jdk-7u80-linux-x64.tar.gz 链接：https://pan.baidu.com/s/1xSCQ8rjABVI-zDFQS5nCPA 提取码：lfze maven apache-maven-3.3.9-bin.tar.gz 链接：https://pan.baidu.com/s/1ddkdkLW7r7ahFZmgACGkVw 提取码：fdfz protobuf protobuf-2.5.0.tar.gz 链接：https://pan.baidu.com/s/1RSNZGd_ThwknMB3vDkEfhQ 提取码：hvc2 注意: 1、编译的JDK版本必须是1.7，1.8的JDK会导致编译失败,采坑 3.安装centos 请参考VM虚拟机安装Centos6.X以及主机和网络配置 4.编译hadoop 4.1安装必要的依赖库 [root@hadoop001 ~]# yum install -y svn ncurses-devel [root@hadoop001 ~]# yum install -y gcc gcc-c++ make cmake [root@hadoop001 ~]# yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool [root@hadoop001 ~]# yum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop autoconf automake cmake 4.2添加用户以及上传软件 [root@hadoop001 ~]# yum install -y lrzsz [root@hadoop001 ~]# useradd hadoop [root@hadoop001 ~]# su - hadoop [hadoop@hadoop001 ~]$ mkdir app soft source lib data maven_repo shell mysql [hadoop@hadoop001 ~]$ cd soft/ [hadoop@hadoop001 soft]$ rz [hadoop@hadoop001 soft]$ ll total 202192 -rw-r--r--. 1 hadoop hadoop 8491533 Apr 7 11:25 apache-maven-3.3.9-bin.tar.gz -rw-r--r--. 1 hadoop hadoop 42610549 Apr 6 16:55 hadoop-2.6.0-cdh5.7.0-src.tar.gz -rw-r--r--. 1 hadoop hadoop 153530841 Apr 7 11:12 jdk-7u80-linux-x64.tar.gz -rw-r--r--. 1 hadoop hadoop 2401901 Apr 7 11:31 protobuf-2.5.0.tar.gz 4.3安装JDK 解压安装包，安装目录必须是/usr/java,安装后记得修改拥有者为root [hadoop@hadoop001 soft]$ exit [root@hadoop001 ~]# mkdir /usr/java [root@hadoop001 ~]# tar -zxvf /home/hadoop/soft/jdk-7u80-linux-x64.tar.gz -C /usr/java [root@hadoop001 ~]# cd /usr/java/ [root@hadoop001 java]# chown -R root:root jdk1.7.0_80 添加环境变量 [root@hadoop001 jdk1.7.0_80]# vim /etc/profile #添加如下两行环境变量 export JAVA_HOME=/usr/java/jdk1.7.0_80 export PATH=$JAVA_HOME/bin:$PATH [root@hadoop001 jdk1.7.0_80]# source /etc/profile #测试java是否安装成功 [root@hadoop001 jdk1.7.0_80]# java -version java version &quot;1.7.0_80&quot; Java(TM) SE Runtime Environment (build 1.7.0_80-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) 4.4安装maven 解压 [root@hadoop001 ~]# su - hadoop [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/apache-maven-3.3.9-bin.tar.gz -C ~/app/ 添加环境变量 #修改haoop用户的环境变量 [hadoop@hadoop001 ~]$ vim ~/.bash_profile #添加或修改如下内容，注意MAVEN_OPTS设置了maven运行的内存，防止内存太小导致编译失败 export MAVEN_HOME=/home/hadoop/app/apache-maven-3.3.9 export MAVEN_OPTS=&quot;-Xms1024m -Xmx1024m&quot; export PATH=$MAVEN_HOME/bin:$PATH [hadoop@hadoop001 ~]$ source ~/.bash_profile [hadoop@hadoop001 ~]$ which mvn ~/app/apache-maven-3.3.9/bin/mvn 配置maven [hadoop@hadoop001 protobuf-2.5.0]$ vim ~/app/apache-maven-3.3.9/conf/settings.xml #配置maven的本地仓库位置 &lt;localRepository&gt;/home/hadoop/maven_repo/repo&lt;/localRepository&gt; #添加阿里云中央仓库地址，注意一定要写在&lt;mirrors&gt;&lt;/mirrors&gt;之间 &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; （可选）添加jars到本地仓库，网络慢可能导致mvn第一次编译时下载需要超长的时间甚至编译失败 #jar包链接 链接：https://pan.baidu.com/s/1vq4iVFqqyJNkYzg90bVrfg 提取码：vugv 复制这段内容后打开百度网盘手机App，操作更方便哦 #下载后 rz上传解压，注意目录层次 [hadoop@hadoop001 maven_repo]$ rz [hadoop@hadoop001 maven_repo]$ tar -zxvf repo.tar.gz 4.5安装protobuf 解压 [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/protobuf-2.5.0.tar.gz -C ~/app/ 编译软件 [hadoop@hadoop001 protobuf-2.5.0]$ cd ~/app/protobuf-2.5.0/ # --prefix= 是用来待会编译好的包放在为路径 [hadoop@hadoop001 protobuf-2.5.0]$ ./configure --prefix=/home/hadoop/app/protobuf-2.5.0 #编译以及安装 [hadoop@hadoop001 protobuf-2.5.0]$ make [hadoop@hadoop001 protobuf-2.5.0]$ make install 添加环境变量 [hadoop@hadoop001 protobuf-2.5.0]$ vim ~/.bash_profile #追加如下两行内容，未编译前是没有bin目录的 export PROTOBUF_HOME=/home/hadoop/app/protobuf-2.5.0 export PATH=$PROTOBUF_HOME/bin:$PATH [hadoop@hadoop001 protobuf-2.5.0]$ source ~/.bash_profile #测试是否生效，若出现libprotoc 2.5.0则为生效 [hadoop@hadoop001 protobuf-2.5.0]$ protoc --version libprotoc 2.5.0 4.6编译hadoop 解压 [hadoop@hadoop001 protobuf-2.5.0]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0-src.tar.gz -C ~/source/ 编译hadoop使其支持压缩：mvn clean package -Pdist,native -DskipTests -Dtar #进入hadoop的源码目录 [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cd ~/source/hadoop-2.6.0-cdh5.7.0/ #进行编译，第一次编译会下载很多依赖的jar包，快慢由网速决定，需耐心等待，本人亲测耗时 [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ mvn clean package -Pdist,native -DskipTests -Dtar 若报异常，主要信息如下（无异常跳过）： [FATAL] Non-resolvable parent POM for org.apache.hadoop:hadoop-main:2.6.0-cdh5.7.0: Could not transfer artifact com.cloudera.cdh:cdh-root:pom:5.7.0 from/to cdh.repo (https://repository.cloudera.com/artifactory/cloudera-repos): Remote host closed connectio #分析：是https://repository.cloudera.com/artifactory/cloudera-repos/com/cloudera/cdh/cdh-root/5.7.0/cdh-root-5.7.0.pom文件下载不了，但是虚拟机确实是ping通远程的仓库，很是费解为什么。 #解决方案：前往本地仓库到目标文件目录，然后 通过wget 文件，来成功获取该文件，重新执行编译命令，或者执行4.5的可选步骤，将需要的jar直接放到本地仓库 查看编译后的包：hadoop-2.6.0-cdh5.7.0.tar.gz #有 BUILD SUCCESS 信息则表示编译成功 [INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 13.592 s] [INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.042 s] [INFO] Apache Hadoop Tools ................................ SUCCESS [ 0.094 s] [INFO] Apache Hadoop Distribution ......................... SUCCESS [01:49 min] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 37:39 min [INFO] Finished at: 2019-04-07T16:48:42+08:00 [INFO] Final Memory: 200M/989M [INFO] ------------------------------------------------------------------------ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ ll /home/hadoop/source/hadoop-2.6.0-cdh5.7.0/hadoop-dist/target/ total 564036 drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:46 antrun drwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 classes -rw-rw-r--. 1 hadoop hadoop 1998 Apr 7 16:46 dist-layout-stitching.sh -rw-rw-r--. 1 hadoop hadoop 690 Apr 7 16:47 dist-tar-stitching.sh drwxrwxr-x. 9 hadoop hadoop 4096 Apr 7 16:47 hadoop-2.6.0-cdh5.7.0 -rw-rw-r--. 1 hadoop hadoop 191880143 Apr 7 16:47 hadoop-2.6.0-cdh5.7.0.tar.gz -rw-rw-r--. 1 hadoop hadoop 7314 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0.jar -rw-rw-r--. 1 hadoop hadoop 385618309 Apr 7 16:48 hadoop-dist-2.6.0-cdh5.7.0-javadoc.jar -rw-rw-r--. 1 hadoop hadoop 4855 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0-sources.jar -rw-rw-r--. 1 hadoop hadoop 4855 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0-test-sources.jar drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:47 javadoc-bundle-options drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:47 maven-archiver drwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 maven-shared-archive-resources drwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 test-classes drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:46 test-dir 5.伪分布式部署 5.1解压安装包 [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cp /home/hadoop/source/hadoop-2.6.0-cdh5.7.0/hadoop-dist/target/hadoop-2.6.0-cdh5.7.0.tar.gz /home/hadoop/soft/ [hadoop@hadoop001 ~]$ cd ~ [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app/ 5.2配置环境变量 [hadoop@hadoop001 ~]$ vim ~/.bash_profile export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH [hadoop@hadoop001 ~]$ source ~/.bash_profile [hadoop@hadoop001 ~]$ which hadoop ~/app/hadoop-2.6.0-cdh5.7.0/bin/hadoop 5.3配置ssh [hadoop@hadoop001 ~]$ rm -rf ~/.ssh [hadoop@hadoop001 ~]$ ssh-keygen [hadoop@hadoop001 ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys [hadoop@hadoop001 ~]$ chmod 600 ~/.ssh/authorized_keys #测试ssh是否成功，用户第一ssh会提示输入是否连接，yes。成功显示时间 [hadoop@hadoop001 ~]$ ssh hadoop001 date 5.4修改配置文件 编辑hadoop-env.sh文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/hadoop-env.sh #将JDK的安装目录修改为绝对路径 export JAVA_HOME=/usr/java/jdk1.7.0_80 编辑core-site.xml文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/core-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:9000&lt;/value&gt; &lt;/property&gt; 编辑hdfs-site.xml文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/hdfs-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop001:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop001:50091&lt;/value&gt; &lt;/property&gt; 添加slaves节点 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/slaves #添加一行内容如下 hadoop001 编辑yarn-site.xml文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/yarn-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 编辑yarn-site.xml文件相关配置 [hadoop@hadoop001 ~]$ cp ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml.template ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 5.5格式化namenode [hadoop@hadoop001 ~]$ hdfs namenode -format #若出现 has been successfully formatted 则表示格式化成功 19/04/07 17:42:31 INFO namenode.FSImage: Allocated new BlockPoolId: BP-565897555-192.168.175.135-1554630151139 19/04/07 17:42:31 INFO common.Storage: Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted. 19/04/07 17:42:32 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0 19/04/07 17:42:32 INFO util.ExitUtil: Exiting with status 0 19/04/07 17:42:32 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at hadoop001/192.168.175.135 ************************************************************/ 5.6启动hadoop [hadoop@hadoop001 ~]$ start-all.sh This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh Starting namenodes on [hadoop001] hadoop001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop001.out hadoop001: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop001.out Starting secondary namenodes [hadoop001] hadoop001: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop001.out starting yarn daemons starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop001.out hadoop001: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop001.out #查询出五个守护进程 [hadoop@hadoop001 ~]$ jps 56598 SecondaryNameNode 56933 Jps 56896 NodeManager 56801 ResourceManager 56470 DataNode 56376 NameNode ###6.验证hadoop 6.1 hdfs验证 [hadoop@hadoop001 ~]$ hdfs dfs -put ~/app/hadoop-2.6.0-cdh5.7.0/README.txt hdfs://hadoop001:9000/ [hadoop@hadoop001 ~]$ hdfs dfs -ls / Found 1 items -rw-r--r-- 1 hadoop supergroup 1366 2019-04-07 17:52 /README.txt 6.2yarn验证 #执行MR用列代码 [hadoop@hadoop001 ~]$ hadoop jar /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 4 10 6.2检测压缩格式 #true表示支持的意思 [hadoop@hadoop001 ~]$ hadoop checknative 19/04/07 17:50:08 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native 19/04/07 17:50:08 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library Native library checking: hadoop: true /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native/libhadoop.so.1.0.0 zlib: true /lib64/libz.so.1 snappy: true /usr/lib64/libsnappy.so.1 lz4: true revision:99 bzip2: true /lib64/libbz2.so.1 openssl: true /usr/lib64/libcrypto.so 扩展1：protobuf是什么？ protobuf它是一种轻便高效的数据格式，类似于Json，平台无关、语言无关、可扩展，可用于通讯协议和数据存储等领域。 优点： 平台无关，语言无关，可扩展； 提供了友好的动态库，使用简单； 解析速度快，比对应的XML快约20-100倍； 序列化数据非常简洁、紧凑，与XML相比，其序列化之后的数据量约为1/3到1/10。 扩展2：mvn命令也是可以进行debug的，mvn -X 命令" />
<meta property="og:description" content="摘要：本文详细记载hadoop-2.6.0-cdh5.7.0源码编译支持压缩以及伪分布式部署的详细步骤，可用于学习以及生产参考 1.需求与设计 1.1需求 直接使用的hadoop-2.6.0-cdh5.7.0.tar.gz包部署的hadoop集群不支持文件压缩，生产上是不可接受的，故需要将hadoop源码下载重新编译支持压缩 1.1概要设计 下载hadoop源码，使用maven编译，使其支持压缩。并成功进行伪分布式集群部署验证压缩功能。 2.环境需求以及部署规划 2.1 硬件环境 一台centos6.X虚拟机 2.2 软件环境： 组件名称 组件版本 百度网盘链接 vm vm10 链接：https://pan.baidu.com/s/1N5i8p8htXz9H_v__YNV1lA 提取码：yasn centos centos6.7 链接：https://pan.baidu.com/s/1Z_6AcQ_WnvKz1ga_VCSI9Q 提取码：a24x Hadoop Hadoop-2.6.0-cdh5.7.0-src.tar.gz 链接：https://pan.baidu.com/s/1uRMGIhLSL9QHT-Ee4F16jw 提取码：jb1d jdk jdk-7u80-linux-x64.tar.gz 链接：https://pan.baidu.com/s/1xSCQ8rjABVI-zDFQS5nCPA 提取码：lfze maven apache-maven-3.3.9-bin.tar.gz 链接：https://pan.baidu.com/s/1ddkdkLW7r7ahFZmgACGkVw 提取码：fdfz protobuf protobuf-2.5.0.tar.gz 链接：https://pan.baidu.com/s/1RSNZGd_ThwknMB3vDkEfhQ 提取码：hvc2 注意: 1、编译的JDK版本必须是1.7，1.8的JDK会导致编译失败,采坑 3.安装centos 请参考VM虚拟机安装Centos6.X以及主机和网络配置 4.编译hadoop 4.1安装必要的依赖库 [root@hadoop001 ~]# yum install -y svn ncurses-devel [root@hadoop001 ~]# yum install -y gcc gcc-c++ make cmake [root@hadoop001 ~]# yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool [root@hadoop001 ~]# yum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop autoconf automake cmake 4.2添加用户以及上传软件 [root@hadoop001 ~]# yum install -y lrzsz [root@hadoop001 ~]# useradd hadoop [root@hadoop001 ~]# su - hadoop [hadoop@hadoop001 ~]$ mkdir app soft source lib data maven_repo shell mysql [hadoop@hadoop001 ~]$ cd soft/ [hadoop@hadoop001 soft]$ rz [hadoop@hadoop001 soft]$ ll total 202192 -rw-r--r--. 1 hadoop hadoop 8491533 Apr 7 11:25 apache-maven-3.3.9-bin.tar.gz -rw-r--r--. 1 hadoop hadoop 42610549 Apr 6 16:55 hadoop-2.6.0-cdh5.7.0-src.tar.gz -rw-r--r--. 1 hadoop hadoop 153530841 Apr 7 11:12 jdk-7u80-linux-x64.tar.gz -rw-r--r--. 1 hadoop hadoop 2401901 Apr 7 11:31 protobuf-2.5.0.tar.gz 4.3安装JDK 解压安装包，安装目录必须是/usr/java,安装后记得修改拥有者为root [hadoop@hadoop001 soft]$ exit [root@hadoop001 ~]# mkdir /usr/java [root@hadoop001 ~]# tar -zxvf /home/hadoop/soft/jdk-7u80-linux-x64.tar.gz -C /usr/java [root@hadoop001 ~]# cd /usr/java/ [root@hadoop001 java]# chown -R root:root jdk1.7.0_80 添加环境变量 [root@hadoop001 jdk1.7.0_80]# vim /etc/profile #添加如下两行环境变量 export JAVA_HOME=/usr/java/jdk1.7.0_80 export PATH=$JAVA_HOME/bin:$PATH [root@hadoop001 jdk1.7.0_80]# source /etc/profile #测试java是否安装成功 [root@hadoop001 jdk1.7.0_80]# java -version java version &quot;1.7.0_80&quot; Java(TM) SE Runtime Environment (build 1.7.0_80-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) 4.4安装maven 解压 [root@hadoop001 ~]# su - hadoop [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/apache-maven-3.3.9-bin.tar.gz -C ~/app/ 添加环境变量 #修改haoop用户的环境变量 [hadoop@hadoop001 ~]$ vim ~/.bash_profile #添加或修改如下内容，注意MAVEN_OPTS设置了maven运行的内存，防止内存太小导致编译失败 export MAVEN_HOME=/home/hadoop/app/apache-maven-3.3.9 export MAVEN_OPTS=&quot;-Xms1024m -Xmx1024m&quot; export PATH=$MAVEN_HOME/bin:$PATH [hadoop@hadoop001 ~]$ source ~/.bash_profile [hadoop@hadoop001 ~]$ which mvn ~/app/apache-maven-3.3.9/bin/mvn 配置maven [hadoop@hadoop001 protobuf-2.5.0]$ vim ~/app/apache-maven-3.3.9/conf/settings.xml #配置maven的本地仓库位置 &lt;localRepository&gt;/home/hadoop/maven_repo/repo&lt;/localRepository&gt; #添加阿里云中央仓库地址，注意一定要写在&lt;mirrors&gt;&lt;/mirrors&gt;之间 &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; （可选）添加jars到本地仓库，网络慢可能导致mvn第一次编译时下载需要超长的时间甚至编译失败 #jar包链接 链接：https://pan.baidu.com/s/1vq4iVFqqyJNkYzg90bVrfg 提取码：vugv 复制这段内容后打开百度网盘手机App，操作更方便哦 #下载后 rz上传解压，注意目录层次 [hadoop@hadoop001 maven_repo]$ rz [hadoop@hadoop001 maven_repo]$ tar -zxvf repo.tar.gz 4.5安装protobuf 解压 [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/protobuf-2.5.0.tar.gz -C ~/app/ 编译软件 [hadoop@hadoop001 protobuf-2.5.0]$ cd ~/app/protobuf-2.5.0/ # --prefix= 是用来待会编译好的包放在为路径 [hadoop@hadoop001 protobuf-2.5.0]$ ./configure --prefix=/home/hadoop/app/protobuf-2.5.0 #编译以及安装 [hadoop@hadoop001 protobuf-2.5.0]$ make [hadoop@hadoop001 protobuf-2.5.0]$ make install 添加环境变量 [hadoop@hadoop001 protobuf-2.5.0]$ vim ~/.bash_profile #追加如下两行内容，未编译前是没有bin目录的 export PROTOBUF_HOME=/home/hadoop/app/protobuf-2.5.0 export PATH=$PROTOBUF_HOME/bin:$PATH [hadoop@hadoop001 protobuf-2.5.0]$ source ~/.bash_profile #测试是否生效，若出现libprotoc 2.5.0则为生效 [hadoop@hadoop001 protobuf-2.5.0]$ protoc --version libprotoc 2.5.0 4.6编译hadoop 解压 [hadoop@hadoop001 protobuf-2.5.0]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0-src.tar.gz -C ~/source/ 编译hadoop使其支持压缩：mvn clean package -Pdist,native -DskipTests -Dtar #进入hadoop的源码目录 [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cd ~/source/hadoop-2.6.0-cdh5.7.0/ #进行编译，第一次编译会下载很多依赖的jar包，快慢由网速决定，需耐心等待，本人亲测耗时 [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ mvn clean package -Pdist,native -DskipTests -Dtar 若报异常，主要信息如下（无异常跳过）： [FATAL] Non-resolvable parent POM for org.apache.hadoop:hadoop-main:2.6.0-cdh5.7.0: Could not transfer artifact com.cloudera.cdh:cdh-root:pom:5.7.0 from/to cdh.repo (https://repository.cloudera.com/artifactory/cloudera-repos): Remote host closed connectio #分析：是https://repository.cloudera.com/artifactory/cloudera-repos/com/cloudera/cdh/cdh-root/5.7.0/cdh-root-5.7.0.pom文件下载不了，但是虚拟机确实是ping通远程的仓库，很是费解为什么。 #解决方案：前往本地仓库到目标文件目录，然后 通过wget 文件，来成功获取该文件，重新执行编译命令，或者执行4.5的可选步骤，将需要的jar直接放到本地仓库 查看编译后的包：hadoop-2.6.0-cdh5.7.0.tar.gz #有 BUILD SUCCESS 信息则表示编译成功 [INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 13.592 s] [INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.042 s] [INFO] Apache Hadoop Tools ................................ SUCCESS [ 0.094 s] [INFO] Apache Hadoop Distribution ......................... SUCCESS [01:49 min] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 37:39 min [INFO] Finished at: 2019-04-07T16:48:42+08:00 [INFO] Final Memory: 200M/989M [INFO] ------------------------------------------------------------------------ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ ll /home/hadoop/source/hadoop-2.6.0-cdh5.7.0/hadoop-dist/target/ total 564036 drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:46 antrun drwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 classes -rw-rw-r--. 1 hadoop hadoop 1998 Apr 7 16:46 dist-layout-stitching.sh -rw-rw-r--. 1 hadoop hadoop 690 Apr 7 16:47 dist-tar-stitching.sh drwxrwxr-x. 9 hadoop hadoop 4096 Apr 7 16:47 hadoop-2.6.0-cdh5.7.0 -rw-rw-r--. 1 hadoop hadoop 191880143 Apr 7 16:47 hadoop-2.6.0-cdh5.7.0.tar.gz -rw-rw-r--. 1 hadoop hadoop 7314 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0.jar -rw-rw-r--. 1 hadoop hadoop 385618309 Apr 7 16:48 hadoop-dist-2.6.0-cdh5.7.0-javadoc.jar -rw-rw-r--. 1 hadoop hadoop 4855 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0-sources.jar -rw-rw-r--. 1 hadoop hadoop 4855 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0-test-sources.jar drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:47 javadoc-bundle-options drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:47 maven-archiver drwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 maven-shared-archive-resources drwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 test-classes drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:46 test-dir 5.伪分布式部署 5.1解压安装包 [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cp /home/hadoop/source/hadoop-2.6.0-cdh5.7.0/hadoop-dist/target/hadoop-2.6.0-cdh5.7.0.tar.gz /home/hadoop/soft/ [hadoop@hadoop001 ~]$ cd ~ [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app/ 5.2配置环境变量 [hadoop@hadoop001 ~]$ vim ~/.bash_profile export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH [hadoop@hadoop001 ~]$ source ~/.bash_profile [hadoop@hadoop001 ~]$ which hadoop ~/app/hadoop-2.6.0-cdh5.7.0/bin/hadoop 5.3配置ssh [hadoop@hadoop001 ~]$ rm -rf ~/.ssh [hadoop@hadoop001 ~]$ ssh-keygen [hadoop@hadoop001 ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys [hadoop@hadoop001 ~]$ chmod 600 ~/.ssh/authorized_keys #测试ssh是否成功，用户第一ssh会提示输入是否连接，yes。成功显示时间 [hadoop@hadoop001 ~]$ ssh hadoop001 date 5.4修改配置文件 编辑hadoop-env.sh文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/hadoop-env.sh #将JDK的安装目录修改为绝对路径 export JAVA_HOME=/usr/java/jdk1.7.0_80 编辑core-site.xml文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/core-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:9000&lt;/value&gt; &lt;/property&gt; 编辑hdfs-site.xml文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/hdfs-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop001:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop001:50091&lt;/value&gt; &lt;/property&gt; 添加slaves节点 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/slaves #添加一行内容如下 hadoop001 编辑yarn-site.xml文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/yarn-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 编辑yarn-site.xml文件相关配置 [hadoop@hadoop001 ~]$ cp ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml.template ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 5.5格式化namenode [hadoop@hadoop001 ~]$ hdfs namenode -format #若出现 has been successfully formatted 则表示格式化成功 19/04/07 17:42:31 INFO namenode.FSImage: Allocated new BlockPoolId: BP-565897555-192.168.175.135-1554630151139 19/04/07 17:42:31 INFO common.Storage: Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted. 19/04/07 17:42:32 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0 19/04/07 17:42:32 INFO util.ExitUtil: Exiting with status 0 19/04/07 17:42:32 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at hadoop001/192.168.175.135 ************************************************************/ 5.6启动hadoop [hadoop@hadoop001 ~]$ start-all.sh This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh Starting namenodes on [hadoop001] hadoop001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop001.out hadoop001: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop001.out Starting secondary namenodes [hadoop001] hadoop001: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop001.out starting yarn daemons starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop001.out hadoop001: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop001.out #查询出五个守护进程 [hadoop@hadoop001 ~]$ jps 56598 SecondaryNameNode 56933 Jps 56896 NodeManager 56801 ResourceManager 56470 DataNode 56376 NameNode ###6.验证hadoop 6.1 hdfs验证 [hadoop@hadoop001 ~]$ hdfs dfs -put ~/app/hadoop-2.6.0-cdh5.7.0/README.txt hdfs://hadoop001:9000/ [hadoop@hadoop001 ~]$ hdfs dfs -ls / Found 1 items -rw-r--r-- 1 hadoop supergroup 1366 2019-04-07 17:52 /README.txt 6.2yarn验证 #执行MR用列代码 [hadoop@hadoop001 ~]$ hadoop jar /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 4 10 6.2检测压缩格式 #true表示支持的意思 [hadoop@hadoop001 ~]$ hadoop checknative 19/04/07 17:50:08 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native 19/04/07 17:50:08 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library Native library checking: hadoop: true /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native/libhadoop.so.1.0.0 zlib: true /lib64/libz.so.1 snappy: true /usr/lib64/libsnappy.so.1 lz4: true revision:99 bzip2: true /lib64/libbz2.so.1 openssl: true /usr/lib64/libcrypto.so 扩展1：protobuf是什么？ protobuf它是一种轻便高效的数据格式，类似于Json，平台无关、语言无关、可扩展，可用于通讯协议和数据存储等领域。 优点： 平台无关，语言无关，可扩展； 提供了友好的动态库，使用简单； 解析速度快，比对应的XML快约20-100倍； 序列化数据非常简洁、紧凑，与XML相比，其序列化之后的数据量约为1/3到1/10。 扩展2：mvn命令也是可以进行debug的，mvn -X 命令" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-07T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"摘要：本文详细记载hadoop-2.6.0-cdh5.7.0源码编译支持压缩以及伪分布式部署的详细步骤，可用于学习以及生产参考 1.需求与设计 1.1需求 直接使用的hadoop-2.6.0-cdh5.7.0.tar.gz包部署的hadoop集群不支持文件压缩，生产上是不可接受的，故需要将hadoop源码下载重新编译支持压缩 1.1概要设计 下载hadoop源码，使用maven编译，使其支持压缩。并成功进行伪分布式集群部署验证压缩功能。 2.环境需求以及部署规划 2.1 硬件环境 一台centos6.X虚拟机 2.2 软件环境： 组件名称 组件版本 百度网盘链接 vm vm10 链接：https://pan.baidu.com/s/1N5i8p8htXz9H_v__YNV1lA 提取码：yasn centos centos6.7 链接：https://pan.baidu.com/s/1Z_6AcQ_WnvKz1ga_VCSI9Q 提取码：a24x Hadoop Hadoop-2.6.0-cdh5.7.0-src.tar.gz 链接：https://pan.baidu.com/s/1uRMGIhLSL9QHT-Ee4F16jw 提取码：jb1d jdk jdk-7u80-linux-x64.tar.gz 链接：https://pan.baidu.com/s/1xSCQ8rjABVI-zDFQS5nCPA 提取码：lfze maven apache-maven-3.3.9-bin.tar.gz 链接：https://pan.baidu.com/s/1ddkdkLW7r7ahFZmgACGkVw 提取码：fdfz protobuf protobuf-2.5.0.tar.gz 链接：https://pan.baidu.com/s/1RSNZGd_ThwknMB3vDkEfhQ 提取码：hvc2 注意: 1、编译的JDK版本必须是1.7，1.8的JDK会导致编译失败,采坑 3.安装centos 请参考VM虚拟机安装Centos6.X以及主机和网络配置 4.编译hadoop 4.1安装必要的依赖库 [root@hadoop001 ~]# yum install -y svn ncurses-devel [root@hadoop001 ~]# yum install -y gcc gcc-c++ make cmake [root@hadoop001 ~]# yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool [root@hadoop001 ~]# yum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop autoconf automake cmake 4.2添加用户以及上传软件 [root@hadoop001 ~]# yum install -y lrzsz [root@hadoop001 ~]# useradd hadoop [root@hadoop001 ~]# su - hadoop [hadoop@hadoop001 ~]$ mkdir app soft source lib data maven_repo shell mysql [hadoop@hadoop001 ~]$ cd soft/ [hadoop@hadoop001 soft]$ rz [hadoop@hadoop001 soft]$ ll total 202192 -rw-r--r--. 1 hadoop hadoop 8491533 Apr 7 11:25 apache-maven-3.3.9-bin.tar.gz -rw-r--r--. 1 hadoop hadoop 42610549 Apr 6 16:55 hadoop-2.6.0-cdh5.7.0-src.tar.gz -rw-r--r--. 1 hadoop hadoop 153530841 Apr 7 11:12 jdk-7u80-linux-x64.tar.gz -rw-r--r--. 1 hadoop hadoop 2401901 Apr 7 11:31 protobuf-2.5.0.tar.gz 4.3安装JDK 解压安装包，安装目录必须是/usr/java,安装后记得修改拥有者为root [hadoop@hadoop001 soft]$ exit [root@hadoop001 ~]# mkdir /usr/java [root@hadoop001 ~]# tar -zxvf /home/hadoop/soft/jdk-7u80-linux-x64.tar.gz -C /usr/java [root@hadoop001 ~]# cd /usr/java/ [root@hadoop001 java]# chown -R root:root jdk1.7.0_80 添加环境变量 [root@hadoop001 jdk1.7.0_80]# vim /etc/profile #添加如下两行环境变量 export JAVA_HOME=/usr/java/jdk1.7.0_80 export PATH=$JAVA_HOME/bin:$PATH [root@hadoop001 jdk1.7.0_80]# source /etc/profile #测试java是否安装成功 [root@hadoop001 jdk1.7.0_80]# java -version java version &quot;1.7.0_80&quot; Java(TM) SE Runtime Environment (build 1.7.0_80-b15) Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) 4.4安装maven 解压 [root@hadoop001 ~]# su - hadoop [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/apache-maven-3.3.9-bin.tar.gz -C ~/app/ 添加环境变量 #修改haoop用户的环境变量 [hadoop@hadoop001 ~]$ vim ~/.bash_profile #添加或修改如下内容，注意MAVEN_OPTS设置了maven运行的内存，防止内存太小导致编译失败 export MAVEN_HOME=/home/hadoop/app/apache-maven-3.3.9 export MAVEN_OPTS=&quot;-Xms1024m -Xmx1024m&quot; export PATH=$MAVEN_HOME/bin:$PATH [hadoop@hadoop001 ~]$ source ~/.bash_profile [hadoop@hadoop001 ~]$ which mvn ~/app/apache-maven-3.3.9/bin/mvn 配置maven [hadoop@hadoop001 protobuf-2.5.0]$ vim ~/app/apache-maven-3.3.9/conf/settings.xml #配置maven的本地仓库位置 &lt;localRepository&gt;/home/hadoop/maven_repo/repo&lt;/localRepository&gt; #添加阿里云中央仓库地址，注意一定要写在&lt;mirrors&gt;&lt;/mirrors&gt;之间 &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; （可选）添加jars到本地仓库，网络慢可能导致mvn第一次编译时下载需要超长的时间甚至编译失败 #jar包链接 链接：https://pan.baidu.com/s/1vq4iVFqqyJNkYzg90bVrfg 提取码：vugv 复制这段内容后打开百度网盘手机App，操作更方便哦 #下载后 rz上传解压，注意目录层次 [hadoop@hadoop001 maven_repo]$ rz [hadoop@hadoop001 maven_repo]$ tar -zxvf repo.tar.gz 4.5安装protobuf 解压 [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/protobuf-2.5.0.tar.gz -C ~/app/ 编译软件 [hadoop@hadoop001 protobuf-2.5.0]$ cd ~/app/protobuf-2.5.0/ # --prefix= 是用来待会编译好的包放在为路径 [hadoop@hadoop001 protobuf-2.5.0]$ ./configure --prefix=/home/hadoop/app/protobuf-2.5.0 #编译以及安装 [hadoop@hadoop001 protobuf-2.5.0]$ make [hadoop@hadoop001 protobuf-2.5.0]$ make install 添加环境变量 [hadoop@hadoop001 protobuf-2.5.0]$ vim ~/.bash_profile #追加如下两行内容，未编译前是没有bin目录的 export PROTOBUF_HOME=/home/hadoop/app/protobuf-2.5.0 export PATH=$PROTOBUF_HOME/bin:$PATH [hadoop@hadoop001 protobuf-2.5.0]$ source ~/.bash_profile #测试是否生效，若出现libprotoc 2.5.0则为生效 [hadoop@hadoop001 protobuf-2.5.0]$ protoc --version libprotoc 2.5.0 4.6编译hadoop 解压 [hadoop@hadoop001 protobuf-2.5.0]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0-src.tar.gz -C ~/source/ 编译hadoop使其支持压缩：mvn clean package -Pdist,native -DskipTests -Dtar #进入hadoop的源码目录 [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cd ~/source/hadoop-2.6.0-cdh5.7.0/ #进行编译，第一次编译会下载很多依赖的jar包，快慢由网速决定，需耐心等待，本人亲测耗时 [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ mvn clean package -Pdist,native -DskipTests -Dtar 若报异常，主要信息如下（无异常跳过）： [FATAL] Non-resolvable parent POM for org.apache.hadoop:hadoop-main:2.6.0-cdh5.7.0: Could not transfer artifact com.cloudera.cdh:cdh-root:pom:5.7.0 from/to cdh.repo (https://repository.cloudera.com/artifactory/cloudera-repos): Remote host closed connectio #分析：是https://repository.cloudera.com/artifactory/cloudera-repos/com/cloudera/cdh/cdh-root/5.7.0/cdh-root-5.7.0.pom文件下载不了，但是虚拟机确实是ping通远程的仓库，很是费解为什么。 #解决方案：前往本地仓库到目标文件目录，然后 通过wget 文件，来成功获取该文件，重新执行编译命令，或者执行4.5的可选步骤，将需要的jar直接放到本地仓库 查看编译后的包：hadoop-2.6.0-cdh5.7.0.tar.gz #有 BUILD SUCCESS 信息则表示编译成功 [INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 13.592 s] [INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.042 s] [INFO] Apache Hadoop Tools ................................ SUCCESS [ 0.094 s] [INFO] Apache Hadoop Distribution ......................... SUCCESS [01:49 min] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 37:39 min [INFO] Finished at: 2019-04-07T16:48:42+08:00 [INFO] Final Memory: 200M/989M [INFO] ------------------------------------------------------------------------ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ ll /home/hadoop/source/hadoop-2.6.0-cdh5.7.0/hadoop-dist/target/ total 564036 drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:46 antrun drwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 classes -rw-rw-r--. 1 hadoop hadoop 1998 Apr 7 16:46 dist-layout-stitching.sh -rw-rw-r--. 1 hadoop hadoop 690 Apr 7 16:47 dist-tar-stitching.sh drwxrwxr-x. 9 hadoop hadoop 4096 Apr 7 16:47 hadoop-2.6.0-cdh5.7.0 -rw-rw-r--. 1 hadoop hadoop 191880143 Apr 7 16:47 hadoop-2.6.0-cdh5.7.0.tar.gz -rw-rw-r--. 1 hadoop hadoop 7314 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0.jar -rw-rw-r--. 1 hadoop hadoop 385618309 Apr 7 16:48 hadoop-dist-2.6.0-cdh5.7.0-javadoc.jar -rw-rw-r--. 1 hadoop hadoop 4855 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0-sources.jar -rw-rw-r--. 1 hadoop hadoop 4855 Apr 7 16:47 hadoop-dist-2.6.0-cdh5.7.0-test-sources.jar drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:47 javadoc-bundle-options drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:47 maven-archiver drwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 maven-shared-archive-resources drwxrwxr-x. 3 hadoop hadoop 4096 Apr 7 16:46 test-classes drwxrwxr-x. 2 hadoop hadoop 4096 Apr 7 16:46 test-dir 5.伪分布式部署 5.1解压安装包 [hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cp /home/hadoop/source/hadoop-2.6.0-cdh5.7.0/hadoop-dist/target/hadoop-2.6.0-cdh5.7.0.tar.gz /home/hadoop/soft/ [hadoop@hadoop001 ~]$ cd ~ [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app/ 5.2配置环境变量 [hadoop@hadoop001 ~]$ vim ~/.bash_profile export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0 export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH [hadoop@hadoop001 ~]$ source ~/.bash_profile [hadoop@hadoop001 ~]$ which hadoop ~/app/hadoop-2.6.0-cdh5.7.0/bin/hadoop 5.3配置ssh [hadoop@hadoop001 ~]$ rm -rf ~/.ssh [hadoop@hadoop001 ~]$ ssh-keygen [hadoop@hadoop001 ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys [hadoop@hadoop001 ~]$ chmod 600 ~/.ssh/authorized_keys #测试ssh是否成功，用户第一ssh会提示输入是否连接，yes。成功显示时间 [hadoop@hadoop001 ~]$ ssh hadoop001 date 5.4修改配置文件 编辑hadoop-env.sh文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/hadoop-env.sh #将JDK的安装目录修改为绝对路径 export JAVA_HOME=/usr/java/jdk1.7.0_80 编辑core-site.xml文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/core-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:9000&lt;/value&gt; &lt;/property&gt; 编辑hdfs-site.xml文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/hdfs-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop001:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;hadoop001:50091&lt;/value&gt; &lt;/property&gt; 添加slaves节点 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/slaves #添加一行内容如下 hadoop001 编辑yarn-site.xml文件相关配置 [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/yarn-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 编辑yarn-site.xml文件相关配置 [hadoop@hadoop001 ~]$ cp ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml.template ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml [hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml #添加如下配置 &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 5.5格式化namenode [hadoop@hadoop001 ~]$ hdfs namenode -format #若出现 has been successfully formatted 则表示格式化成功 19/04/07 17:42:31 INFO namenode.FSImage: Allocated new BlockPoolId: BP-565897555-192.168.175.135-1554630151139 19/04/07 17:42:31 INFO common.Storage: Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted. 19/04/07 17:42:32 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0 19/04/07 17:42:32 INFO util.ExitUtil: Exiting with status 0 19/04/07 17:42:32 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at hadoop001/192.168.175.135 ************************************************************/ 5.6启动hadoop [hadoop@hadoop001 ~]$ start-all.sh This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh Starting namenodes on [hadoop001] hadoop001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop001.out hadoop001: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop001.out Starting secondary namenodes [hadoop001] hadoop001: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop001.out starting yarn daemons starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop001.out hadoop001: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop001.out #查询出五个守护进程 [hadoop@hadoop001 ~]$ jps 56598 SecondaryNameNode 56933 Jps 56896 NodeManager 56801 ResourceManager 56470 DataNode 56376 NameNode ###6.验证hadoop 6.1 hdfs验证 [hadoop@hadoop001 ~]$ hdfs dfs -put ~/app/hadoop-2.6.0-cdh5.7.0/README.txt hdfs://hadoop001:9000/ [hadoop@hadoop001 ~]$ hdfs dfs -ls / Found 1 items -rw-r--r-- 1 hadoop supergroup 1366 2019-04-07 17:52 /README.txt 6.2yarn验证 #执行MR用列代码 [hadoop@hadoop001 ~]$ hadoop jar /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 4 10 6.2检测压缩格式 #true表示支持的意思 [hadoop@hadoop001 ~]$ hadoop checknative 19/04/07 17:50:08 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native 19/04/07 17:50:08 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library Native library checking: hadoop: true /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native/libhadoop.so.1.0.0 zlib: true /lib64/libz.so.1 snappy: true /usr/lib64/libsnappy.so.1 lz4: true revision:99 bzip2: true /lib64/libbz2.so.1 openssl: true /usr/lib64/libcrypto.so 扩展1：protobuf是什么？ protobuf它是一种轻便高效的数据格式，类似于Json，平台无关、语言无关、可扩展，可用于通讯协议和数据存储等领域。 优点： 平台无关，语言无关，可扩展； 提供了友好的动态库，使用简单； 解析速度快，比对应的XML快约20-100倍； 序列化数据非常简洁、紧凑，与XML相比，其序列化之后的数据量约为1/3到1/10。 扩展2：mvn命令也是可以进行debug的，mvn -X 命令","@type":"BlogPosting","url":"/2019/04/07/728326.html","headline":"hadoop之hadoop-2.6.0-cdh5.7.0源码编译支持压缩以及伪分布式部署","dateModified":"2019-04-07T00:00:00+08:00","datePublished":"2019-04-07T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/04/07/728326.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>hadoop之hadoop-2.6.0-cdh5.7.0源码编译支持压缩以及伪分布式部署</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-tomorrow-night-eighties"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p><strong>摘要：本文详细记载hadoop-2.6.0-cdh5.7.0源码编译支持压缩以及伪分布式部署的详细步骤，可用于学习以及生产参考</strong></p> 
  <h3><a id="1_2"></a>1.需求与设计</h3> 
  <h4><a id="11_3"></a>1.1需求</h4> 
  <p>直接使用的hadoop-2.6.0-cdh5.7.0.tar.gz包部署的hadoop集群不支持文件压缩，生产上是不可接受的，故需要将hadoop源码下载重新编译支持压缩</p> 
  <h4><a id="11_5"></a>1.1概要设计</h4> 
  <p>下载hadoop源码，使用maven编译，使其支持压缩。并成功进行伪分布式集群部署验证压缩功能。</p> 
  <h3><a id="2_8"></a>2.环境需求以及部署规划</h3> 
  <h4><a id="21__9"></a>2.1 硬件环境</h4> 
  <p>一台centos6.X虚拟机</p> 
  <h4><a id="22__11"></a>2.2 软件环境：</h4> 
  <table> 
   <thead> 
    <tr> 
     <th align="left">组件名称</th> 
     <th align="left">组件版本</th> 
     <th align="left">百度网盘链接</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td align="left">vm</td> 
     <td align="left">vm10</td> 
     <td align="left">链接：<a href="https://pan.baidu.com/s/1N5i8p8htXz9H_v__YNV1lA" rel="nofollow">https://pan.baidu.com/s/1N5i8p8htXz9H_v__YNV1lA</a> 提取码：yasn</td> 
    </tr> 
    <tr> 
     <td align="left">centos</td> 
     <td align="left">centos6.7</td> 
     <td align="left">链接：<a href="https://pan.baidu.com/s/1Z_6AcQ_WnvKz1ga_VCSI9Q" rel="nofollow">https://pan.baidu.com/s/1Z_6AcQ_WnvKz1ga_VCSI9Q</a> 提取码：a24x</td> 
    </tr> 
    <tr> 
     <td align="left">Hadoop</td> 
     <td align="left">Hadoop-2.6.0-cdh5.7.0-src.tar.gz</td> 
     <td align="left">链接：<a href="https://pan.baidu.com/s/1uRMGIhLSL9QHT-Ee4F16jw" rel="nofollow">https://pan.baidu.com/s/1uRMGIhLSL9QHT-Ee4F16jw</a> 提取码：jb1d</td> 
    </tr> 
    <tr> 
     <td align="left">jdk</td> 
     <td align="left">jdk-7u80-linux-x64.tar.gz</td> 
     <td align="left">链接：<a href="https://pan.baidu.com/s/1xSCQ8rjABVI-zDFQS5nCPA" rel="nofollow">https://pan.baidu.com/s/1xSCQ8rjABVI-zDFQS5nCPA</a> 提取码：lfze</td> 
    </tr> 
    <tr> 
     <td align="left">maven</td> 
     <td align="left">apache-maven-3.3.9-bin.tar.gz</td> 
     <td align="left">链接：<a href="https://pan.baidu.com/s/1ddkdkLW7r7ahFZmgACGkVw" rel="nofollow">https://pan.baidu.com/s/1ddkdkLW7r7ahFZmgACGkVw</a> 提取码：fdfz</td> 
    </tr> 
    <tr> 
     <td align="left">protobuf</td> 
     <td align="left">protobuf-2.5.0.tar.gz</td> 
     <td align="left">链接：<a href="https://pan.baidu.com/s/1RSNZGd_ThwknMB3vDkEfhQ" rel="nofollow">https://pan.baidu.com/s/1RSNZGd_ThwknMB3vDkEfhQ</a> 提取码：hvc2</td> 
    </tr> 
   </tbody> 
  </table>
  <p><font color="#FF0000"><strong>注意:</strong></font><br> 1、编译的JDK版本必须是1.7，1.8的JDK会导致编译失败,采坑</p> 
  <h3><a id="3centos_24"></a>3.安装centos</h3> 
  <p>请参考<a href="https://blog.csdn.net/qq_32641659/article/details/87429652" rel="nofollow">VM虚拟机安装Centos6.X以及主机和网络配置</a></p> 
  <h3><a id="4hadoop_27"></a>4.编译hadoop</h3> 
  <h4><a id="41_28"></a>4.1安装必要的依赖库</h4> 
  <pre><code>[root@hadoop001 ~]# yum install -y svn ncurses-devel
[root@hadoop001 ~]# yum install -y gcc gcc-c++ make cmake
[root@hadoop001 ~]# yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool
[root@hadoop001 ~]# yum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop autoconf automake cmake 
</code></pre> 
  <h4><a id="42_36"></a>4.2添加用户以及上传软件</h4> 
  <pre><code>[root@hadoop001 ~]# yum install -y lrzsz
[root@hadoop001 ~]# useradd hadoop
[root@hadoop001 ~]# su - hadoop
[hadoop@hadoop001 ~]$ mkdir app soft source lib data maven_repo shell mysql
[hadoop@hadoop001 ~]$ cd soft/
[hadoop@hadoop001 soft]$ rz

[hadoop@hadoop001 soft]$ ll
total 202192
-rw-r--r--. 1 hadoop hadoop   8491533 Apr  7 11:25 apache-maven-3.3.9-bin.tar.gz
-rw-r--r--. 1 hadoop hadoop  42610549 Apr  6 16:55 hadoop-2.6.0-cdh5.7.0-src.tar.gz
-rw-r--r--. 1 hadoop hadoop 153530841 Apr  7 11:12 jdk-7u80-linux-x64.tar.gz
-rw-r--r--. 1 hadoop hadoop   2401901 Apr  7 11:31 protobuf-2.5.0.tar.gz
</code></pre> 
  <h4><a id="43JDK_52"></a>4.3安装JDK</h4> 
  <ul> 
   <li>解压安装包，安装目录必须是/usr/java,安装后记得修改拥有者为root</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 soft]$ exit
[root@hadoop001 ~]# mkdir /usr/java
[root@hadoop001 ~]# tar -zxvf /home/hadoop/soft/jdk-7u80-linux-x64.tar.gz -C /usr/java
[root@hadoop001 ~]# cd /usr/java/
[root@hadoop001 java]# chown -R  root:root jdk1.7.0_80
</code></pre> 
  <ul> 
   <li>添加环境变量</li> 
  </ul> 
  <pre><code>[root@hadoop001 jdk1.7.0_80]# vim /etc/profile 
#添加如下两行环境变量
export JAVA_HOME=/usr/java/jdk1.7.0_80
export PATH=$JAVA_HOME/bin:$PATH
[root@hadoop001 jdk1.7.0_80]# source /etc/profile
#测试java是否安装成功
[root@hadoop001 jdk1.7.0_80]# java -version
java version "1.7.0_80"
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)
</code></pre> 
  <h4><a id="44maven_75"></a>4.4安装maven</h4> 
  <ul> 
   <li>解压</li> 
  </ul> 
  <pre><code>[root@hadoop001 ~]# su - hadoop
[hadoop@hadoop001 ~]$ tar -zxvf ~/soft/apache-maven-3.3.9-bin.tar.gz -C ~/app/
</code></pre> 
  <ul> 
   <li>添加环境变量</li> 
  </ul> 
  <pre><code>#修改haoop用户的环境变量
[hadoop@hadoop001 ~]$ vim ~/.bash_profile
#添加或修改如下内容，注意MAVEN_OPTS设置了maven运行的内存，防止内存太小导致编译失败
export MAVEN_HOME=/home/hadoop/app/apache-maven-3.3.9
export MAVEN_OPTS="-Xms1024m -Xmx1024m"
export PATH=$MAVEN_HOME/bin:$PATH
[hadoop@hadoop001 ~]$ source ~/.bash_profile
[hadoop@hadoop001 ~]$ which mvn
~/app/apache-maven-3.3.9/bin/mvn
</code></pre> 
  <ul> 
   <li>配置maven</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 protobuf-2.5.0]$ vim ~/app/apache-maven-3.3.9/conf/settings.xml
#配置maven的本地仓库位置
&lt;localRepository&gt;/home/hadoop/maven_repo/repo&lt;/localRepository&gt;
#添加阿里云中央仓库地址，注意一定要写在&lt;mirrors&gt;&lt;/mirrors&gt;之间
&lt;mirror&gt;
     &lt;id&gt;nexus-aliyun&lt;/id&gt;
     &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;
     &lt;name&gt;Nexus aliyun&lt;/name&gt;
     &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;
&lt;/mirror&gt;
</code></pre> 
  <ul> 
   <li>（可选）添加jars到本地仓库，网络慢可能导致mvn第一次编译时下载需要超长的时间甚至编译失败</li> 
  </ul> 
  <pre><code>#jar包链接
链接：https://pan.baidu.com/s/1vq4iVFqqyJNkYzg90bVrfg 
提取码：vugv 
复制这段内容后打开百度网盘手机App，操作更方便哦
#下载后 rz上传解压，注意目录层次
[hadoop@hadoop001 maven_repo]$ rz
[hadoop@hadoop001 maven_repo]$ tar -zxvf repo.tar.gz 
</code></pre> 
  <h4><a id="45protobuf_119"></a>4.5安装protobuf</h4> 
  <ul> 
   <li>解压</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 ~]$ tar -zxvf ~/soft/protobuf-2.5.0.tar.gz -C ~/app/
</code></pre> 
  <ul> 
   <li>编译软件</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 protobuf-2.5.0]$ cd ~/app/protobuf-2.5.0/
#  --prefix= 是用来待会编译好的包放在为路径
[hadoop@hadoop001 protobuf-2.5.0]$ ./configure  --prefix=/home/hadoop/app/protobuf-2.5.0
#编译以及安装
[hadoop@hadoop001 protobuf-2.5.0]$ make
[hadoop@hadoop001 protobuf-2.5.0]$ make install
</code></pre> 
  <ul> 
   <li>添加环境变量</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 protobuf-2.5.0]$ vim ~/.bash_profile
#追加如下两行内容，未编译前是没有bin目录的
export PROTOBUF_HOME=/home/hadoop/app/protobuf-2.5.0
export PATH=$PROTOBUF_HOME/bin:$PATH
[hadoop@hadoop001 protobuf-2.5.0]$ source ~/.bash_profile 
#测试是否生效，若出现libprotoc 2.5.0则为生效
[hadoop@hadoop001 protobuf-2.5.0]$ protoc --version
libprotoc 2.5.0
</code></pre> 
  <h4><a id="46hadoop_145"></a>4.6编译hadoop</h4> 
  <ul> 
   <li>解压</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 protobuf-2.5.0]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0-src.tar.gz -C ~/source/
</code></pre> 
  <ul> 
   <li><font color="#FF0000">编译hadoop使其支持压缩</font>：mvn clean package -Pdist,native -DskipTests -Dtar</li> 
  </ul> 
  <pre><code>#进入hadoop的源码目录
[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cd ~/source/hadoop-2.6.0-cdh5.7.0/
#进行编译，第一次编译会下载很多依赖的jar包，快慢由网速决定，需耐心等待，本人亲测耗时
[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ mvn clean package -Pdist,native -DskipTests -Dtar
</code></pre> 
  <ul> 
   <li>若报异常，主要信息如下（无异常跳过）：</li> 
  </ul> 
  <pre><code>[FATAL] Non-resolvable parent POM for org.apache.hadoop:hadoop-main:2.6.0-cdh5.7.0: Could not transfer artifact com.cloudera.cdh:cdh-root:pom:5.7.0 from/to cdh.repo (https://repository.cloudera.com/artifactory/cloudera-repos): Remote host closed connectio
#分析：是https://repository.cloudera.com/artifactory/cloudera-repos/com/cloudera/cdh/cdh-root/5.7.0/cdh-root-5.7.0.pom文件下载不了，但是虚拟机确实是ping通远程的仓库，很是费解为什么。
#解决方案：前往本地仓库到目标文件目录，然后 通过wget 文件，来成功获取该文件，重新执行编译命令，或者执行4.5的可选步骤，将需要的jar直接放到本地仓库
</code></pre> 
  <ul> 
   <li>查看编译后的包：hadoop-2.6.0-cdh5.7.0.tar.gz</li> 
  </ul> 
  <pre><code>#有 BUILD SUCCESS 信息则表示编译成功
[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 13.592 s]
[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 12.042 s]
[INFO] Apache Hadoop Tools ................................ SUCCESS [  0.094 s]
[INFO] Apache Hadoop Distribution ......................... SUCCESS [01:49 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 37:39 min
[INFO] Finished at: 2019-04-07T16:48:42+08:00
[INFO] Final Memory: 200M/989M
[INFO] ------------------------------------------------------------------------
[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ 
[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ 
[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ 
[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ 
[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ ll /home/hadoop/source/hadoop-2.6.0-cdh5.7.0/hadoop-dist/target/
total 564036
drwxrwxr-x. 2 hadoop hadoop      4096 Apr  7 16:46 antrun
drwxrwxr-x. 3 hadoop hadoop      4096 Apr  7 16:46 classes
-rw-rw-r--. 1 hadoop hadoop      1998 Apr  7 16:46 dist-layout-stitching.sh
-rw-rw-r--. 1 hadoop hadoop       690 Apr  7 16:47 dist-tar-stitching.sh
drwxrwxr-x. 9 hadoop hadoop      4096 Apr  7 16:47 hadoop-2.6.0-cdh5.7.0
-rw-rw-r--. 1 hadoop hadoop 191880143 Apr  7 16:47 hadoop-2.6.0-cdh5.7.0.tar.gz
-rw-rw-r--. 1 hadoop hadoop      7314 Apr  7 16:47 hadoop-dist-2.6.0-cdh5.7.0.jar
-rw-rw-r--. 1 hadoop hadoop 385618309 Apr  7 16:48 hadoop-dist-2.6.0-cdh5.7.0-javadoc.jar
-rw-rw-r--. 1 hadoop hadoop      4855 Apr  7 16:47 hadoop-dist-2.6.0-cdh5.7.0-sources.jar
-rw-rw-r--. 1 hadoop hadoop      4855 Apr  7 16:47 hadoop-dist-2.6.0-cdh5.7.0-test-sources.jar
drwxrwxr-x. 2 hadoop hadoop      4096 Apr  7 16:47 javadoc-bundle-options
drwxrwxr-x. 2 hadoop hadoop      4096 Apr  7 16:47 maven-archiver
drwxrwxr-x. 3 hadoop hadoop      4096 Apr  7 16:46 maven-shared-archive-resources
drwxrwxr-x. 3 hadoop hadoop      4096 Apr  7 16:46 test-classes
drwxrwxr-x. 2 hadoop hadoop      4096 Apr  7 16:46 test-dir
</code></pre> 
  <h3><a id="5_200"></a>5.伪分布式部署</h3> 
  <h4><a id="51_201"></a>5.1解压安装包</h4> 
  <pre><code>[hadoop@hadoop001 hadoop-2.6.0-cdh5.7.0]$ cp /home/hadoop/source/hadoop-2.6.0-cdh5.7.0/hadoop-dist/target/hadoop-2.6.0-cdh5.7.0.tar.gz /home/hadoop/soft/
[hadoop@hadoop001 ~]$ cd ~
[hadoop@hadoop001 ~]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app/
</code></pre> 
  <h4><a id="52_207"></a>5.2配置环境变量</h4> 
  <pre><code>[hadoop@hadoop001 ~]$ vim ~/.bash_profile 
export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
[hadoop@hadoop001 ~]$ source ~/.bash_profile 
[hadoop@hadoop001 ~]$ which hadoop
~/app/hadoop-2.6.0-cdh5.7.0/bin/hadoop
</code></pre> 
  <h4><a id="53ssh_216"></a>5.3配置ssh</h4> 
  <pre><code>[hadoop@hadoop001 ~]$ rm -rf ~/.ssh
[hadoop@hadoop001 ~]$ ssh-keygen 
[hadoop@hadoop001 ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
[hadoop@hadoop001 ~]$ chmod 600 ~/.ssh/authorized_keys
#测试ssh是否成功，用户第一ssh会提示输入是否连接，yes。成功显示时间
[hadoop@hadoop001 ~]$ ssh hadoop001 date 

</code></pre> 
  <h4><a id="54_226"></a>5.4修改配置文件</h4> 
  <ul> 
   <li>编辑hadoop-env.sh文件相关配置</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/hadoop-env.sh 
#将JDK的安装目录修改为绝对路径
export JAVA_HOME=/usr/java/jdk1.7.0_80
</code></pre> 
  <ul> 
   <li>编辑core-site.xml文件相关配置</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/core-site.xml 
#添加如下配置
&lt;property&gt;
  &lt;name&gt;fs.defaultFS&lt;/name&gt;
  &lt;value&gt;hdfs://hadoop001:9000&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <ul> 
   <li>编辑hdfs-site.xml文件相关配置</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/hdfs-site.xml 
#添加如下配置
&lt;property&gt;
  &lt;name&gt;dfs.replication&lt;/name&gt;
  &lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
  &lt;value&gt;hadoop001:50090&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt;
    &lt;value&gt;hadoop001:50091&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <ul> 
   <li>添加slaves节点</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/slaves
#添加一行内容如下
hadoop001
</code></pre> 
  <ul> 
   <li>编辑yarn-site.xml文件相关配置</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/yarn-site.xml 
#添加如下配置
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <ul> 
   <li>编辑yarn-site.xml文件相关配置</li> 
  </ul> 
  <pre><code>[hadoop@hadoop001 ~]$ cp ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml.template  ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml
[hadoop@hadoop001 ~]$ vim ~/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/mapred-site.xml
#添加如下配置
&lt;property&gt;
    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre> 
  <h4><a id="55namenode_288"></a>5.5格式化namenode</h4> 
  <pre><code>[hadoop@hadoop001 ~]$ hdfs namenode -format

#若出现 has been successfully formatted 则表示格式化成功
19/04/07 17:42:31 INFO namenode.FSImage: Allocated new BlockPoolId: BP-565897555-192.168.175.135-1554630151139
19/04/07 17:42:31 INFO common.Storage: Storage directory /tmp/hadoop-hadoop/dfs/name has been successfully formatted.
19/04/07 17:42:32 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0
19/04/07 17:42:32 INFO util.ExitUtil: Exiting with status 0
19/04/07 17:42:32 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at hadoop001/192.168.175.135
************************************************************/

</code></pre> 
  <h4><a id="56hadoop_304"></a>5.6启动hadoop</h4> 
  <pre><code>[hadoop@hadoop001 ~]$ start-all.sh 
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [hadoop001]
hadoop001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop001.out
hadoop001: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop001.out
Starting secondary namenodes [hadoop001]
hadoop001: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop001.out
starting yarn daemons
starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop001.out
hadoop001: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop001.out

#查询出五个守护进程
[hadoop@hadoop001 ~]$ jps
56598 SecondaryNameNode
56933 Jps
56896 NodeManager
56801 ResourceManager
56470 DataNode
56376 NameNode
</code></pre> 
  <p>###6.验证hadoop</p> 
  <h4><a id="61_hdfs_327"></a>6.1 hdfs验证</h4> 
  <pre><code>[hadoop@hadoop001 ~]$ hdfs dfs -put ~/app/hadoop-2.6.0-cdh5.7.0/README.txt hdfs://hadoop001:9000/
[hadoop@hadoop001 ~]$ hdfs dfs -ls /
Found 1 items
-rw-r--r--   1 hadoop supergroup       1366 2019-04-07 17:52 /README.txt

</code></pre> 
  <h4><a id="62yarn_335"></a>6.2yarn验证</h4> 
  <pre><code>#执行MR用列代码
[hadoop@hadoop001 ~]$ hadoop jar  /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar  pi 4 10

</code></pre> 
  <h4><a id="62_341"></a>6.2检测压缩格式</h4> 
  <pre><code>#true表示支持的意思
[hadoop@hadoop001 ~]$ hadoop checknative
19/04/07 17:50:08 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native
19/04/07 17:50:08 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library
Native library checking:
hadoop:  true /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/lib/native/libhadoop.so.1.0.0
zlib:    true /lib64/libz.so.1
snappy:  true /usr/lib64/libsnappy.so.1
lz4:     true revision:99
bzip2:   true /lib64/libbz2.so.1
openssl: true /usr/lib64/libcrypto.so

</code></pre> 
  <p><strong>扩展1</strong>：protobuf是什么？</p> 
  <blockquote> 
   <p>protobuf它是一种轻便高效的数据格式，类似于Json，平台无关、语言无关、可扩展，可用于通讯协议和数据存储等领域。<br> 优点：<br> 平台无关，语言无关，可扩展；<br> 提供了友好的动态库，使用简单；<br> 解析速度快，比对应的XML快约20-100倍；<br> 序列化数据非常简洁、紧凑，与XML相比，其序列化之后的数据量约为1/3到1/10。</p> 
  </blockquote> 
  <p><strong>扩展2</strong>：mvn命令也是可以进行debug的，mvn -X 命令</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
