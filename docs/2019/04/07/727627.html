<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>文本智能处理挑战赛-第二天 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="文本智能处理挑战赛-第二天" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="TF-IDF原理及使用 1.TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文本频率). 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章. TF(x) TF反映了一个词在当前文章中出现的频率，如果一个词在当前的文本中多次出现，那么它的TF值应该比较高，这个词出现的频率越高，越接近1. TF(词频) =&nbsp;该文章出现此词的次数/文章的总次数 IDF(x) IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如比如单词 “is”、“and”等。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; N 代表语料库中文本的总数，而N(x)代表语料库中包含词xx的文本总数， 上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。为了防止一些热词会有N(x)=N,导致IDF值为0， 接着TF-IDF也为0，影响整个算法的过程，所以常用的IDF我们需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; N 代表语料库中文本的总数，而N(x)代表语料库中包含词xx的文本总数。 TF−IDF(x)&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;(该词的词频（TF） 乘以 权重(IDF) 总结： TF-IDF算法的优点是简单快速，结果比较符合实际情况。 缺点是，单纯以&quot;词频&quot;衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。 而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。 （一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。） 2.应用 达观杯文本分类数据 from sklearn.feature_extraction.text import TfidfVectorizer vec = TfidfVectorizer() tfidfX_train = vec.fit_transform(X_train[&quot;word_seg&quot;]) # 也可以用CountVectorizer+TfidfTransformer的组合 print(tfidfX_train) &nbsp; &nbsp; (0, 555859) 0.13626336767206662 (0, 10960) 0.0079950202415356 (0, 60380) 0.04774102230403789 (0, 462401) 0.01628029763917735 (0, 129942) 0.010093582790523453 (0, 481177) 0.035661753236321043 (0, 500619) 0.12223405155311452 (0, 590042) 0.028672919391752878 (0, 519491) 0.08577796139666678 (0, 40910) 0.028673522485787323 (0, 222611) 0.02247056347865981 (0, 328449) 0.16106834459267144 (0, 626282) 0.11479395755145853 (0, 641245) 0.11028263921075573 (0, 524555) 0.10328630075775601 (0, 397724) 0.09830988691442702 (0, 563778) 0.13810109074225443 (0, 231370) 0.3315637062041968 (0, 19379) 0.15115207357802815 (0, 36284) 0.16954570349997508 (0, 385755) 0.13129785407001648 (0, 526982) 0.006581830547107576 (0, 536252) 0.028896303792333967 (0, 55857) 0.009656707143037036 (0, 214848) 0.052272118006157685 : : (61365, 114238) 0.04736859756036005 (61365, 123659) 0.050969811781371215 (61365, 568756) 0.04246262332849717 (61365, 287266) 0.04058915768431871 (61365, 548971) 0.05790141639728023 (61365, 584876) 0.045807817071433395 (61365, 574990) 0.050969811781371215 (61365, 65017) 0.05249321078115605 (61365, 554972) 0.10454950569571177 (61365, 427635) 0.05453930588513383 (61365, 423523) 0.15203464207379222 (61365, 80633) 0.057145149801019895 (61365, 13588) 0.06344151009356525 (61365, 498617) 0.06036211290321397 (61365, 391043) 0.06380192229996137 (61365, 154013) 0.05220341723482519 (61365, 162657) 0.053111323147109056 (61365, 168619) 0.06344151009356525 (61365, 637784) 0.05992281376921672 (61365, 455567) 0.05596032272833658 (61365, 169421) 0.05729003117165101 (61365, 591423) 0.06987982465920654 (61365, 132000) 0.13001885541208508 (61365, 592358) 0.15924123713106914 (61365, 364936) 0.07962061856553457 参考博客：https://blog.csdn.net/devcy/article/details/89071572" />
<meta property="og:description" content="TF-IDF原理及使用 1.TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文本频率). 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章. TF(x) TF反映了一个词在当前文章中出现的频率，如果一个词在当前的文本中多次出现，那么它的TF值应该比较高，这个词出现的频率越高，越接近1. TF(词频) =&nbsp;该文章出现此词的次数/文章的总次数 IDF(x) IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如比如单词 “is”、“and”等。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; N 代表语料库中文本的总数，而N(x)代表语料库中包含词xx的文本总数， 上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。为了防止一些热词会有N(x)=N,导致IDF值为0， 接着TF-IDF也为0，影响整个算法的过程，所以常用的IDF我们需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; N 代表语料库中文本的总数，而N(x)代表语料库中包含词xx的文本总数。 TF−IDF(x)&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;(该词的词频（TF） 乘以 权重(IDF) 总结： TF-IDF算法的优点是简单快速，结果比较符合实际情况。 缺点是，单纯以&quot;词频&quot;衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。 而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。 （一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。） 2.应用 达观杯文本分类数据 from sklearn.feature_extraction.text import TfidfVectorizer vec = TfidfVectorizer() tfidfX_train = vec.fit_transform(X_train[&quot;word_seg&quot;]) # 也可以用CountVectorizer+TfidfTransformer的组合 print(tfidfX_train) &nbsp; &nbsp; (0, 555859) 0.13626336767206662 (0, 10960) 0.0079950202415356 (0, 60380) 0.04774102230403789 (0, 462401) 0.01628029763917735 (0, 129942) 0.010093582790523453 (0, 481177) 0.035661753236321043 (0, 500619) 0.12223405155311452 (0, 590042) 0.028672919391752878 (0, 519491) 0.08577796139666678 (0, 40910) 0.028673522485787323 (0, 222611) 0.02247056347865981 (0, 328449) 0.16106834459267144 (0, 626282) 0.11479395755145853 (0, 641245) 0.11028263921075573 (0, 524555) 0.10328630075775601 (0, 397724) 0.09830988691442702 (0, 563778) 0.13810109074225443 (0, 231370) 0.3315637062041968 (0, 19379) 0.15115207357802815 (0, 36284) 0.16954570349997508 (0, 385755) 0.13129785407001648 (0, 526982) 0.006581830547107576 (0, 536252) 0.028896303792333967 (0, 55857) 0.009656707143037036 (0, 214848) 0.052272118006157685 : : (61365, 114238) 0.04736859756036005 (61365, 123659) 0.050969811781371215 (61365, 568756) 0.04246262332849717 (61365, 287266) 0.04058915768431871 (61365, 548971) 0.05790141639728023 (61365, 584876) 0.045807817071433395 (61365, 574990) 0.050969811781371215 (61365, 65017) 0.05249321078115605 (61365, 554972) 0.10454950569571177 (61365, 427635) 0.05453930588513383 (61365, 423523) 0.15203464207379222 (61365, 80633) 0.057145149801019895 (61365, 13588) 0.06344151009356525 (61365, 498617) 0.06036211290321397 (61365, 391043) 0.06380192229996137 (61365, 154013) 0.05220341723482519 (61365, 162657) 0.053111323147109056 (61365, 168619) 0.06344151009356525 (61365, 637784) 0.05992281376921672 (61365, 455567) 0.05596032272833658 (61365, 169421) 0.05729003117165101 (61365, 591423) 0.06987982465920654 (61365, 132000) 0.13001885541208508 (61365, 592358) 0.15924123713106914 (61365, 364936) 0.07962061856553457 参考博客：https://blog.csdn.net/devcy/article/details/89071572" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-07T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"TF-IDF原理及使用 1.TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文本频率). 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章. TF(x) TF反映了一个词在当前文章中出现的频率，如果一个词在当前的文本中多次出现，那么它的TF值应该比较高，这个词出现的频率越高，越接近1. TF(词频) =&nbsp;该文章出现此词的次数/文章的总次数 IDF(x) IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如比如单词 “is”、“and”等。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; N 代表语料库中文本的总数，而N(x)代表语料库中包含词xx的文本总数， 上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。为了防止一些热词会有N(x)=N,导致IDF值为0， 接着TF-IDF也为0，影响整个算法的过程，所以常用的IDF我们需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; N 代表语料库中文本的总数，而N(x)代表语料库中包含词xx的文本总数。 TF−IDF(x)&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;(该词的词频（TF） 乘以 权重(IDF) 总结： TF-IDF算法的优点是简单快速，结果比较符合实际情况。 缺点是，单纯以&quot;词频&quot;衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。 而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。 （一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。） 2.应用 达观杯文本分类数据 from sklearn.feature_extraction.text import TfidfVectorizer vec = TfidfVectorizer() tfidfX_train = vec.fit_transform(X_train[&quot;word_seg&quot;]) # 也可以用CountVectorizer+TfidfTransformer的组合 print(tfidfX_train) &nbsp; &nbsp; (0, 555859) 0.13626336767206662 (0, 10960) 0.0079950202415356 (0, 60380) 0.04774102230403789 (0, 462401) 0.01628029763917735 (0, 129942) 0.010093582790523453 (0, 481177) 0.035661753236321043 (0, 500619) 0.12223405155311452 (0, 590042) 0.028672919391752878 (0, 519491) 0.08577796139666678 (0, 40910) 0.028673522485787323 (0, 222611) 0.02247056347865981 (0, 328449) 0.16106834459267144 (0, 626282) 0.11479395755145853 (0, 641245) 0.11028263921075573 (0, 524555) 0.10328630075775601 (0, 397724) 0.09830988691442702 (0, 563778) 0.13810109074225443 (0, 231370) 0.3315637062041968 (0, 19379) 0.15115207357802815 (0, 36284) 0.16954570349997508 (0, 385755) 0.13129785407001648 (0, 526982) 0.006581830547107576 (0, 536252) 0.028896303792333967 (0, 55857) 0.009656707143037036 (0, 214848) 0.052272118006157685 : : (61365, 114238) 0.04736859756036005 (61365, 123659) 0.050969811781371215 (61365, 568756) 0.04246262332849717 (61365, 287266) 0.04058915768431871 (61365, 548971) 0.05790141639728023 (61365, 584876) 0.045807817071433395 (61365, 574990) 0.050969811781371215 (61365, 65017) 0.05249321078115605 (61365, 554972) 0.10454950569571177 (61365, 427635) 0.05453930588513383 (61365, 423523) 0.15203464207379222 (61365, 80633) 0.057145149801019895 (61365, 13588) 0.06344151009356525 (61365, 498617) 0.06036211290321397 (61365, 391043) 0.06380192229996137 (61365, 154013) 0.05220341723482519 (61365, 162657) 0.053111323147109056 (61365, 168619) 0.06344151009356525 (61365, 637784) 0.05992281376921672 (61365, 455567) 0.05596032272833658 (61365, 169421) 0.05729003117165101 (61365, 591423) 0.06987982465920654 (61365, 132000) 0.13001885541208508 (61365, 592358) 0.15924123713106914 (61365, 364936) 0.07962061856553457 参考博客：https://blog.csdn.net/devcy/article/details/89071572","@type":"BlogPosting","url":"/2019/04/07/727627.html","headline":"文本智能处理挑战赛-第二天","dateModified":"2019-04-07T00:00:00+08:00","datePublished":"2019-04-07T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/04/07/727627.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>文本智能处理挑战赛-第二天</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h3>TF-IDF原理及使用</h3> 
  <h2>1.TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文本频率).</h2> 
  <p>一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.</p> 
  <p>TF<img alt="" class="mathcode" src="https://private.codecogs.com/gif.latex?">(x)</p> 
  <blockquote> 
   <p>TF反映了一个词在当前文章中出现的频率，如果一个词在当前的文本中多次出现，那么它的TF值应该比较高，这个词出现的频率越高，越接近1.</p> 
   <p>TF(词频) =&nbsp;该文章出现此词的次数/文章的总次数</p> 
  </blockquote> 
  <p>IDF(x)</p> 
  <blockquote> 
   <p>IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如比如单词 “is”、“and”等。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。</p> 
   <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<img alt="IDF(x) = log(\frac{N}{N(x)})" class="mathcode" src="https://private.codecogs.com/gif.latex?IDF%28x%29%20%3D%20log%28%5Cfrac%7BN%7D%7BN%28x%29%7D%29"></p> 
   <p>N 代表语料库中文本的总数，而N(x)代表语料库中包含词xx的文本总数，</p> 
   <p>上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。为了防止一些热词会有N(x)=N,导致IDF值为0， 接着TF-IDF也为0，影响整个算法的过程，所以常用的IDF我们需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为：</p> 
   <p>&nbsp;</p> 
   <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img alt="IDF(x) = log(\frac{N+1}{N(x)+1}+1)" class="mathcode" src="https://private.codecogs.com/gif.latex?IDF%28x%29%20%3D%20log%28%5Cfrac%7BN&amp;plus;1%7D%7BN%28x%29&amp;plus;1%7D&amp;plus;1%29"></p> 
   <p>N 代表语料库中文本的总数，而N(x)代表语料库中包含词xx的文本总数。</p> 
  </blockquote> 
  <p>TF−IDF(x)&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</p> 
  <blockquote> 
   <p><img alt="TF - IDF(x) = TF(x)*IDF(x)" class="mathcode" src="https://private.codecogs.com/gif.latex?TF%20-%20IDF%28x%29%20%3D%20TF%28x%29*IDF%28x%29">&nbsp; &nbsp;(该词的词频（TF） 乘以 权重(IDF)</p> 
  </blockquote> 
  <p>总结：</p> 
  <blockquote> 
   <p>TF-IDF算法的优点是简单快速，结果比较符合实际情况。</p> 
   <p>缺点是，单纯以"词频"衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。</p> 
   <p>而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。</p> 
   <p>（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。）</p> 
  </blockquote> 
  <h2>2.应用</h2> 
  <p>达观杯文本分类数据</p> 
  <pre class="has">
<code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer()
tfidfX_train = vec.fit_transform(X_train["word_seg"]) # 也可以用CountVectorizer+TfidfTransformer的组合
print(tfidfX_train)
</code></pre> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <pre class="has">
<code class="language-python"> (0, 555859)	0.13626336767206662
  (0, 10960)	0.0079950202415356
  (0, 60380)	0.04774102230403789
  (0, 462401)	0.01628029763917735
  (0, 129942)	0.010093582790523453
  (0, 481177)	0.035661753236321043
  (0, 500619)	0.12223405155311452
  (0, 590042)	0.028672919391752878
  (0, 519491)	0.08577796139666678
  (0, 40910)	0.028673522485787323
  (0, 222611)	0.02247056347865981
  (0, 328449)	0.16106834459267144
  (0, 626282)	0.11479395755145853
  (0, 641245)	0.11028263921075573
  (0, 524555)	0.10328630075775601
  (0, 397724)	0.09830988691442702
  (0, 563778)	0.13810109074225443
  (0, 231370)	0.3315637062041968
  (0, 19379)	0.15115207357802815
  (0, 36284)	0.16954570349997508
  (0, 385755)	0.13129785407001648
  (0, 526982)	0.006581830547107576
  (0, 536252)	0.028896303792333967
  (0, 55857)	0.009656707143037036
  (0, 214848)	0.052272118006157685
  :	:
  (61365, 114238)	0.04736859756036005
  (61365, 123659)	0.050969811781371215
  (61365, 568756)	0.04246262332849717
  (61365, 287266)	0.04058915768431871
  (61365, 548971)	0.05790141639728023
  (61365, 584876)	0.045807817071433395
  (61365, 574990)	0.050969811781371215
  (61365, 65017)	0.05249321078115605
  (61365, 554972)	0.10454950569571177
  (61365, 427635)	0.05453930588513383
  (61365, 423523)	0.15203464207379222
  (61365, 80633)	0.057145149801019895
  (61365, 13588)	0.06344151009356525
  (61365, 498617)	0.06036211290321397
  (61365, 391043)	0.06380192229996137
  (61365, 154013)	0.05220341723482519
  (61365, 162657)	0.053111323147109056
  (61365, 168619)	0.06344151009356525
  (61365, 637784)	0.05992281376921672
  (61365, 455567)	0.05596032272833658
  (61365, 169421)	0.05729003117165101
  (61365, 591423)	0.06987982465920654
  (61365, 132000)	0.13001885541208508
  (61365, 592358)	0.15924123713106914
  (61365, 364936)	0.07962061856553457</code></pre> 
  <p>参考博客：<a href="https://blog.csdn.net/devcy/article/details/89071572" rel="nofollow">https://blog.csdn.net/devcy/article/details/89071572</a></p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
