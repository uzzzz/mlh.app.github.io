<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>机器学习-GBDT算法梳理 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="机器学习-GBDT算法梳理" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="学习任务： 前向分步算法 负梯度拟合 损失函数 回归 二分类，多分类 正则化 优缺点 sklearn参数 应用场景 前言 提升(Boosting)方法是集成学习方法的一种，在分类问题中，它通过改变训练样本的权重，学习多个分类器，将弱分类器组装成一个强分类器，提高分类的性能。其最具代表性的是AdaBoost算法。 提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。提升树(Boosting Tree)是以决策树为基函数的提升方法。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。 提升树模型可以表示为决策树的加法模型： 其中T(x;Θm)表示决策树；Θm表示决策树的参数；M为树的个数。 不同问题的提升树学习算法，其主要区别在于损失函数不同。 平方损失函数常用于回归问题， 指数损失函数用于分类问题， 一般损失函数用一般于决策问题。 GBDT有很多简称，有GBT（Gradient Boosting Tree） GTB(Gradient Tree Boosting ) GBRT(Gradient Boosting Regression Tree) MART(Multiple Additive Regression Tree) 其实都是一种算法。 GBDT也是使用的前向分步算法和加法模型。 1.前向分步算法 加法模型（Additive Model）： 其中，b(x;γm)为基函数，βm为基函数的系数。 在给定训练数据及损失函数L(y,f(x))的条件下，学习加法模型f(x)称为经验风险极小化，即损失函数极小化的问题： 通常这是一个复杂的优化问题。前向分布算法（forward stagwise algorithm）求解这一优化问题的思路是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式 ： 那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数: 前向分步算法步骤见：李航. ch8.p144. 2.负梯度拟合 针对一个一般损失函数，Freidman提出了梯度提升的算法，利用损失函数的负梯度在当前模型的值 来拟合本轮损失的近似值，进而拟合一个回归树。 对于分类问题和回归问题，通过其损失函数的负梯度的拟合，就可以用 GBDT 来解决分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。 3.损失函数 对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种: 指数损失函数：则损失函数表达式为：L(y,f(x))=exp(−yfL(y,f(x))=exp(−yf(x)) 对数损失函数：分为二元分类和多元分类两种。 对于回归算法，常用损失函数有如下4种： 均方差 绝对损失 Huber损失 分位数损失 4.回归 输入：训练集样本 最大迭代次数T, 损失函数L。 输出：强学习器f(x) 过程见：https://blog.csdn.net/Kaiyuan_sjtu/article/details/79991452 5.分类（二分类、多分类） GBDT的弱学习器限定了只能使用CART回归树模型，在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x))=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 对于二元分类GBDT问题： 可选择损失函数为： 此时的负梯度误差为： 对于多元分类问题： 假设有K类，则此时对数似然函数为： 其中如果样本输出类别为k， 则yk = 1。第k类的概率为： 结合上述两式，可以算出第t轮的第i个样本对应类别 l 的负梯度误差为： 6.正则化 第一种是和Adaboost类似的正则化项，即步长(learning rate)。 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。 第三种是对于弱学习器即CART回归树进行正则化剪枝。与之前一篇决策树内容讲过的相同。 7.优缺点 GBDT主要的优点有： 可以灵活处理各种类型的数据，包括连续值和离散值。 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。 使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 GBDT的主要缺点有： 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。 由于GBDT算法的卓越性，使其成为机器学习研究必须掌握的算法之一，很多面试的问题也都会涉及这个方面，包括其原理、实现以及参数调优等。目前GBDT的算法比较好的库是xgboost，sklearn。 8.sklearn参数 from sklearn.ensemble import GradientBoostingClassifier/GradientBoostingRegressor 参数： https://blog.csdn.net/Kaiyuan_sjtu/article/details/80002855" />
<meta property="og:description" content="学习任务： 前向分步算法 负梯度拟合 损失函数 回归 二分类，多分类 正则化 优缺点 sklearn参数 应用场景 前言 提升(Boosting)方法是集成学习方法的一种，在分类问题中，它通过改变训练样本的权重，学习多个分类器，将弱分类器组装成一个强分类器，提高分类的性能。其最具代表性的是AdaBoost算法。 提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。提升树(Boosting Tree)是以决策树为基函数的提升方法。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。 提升树模型可以表示为决策树的加法模型： 其中T(x;Θm)表示决策树；Θm表示决策树的参数；M为树的个数。 不同问题的提升树学习算法，其主要区别在于损失函数不同。 平方损失函数常用于回归问题， 指数损失函数用于分类问题， 一般损失函数用一般于决策问题。 GBDT有很多简称，有GBT（Gradient Boosting Tree） GTB(Gradient Tree Boosting ) GBRT(Gradient Boosting Regression Tree) MART(Multiple Additive Regression Tree) 其实都是一种算法。 GBDT也是使用的前向分步算法和加法模型。 1.前向分步算法 加法模型（Additive Model）： 其中，b(x;γm)为基函数，βm为基函数的系数。 在给定训练数据及损失函数L(y,f(x))的条件下，学习加法模型f(x)称为经验风险极小化，即损失函数极小化的问题： 通常这是一个复杂的优化问题。前向分布算法（forward stagwise algorithm）求解这一优化问题的思路是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式 ： 那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数: 前向分步算法步骤见：李航. ch8.p144. 2.负梯度拟合 针对一个一般损失函数，Freidman提出了梯度提升的算法，利用损失函数的负梯度在当前模型的值 来拟合本轮损失的近似值，进而拟合一个回归树。 对于分类问题和回归问题，通过其损失函数的负梯度的拟合，就可以用 GBDT 来解决分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。 3.损失函数 对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种: 指数损失函数：则损失函数表达式为：L(y,f(x))=exp(−yfL(y,f(x))=exp(−yf(x)) 对数损失函数：分为二元分类和多元分类两种。 对于回归算法，常用损失函数有如下4种： 均方差 绝对损失 Huber损失 分位数损失 4.回归 输入：训练集样本 最大迭代次数T, 损失函数L。 输出：强学习器f(x) 过程见：https://blog.csdn.net/Kaiyuan_sjtu/article/details/79991452 5.分类（二分类、多分类） GBDT的弱学习器限定了只能使用CART回归树模型，在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x))=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 对于二元分类GBDT问题： 可选择损失函数为： 此时的负梯度误差为： 对于多元分类问题： 假设有K类，则此时对数似然函数为： 其中如果样本输出类别为k， 则yk = 1。第k类的概率为： 结合上述两式，可以算出第t轮的第i个样本对应类别 l 的负梯度误差为： 6.正则化 第一种是和Adaboost类似的正则化项，即步长(learning rate)。 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。 第三种是对于弱学习器即CART回归树进行正则化剪枝。与之前一篇决策树内容讲过的相同。 7.优缺点 GBDT主要的优点有： 可以灵活处理各种类型的数据，包括连续值和离散值。 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。 使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 GBDT的主要缺点有： 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。 由于GBDT算法的卓越性，使其成为机器学习研究必须掌握的算法之一，很多面试的问题也都会涉及这个方面，包括其原理、实现以及参数调优等。目前GBDT的算法比较好的库是xgboost，sklearn。 8.sklearn参数 from sklearn.ensemble import GradientBoostingClassifier/GradientBoostingRegressor 参数： https://blog.csdn.net/Kaiyuan_sjtu/article/details/80002855" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-07T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"学习任务： 前向分步算法 负梯度拟合 损失函数 回归 二分类，多分类 正则化 优缺点 sklearn参数 应用场景 前言 提升(Boosting)方法是集成学习方法的一种，在分类问题中，它通过改变训练样本的权重，学习多个分类器，将弱分类器组装成一个强分类器，提高分类的性能。其最具代表性的是AdaBoost算法。 提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。提升树(Boosting Tree)是以决策树为基函数的提升方法。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。 提升树模型可以表示为决策树的加法模型： 其中T(x;Θm)表示决策树；Θm表示决策树的参数；M为树的个数。 不同问题的提升树学习算法，其主要区别在于损失函数不同。 平方损失函数常用于回归问题， 指数损失函数用于分类问题， 一般损失函数用一般于决策问题。 GBDT有很多简称，有GBT（Gradient Boosting Tree） GTB(Gradient Tree Boosting ) GBRT(Gradient Boosting Regression Tree) MART(Multiple Additive Regression Tree) 其实都是一种算法。 GBDT也是使用的前向分步算法和加法模型。 1.前向分步算法 加法模型（Additive Model）： 其中，b(x;γm)为基函数，βm为基函数的系数。 在给定训练数据及损失函数L(y,f(x))的条件下，学习加法模型f(x)称为经验风险极小化，即损失函数极小化的问题： 通常这是一个复杂的优化问题。前向分布算法（forward stagwise algorithm）求解这一优化问题的思路是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式 ： 那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数: 前向分步算法步骤见：李航. ch8.p144. 2.负梯度拟合 针对一个一般损失函数，Freidman提出了梯度提升的算法，利用损失函数的负梯度在当前模型的值 来拟合本轮损失的近似值，进而拟合一个回归树。 对于分类问题和回归问题，通过其损失函数的负梯度的拟合，就可以用 GBDT 来解决分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。 3.损失函数 对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种: 指数损失函数：则损失函数表达式为：L(y,f(x))=exp(−yfL(y,f(x))=exp(−yf(x)) 对数损失函数：分为二元分类和多元分类两种。 对于回归算法，常用损失函数有如下4种： 均方差 绝对损失 Huber损失 分位数损失 4.回归 输入：训练集样本 最大迭代次数T, 损失函数L。 输出：强学习器f(x) 过程见：https://blog.csdn.net/Kaiyuan_sjtu/article/details/79991452 5.分类（二分类、多分类） GBDT的弱学习器限定了只能使用CART回归树模型，在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是ft−1(x), 损失函数是L(y,ft−1(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器ht(x)，让本轮的损失损失L(y,ft(x))=L(y,ft−1(x)+ht(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。 对于二元分类GBDT问题： 可选择损失函数为： 此时的负梯度误差为： 对于多元分类问题： 假设有K类，则此时对数似然函数为： 其中如果样本输出类别为k， 则yk = 1。第k类的概率为： 结合上述两式，可以算出第t轮的第i个样本对应类别 l 的负梯度误差为： 6.正则化 第一种是和Adaboost类似的正则化项，即步长(learning rate)。 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。 第三种是对于弱学习器即CART回归树进行正则化剪枝。与之前一篇决策树内容讲过的相同。 7.优缺点 GBDT主要的优点有： 可以灵活处理各种类型的数据，包括连续值和离散值。 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。 使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。 GBDT的主要缺点有： 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。 由于GBDT算法的卓越性，使其成为机器学习研究必须掌握的算法之一，很多面试的问题也都会涉及这个方面，包括其原理、实现以及参数调优等。目前GBDT的算法比较好的库是xgboost，sklearn。 8.sklearn参数 from sklearn.ensemble import GradientBoostingClassifier/GradientBoostingRegressor 参数： https://blog.csdn.net/Kaiyuan_sjtu/article/details/80002855","@type":"BlogPosting","url":"/2019/04/07/727186.html","headline":"机器学习-GBDT算法梳理","dateModified":"2019-04-07T00:00:00+08:00","datePublished":"2019-04-07T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/04/07/727186.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>机器学习-GBDT算法梳理</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p><strong>学习任务：</strong><br> 前向分步算法<br> 负梯度拟合<br> 损失函数<br> 回归<br> 二分类，多分类<br> 正则化<br> 优缺点<br> sklearn参数<br> 应用场景</p> 
  <p><strong>前言</strong><br> 提升(Boosting)方法是集成学习方法的一种，在分类问题中，它通过改变训练样本的权重，学习多个分类器，将弱分类器组装成一个强分类器，提高分类的性能。其最具代表性的是AdaBoost算法。</p> 
  <p>提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。提升树(Boosting Tree)是以决策树为基函数的提升方法。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。</p> 
  <p>提升树模型可以表示为决策树的加法模型：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407212505339.png" alt="在这里插入图片描述"><br> 其中T(x;Θm)表示决策树；Θm表示决策树的参数；M为树的个数。</p> 
  <p>不同问题的提升树学习算法，其主要区别在于损失函数不同。</p> 
  <ul> 
   <li>平方损失函数常用于回归问题，</li> 
   <li>指数损失函数用于分类问题，</li> 
   <li>一般损失函数用一般于决策问题。</li> 
  </ul> 
  <p>GBDT有很多简称，有GBT（Gradient Boosting Tree） GTB(Gradient Tree Boosting ) GBRT(Gradient Boosting Regression Tree) MART(Multiple Additive Regression Tree) 其实都是一种算法。</p> 
  <p><strong>GBDT也是使用的前向分步算法和加法模型</strong>。</p> 
  <h6><a id="1_30"></a>1.前向分步算法</h6> 
  <p><strong>加法模型</strong>（Additive Model）：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407213422815.png" alt="在这里插入图片描述"><br> 其中，b(x;γ<sub>m</sub>)为基函数，β<sub>m</sub>为基函数的系数。</p> 
  <p>在给定训练数据及损失函数L(y,f(x))的条件下，学习加法模型f(x)称为经验风险极小化，即损失函数极小化的问题：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407213522216.png" alt="在这里插入图片描述"><br> 通常这是一个复杂的优化问题。<strong>前向分布算法</strong>（forward stagwise algorithm）求解这一优化问题的思路是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式 ：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407213600515.png" alt="在这里插入图片描述"><br> 那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数:<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019040721361960.png" alt="在这里插入图片描述"><br> 前向分步算法步骤见：李航. ch8.p144.</p> 
  <h6><a id="2_43"></a>2.负梯度拟合</h6> 
  <p>针对一个一般损失函数，Freidman提出了梯度提升的算法，利用损失函数的负梯度在当前模型的值</p> 
  <p><img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407204144815.png" alt="在这里插入图片描述"><br> 来拟合本轮损失的近似值，进而拟合一个回归树。</p> 
  <p>对于分类问题和回归问题，通过其损失函数的负梯度的拟合，就可以用 GBDT 来解决分类回归问题。区别仅仅在于损失函数不同导致的负梯度不同而已。</p> 
  <h6><a id="3_51"></a>3.损失函数</h6> 
  <p>对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种:</p> 
  <ul> 
   <li>指数损失函数：则损失函数表达式为：L(y,f(x))=exp(−yfL(y,f(x))=exp(−yf(x))</li> 
   <li>对数损失函数：分为二元分类和多元分类两种。</li> 
  </ul> 
  <p>对于回归算法，常用损失函数有如下4种：</p> 
  <ul> 
   <li>均方差</li> 
   <li>绝对损失</li> 
   <li>Huber损失</li> 
   <li>分位数损失</li> 
  </ul> 
  <h6><a id="4_64"></a>4.回归</h6> 
  <p>输入：训练集样本<img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407214751420.png" alt="在这里插入图片描述"><br> 最大迭代次数T, 损失函数L。</p> 
  <p>输出：强学习器f(x)</p> 
  <p>过程见：<a href="https://blog.csdn.net/Kaiyuan_sjtu/article/details/79991452" rel="nofollow">https://blog.csdn.net/Kaiyuan_sjtu/article/details/79991452</a></p> 
  <h6><a id="5_72"></a>5.分类（二分类、多分类）</h6> 
  <p>GBDT的弱学习器限定了只能使用CART回归树模型，在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是f<sub>t−1</sub>(x), 损失函数是L(y,f<sub>t−1</sub>(x)), 我们本轮迭代的目标是找到一个CART回归树模型的弱学习器h<sub>t</sub>(x)，让本轮的损失损失L(y,f<sub>t</sub>(x))=L(y,f<sub>t−1</sub>(x)+h<sub>t</sub>(x))最小。也就是说，本轮迭代找到决策树，要让样本的损失尽量变得更小。</p> 
  <p>对于二元分类GBDT问题：</p> 
  <ul> 
   <li> <p>可选择损失函数为：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407214909449.png" alt="在这里插入图片描述"></p> </li> 
   <li> <p>此时的负梯度误差为：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407214920811.png" alt="在这里插入图片描述"><br> 对于多元分类问题：</p> </li> 
   <li> <p>假设有K类，则此时对数似然函数为：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407215003514.png" alt="在这里插入图片描述"></p> </li> 
   <li> <p>其中如果样本输出类别为k， 则yk = 1。第k类的概率为：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407215024100.png" alt="在这里插入图片描述"></p> </li> 
   <li> <p>结合上述两式，可以算出第t轮的第i个样本对应类别 l 的负梯度误差为：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407215038446.png" alt="在这里插入图片描述"></p> </li> 
  </ul> 
  <h6><a id="6_88"></a>6.正则化</h6> 
  <p>第一种是和Adaboost类似的正则化项，即步长(learning rate)。</p> 
  <p>第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。</p> 
  <p>第三种是对于弱学习器即CART回归树进行正则化剪枝。与之前一篇决策树内容讲过的相同。</p> 
  <h6><a id="7_95"></a>7.优缺点</h6> 
  <p>GBDT主要的优点有：</p> 
  <ol> 
   <li>可以灵活处理各种类型的数据，包括连续值和离散值。</li> 
   <li>在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。</li> 
   <li>使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</li> 
  </ol> 
  <p>GBDT的主要缺点有：</p> 
  <ul> 
   <li>由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</li> 
  </ul> 
  <p>由于GBDT算法的卓越性，使其成为机器学习研究必须掌握的算法之一，很多面试的问题也都会涉及这个方面，包括其原理、实现以及参数调优等。目前GBDT的算法比较好的库是xgboost，sklearn。</p> 
  <h6><a id="8sklearn_108"></a>8.sklearn参数</h6> 
  <p>from sklearn.ensemble import GradientBoostingClassifier/GradientBoostingRegressor</p> 
  <p>参数：<br> <a href="https://blog.csdn.net/Kaiyuan_sjtu/article/details/80002855" rel="nofollow">https://blog.csdn.net/Kaiyuan_sjtu/article/details/80002855</a></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
