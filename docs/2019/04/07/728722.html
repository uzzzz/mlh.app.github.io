<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>达观杯文本智能处理挑战赛练习(二) | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="达观杯文本智能处理挑战赛练习(二)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="文章目录 Task2 TF-IDF TF IDF TF-IDF 使用TF-IDF表示文本 Reference Task3 one-hot表示 分布式表示（distribution representation） word2vec 实践 Task4 LR模型预测 逻辑回归 用LR模型预测 SVM模型预测 SVM 使用svm进行预测 Reference Task5 LightGBM Reference Task6 模型融合 Task2 学习TF-IDF理论并实践，使用TF-IDF表示文本。 TF-IDF TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率). TF 词频，就是每个词在文本中出现的次数。 假设现在有一段文本 corpus=[“I come to China to travel”, “This is a car polupar in China”, &quot;I love tea and Apple &quot;, “The work is to write some papers in science”] 不考虑停用词，处理后得到的词向量如下： [[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0] [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0] [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0] [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]] 如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现 “come” , “China” 和 “Travel” 各出现1次，而“to“出现了两次。似乎看起来这个文本与 ”to“ 这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的 “China” 和 “Travel” 要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。 IDF IDF即逆文本频率。在上面的例子中可以看到到几乎所有文本都会出现的&quot;to&quot;其词频虽然高，但是重要性却应该比词频低的&quot;China&quot;和“Travel”要低。我们的IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。 概括来讲， IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高，说明词条具有很好的类别区分能力。 IDF的公式为： TF-IDF 将TF和IDF结合得到TF-IDF 使用TF-IDF表示文本 用TF-IDF表示达观杯文本比赛的数据 import pickle import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer data_path=&#39;E:/dataset/daguan/new_data/&#39; #读取数据 train_data=pd.read_csv(data_path+&quot;train_set.csv&quot;) test_data=pd.read_csv(data_path+&quot;test_set.csv&quot;) columns=[&#39;article&#39;,&#39;word_seg&#39;] X_train,X_val,y_train,y_val=train_test_split(train_data[columns],train_data[&#39;class&#39;],test_size=0.3, random_state=2019) vectorizer = TfidfVectorizer() vectorizer.fit(X_train[&#39;word_seg&#39;]) Xtrain=vectorizer.fit_transform(X_train[&#39;word_seg&#39;]) Xval=vectorizer.fit_transform(X_train[&#39;word_seg&#39;]) print(Xtrain,Xval) data=(Xtrain,Xval,y_train,y_val) #保存到本地 fp = open(&#39;./feat/data_tfidf.pickl&#39;, &#39;wb&#39;) pickle.dump(data, fp) fp.close() 使用tf-idf表示的数据： Reference https://www.cnblogs.com/pinard/p/6693230.html https://github.com/Heitao5200/DGB/blob/master/feature/feature_code/tfidf.py Task3 one-hot表示 传统的基于规则或基于统计的自然语义处理方法将单词看作一个原子符号 被称作one-hot representation。one-hot representation把每个词表示为一个长向量。这个向量的维度是词表大小，向量中只有一个维度的值为1，其余维度为0，这个维度就代表了当前的词。 例如： 苹果 [0，0，0，1，0，0，0，0，0，……] 不足：难以发现词之间的关系，以及难以捕捉句法（结构）和语义（意思）之间的关系 分布式表示（distribution representation） word embedding指的是将词转化成一种分布式表示，又称词向量。分布式表示将词表示成一个定长的连续的稠密向量。这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, …]。 分布式表示优点: (1)词之间存在相似关系： 是词之间存在“距离”概念，这对很多自然语言处理的任务非常有帮助。 (2)包含更多信息： 词向量能够包含更多信息，并且每一维都有特定的含义。在采用one-hot特征时，可以对特征向量进行删减，词向量则不能。 Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。 比如下图我们将词汇表里的词用&quot;Royalty&quot;,“Masculinity”, &quot;Femininity&quot;和&quot;Age&quot;4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。 我们将king这个词从一个可能非常稀疏的向量坐在的空间，映射到现在这个四维向量所在的空间，必须满足以下性质： （1）这个映射是单设）； （2）映射之后的向量不会丢失之前的那种向量所含的信息。 这个过程称为word embedding（词嵌入），即将高维词向量嵌入到一个低维空间。 word2vec word2vec模型其实就是简单化的神经网络。 输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵。 这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。　Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。 CBOW模型 Skip-Gram模型 实践 import pandas as pd import gensim import time import pickle import numpy as np import csv,sys vector_size = 100 maxInt = sys.maxsize decrement = True while decrement: # decrease the maxInt value by factor 10 # as long as the OverflowError occurs. decrement = False try: csv.field_size_limit(maxInt) except OverflowError: maxInt = int(maxInt/10) decrement = True #======================================================================================================================= # 0 辅助函数 #======================================================================================================================= def sentence2list(sentence): return sentence.strip().split() start_time = time.time() data_path = &#39;E:/MyPython/机器学习——达观杯/data_set/&#39; feature_path = &#39;E:/MyPython/机器学习——达观杯/feature/feature_file/&#39; proba_path = &#39;E:/MyPython/机器学习——达观杯/proba/proba_file/&#39; model_path = &#39;E:/MyPython/机器学习——达观杯/model/model_file/&#39; result_path =&quot;E:/MyPython/机器学习——达观杯/result/&quot; #======================================================================================================================= # 1 准备训练数据 #======================================================================================================================= print(&quot;准备数据................ &quot;) df_train = pd.read_csv(data_path +&#39;train_set1.csv&#39;,engine=&#39;python&#39;) df_test = pd.read_csv(data_path +&#39;test_set1.csv&#39;,engine=&#39;python&#39;) sentences_train = list(df_train.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences_test = list(df_test.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences = sentences_train + sentences_test print(&quot;准备数据完成! &quot;) #======================================================================================================================= # 2 训练 #======================================================================================================================= print(&quot;开始训练................ &quot;) model = gensim.models.Word2Vec(sentences=sentences, size=vector_size, window=5, min_count=5, workers=8, sg=0, iter=5) print(&quot;训练完成! &quot;) #======================================================================================================================= # 3 提取词汇表及vectors,并保存 #======================================================================================================================= print(&quot; 保存训练结果........... &quot;) wv = model.wv vocab_list = wv.index2word word_idx_dict = {} for idx, word in enumerate(vocab_list): word_idx_dict[word] = idx vectors_arr = wv.vectors vectors_arr = np.concatenate((np.zeros(vector_size)[np.newaxis, :], vectors_arr), axis=0)#第0位置的vector为&#39;unk&#39;的vector f_wordidx = open(feature_path + &#39;word_seg_word_idx_dict.pkl&#39;, &#39;wb&#39;) f_vectors = open(feature_path + &#39;word_seg_vectors_arr.pkl&#39;, &#39;wb&#39;) pickle.dump(word_idx_dict, f_wordidx) pickle.dump(vectors_arr, f_vectors) f_wordidx.close() f_vectors.close() print(&quot;训练结果已保存到该目录下！ &quot;) end_time = time.time() print(&quot;耗时：{}s &quot;.format(end_time - start_time)) Task4 LR模型预测 逻辑回归 如果我们忽略二分类问题中y的取值是一个离散的取值（0或1），我们继续使用线性回归来预测y的取值。这样做会导致y的取值并不为0或1。逻辑回归使用一个函数来归一化y值，使y的取值在区间(0,1)内，这个函数称为Logistic函数(logistic function)，也称为Sigmoid函数(sigmoid function)。函数公式如下： Logistic函数当z趋近于无穷大时，g(z)趋近于1；当z趋近于无穷小时，g(z)趋近于0。Logistic函数的图形如下： 逻辑回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数g(z)作为最终假设函数来预测。g(z)可以将连续值映射到0到1之间。线性回归模型的表达式带入g(z)，就得到逻辑回归的表达式: 用LR模型预测 from sklearn.model_selection import GridSearchCV import pickle import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.externals import joblib from sklearn.metrics import roc_auc_score,f1_score,accuracy_score from sklearn.model_selection import GridSearchCV from sklearn import svm from sklearn.model_selection import train_test_split feat_path=&quot;D:/Code/NLPPractice/feat/&quot; model_path=&quot;D:/Code/NLPPractice/model/&quot; result_path=&quot;D:/Code/NLPPractice/result/&quot; #读取特征 data_fp=open(feat_path + &quot;data_w_tfidf.pkl&quot;, &#39;rb&#39;) x_train, y_train, x_test = pickle.load(data_fp) xTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train, test_size=0.30, random_state=531) &#39;&#39;&#39; #模型训练 lr = LogisticRegression(C=120,dual=True) lr.fit(xTrain,yTrain) #保存模型 joblib.dump(lr, model_path + &quot;LR(120)_data_w_tfidf.m&quot;) &#39;&#39;&#39; lr=joblib.load(model_path+&quot;LR(120)_data_w_tfidf.m&quot;) #预测结果 predicts = lr.predict(xVal) accuracy=accuracy_score(yVal,predicts) print(&#39;Accuracy :%2f%%&#39;% (accuracy*100.0)) f1score=f1_score(yVal,predicts,average=&#39;micro&#39;) print(&#39;F1_score :%f1&#39;% f1score) 准确率：78.58%，F1评分为0.78,本来想使用gridsearchcv调参的，但是搞了半天，好像不能对LR多分类模型进行调参，也可能是我没找到，以前只对二分类调过参。 SVM模型预测 SVM SVM中最关键的思想之一就是引入和定义了“间隔”这个概念。这个概念本身很简单，以二维空间为例，就是点到分类直线之间的距离。假设直线为y=wx+b，那么只要使所有正分类点到该直线的距离与所有负分类点到该直线的距离的总和达到最大，这条直线就是最优分类直线。这样，原问题就转化为一个约束优化问题，可以直接求解。这叫做硬间隔最大化，得到的SVM模型称作硬间隔支持向量机。 但是新问题出现了，在实际应用中，我们得到的数据并不总是完美的线性可分的，其中可能会有个别噪声点，他们错误的被分类到了其他类中。如果将这些特异的噪点去除后，可以很容易的线性可分。但是，我们对于数据集中哪些是噪声点却是不知道的，如果以之前的方法进行求解，会无法进行线性分开。是不是就没办法了呢？假设在y=x+1直线上下分为两类，若两类中各有对方的几个噪点，在人的眼中，仍然是可以将两类分开的。这是因为在人脑中是可以容忍一定的误差的，仍然使用y=x+1直线分类，可以在最小误差的情况下进行最优的分类。同样的道理，我们在SVM中引入误差的概念，将其称作“松弛变量”。通过加入松弛变量，在原距离函数中需要加入新的松弛变量带来的误差，这样，最终的优化目标函数变成了两个部分组成：距离函数和松弛变量误差。这两个部分的重要程度并不是相等的，而是需要依据具体问题而定的，因此，我们加入权重参数C，将其与目标函数中的松弛变量误差相乘，这样，就可以通过调整C来对二者的系数进行调和。如果我们能够容忍噪声，那就把C调小，让他的权重降下来，从而变得不重要；反之，我们需要很严格的噪声小的模型，则将C调大一点，权重提升上去，变得更加重要。通过对参数C的调整，可以对模型进行控制。这叫做软间隔最大化，得到的SVM称作软间隔支持向量机。 之前的硬间隔支持向量机和软间隔支持向量机都是解决线性可分数据集或近似线性可分数据集的问题的。但是如果噪点很多，甚至会造成数据变成了线性不可分的，那该怎么办？最常见的例子是在二维平面笛卡尔坐标系下，以原点(0,0)为圆心，以1为半径画圆，则圆内的点和圆外的点在二维空间中是肯定无法线性分开的。但是，学过初中几何就知道，对于圆圈内（含圆圈）的点：x2+y2≤1，圆圈外的则x2+y2＞1。我们假设第三个维度：z=x2+y2，那么在第三维空间中，可以通过z是否大于1来判断该点是否在圆内还是圆外。这样，在二维空间中线性不可分的数据在第三维空间很容易的线性可分了。这就是非线性支持向量机。 使用svm进行预测 from sklearn.model_selection import GridSearchCV import pickle import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.externals import joblib from sklearn.metrics import roc_auc_score,f1_score,accuracy_score from sklearn.model_selection import GridSearchCV from sklearn import svm from sklearn.model_selection import train_test_split feat_path=&quot;D:/Code/NLPPractice/feat/&quot; model_path=&quot;D:/Code/NLPPractice/model/&quot; result_path=&quot;D:/Code/NLPPractice/result/&quot; #读取特征 data_fp=open(feat_path + &quot;data_w_tfidf.pkl&quot;, &#39;rb&#39;) x_train, y_train, x_test = pickle.load(data_fp) xTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train, test_size=0.30, random_state=531) &#39;&#39;&#39; #模型训练 clf = svm.LinearSVC(C=5,dual=False) clf.fit(xTrain,yTrain) #保存模型 joblib.dump(clf, model_path + &quot;SVM(c5)_data_w_tfidf.m&quot;) &#39;&#39;&#39; clf=joblib.load(model_path+&quot;SVM(c5)_data_w_tfidf.m&quot;) #预测结果 predicts = clf.predict(xVal) print(predicts) accuracy=accuracy_score(yVal,predicts) print(&#39;Accuracy :%2f%%&#39;% (accuracy*100.0)) f1score=f1_score(yVal,predicts,average=&#39;micro&#39;) print(&#39;F1_score :%f1&#39;% f1score) 准确率：78.78%，F1评分为0.79 Reference https://www.cnblogs.com/zhizhan/p/4430253.html https://github.com/Heitao5200/DGB/blob/master/model/model_code/SVM_data_w_tfidf.py Task5 LightGBM from sklearn.model_selection import GridSearchCV import pickle import pandas as pd from sklearn.externals import joblib from sklearn.metrics import roc_auc_score,f1_score,accuracy_score from sklearn.model_selection import GridSearchCV from sklearn.model_selection import train_test_split import lightgbm as LGB import numpy as np feat_path=&quot;D:/Code/NLPPractice/feat/&quot; model_path=&quot;D:/Code/NLPPractice/model/&quot; result_path=&quot;D:/Code/NLPPractice/result/&quot; #自定义验证集的评价函数 def f1_score_vali(preds, data_vali): labels = data_vali.get_label() preds = np.argmax(preds.reshape(19, -1), axis=0) score_vali = f1_score(y_true=labels, y_pred=preds, average=&#39;macro&#39;) return &#39;f1_score&#39;, score_vali, True #读取特征 data_fp=open(feat_path + &quot;data_w_tfidf.pkl&quot;, &#39;rb&#39;) x_train, y_train, x_test = pickle.load(data_fp) xTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train, test_size=0.30, random_state=531) d_train = LGB.Dataset(data=xTrain, label=yTrain) d_vali = LGB.Dataset(data=xVal, label=yVal) #构建模型 params = { &#39;boosting&#39;: &#39;gbdt&#39;, &#39;application&#39;: &#39;multiclassova&#39;, &#39;num_class&#39;: 19, &#39;learning_rate&#39;: 0.1, &#39;num_leaves&#39;: 31, &#39;max_depth&#39;: -1, &#39;lambda_l1&#39;: 0, &#39;lambda_l2&#39;: 0.5, &#39;bagging_fraction&#39;: 1.0, } &#39;&#39;&#39; bst = LGB.train(params, d_train, num_boost_round=800, valid_sets=d_vali, feval=f1_score_vali, early_stopping_rounds=None, verbose_eval=True) joblib.dump(bst, model_path + &quot;LGB_data_w_tfidf.m&quot;) &#39;&#39;&#39; #bst=joblib.load(model_path+&quot;LGB_data_w_tfidf.m&quot;) y_proba = bst.predict(xVal) predicts = np.argmax(y_proba.reshape(19, -1), axis=0) print(len(predicts),len(yVal)) accuracy=accuracy_score(yVal,predicts) print(&#39;Accuracy :%2f%%&#39;% (accuracy*100.0)) f1score=f1_score(yVal,predicts,average=&#39;micro&#39;) print(&#39;F1_score :%f1&#39;% f1score) 这个f1score和accuracy都很低，我觉得应该是轮数太少的原因，我电脑配置太差，跑下来太慢了，用gridsearchcv调参的话，应该更慢，过两天有时间再回来找下原因。 Reference https://github.com/Heitao5200/DGB/blob/master/model/model_code/LGB_data_w_tfidf.py Task6 模型融合 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn import svm from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.linear_model import LogisticRegression import lightgbm as lgb from sklearn.model_selection import GridSearchCV import gensim import time import pickle import csv,sys # read data df = pd.read_csv(&#39;data/train_set.csv&#39;, nrows=5000) df.drop(columns=&#39;article&#39;, inplace=True) # observe data # print(df[&#39;class&#39;].value_counts(normalize=True, ascending=False)) # TF-IDF vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True) vectorizer.fit(df[&#39;word_seg&#39;]) x_train = vectorizer.transform(df[&#39;word_seg&#39;]) # split training set and validation set predictor = [&#39;word_seg&#39;] x_train, x_validation, y_train, y_validation = train_test_split(x_train, df[&#39;class&#39;], test_size=0.2) clf = LogisticRegression(C=10, max_iter=20) clf = svm.LinearSVC(C=1, max_iter=20) clf = lgb.sklearn.LGBMClassifier(learning_rate=0.1, n_estimators=50, num_leaves=10) algorithms=[ LogisticRegression(C=10, max_iter=20), svm.LinearSVC(C=1, max_iter=20), ] full_predictions = [] for alg in algorithms: # Fit the algorithm using the full training data. alg.fit(x_train, y_train) # Predict using the test dataset. We have to convert all the columns to floats to avoid an error. predictions = alg.decision_function(x_validation.astype(float)) full_predictions.append(predictions) y_prediction = (full_predictions[0] + full_predictions[1]) / 2 # adjust labels from 1 to 19 y_prediction = np.argmax(y_prediction, axis=1)+1 # # grid search for model # param_grid = { # &#39;num_leaves&#39;: [10, 20, 30], # &#39;learning_rate&#39;: [0.01, 0.05, 0.1], # &#39;n_estimators&#39;: [10, 20, 50] # } # gbm = GridSearchCV(clf, param_grid, cv=5, scoring=&#39;f1_micro&#39;, n_jobs=4, verbose=1) # gbm.fit(x_train, y_train) # print(&#39;网格搜索得到的最优参数是:&#39;, gbm.best_params_) # test model label = [] for i in range(1, 20): label.append(i) f1 = f1_score(y_validation, y_prediction, labels=label, average=&#39;micro&#39;) print(&#39;The F1 Score: &#39; + str(&quot;%.4f&quot; % f1))" />
<meta property="og:description" content="文章目录 Task2 TF-IDF TF IDF TF-IDF 使用TF-IDF表示文本 Reference Task3 one-hot表示 分布式表示（distribution representation） word2vec 实践 Task4 LR模型预测 逻辑回归 用LR模型预测 SVM模型预测 SVM 使用svm进行预测 Reference Task5 LightGBM Reference Task6 模型融合 Task2 学习TF-IDF理论并实践，使用TF-IDF表示文本。 TF-IDF TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率). TF 词频，就是每个词在文本中出现的次数。 假设现在有一段文本 corpus=[“I come to China to travel”, “This is a car polupar in China”, &quot;I love tea and Apple &quot;, “The work is to write some papers in science”] 不考虑停用词，处理后得到的词向量如下： [[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0] [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0] [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0] [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]] 如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现 “come” , “China” 和 “Travel” 各出现1次，而“to“出现了两次。似乎看起来这个文本与 ”to“ 这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的 “China” 和 “Travel” 要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。 IDF IDF即逆文本频率。在上面的例子中可以看到到几乎所有文本都会出现的&quot;to&quot;其词频虽然高，但是重要性却应该比词频低的&quot;China&quot;和“Travel”要低。我们的IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。 概括来讲， IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高，说明词条具有很好的类别区分能力。 IDF的公式为： TF-IDF 将TF和IDF结合得到TF-IDF 使用TF-IDF表示文本 用TF-IDF表示达观杯文本比赛的数据 import pickle import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer data_path=&#39;E:/dataset/daguan/new_data/&#39; #读取数据 train_data=pd.read_csv(data_path+&quot;train_set.csv&quot;) test_data=pd.read_csv(data_path+&quot;test_set.csv&quot;) columns=[&#39;article&#39;,&#39;word_seg&#39;] X_train,X_val,y_train,y_val=train_test_split(train_data[columns],train_data[&#39;class&#39;],test_size=0.3, random_state=2019) vectorizer = TfidfVectorizer() vectorizer.fit(X_train[&#39;word_seg&#39;]) Xtrain=vectorizer.fit_transform(X_train[&#39;word_seg&#39;]) Xval=vectorizer.fit_transform(X_train[&#39;word_seg&#39;]) print(Xtrain,Xval) data=(Xtrain,Xval,y_train,y_val) #保存到本地 fp = open(&#39;./feat/data_tfidf.pickl&#39;, &#39;wb&#39;) pickle.dump(data, fp) fp.close() 使用tf-idf表示的数据： Reference https://www.cnblogs.com/pinard/p/6693230.html https://github.com/Heitao5200/DGB/blob/master/feature/feature_code/tfidf.py Task3 one-hot表示 传统的基于规则或基于统计的自然语义处理方法将单词看作一个原子符号 被称作one-hot representation。one-hot representation把每个词表示为一个长向量。这个向量的维度是词表大小，向量中只有一个维度的值为1，其余维度为0，这个维度就代表了当前的词。 例如： 苹果 [0，0，0，1，0，0，0，0，0，……] 不足：难以发现词之间的关系，以及难以捕捉句法（结构）和语义（意思）之间的关系 分布式表示（distribution representation） word embedding指的是将词转化成一种分布式表示，又称词向量。分布式表示将词表示成一个定长的连续的稠密向量。这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, …]。 分布式表示优点: (1)词之间存在相似关系： 是词之间存在“距离”概念，这对很多自然语言处理的任务非常有帮助。 (2)包含更多信息： 词向量能够包含更多信息，并且每一维都有特定的含义。在采用one-hot特征时，可以对特征向量进行删减，词向量则不能。 Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。 比如下图我们将词汇表里的词用&quot;Royalty&quot;,“Masculinity”, &quot;Femininity&quot;和&quot;Age&quot;4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。 我们将king这个词从一个可能非常稀疏的向量坐在的空间，映射到现在这个四维向量所在的空间，必须满足以下性质： （1）这个映射是单设）； （2）映射之后的向量不会丢失之前的那种向量所含的信息。 这个过程称为word embedding（词嵌入），即将高维词向量嵌入到一个低维空间。 word2vec word2vec模型其实就是简单化的神经网络。 输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵。 这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。　Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。 CBOW模型 Skip-Gram模型 实践 import pandas as pd import gensim import time import pickle import numpy as np import csv,sys vector_size = 100 maxInt = sys.maxsize decrement = True while decrement: # decrease the maxInt value by factor 10 # as long as the OverflowError occurs. decrement = False try: csv.field_size_limit(maxInt) except OverflowError: maxInt = int(maxInt/10) decrement = True #======================================================================================================================= # 0 辅助函数 #======================================================================================================================= def sentence2list(sentence): return sentence.strip().split() start_time = time.time() data_path = &#39;E:/MyPython/机器学习——达观杯/data_set/&#39; feature_path = &#39;E:/MyPython/机器学习——达观杯/feature/feature_file/&#39; proba_path = &#39;E:/MyPython/机器学习——达观杯/proba/proba_file/&#39; model_path = &#39;E:/MyPython/机器学习——达观杯/model/model_file/&#39; result_path =&quot;E:/MyPython/机器学习——达观杯/result/&quot; #======================================================================================================================= # 1 准备训练数据 #======================================================================================================================= print(&quot;准备数据................ &quot;) df_train = pd.read_csv(data_path +&#39;train_set1.csv&#39;,engine=&#39;python&#39;) df_test = pd.read_csv(data_path +&#39;test_set1.csv&#39;,engine=&#39;python&#39;) sentences_train = list(df_train.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences_test = list(df_test.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences = sentences_train + sentences_test print(&quot;准备数据完成! &quot;) #======================================================================================================================= # 2 训练 #======================================================================================================================= print(&quot;开始训练................ &quot;) model = gensim.models.Word2Vec(sentences=sentences, size=vector_size, window=5, min_count=5, workers=8, sg=0, iter=5) print(&quot;训练完成! &quot;) #======================================================================================================================= # 3 提取词汇表及vectors,并保存 #======================================================================================================================= print(&quot; 保存训练结果........... &quot;) wv = model.wv vocab_list = wv.index2word word_idx_dict = {} for idx, word in enumerate(vocab_list): word_idx_dict[word] = idx vectors_arr = wv.vectors vectors_arr = np.concatenate((np.zeros(vector_size)[np.newaxis, :], vectors_arr), axis=0)#第0位置的vector为&#39;unk&#39;的vector f_wordidx = open(feature_path + &#39;word_seg_word_idx_dict.pkl&#39;, &#39;wb&#39;) f_vectors = open(feature_path + &#39;word_seg_vectors_arr.pkl&#39;, &#39;wb&#39;) pickle.dump(word_idx_dict, f_wordidx) pickle.dump(vectors_arr, f_vectors) f_wordidx.close() f_vectors.close() print(&quot;训练结果已保存到该目录下！ &quot;) end_time = time.time() print(&quot;耗时：{}s &quot;.format(end_time - start_time)) Task4 LR模型预测 逻辑回归 如果我们忽略二分类问题中y的取值是一个离散的取值（0或1），我们继续使用线性回归来预测y的取值。这样做会导致y的取值并不为0或1。逻辑回归使用一个函数来归一化y值，使y的取值在区间(0,1)内，这个函数称为Logistic函数(logistic function)，也称为Sigmoid函数(sigmoid function)。函数公式如下： Logistic函数当z趋近于无穷大时，g(z)趋近于1；当z趋近于无穷小时，g(z)趋近于0。Logistic函数的图形如下： 逻辑回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数g(z)作为最终假设函数来预测。g(z)可以将连续值映射到0到1之间。线性回归模型的表达式带入g(z)，就得到逻辑回归的表达式: 用LR模型预测 from sklearn.model_selection import GridSearchCV import pickle import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.externals import joblib from sklearn.metrics import roc_auc_score,f1_score,accuracy_score from sklearn.model_selection import GridSearchCV from sklearn import svm from sklearn.model_selection import train_test_split feat_path=&quot;D:/Code/NLPPractice/feat/&quot; model_path=&quot;D:/Code/NLPPractice/model/&quot; result_path=&quot;D:/Code/NLPPractice/result/&quot; #读取特征 data_fp=open(feat_path + &quot;data_w_tfidf.pkl&quot;, &#39;rb&#39;) x_train, y_train, x_test = pickle.load(data_fp) xTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train, test_size=0.30, random_state=531) &#39;&#39;&#39; #模型训练 lr = LogisticRegression(C=120,dual=True) lr.fit(xTrain,yTrain) #保存模型 joblib.dump(lr, model_path + &quot;LR(120)_data_w_tfidf.m&quot;) &#39;&#39;&#39; lr=joblib.load(model_path+&quot;LR(120)_data_w_tfidf.m&quot;) #预测结果 predicts = lr.predict(xVal) accuracy=accuracy_score(yVal,predicts) print(&#39;Accuracy :%2f%%&#39;% (accuracy*100.0)) f1score=f1_score(yVal,predicts,average=&#39;micro&#39;) print(&#39;F1_score :%f1&#39;% f1score) 准确率：78.58%，F1评分为0.78,本来想使用gridsearchcv调参的，但是搞了半天，好像不能对LR多分类模型进行调参，也可能是我没找到，以前只对二分类调过参。 SVM模型预测 SVM SVM中最关键的思想之一就是引入和定义了“间隔”这个概念。这个概念本身很简单，以二维空间为例，就是点到分类直线之间的距离。假设直线为y=wx+b，那么只要使所有正分类点到该直线的距离与所有负分类点到该直线的距离的总和达到最大，这条直线就是最优分类直线。这样，原问题就转化为一个约束优化问题，可以直接求解。这叫做硬间隔最大化，得到的SVM模型称作硬间隔支持向量机。 但是新问题出现了，在实际应用中，我们得到的数据并不总是完美的线性可分的，其中可能会有个别噪声点，他们错误的被分类到了其他类中。如果将这些特异的噪点去除后，可以很容易的线性可分。但是，我们对于数据集中哪些是噪声点却是不知道的，如果以之前的方法进行求解，会无法进行线性分开。是不是就没办法了呢？假设在y=x+1直线上下分为两类，若两类中各有对方的几个噪点，在人的眼中，仍然是可以将两类分开的。这是因为在人脑中是可以容忍一定的误差的，仍然使用y=x+1直线分类，可以在最小误差的情况下进行最优的分类。同样的道理，我们在SVM中引入误差的概念，将其称作“松弛变量”。通过加入松弛变量，在原距离函数中需要加入新的松弛变量带来的误差，这样，最终的优化目标函数变成了两个部分组成：距离函数和松弛变量误差。这两个部分的重要程度并不是相等的，而是需要依据具体问题而定的，因此，我们加入权重参数C，将其与目标函数中的松弛变量误差相乘，这样，就可以通过调整C来对二者的系数进行调和。如果我们能够容忍噪声，那就把C调小，让他的权重降下来，从而变得不重要；反之，我们需要很严格的噪声小的模型，则将C调大一点，权重提升上去，变得更加重要。通过对参数C的调整，可以对模型进行控制。这叫做软间隔最大化，得到的SVM称作软间隔支持向量机。 之前的硬间隔支持向量机和软间隔支持向量机都是解决线性可分数据集或近似线性可分数据集的问题的。但是如果噪点很多，甚至会造成数据变成了线性不可分的，那该怎么办？最常见的例子是在二维平面笛卡尔坐标系下，以原点(0,0)为圆心，以1为半径画圆，则圆内的点和圆外的点在二维空间中是肯定无法线性分开的。但是，学过初中几何就知道，对于圆圈内（含圆圈）的点：x2+y2≤1，圆圈外的则x2+y2＞1。我们假设第三个维度：z=x2+y2，那么在第三维空间中，可以通过z是否大于1来判断该点是否在圆内还是圆外。这样，在二维空间中线性不可分的数据在第三维空间很容易的线性可分了。这就是非线性支持向量机。 使用svm进行预测 from sklearn.model_selection import GridSearchCV import pickle import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.externals import joblib from sklearn.metrics import roc_auc_score,f1_score,accuracy_score from sklearn.model_selection import GridSearchCV from sklearn import svm from sklearn.model_selection import train_test_split feat_path=&quot;D:/Code/NLPPractice/feat/&quot; model_path=&quot;D:/Code/NLPPractice/model/&quot; result_path=&quot;D:/Code/NLPPractice/result/&quot; #读取特征 data_fp=open(feat_path + &quot;data_w_tfidf.pkl&quot;, &#39;rb&#39;) x_train, y_train, x_test = pickle.load(data_fp) xTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train, test_size=0.30, random_state=531) &#39;&#39;&#39; #模型训练 clf = svm.LinearSVC(C=5,dual=False) clf.fit(xTrain,yTrain) #保存模型 joblib.dump(clf, model_path + &quot;SVM(c5)_data_w_tfidf.m&quot;) &#39;&#39;&#39; clf=joblib.load(model_path+&quot;SVM(c5)_data_w_tfidf.m&quot;) #预测结果 predicts = clf.predict(xVal) print(predicts) accuracy=accuracy_score(yVal,predicts) print(&#39;Accuracy :%2f%%&#39;% (accuracy*100.0)) f1score=f1_score(yVal,predicts,average=&#39;micro&#39;) print(&#39;F1_score :%f1&#39;% f1score) 准确率：78.78%，F1评分为0.79 Reference https://www.cnblogs.com/zhizhan/p/4430253.html https://github.com/Heitao5200/DGB/blob/master/model/model_code/SVM_data_w_tfidf.py Task5 LightGBM from sklearn.model_selection import GridSearchCV import pickle import pandas as pd from sklearn.externals import joblib from sklearn.metrics import roc_auc_score,f1_score,accuracy_score from sklearn.model_selection import GridSearchCV from sklearn.model_selection import train_test_split import lightgbm as LGB import numpy as np feat_path=&quot;D:/Code/NLPPractice/feat/&quot; model_path=&quot;D:/Code/NLPPractice/model/&quot; result_path=&quot;D:/Code/NLPPractice/result/&quot; #自定义验证集的评价函数 def f1_score_vali(preds, data_vali): labels = data_vali.get_label() preds = np.argmax(preds.reshape(19, -1), axis=0) score_vali = f1_score(y_true=labels, y_pred=preds, average=&#39;macro&#39;) return &#39;f1_score&#39;, score_vali, True #读取特征 data_fp=open(feat_path + &quot;data_w_tfidf.pkl&quot;, &#39;rb&#39;) x_train, y_train, x_test = pickle.load(data_fp) xTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train, test_size=0.30, random_state=531) d_train = LGB.Dataset(data=xTrain, label=yTrain) d_vali = LGB.Dataset(data=xVal, label=yVal) #构建模型 params = { &#39;boosting&#39;: &#39;gbdt&#39;, &#39;application&#39;: &#39;multiclassova&#39;, &#39;num_class&#39;: 19, &#39;learning_rate&#39;: 0.1, &#39;num_leaves&#39;: 31, &#39;max_depth&#39;: -1, &#39;lambda_l1&#39;: 0, &#39;lambda_l2&#39;: 0.5, &#39;bagging_fraction&#39;: 1.0, } &#39;&#39;&#39; bst = LGB.train(params, d_train, num_boost_round=800, valid_sets=d_vali, feval=f1_score_vali, early_stopping_rounds=None, verbose_eval=True) joblib.dump(bst, model_path + &quot;LGB_data_w_tfidf.m&quot;) &#39;&#39;&#39; #bst=joblib.load(model_path+&quot;LGB_data_w_tfidf.m&quot;) y_proba = bst.predict(xVal) predicts = np.argmax(y_proba.reshape(19, -1), axis=0) print(len(predicts),len(yVal)) accuracy=accuracy_score(yVal,predicts) print(&#39;Accuracy :%2f%%&#39;% (accuracy*100.0)) f1score=f1_score(yVal,predicts,average=&#39;micro&#39;) print(&#39;F1_score :%f1&#39;% f1score) 这个f1score和accuracy都很低，我觉得应该是轮数太少的原因，我电脑配置太差，跑下来太慢了，用gridsearchcv调参的话，应该更慢，过两天有时间再回来找下原因。 Reference https://github.com/Heitao5200/DGB/blob/master/model/model_code/LGB_data_w_tfidf.py Task6 模型融合 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn import svm from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.linear_model import LogisticRegression import lightgbm as lgb from sklearn.model_selection import GridSearchCV import gensim import time import pickle import csv,sys # read data df = pd.read_csv(&#39;data/train_set.csv&#39;, nrows=5000) df.drop(columns=&#39;article&#39;, inplace=True) # observe data # print(df[&#39;class&#39;].value_counts(normalize=True, ascending=False)) # TF-IDF vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True) vectorizer.fit(df[&#39;word_seg&#39;]) x_train = vectorizer.transform(df[&#39;word_seg&#39;]) # split training set and validation set predictor = [&#39;word_seg&#39;] x_train, x_validation, y_train, y_validation = train_test_split(x_train, df[&#39;class&#39;], test_size=0.2) clf = LogisticRegression(C=10, max_iter=20) clf = svm.LinearSVC(C=1, max_iter=20) clf = lgb.sklearn.LGBMClassifier(learning_rate=0.1, n_estimators=50, num_leaves=10) algorithms=[ LogisticRegression(C=10, max_iter=20), svm.LinearSVC(C=1, max_iter=20), ] full_predictions = [] for alg in algorithms: # Fit the algorithm using the full training data. alg.fit(x_train, y_train) # Predict using the test dataset. We have to convert all the columns to floats to avoid an error. predictions = alg.decision_function(x_validation.astype(float)) full_predictions.append(predictions) y_prediction = (full_predictions[0] + full_predictions[1]) / 2 # adjust labels from 1 to 19 y_prediction = np.argmax(y_prediction, axis=1)+1 # # grid search for model # param_grid = { # &#39;num_leaves&#39;: [10, 20, 30], # &#39;learning_rate&#39;: [0.01, 0.05, 0.1], # &#39;n_estimators&#39;: [10, 20, 50] # } # gbm = GridSearchCV(clf, param_grid, cv=5, scoring=&#39;f1_micro&#39;, n_jobs=4, verbose=1) # gbm.fit(x_train, y_train) # print(&#39;网格搜索得到的最优参数是:&#39;, gbm.best_params_) # test model label = [] for i in range(1, 20): label.append(i) f1 = f1_score(y_validation, y_prediction, labels=label, average=&#39;micro&#39;) print(&#39;The F1 Score: &#39; + str(&quot;%.4f&quot; % f1))" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-07T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"文章目录 Task2 TF-IDF TF IDF TF-IDF 使用TF-IDF表示文本 Reference Task3 one-hot表示 分布式表示（distribution representation） word2vec 实践 Task4 LR模型预测 逻辑回归 用LR模型预测 SVM模型预测 SVM 使用svm进行预测 Reference Task5 LightGBM Reference Task6 模型融合 Task2 学习TF-IDF理论并实践，使用TF-IDF表示文本。 TF-IDF TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率). TF 词频，就是每个词在文本中出现的次数。 假设现在有一段文本 corpus=[“I come to China to travel”, “This is a car polupar in China”, &quot;I love tea and Apple &quot;, “The work is to write some papers in science”] 不考虑停用词，处理后得到的词向量如下： [[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0] [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0] [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0] [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]] 如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现 “come” , “China” 和 “Travel” 各出现1次，而“to“出现了两次。似乎看起来这个文本与 ”to“ 这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的 “China” 和 “Travel” 要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。 IDF IDF即逆文本频率。在上面的例子中可以看到到几乎所有文本都会出现的&quot;to&quot;其词频虽然高，但是重要性却应该比词频低的&quot;China&quot;和“Travel”要低。我们的IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。 概括来讲， IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高，说明词条具有很好的类别区分能力。 IDF的公式为： TF-IDF 将TF和IDF结合得到TF-IDF 使用TF-IDF表示文本 用TF-IDF表示达观杯文本比赛的数据 import pickle import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer data_path=&#39;E:/dataset/daguan/new_data/&#39; #读取数据 train_data=pd.read_csv(data_path+&quot;train_set.csv&quot;) test_data=pd.read_csv(data_path+&quot;test_set.csv&quot;) columns=[&#39;article&#39;,&#39;word_seg&#39;] X_train,X_val,y_train,y_val=train_test_split(train_data[columns],train_data[&#39;class&#39;],test_size=0.3, random_state=2019) vectorizer = TfidfVectorizer() vectorizer.fit(X_train[&#39;word_seg&#39;]) Xtrain=vectorizer.fit_transform(X_train[&#39;word_seg&#39;]) Xval=vectorizer.fit_transform(X_train[&#39;word_seg&#39;]) print(Xtrain,Xval) data=(Xtrain,Xval,y_train,y_val) #保存到本地 fp = open(&#39;./feat/data_tfidf.pickl&#39;, &#39;wb&#39;) pickle.dump(data, fp) fp.close() 使用tf-idf表示的数据： Reference https://www.cnblogs.com/pinard/p/6693230.html https://github.com/Heitao5200/DGB/blob/master/feature/feature_code/tfidf.py Task3 one-hot表示 传统的基于规则或基于统计的自然语义处理方法将单词看作一个原子符号 被称作one-hot representation。one-hot representation把每个词表示为一个长向量。这个向量的维度是词表大小，向量中只有一个维度的值为1，其余维度为0，这个维度就代表了当前的词。 例如： 苹果 [0，0，0，1，0，0，0，0，0，……] 不足：难以发现词之间的关系，以及难以捕捉句法（结构）和语义（意思）之间的关系 分布式表示（distribution representation） word embedding指的是将词转化成一种分布式表示，又称词向量。分布式表示将词表示成一个定长的连续的稠密向量。这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, …]。 分布式表示优点: (1)词之间存在相似关系： 是词之间存在“距离”概念，这对很多自然语言处理的任务非常有帮助。 (2)包含更多信息： 词向量能够包含更多信息，并且每一维都有特定的含义。在采用one-hot特征时，可以对特征向量进行删减，词向量则不能。 Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。 比如下图我们将词汇表里的词用&quot;Royalty&quot;,“Masculinity”, &quot;Femininity&quot;和&quot;Age&quot;4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。 我们将king这个词从一个可能非常稀疏的向量坐在的空间，映射到现在这个四维向量所在的空间，必须满足以下性质： （1）这个映射是单设）； （2）映射之后的向量不会丢失之前的那种向量所含的信息。 这个过程称为word embedding（词嵌入），即将高维词向量嵌入到一个低维空间。 word2vec word2vec模型其实就是简单化的神经网络。 输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵。 这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。　Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。 CBOW模型 Skip-Gram模型 实践 import pandas as pd import gensim import time import pickle import numpy as np import csv,sys vector_size = 100 maxInt = sys.maxsize decrement = True while decrement: # decrease the maxInt value by factor 10 # as long as the OverflowError occurs. decrement = False try: csv.field_size_limit(maxInt) except OverflowError: maxInt = int(maxInt/10) decrement = True #======================================================================================================================= # 0 辅助函数 #======================================================================================================================= def sentence2list(sentence): return sentence.strip().split() start_time = time.time() data_path = &#39;E:/MyPython/机器学习——达观杯/data_set/&#39; feature_path = &#39;E:/MyPython/机器学习——达观杯/feature/feature_file/&#39; proba_path = &#39;E:/MyPython/机器学习——达观杯/proba/proba_file/&#39; model_path = &#39;E:/MyPython/机器学习——达观杯/model/model_file/&#39; result_path =&quot;E:/MyPython/机器学习——达观杯/result/&quot; #======================================================================================================================= # 1 准备训练数据 #======================================================================================================================= print(&quot;准备数据................ &quot;) df_train = pd.read_csv(data_path +&#39;train_set1.csv&#39;,engine=&#39;python&#39;) df_test = pd.read_csv(data_path +&#39;test_set1.csv&#39;,engine=&#39;python&#39;) sentences_train = list(df_train.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences_test = list(df_test.loc[:, &#39;word_seg&#39;].apply(sentence2list)) sentences = sentences_train + sentences_test print(&quot;准备数据完成! &quot;) #======================================================================================================================= # 2 训练 #======================================================================================================================= print(&quot;开始训练................ &quot;) model = gensim.models.Word2Vec(sentences=sentences, size=vector_size, window=5, min_count=5, workers=8, sg=0, iter=5) print(&quot;训练完成! &quot;) #======================================================================================================================= # 3 提取词汇表及vectors,并保存 #======================================================================================================================= print(&quot; 保存训练结果........... &quot;) wv = model.wv vocab_list = wv.index2word word_idx_dict = {} for idx, word in enumerate(vocab_list): word_idx_dict[word] = idx vectors_arr = wv.vectors vectors_arr = np.concatenate((np.zeros(vector_size)[np.newaxis, :], vectors_arr), axis=0)#第0位置的vector为&#39;unk&#39;的vector f_wordidx = open(feature_path + &#39;word_seg_word_idx_dict.pkl&#39;, &#39;wb&#39;) f_vectors = open(feature_path + &#39;word_seg_vectors_arr.pkl&#39;, &#39;wb&#39;) pickle.dump(word_idx_dict, f_wordidx) pickle.dump(vectors_arr, f_vectors) f_wordidx.close() f_vectors.close() print(&quot;训练结果已保存到该目录下！ &quot;) end_time = time.time() print(&quot;耗时：{}s &quot;.format(end_time - start_time)) Task4 LR模型预测 逻辑回归 如果我们忽略二分类问题中y的取值是一个离散的取值（0或1），我们继续使用线性回归来预测y的取值。这样做会导致y的取值并不为0或1。逻辑回归使用一个函数来归一化y值，使y的取值在区间(0,1)内，这个函数称为Logistic函数(logistic function)，也称为Sigmoid函数(sigmoid function)。函数公式如下： Logistic函数当z趋近于无穷大时，g(z)趋近于1；当z趋近于无穷小时，g(z)趋近于0。Logistic函数的图形如下： 逻辑回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数g(z)作为最终假设函数来预测。g(z)可以将连续值映射到0到1之间。线性回归模型的表达式带入g(z)，就得到逻辑回归的表达式: 用LR模型预测 from sklearn.model_selection import GridSearchCV import pickle import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.externals import joblib from sklearn.metrics import roc_auc_score,f1_score,accuracy_score from sklearn.model_selection import GridSearchCV from sklearn import svm from sklearn.model_selection import train_test_split feat_path=&quot;D:/Code/NLPPractice/feat/&quot; model_path=&quot;D:/Code/NLPPractice/model/&quot; result_path=&quot;D:/Code/NLPPractice/result/&quot; #读取特征 data_fp=open(feat_path + &quot;data_w_tfidf.pkl&quot;, &#39;rb&#39;) x_train, y_train, x_test = pickle.load(data_fp) xTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train, test_size=0.30, random_state=531) &#39;&#39;&#39; #模型训练 lr = LogisticRegression(C=120,dual=True) lr.fit(xTrain,yTrain) #保存模型 joblib.dump(lr, model_path + &quot;LR(120)_data_w_tfidf.m&quot;) &#39;&#39;&#39; lr=joblib.load(model_path+&quot;LR(120)_data_w_tfidf.m&quot;) #预测结果 predicts = lr.predict(xVal) accuracy=accuracy_score(yVal,predicts) print(&#39;Accuracy :%2f%%&#39;% (accuracy*100.0)) f1score=f1_score(yVal,predicts,average=&#39;micro&#39;) print(&#39;F1_score :%f1&#39;% f1score) 准确率：78.58%，F1评分为0.78,本来想使用gridsearchcv调参的，但是搞了半天，好像不能对LR多分类模型进行调参，也可能是我没找到，以前只对二分类调过参。 SVM模型预测 SVM SVM中最关键的思想之一就是引入和定义了“间隔”这个概念。这个概念本身很简单，以二维空间为例，就是点到分类直线之间的距离。假设直线为y=wx+b，那么只要使所有正分类点到该直线的距离与所有负分类点到该直线的距离的总和达到最大，这条直线就是最优分类直线。这样，原问题就转化为一个约束优化问题，可以直接求解。这叫做硬间隔最大化，得到的SVM模型称作硬间隔支持向量机。 但是新问题出现了，在实际应用中，我们得到的数据并不总是完美的线性可分的，其中可能会有个别噪声点，他们错误的被分类到了其他类中。如果将这些特异的噪点去除后，可以很容易的线性可分。但是，我们对于数据集中哪些是噪声点却是不知道的，如果以之前的方法进行求解，会无法进行线性分开。是不是就没办法了呢？假设在y=x+1直线上下分为两类，若两类中各有对方的几个噪点，在人的眼中，仍然是可以将两类分开的。这是因为在人脑中是可以容忍一定的误差的，仍然使用y=x+1直线分类，可以在最小误差的情况下进行最优的分类。同样的道理，我们在SVM中引入误差的概念，将其称作“松弛变量”。通过加入松弛变量，在原距离函数中需要加入新的松弛变量带来的误差，这样，最终的优化目标函数变成了两个部分组成：距离函数和松弛变量误差。这两个部分的重要程度并不是相等的，而是需要依据具体问题而定的，因此，我们加入权重参数C，将其与目标函数中的松弛变量误差相乘，这样，就可以通过调整C来对二者的系数进行调和。如果我们能够容忍噪声，那就把C调小，让他的权重降下来，从而变得不重要；反之，我们需要很严格的噪声小的模型，则将C调大一点，权重提升上去，变得更加重要。通过对参数C的调整，可以对模型进行控制。这叫做软间隔最大化，得到的SVM称作软间隔支持向量机。 之前的硬间隔支持向量机和软间隔支持向量机都是解决线性可分数据集或近似线性可分数据集的问题的。但是如果噪点很多，甚至会造成数据变成了线性不可分的，那该怎么办？最常见的例子是在二维平面笛卡尔坐标系下，以原点(0,0)为圆心，以1为半径画圆，则圆内的点和圆外的点在二维空间中是肯定无法线性分开的。但是，学过初中几何就知道，对于圆圈内（含圆圈）的点：x2+y2≤1，圆圈外的则x2+y2＞1。我们假设第三个维度：z=x2+y2，那么在第三维空间中，可以通过z是否大于1来判断该点是否在圆内还是圆外。这样，在二维空间中线性不可分的数据在第三维空间很容易的线性可分了。这就是非线性支持向量机。 使用svm进行预测 from sklearn.model_selection import GridSearchCV import pickle import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.externals import joblib from sklearn.metrics import roc_auc_score,f1_score,accuracy_score from sklearn.model_selection import GridSearchCV from sklearn import svm from sklearn.model_selection import train_test_split feat_path=&quot;D:/Code/NLPPractice/feat/&quot; model_path=&quot;D:/Code/NLPPractice/model/&quot; result_path=&quot;D:/Code/NLPPractice/result/&quot; #读取特征 data_fp=open(feat_path + &quot;data_w_tfidf.pkl&quot;, &#39;rb&#39;) x_train, y_train, x_test = pickle.load(data_fp) xTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train, test_size=0.30, random_state=531) &#39;&#39;&#39; #模型训练 clf = svm.LinearSVC(C=5,dual=False) clf.fit(xTrain,yTrain) #保存模型 joblib.dump(clf, model_path + &quot;SVM(c5)_data_w_tfidf.m&quot;) &#39;&#39;&#39; clf=joblib.load(model_path+&quot;SVM(c5)_data_w_tfidf.m&quot;) #预测结果 predicts = clf.predict(xVal) print(predicts) accuracy=accuracy_score(yVal,predicts) print(&#39;Accuracy :%2f%%&#39;% (accuracy*100.0)) f1score=f1_score(yVal,predicts,average=&#39;micro&#39;) print(&#39;F1_score :%f1&#39;% f1score) 准确率：78.78%，F1评分为0.79 Reference https://www.cnblogs.com/zhizhan/p/4430253.html https://github.com/Heitao5200/DGB/blob/master/model/model_code/SVM_data_w_tfidf.py Task5 LightGBM from sklearn.model_selection import GridSearchCV import pickle import pandas as pd from sklearn.externals import joblib from sklearn.metrics import roc_auc_score,f1_score,accuracy_score from sklearn.model_selection import GridSearchCV from sklearn.model_selection import train_test_split import lightgbm as LGB import numpy as np feat_path=&quot;D:/Code/NLPPractice/feat/&quot; model_path=&quot;D:/Code/NLPPractice/model/&quot; result_path=&quot;D:/Code/NLPPractice/result/&quot; #自定义验证集的评价函数 def f1_score_vali(preds, data_vali): labels = data_vali.get_label() preds = np.argmax(preds.reshape(19, -1), axis=0) score_vali = f1_score(y_true=labels, y_pred=preds, average=&#39;macro&#39;) return &#39;f1_score&#39;, score_vali, True #读取特征 data_fp=open(feat_path + &quot;data_w_tfidf.pkl&quot;, &#39;rb&#39;) x_train, y_train, x_test = pickle.load(data_fp) xTrain, xVal, yTrain, yVal = train_test_split(x_train, y_train, test_size=0.30, random_state=531) d_train = LGB.Dataset(data=xTrain, label=yTrain) d_vali = LGB.Dataset(data=xVal, label=yVal) #构建模型 params = { &#39;boosting&#39;: &#39;gbdt&#39;, &#39;application&#39;: &#39;multiclassova&#39;, &#39;num_class&#39;: 19, &#39;learning_rate&#39;: 0.1, &#39;num_leaves&#39;: 31, &#39;max_depth&#39;: -1, &#39;lambda_l1&#39;: 0, &#39;lambda_l2&#39;: 0.5, &#39;bagging_fraction&#39;: 1.0, } &#39;&#39;&#39; bst = LGB.train(params, d_train, num_boost_round=800, valid_sets=d_vali, feval=f1_score_vali, early_stopping_rounds=None, verbose_eval=True) joblib.dump(bst, model_path + &quot;LGB_data_w_tfidf.m&quot;) &#39;&#39;&#39; #bst=joblib.load(model_path+&quot;LGB_data_w_tfidf.m&quot;) y_proba = bst.predict(xVal) predicts = np.argmax(y_proba.reshape(19, -1), axis=0) print(len(predicts),len(yVal)) accuracy=accuracy_score(yVal,predicts) print(&#39;Accuracy :%2f%%&#39;% (accuracy*100.0)) f1score=f1_score(yVal,predicts,average=&#39;micro&#39;) print(&#39;F1_score :%f1&#39;% f1score) 这个f1score和accuracy都很低，我觉得应该是轮数太少的原因，我电脑配置太差，跑下来太慢了，用gridsearchcv调参的话，应该更慢，过两天有时间再回来找下原因。 Reference https://github.com/Heitao5200/DGB/blob/master/model/model_code/LGB_data_w_tfidf.py Task6 模型融合 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn import svm from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.linear_model import LogisticRegression import lightgbm as lgb from sklearn.model_selection import GridSearchCV import gensim import time import pickle import csv,sys # read data df = pd.read_csv(&#39;data/train_set.csv&#39;, nrows=5000) df.drop(columns=&#39;article&#39;, inplace=True) # observe data # print(df[&#39;class&#39;].value_counts(normalize=True, ascending=False)) # TF-IDF vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True) vectorizer.fit(df[&#39;word_seg&#39;]) x_train = vectorizer.transform(df[&#39;word_seg&#39;]) # split training set and validation set predictor = [&#39;word_seg&#39;] x_train, x_validation, y_train, y_validation = train_test_split(x_train, df[&#39;class&#39;], test_size=0.2) clf = LogisticRegression(C=10, max_iter=20) clf = svm.LinearSVC(C=1, max_iter=20) clf = lgb.sklearn.LGBMClassifier(learning_rate=0.1, n_estimators=50, num_leaves=10) algorithms=[ LogisticRegression(C=10, max_iter=20), svm.LinearSVC(C=1, max_iter=20), ] full_predictions = [] for alg in algorithms: # Fit the algorithm using the full training data. alg.fit(x_train, y_train) # Predict using the test dataset. We have to convert all the columns to floats to avoid an error. predictions = alg.decision_function(x_validation.astype(float)) full_predictions.append(predictions) y_prediction = (full_predictions[0] + full_predictions[1]) / 2 # adjust labels from 1 to 19 y_prediction = np.argmax(y_prediction, axis=1)+1 # # grid search for model # param_grid = { # &#39;num_leaves&#39;: [10, 20, 30], # &#39;learning_rate&#39;: [0.01, 0.05, 0.1], # &#39;n_estimators&#39;: [10, 20, 50] # } # gbm = GridSearchCV(clf, param_grid, cv=5, scoring=&#39;f1_micro&#39;, n_jobs=4, verbose=1) # gbm.fit(x_train, y_train) # print(&#39;网格搜索得到的最优参数是:&#39;, gbm.best_params_) # test model label = [] for i in range(1, 20): label.append(i) f1 = f1_score(y_validation, y_prediction, labels=label, average=&#39;micro&#39;) print(&#39;The F1 Score: &#39; + str(&quot;%.4f&quot; % f1))","@type":"BlogPosting","url":"/2019/04/07/728722.html","headline":"达观杯文本智能处理挑战赛练习(二)","dateModified":"2019-04-07T00:00:00+08:00","datePublished":"2019-04-07T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/04/07/728722.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>达观杯文本智能处理挑战赛练习(二)</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <li><a href="#Task2_1" rel="nofollow">Task2</a></li>
    <ul>
     <li><a href="#TFIDF_3" rel="nofollow">TF-IDF</a></li>
     <ul>
      <li><a href="#TF_5" rel="nofollow">TF</a></li>
      <li><a href="#IDF_20" rel="nofollow">IDF</a></li>
      <li><a href="#TFIDF_26" rel="nofollow">TF-IDF</a></li>
     </ul>
     <li><a href="#TFIDF_29" rel="nofollow">使用TF-IDF表示文本</a></li>
     <li><a href="#Reference_57" rel="nofollow">Reference</a></li>
    </ul>
    <li><a href="#Task3_60" rel="nofollow">Task3</a></li>
    <ul>
     <li><a href="#onehot_61" rel="nofollow">one-hot表示</a></li>
     <li><a href="#distribution_representation_66" rel="nofollow">分布式表示（distribution representation）</a></li>
     <li><a href="#word2vec_82" rel="nofollow">word2vec</a></li>
     <li><a href="#_92" rel="nofollow">实践</a></li>
    </ul>
    <li><a href="#Task4_171" rel="nofollow">Task4</a></li>
    <ul>
     <li><a href="#LR_172" rel="nofollow">LR模型预测</a></li>
     <ul>
      <li><a href="#_173" rel="nofollow">逻辑回归</a></li>
      <li><a href="#LR_180" rel="nofollow">用LR模型预测</a></li>
     </ul>
     <li><a href="#SVM_221" rel="nofollow">SVM模型预测</a></li>
     <ul>
      <li><a href="#SVM_222" rel="nofollow">SVM</a></li>
      <li><a href="#svm_226" rel="nofollow">使用svm进行预测</a></li>
     </ul>
     <li><a href="#Reference_271" rel="nofollow">Reference</a></li>
    </ul>
    <li><a href="#Task5_274" rel="nofollow">Task5</a></li>
    <ul>
     <li><a href="#LightGBM_275" rel="nofollow">LightGBM</a></li>
     <li><a href="#Reference_337" rel="nofollow">Reference</a></li>
    </ul>
    <li><a href="#Task6_340" rel="nofollow">Task6</a></li>
    <ul>
     <li><a href="#_341" rel="nofollow">模型融合</a></li>
    </ul>
   </ul>
  </div>
  <p></p> 
  <h1><a id="Task2_1"></a>Task2</h1> 
  <p>学习TF-IDF理论并实践，使用TF-IDF表示文本。</p> 
  <h2><a id="TFIDF_3"></a>TF-IDF</h2> 
  <p>TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文件频率).</p> 
  <h3><a id="TF_5"></a>TF</h3> 
  <p>词频，就是每个词在文本中出现的次数。<br> 假设现在有一段文本<br> corpus=[“I come to China to travel”,<br> “This is a car polupar in China”,<br> "I love tea and Apple ",<br> “The work is to write some papers in science”]<br> 不考虑停用词，处理后得到的词向量如下：</p> 
  <p>[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]<br> [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]<br> [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]<br> [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]</p> 
  <p>如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现 “come” , “China” 和 “Travel” 各出现1次，而“to“出现了两次。似乎看起来这个文本与 ”to“ 这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的 “China” 和 “Travel” 要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。</p> 
  <h3><a id="IDF_20"></a>IDF</h3> 
  <p>IDF即逆文本频率。在上面的例子中可以看到到几乎所有文本都会出现的"to"其词频虽然高，但是重要性却应该比词频低的"China"和“Travel”要低。我们的IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。</p> 
  <p>概括来讲， IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高，说明词条具有很好的类别区分能力。<br> IDF的公式为：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190407205514843.png" alt="在这里插入图片描述"></p> 
  <h3><a id="TFIDF_26"></a>TF-IDF</h3> 
  <p>将TF和IDF结合得到TF-IDF<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190407205852276.png" alt="在这里插入图片描述"></p> 
  <h2><a id="TFIDF_29"></a>使用TF-IDF表示文本</h2> 
  <p>用TF-IDF表示达观杯文本比赛的数据</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> pickle
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfVectorizer

data_path<span class="token operator">=</span><span class="token string">'E:/dataset/daguan/new_data/'</span>
<span class="token comment">#读取数据</span>
train_data<span class="token operator">=</span>pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>data_path<span class="token operator">+</span><span class="token string">"train_set.csv"</span><span class="token punctuation">)</span>
test_data<span class="token operator">=</span>pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>data_path<span class="token operator">+</span><span class="token string">"test_set.csv"</span><span class="token punctuation">)</span>
columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'article'</span><span class="token punctuation">,</span><span class="token string">'word_seg'</span><span class="token punctuation">]</span>
X_train<span class="token punctuation">,</span>X_val<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>y_val<span class="token operator">=</span>train_test_split<span class="token punctuation">(</span>train_data<span class="token punctuation">[</span>columns<span class="token punctuation">]</span><span class="token punctuation">,</span>train_data<span class="token punctuation">[</span><span class="token string">'class'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>test_size<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">2019</span><span class="token punctuation">)</span>

vectorizer <span class="token operator">=</span> TfidfVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span>
vectorizer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span><span class="token string">'word_seg'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xtrain<span class="token operator">=</span>vectorizer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span><span class="token string">'word_seg'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Xval<span class="token operator">=</span>vectorizer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span><span class="token string">'word_seg'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>Xtrain<span class="token punctuation">,</span>Xval<span class="token punctuation">)</span>
data<span class="token operator">=</span><span class="token punctuation">(</span>Xtrain<span class="token punctuation">,</span>Xval<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>y_val<span class="token punctuation">)</span>
<span class="token comment">#保存到本地</span>
fp <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'./feat/data_tfidf.pickl'</span><span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span>
pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>data<span class="token punctuation">,</span> fp<span class="token punctuation">)</span>
fp<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p>使用tf-idf表示的数据：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190407214852511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqZnd6MTUzMDc2MDI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="Reference_57"></a>Reference</h2> 
  <p><a href="https://www.cnblogs.com/pinard/p/6693230.html" rel="nofollow">https://www.cnblogs.com/pinard/p/6693230.html</a><br> <a href="https://github.com/Heitao5200/DGB/blob/master/feature/feature_code/tfidf.py" rel="nofollow">https://github.com/Heitao5200/DGB/blob/master/feature/feature_code/tfidf.py</a></p> 
  <h1><a id="Task3_60"></a>Task3</h1> 
  <h2><a id="onehot_61"></a>one-hot表示</h2> 
  <p>传统的基于规则或基于统计的自然语义处理方法将单词看作一个原子符号<br> 被称作one-hot representation。one-hot representation把每个词表示为一个长向量。这个向量的维度是词表大小，向量中只有一个维度的值为1，其余维度为0，这个维度就代表了当前的词。<br> 例如： 苹果 [0，0，0，1，0，0，0，0，0，……]<br> 不足：难以发现词之间的关系，以及难以捕捉句法（结构）和语义（意思）之间的关系</p> 
  <h2><a id="distribution_representation_66"></a>分布式表示（distribution representation）</h2> 
  <p>word embedding指的是将词转化成一种分布式表示，又称词向量。分布式表示将词表示成一个定长的连续的稠密向量。这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, …]。<br> 分布式表示优点:<br> (1)词之间存在相似关系：<br> 是词之间存在“距离”概念，这对很多自然语言处理的任务非常有帮助。<br> (2)包含更多信息：<br> 词向量能够包含更多信息，并且每一维都有特定的含义。在采用one-hot特征时，可以对特征向量进行删减，词向量则不能。<br> Dristributed representation可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。<br> 比如下图我们将词汇表里的词用"Royalty",“Masculinity”, "Femininity"和"Age"4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190409215703924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqZnd6MTUzMDc2MDI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 我们将king这个词从一个可能非常稀疏的向量坐在的空间，映射到现在这个四维向量所在的空间，必须满足以下性质：<br> （1）这个映射是单设）；<br> （2）映射之后的向量不会丢失之前的那种向量所含的信息。<br> 这个过程称为word embedding（词嵌入），即将高维词向量嵌入到一个低维空间。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190409215751859.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqZnd6MTUzMDc2MDI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="word2vec_82"></a>word2vec</h2> 
  <p>word2vec模型其实就是简单化的神经网络。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190409215900983.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqZnd6MTUzMDc2MDI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵。</p> 
  <p>这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。　Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190409215937846.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqZnd6MTUzMDc2MDI0,size_16,color_FFFFFF,t_70" alt="CBOW模型"><br> CBOW模型<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190409220039158.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqZnd6MTUzMDc2MDI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> Skip-Gram模型</p> 
  <h2><a id="_92"></a>实践</h2> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> gensim
<span class="token keyword">import</span> time
<span class="token keyword">import</span> pickle
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> csv<span class="token punctuation">,</span>sys
vector_size <span class="token operator">=</span> <span class="token number">100</span>

maxInt <span class="token operator">=</span> sys<span class="token punctuation">.</span>maxsize
decrement <span class="token operator">=</span> <span class="token boolean">True</span>
<span class="token keyword">while</span> decrement<span class="token punctuation">:</span>
    <span class="token comment"># decrease the maxInt value by factor 10</span>
    <span class="token comment"># as long as the OverflowError occurs.</span>
    decrement <span class="token operator">=</span> <span class="token boolean">False</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
        csv<span class="token punctuation">.</span>field_size_limit<span class="token punctuation">(</span>maxInt<span class="token punctuation">)</span>
    <span class="token keyword">except</span> OverflowError<span class="token punctuation">:</span>
        maxInt <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>maxInt<span class="token operator">/</span><span class="token number">10</span><span class="token punctuation">)</span>
        decrement <span class="token operator">=</span> <span class="token boolean">True</span>

<span class="token comment">#=======================================================================================================================</span>
<span class="token comment"># 0 辅助函数</span>
<span class="token comment">#=======================================================================================================================</span>

<span class="token keyword">def</span> <span class="token function">sentence2list</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> sentence<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>

start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

data_path <span class="token operator">=</span> <span class="token string">'E:/MyPython/机器学习——达观杯/data_set/'</span>
feature_path <span class="token operator">=</span> <span class="token string">'E:/MyPython/机器学习——达观杯/feature/feature_file/'</span>
proba_path <span class="token operator">=</span> <span class="token string">'E:/MyPython/机器学习——达观杯/proba/proba_file/'</span>
model_path <span class="token operator">=</span> <span class="token string">'E:/MyPython/机器学习——达观杯/model/model_file/'</span>
result_path <span class="token operator">=</span><span class="token string">"E:/MyPython/机器学习——达观杯/result/"</span>
<span class="token comment">#=======================================================================================================================</span>
<span class="token comment"># 1 准备训练数据</span>
<span class="token comment">#=======================================================================================================================</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"准备数据................ "</span><span class="token punctuation">)</span>
df_train <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>data_path <span class="token operator">+</span><span class="token string">'train_set1.csv'</span><span class="token punctuation">,</span>engine<span class="token operator">=</span><span class="token string">'python'</span><span class="token punctuation">)</span>
df_test <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>data_path <span class="token operator">+</span><span class="token string">'test_set1.csv'</span><span class="token punctuation">,</span>engine<span class="token operator">=</span><span class="token string">'python'</span><span class="token punctuation">)</span>
sentences_train <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>df_train<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token string">'word_seg'</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>sentence2list<span class="token punctuation">)</span><span class="token punctuation">)</span>
sentences_test <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>df_test<span class="token punctuation">.</span>loc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token string">'word_seg'</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>sentence2list<span class="token punctuation">)</span><span class="token punctuation">)</span>
sentences <span class="token operator">=</span> sentences_train <span class="token operator">+</span> sentences_test
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"准备数据完成! "</span><span class="token punctuation">)</span>

<span class="token comment">#=======================================================================================================================</span>
<span class="token comment"># 2 训练</span>
<span class="token comment">#=======================================================================================================================</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"开始训练................ "</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> gensim<span class="token punctuation">.</span>models<span class="token punctuation">.</span>Word2Vec<span class="token punctuation">(</span>sentences<span class="token operator">=</span>sentences<span class="token punctuation">,</span> size<span class="token operator">=</span>vector_size<span class="token punctuation">,</span> window<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> min_count<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> workers<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> sg<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">iter</span><span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"训练完成! "</span><span class="token punctuation">)</span>

<span class="token comment">#=======================================================================================================================</span>
<span class="token comment"># 3 提取词汇表及vectors,并保存</span>
<span class="token comment">#=======================================================================================================================</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">" 保存训练结果........... "</span><span class="token punctuation">)</span>
wv <span class="token operator">=</span> model<span class="token punctuation">.</span>wv
vocab_list <span class="token operator">=</span> wv<span class="token punctuation">.</span>index2word
word_idx_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
<span class="token keyword">for</span> idx<span class="token punctuation">,</span> word <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>vocab_list<span class="token punctuation">)</span><span class="token punctuation">:</span>
    word_idx_dict<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> idx
    
vectors_arr <span class="token operator">=</span> wv<span class="token punctuation">.</span>vectors
vectors_arr <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>vector_size<span class="token punctuation">)</span><span class="token punctuation">[</span>np<span class="token punctuation">.</span>newaxis<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> vectors_arr<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment">#第0位置的vector为'unk'的vector</span>

f_wordidx <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>feature_path <span class="token operator">+</span> <span class="token string">'word_seg_word_idx_dict.pkl'</span><span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span>
f_vectors <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>feature_path <span class="token operator">+</span> <span class="token string">'word_seg_vectors_arr.pkl'</span><span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span>
pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>word_idx_dict<span class="token punctuation">,</span> f_wordidx<span class="token punctuation">)</span>
pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>vectors_arr<span class="token punctuation">,</span> f_vectors<span class="token punctuation">)</span>
f_wordidx<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
f_vectors<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"训练结果已保存到该目录下！ "</span><span class="token punctuation">)</span>

end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"耗时：{}s "</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>end_time <span class="token operator">-</span> start_time<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <h1><a id="Task4_171"></a>Task4</h1> 
  <h2><a id="LR_172"></a>LR模型预测</h2> 
  <h3><a id="_173"></a>逻辑回归</h3> 
  <p>如果我们忽略二分类问题中y的取值是一个离散的取值（0或1），我们继续使用线性回归来预测y的取值。这样做会导致y的取值并不为0或1。逻辑回归使用一个函数来归一化y值，使y的取值在区间(0,1)内，这个函数称为Logistic函数(logistic function)，也称为Sigmoid函数(sigmoid function)。函数公式如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190411234544266.png" alt="在这里插入图片描述"><br> Logistic函数当z趋近于无穷大时，g(z)趋近于1；当z趋近于无穷小时，g(z)趋近于0。Logistic函数的图形如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190411234619114.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqZnd6MTUzMDc2MDI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 逻辑回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数g(z)作为最终假设函数来预测。g(z)可以将连续值映射到0到1之间。线性回归模型的表达式带入g(z)，就得到逻辑回归的表达式:<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190411234740975.png" alt="在这里插入图片描述"></p> 
  <h3><a id="LR_180"></a>用LR模型预测</h3> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV
<span class="token keyword">import</span> pickle
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>externals <span class="token keyword">import</span> joblib
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> roc_auc_score<span class="token punctuation">,</span>f1_score<span class="token punctuation">,</span>accuracy_score
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> svm
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split

feat_path<span class="token operator">=</span><span class="token string">"D:/Code/NLPPractice/feat/"</span>
model_path<span class="token operator">=</span><span class="token string">"D:/Code/NLPPractice/model/"</span>
result_path<span class="token operator">=</span><span class="token string">"D:/Code/NLPPractice/result/"</span>
<span class="token comment">#读取特征</span>
data_fp<span class="token operator">=</span><span class="token builtin">open</span><span class="token punctuation">(</span>feat_path <span class="token operator">+</span> <span class="token string">"data_w_tfidf.pkl"</span><span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span>
x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> x_test <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>data_fp<span class="token punctuation">)</span>
xTrain<span class="token punctuation">,</span> xVal<span class="token punctuation">,</span> yTrain<span class="token punctuation">,</span> yVal <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.30</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">531</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">''' #模型训练 lr = LogisticRegression(C=120,dual=True) lr.fit(xTrain,yTrain) #保存模型 joblib.dump(lr, model_path + "LR(120)_data_w_tfidf.m") '''</span>
lr<span class="token operator">=</span>joblib<span class="token punctuation">.</span>load<span class="token punctuation">(</span>model_path<span class="token operator">+</span><span class="token string">"LR(120)_data_w_tfidf.m"</span><span class="token punctuation">)</span>

<span class="token comment">#预测结果</span>
predicts <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>xVal<span class="token punctuation">)</span>
accuracy<span class="token operator">=</span>accuracy_score<span class="token punctuation">(</span>yVal<span class="token punctuation">,</span>predicts<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Accuracy :%2f%%'</span><span class="token operator">%</span> <span class="token punctuation">(</span>accuracy<span class="token operator">*</span><span class="token number">100.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

f1score<span class="token operator">=</span>f1_score<span class="token punctuation">(</span>yVal<span class="token punctuation">,</span>predicts<span class="token punctuation">,</span>average<span class="token operator">=</span><span class="token string">'micro'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'F1_score :%f1'</span><span class="token operator">%</span> f1score<span class="token punctuation">)</span>

</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190412204058676.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqZnd6MTUzMDc2MDI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 准确率：78.58%，F1评分为0.78,本来想使用gridsearchcv调参的，但是搞了半天，好像不能对LR多分类模型进行调参，也可能是我没找到，以前只对二分类调过参。</p> 
  <h2><a id="SVM_221"></a>SVM模型预测</h2> 
  <h3><a id="SVM_222"></a>SVM</h3> 
  <p>SVM中最关键的思想之一就是引入和定义了“间隔”这个概念。这个概念本身很简单，以二维空间为例，就是点到分类直线之间的距离。假设直线为y=wx+b，那么只要使所有正分类点到该直线的距离与所有负分类点到该直线的距离的总和达到最大，这条直线就是最优分类直线。这样，原问题就转化为一个约束优化问题，可以直接求解。这叫做硬间隔最大化，得到的SVM模型称作硬间隔支持向量机。<br> 但是新问题出现了，在实际应用中，我们得到的数据并不总是完美的线性可分的，其中可能会有个别噪声点，他们错误的被分类到了其他类中。如果将这些特异的噪点去除后，可以很容易的线性可分。但是，我们对于数据集中哪些是噪声点却是不知道的，如果以之前的方法进行求解，会无法进行线性分开。是不是就没办法了呢？假设在y=x+1直线上下分为两类，若两类中各有对方的几个噪点，在人的眼中，仍然是可以将两类分开的。这是因为在人脑中是可以容忍一定的误差的，仍然使用y=x+1直线分类，可以在最小误差的情况下进行最优的分类。同样的道理，我们在SVM中引入误差的概念，将其称作“松弛变量”。通过加入松弛变量，在原距离函数中需要加入新的松弛变量带来的误差，这样，最终的优化目标函数变成了两个部分组成：距离函数和松弛变量误差。这两个部分的重要程度并不是相等的，而是需要依据具体问题而定的，因此，我们加入权重参数C，将其与目标函数中的松弛变量误差相乘，这样，就可以通过调整C来对二者的系数进行调和。如果我们能够容忍噪声，那就把C调小，让他的权重降下来，从而变得不重要；反之，我们需要很严格的噪声小的模型，则将C调大一点，权重提升上去，变得更加重要。通过对参数C的调整，可以对模型进行控制。这叫做软间隔最大化，得到的SVM称作软间隔支持向量机。<br> 之前的硬间隔支持向量机和软间隔支持向量机都是解决线性可分数据集或近似线性可分数据集的问题的。但是如果噪点很多，甚至会造成数据变成了线性不可分的，那该怎么办？最常见的例子是在二维平面笛卡尔坐标系下，以原点(0,0)为圆心，以1为半径画圆，则圆内的点和圆外的点在二维空间中是肯定无法线性分开的。但是，学过初中几何就知道，对于圆圈内（含圆圈）的点：x<sup>2+y</sup>2≤1，圆圈外的则x<sup>2+y</sup>2＞1。我们假设第三个维度：z=x<sup>2+y</sup>2，那么在第三维空间中，可以通过z是否大于1来判断该点是否在圆内还是圆外。这样，在二维空间中线性不可分的数据在第三维空间很容易的线性可分了。这就是非线性支持向量机。</p> 
  <h3><a id="svm_226"></a>使用svm进行预测</h3> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV
<span class="token keyword">import</span> pickle
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>externals <span class="token keyword">import</span> joblib
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> roc_auc_score<span class="token punctuation">,</span>f1_score<span class="token punctuation">,</span>accuracy_score
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> svm
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split

feat_path<span class="token operator">=</span><span class="token string">"D:/Code/NLPPractice/feat/"</span>
model_path<span class="token operator">=</span><span class="token string">"D:/Code/NLPPractice/model/"</span>
result_path<span class="token operator">=</span><span class="token string">"D:/Code/NLPPractice/result/"</span>
<span class="token comment">#读取特征</span>
data_fp<span class="token operator">=</span><span class="token builtin">open</span><span class="token punctuation">(</span>feat_path <span class="token operator">+</span> <span class="token string">"data_w_tfidf.pkl"</span><span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span>
x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> x_test <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>data_fp<span class="token punctuation">)</span>
xTrain<span class="token punctuation">,</span> xVal<span class="token punctuation">,</span> yTrain<span class="token punctuation">,</span> yVal <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.30</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">531</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">''' #模型训练 clf = svm.LinearSVC(C=5,dual=False) clf.fit(xTrain,yTrain) #保存模型 joblib.dump(clf, model_path + "SVM(c5)_data_w_tfidf.m") '''</span>


clf<span class="token operator">=</span>joblib<span class="token punctuation">.</span>load<span class="token punctuation">(</span>model_path<span class="token operator">+</span><span class="token string">"SVM(c5)_data_w_tfidf.m"</span><span class="token punctuation">)</span>

<span class="token comment">#预测结果</span>
predicts <span class="token operator">=</span> clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>xVal<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>predicts<span class="token punctuation">)</span>
accuracy<span class="token operator">=</span>accuracy_score<span class="token punctuation">(</span>yVal<span class="token punctuation">,</span>predicts<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Accuracy :%2f%%'</span><span class="token operator">%</span> <span class="token punctuation">(</span>accuracy<span class="token operator">*</span><span class="token number">100.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

f1score<span class="token operator">=</span>f1_score<span class="token punctuation">(</span>yVal<span class="token punctuation">,</span>predicts<span class="token punctuation">,</span>average<span class="token operator">=</span><span class="token string">'micro'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'F1_score :%f1'</span><span class="token operator">%</span> f1score<span class="token punctuation">)</span>

</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190412203850795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqZnd6MTUzMDc2MDI0,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 准确率：78.78%，F1评分为0.79</p> 
  <h2><a id="Reference_271"></a>Reference</h2> 
  <p><a href="https://www.cnblogs.com/zhizhan/p/4430253.html" rel="nofollow">https://www.cnblogs.com/zhizhan/p/4430253.html</a><br> <a href="https://github.com/Heitao5200/DGB/blob/master/model/model_code/SVM_data_w_tfidf.py" rel="nofollow">https://github.com/Heitao5200/DGB/blob/master/model/model_code/SVM_data_w_tfidf.py</a></p> 
  <h1><a id="Task5_274"></a>Task5</h1> 
  <h2><a id="LightGBM_275"></a>LightGBM</h2> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV
<span class="token keyword">import</span> pickle
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>externals <span class="token keyword">import</span> joblib
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> roc_auc_score<span class="token punctuation">,</span>f1_score<span class="token punctuation">,</span>accuracy_score
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">import</span> lightgbm <span class="token keyword">as</span> LGB
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

feat_path<span class="token operator">=</span><span class="token string">"D:/Code/NLPPractice/feat/"</span>
model_path<span class="token operator">=</span><span class="token string">"D:/Code/NLPPractice/model/"</span>
result_path<span class="token operator">=</span><span class="token string">"D:/Code/NLPPractice/result/"</span>

<span class="token comment">#自定义验证集的评价函数</span>
<span class="token keyword">def</span> <span class="token function">f1_score_vali</span><span class="token punctuation">(</span>preds<span class="token punctuation">,</span> data_vali<span class="token punctuation">)</span><span class="token punctuation">:</span>
    labels <span class="token operator">=</span> data_vali<span class="token punctuation">.</span>get_label<span class="token punctuation">(</span><span class="token punctuation">)</span>
    preds <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>preds<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">19</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    score_vali <span class="token operator">=</span> f1_score<span class="token punctuation">(</span>y_true<span class="token operator">=</span>labels<span class="token punctuation">,</span> y_pred<span class="token operator">=</span>preds<span class="token punctuation">,</span> average<span class="token operator">=</span><span class="token string">'macro'</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token string">'f1_score'</span><span class="token punctuation">,</span> score_vali<span class="token punctuation">,</span> <span class="token boolean">True</span>

<span class="token comment">#读取特征</span>
data_fp<span class="token operator">=</span><span class="token builtin">open</span><span class="token punctuation">(</span>feat_path <span class="token operator">+</span> <span class="token string">"data_w_tfidf.pkl"</span><span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span>
x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> x_test <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>data_fp<span class="token punctuation">)</span>
xTrain<span class="token punctuation">,</span> xVal<span class="token punctuation">,</span> yTrain<span class="token punctuation">,</span> yVal <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.30</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">531</span><span class="token punctuation">)</span>

d_train <span class="token operator">=</span> LGB<span class="token punctuation">.</span>Dataset<span class="token punctuation">(</span>data<span class="token operator">=</span>xTrain<span class="token punctuation">,</span> label<span class="token operator">=</span>yTrain<span class="token punctuation">)</span>
d_vali <span class="token operator">=</span> LGB<span class="token punctuation">.</span>Dataset<span class="token punctuation">(</span>data<span class="token operator">=</span>xVal<span class="token punctuation">,</span> label<span class="token operator">=</span>yVal<span class="token punctuation">)</span>
<span class="token comment">#构建模型</span>
params <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">'boosting'</span><span class="token punctuation">:</span> <span class="token string">'gbdt'</span><span class="token punctuation">,</span>
    <span class="token string">'application'</span><span class="token punctuation">:</span> <span class="token string">'multiclassova'</span><span class="token punctuation">,</span>
    <span class="token string">'num_class'</span><span class="token punctuation">:</span> <span class="token number">19</span><span class="token punctuation">,</span>
    <span class="token string">'learning_rate'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>
    <span class="token string">'num_leaves'</span><span class="token punctuation">:</span> <span class="token number">31</span><span class="token punctuation">,</span>
    <span class="token string">'max_depth'</span><span class="token punctuation">:</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>
    <span class="token string">'lambda_l1'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
    <span class="token string">'lambda_l2'</span><span class="token punctuation">:</span> <span class="token number">0.5</span><span class="token punctuation">,</span>
    <span class="token string">'bagging_fraction'</span><span class="token punctuation">:</span> <span class="token number">1.0</span><span class="token punctuation">,</span>

<span class="token punctuation">}</span>
<span class="token triple-quoted-string string">''' bst = LGB.train(params, d_train, num_boost_round=800, valid_sets=d_vali, feval=f1_score_vali, early_stopping_rounds=None, verbose_eval=True) joblib.dump(bst, model_path + "LGB_data_w_tfidf.m") '''</span>
<span class="token comment">#bst=joblib.load(model_path+"LGB_data_w_tfidf.m")</span>
y_proba <span class="token operator">=</span> bst<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>xVal<span class="token punctuation">)</span>
predicts <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_proba<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">19</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>predicts<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>yVal<span class="token punctuation">)</span><span class="token punctuation">)</span>
accuracy<span class="token operator">=</span>accuracy_score<span class="token punctuation">(</span>yVal<span class="token punctuation">,</span>predicts<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Accuracy :%2f%%'</span><span class="token operator">%</span> <span class="token punctuation">(</span>accuracy<span class="token operator">*</span><span class="token number">100.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

f1score<span class="token operator">=</span>f1_score<span class="token punctuation">(</span>yVal<span class="token punctuation">,</span>predicts<span class="token punctuation">,</span>average<span class="token operator">=</span><span class="token string">'micro'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'F1_score :%f1'</span><span class="token operator">%</span> f1score<span class="token punctuation">)</span>
</code></pre> 
  <p>这个f1score和accuracy都很低，我觉得应该是轮数太少的原因，我电脑配置太差，跑下来太慢了，用gridsearchcv调参的话，应该更慢，过两天有时间再回来找下原因。</p> 
  <h2><a id="Reference_337"></a>Reference</h2> 
  <p><a href="https://github.com/Heitao5200/DGB/blob/master/model/model_code/LGB_data_w_tfidf.py" rel="nofollow">https://github.com/Heitao5200/DGB/blob/master/model/model_code/LGB_data_w_tfidf.py</a></p> 
  <h1><a id="Task6_340"></a>Task6</h1> 
  <h2><a id="_341"></a>模型融合</h2> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfVectorizer
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> svm
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> accuracy_score<span class="token punctuation">,</span> precision_score<span class="token punctuation">,</span> recall_score<span class="token punctuation">,</span> f1_score
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression
<span class="token keyword">import</span> lightgbm <span class="token keyword">as</span> lgb
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV
<span class="token keyword">import</span> gensim
<span class="token keyword">import</span> time
<span class="token keyword">import</span> pickle
<span class="token keyword">import</span> csv<span class="token punctuation">,</span>sys

<span class="token comment"># read data</span>
df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">'data/train_set.csv'</span><span class="token punctuation">,</span> nrows<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">)</span>
df<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>columns<span class="token operator">=</span><span class="token string">'article'</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># observe data</span>
<span class="token comment"># print(df['class'].value_counts(normalize=True, ascending=False))</span>

<span class="token comment"># TF-IDF</span>
vectorizer <span class="token operator">=</span> TfidfVectorizer<span class="token punctuation">(</span>ngram_range<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> min_df<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> max_df<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span> sublinear_tf<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
vectorizer<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>df<span class="token punctuation">[</span><span class="token string">'word_seg'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
x_train <span class="token operator">=</span> vectorizer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>df<span class="token punctuation">[</span><span class="token string">'word_seg'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># split training set and validation set</span>
predictor <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'word_seg'</span><span class="token punctuation">]</span>
x_train<span class="token punctuation">,</span> x_validation<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_validation <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> df<span class="token punctuation">[</span><span class="token string">'class'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>


clf <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span>C<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>
clf <span class="token operator">=</span> svm<span class="token punctuation">.</span>LinearSVC<span class="token punctuation">(</span>C<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>
clf <span class="token operator">=</span> lgb<span class="token punctuation">.</span>sklearn<span class="token punctuation">.</span>LGBMClassifier<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> n_estimators<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> num_leaves<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>

algorithms<span class="token operator">=</span><span class="token punctuation">[</span>
LogisticRegression<span class="token punctuation">(</span>C<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
svm<span class="token punctuation">.</span>LinearSVC<span class="token punctuation">(</span>C<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>
full_predictions <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> alg <span class="token keyword">in</span> algorithms<span class="token punctuation">:</span>
    <span class="token comment"># Fit the algorithm using the full training data.</span>
    alg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
    <span class="token comment"># Predict using the test dataset. We have to convert all the columns to floats to avoid an error.</span>
    predictions <span class="token operator">=</span> alg<span class="token punctuation">.</span>decision_function<span class="token punctuation">(</span>x_validation<span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    full_predictions<span class="token punctuation">.</span>append<span class="token punctuation">(</span>predictions<span class="token punctuation">)</span>


y_prediction <span class="token operator">=</span> <span class="token punctuation">(</span>full_predictions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> full_predictions<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>

<span class="token comment"># adjust labels from 1 to 19</span>
y_prediction <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_prediction<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span>


<span class="token comment"># # grid search for model</span>
<span class="token comment"># param_grid = {</span>
<span class="token comment"># 'num_leaves': [10, 20, 30],</span>
<span class="token comment"># 'learning_rate': [0.01, 0.05, 0.1],</span>
<span class="token comment"># 'n_estimators': [10, 20, 50]</span>
<span class="token comment"># }</span>
<span class="token comment"># gbm = GridSearchCV(clf, param_grid, cv=5, scoring='f1_micro', n_jobs=4, verbose=1)</span>
<span class="token comment"># gbm.fit(x_train, y_train)</span>
<span class="token comment"># print('网格搜索得到的最优参数是:', gbm.best_params_)</span>

<span class="token comment"># test model</span>
label <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    label<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token punctuation">)</span>
f1 <span class="token operator">=</span> f1_score<span class="token punctuation">(</span>y_validation<span class="token punctuation">,</span> y_prediction<span class="token punctuation">,</span> labels<span class="token operator">=</span>label<span class="token punctuation">,</span> average<span class="token operator">=</span><span class="token string">'micro'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'The F1 Score: '</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span><span class="token string">"%.4f"</span> <span class="token operator">%</span> f1<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
