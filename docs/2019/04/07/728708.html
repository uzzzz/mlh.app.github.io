<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>hadoop之HA生产集群部署 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="hadoop之HA生产集群部署" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="摘要：本文详细记载hadoop-2.6.0-cdh5.7.0在生产中HA集群部署流程，可用于学习以及生产环境部署借鉴参考。 文章目录 1.环境需求以及部署规划 1.1 硬件环境 1.2 软件环境： 1.3 进程部署规划图： 2.Hadoop Ha架构剖析 2.1 HDFS HA架构详解 2.2 YARN HA架构详解 3.HA部署流程 3.1 上传相关安装包 3.2 关闭防火墙 3.3 配置host文件 3.4 配置SSH免密码通信 3.5 部署JDK 3.6 部署ZK集群 3.6 部署HADOOP HA集群 3.7测试集群是否部署成功 4.卸载HADOOP HA集群 1.环境需求以及部署规划 1.1 硬件环境 三台阿里云主机、每台2vcore、4G内存。 1.2 软件环境： 组件名称 组件版本 Hadoop Hadoop-2.6.0-cdh5.7.0 Zookeeper Zookeeper-3.4.5 jdk Jdk-8u45-linux-x64 1.3 进程部署规划图： 主机名称 ZK NN ZkFC JN DN RM(ZKFC) NM Hadoop001 1 1 1 1 1 1 1 Hadoop002 1 1 1 1 1 1 1 Hadoop003 1 0 0 1 1 0 1 注意：1.、1表示部署在该主机上部署相应的进程，0表示不部署 2.Hadoop Ha架构剖析 2.1 HDFS HA架构详解 请参考：https://blog.csdn.net/qq_32641659/article/details/88964464 2.2 YARN HA架构详解 请参考：https://blog.csdn.net/qq_32641659/article/details/88965006 3.HA部署流程 3.1 上传相关安装包 安装包百度网盘地址： 安装包百度网盘地址： 链接：https://pan.baidu.com/s/1NfOv2ODV9ktKXM8zfaofzQ 提取码：mgwr 复制这段内容后打开百度网盘手机App，操作更方便哦 添加用户以及上传安装包： #####三台机器时执行如下命令######## useradd hadoop su - hadoop mkdir app soft lib source data exit yum install -y lrzsz #安装lrzsz软件 su - hadoop cd ~/soft/ rz #上传安装包，先上传到hadoop001，个人测试xftp传输速度大于rz #scp，将安装包传到另外两台机器，注意使用是内网ip scp -r ~/soft/* root@172.19.121.241:/home/hadoop/soft scp -r ~/soft/* root@172.19.121.242:/home/hadoop/soft [hadoop@hadoop001 soft]$ ll total 490792 -rw-r--r-- 1 root root 311585484 Apr 3 15:52 hadoop-2.6.0-cdh5.7.0.tar.gz -rw-r--r-- 1 root root 173271626 Apr 3 15:49 jdk-8u45-linux-x64.gz -rw-r--r-- 1 root root 17699306 Apr 3 15:50 zookeeper-3.4.6.tar.gz 3.2 关闭防火墙 ##三台机器都需要执行如下命令 #清空防火墙规则 [root@hadoop001 ~]# iptables -F [root@hadoop001 ~]# iptables -L #永久关闭防火墙 [root@hadoop001 ~]# service iptables stop [root@hadoop001 ~]# chkconfig iptables off [root@hadoop001 ~]# service iptables status iptables: Firewall is not running. 3.3 配置host文件 三台机器配置相同的host文件，如下（只列举了hadoop001）： #采坑1：第一第二行的内容永远不要自作聪明去改动，不然后面会遇坑的 [root@hadoop001 ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.19.121.243 hadoop001 hadoop001 172.19.121.241 hadoop002 hadoop002 172.19.121.242 hadoop003 hadoop003 [root@hadoop001 ~]# ping hadoop001 [root@hadoop001 ~]# ping hadoop002 [root@hadoop001 ~]# ping hadoop003 3.4 配置SSH免密码通信 三台机器各自生成秘钥： [root@hadoop001 ~]# su - hadoop [hadoop@hadoop001 ~]$ rm -rf ./.ssh [hadoop@hadoop001 ~]$ ssh-keygen #连续生产四个回车 [hadoop@hadoop001 ~]$ cd ~/.ssh [hadoop@hadoop001 .ssh]$ ll total 8 -rw------- 1 hadoop hadoop 1675 Apr 3 16:26 id_rsa -rw-r--r-- 1 hadoop hadoop 398 Apr 3 16:26 id_rsa.pub 合成公钥**(注意命令操作的机器)**： [hadoop@hadoop001 .ssh]$ cat id_rsa.pub &gt;&gt;authorized_keys [hadoop@hadoop002 .ssh]$ scp -r ~/.ssh/id_rsa.pub root@172.19.121.243:/home/hadoop/.ssh/id_rsa2 [hadoop@hadoop003 .ssh]$ scp -r ~/.ssh/id_rsa.pub root@172.19.121.243:/home/hadoop/.ssh/id_rsa3 [hadoop@hadoop001 .ssh]$ ll total 20 -rw-rw-r-- 1 hadoop hadoop 398 Apr 3 16:37 authorized_keys -rw------- 1 hadoop hadoop 1675 Apr 3 16:37 id_rsa -rw-r--r-- 1 root root 398 Apr 3 16:38 id_rsa2 -rw-r--r-- 1 root root 398 Apr 3 16:38 id_rsa3 -rw-r--r-- 1 hadoop hadoop 398 Apr 3 16:37 id_rsa.pub [hadoop@hadoop001 .ssh]$ cat ./id_rsa2 &gt;&gt; authorized_keys [hadoop@hadoop001 .ssh]$ cat ./id_rsa3 &gt;&gt; authorized_keys [hadoop@hadoop001 .ssh]$ cat authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAwZuESml5aeRFyAmZPhzh0WG3waHqGChV4SHWBkjHrkcisLpqpXXotEn0Ap1yWuPYCUKNLIgyLD8tSubnLyj5nNdOXPYnzSyTw0NVIKzKkhLqrYMnpTrckodGjwkhSlaZbIRngBHGB7cUOW8AaWeA79UzEydr1/8Q/arizt82R/K8+t0SAIsk1MUu7+oUGJAzPXpNU76pq69ARb/hJUs0xRMMjOFetqrp8dh8pHoBjgcgUX+fyc5FB/dqJlaCXNJDmNtWclOo8flprB27qj4+1jfCs78wU6AAfewQqo4jJ/2NoD527Vu/SDGysQdlsKpSYBygLB1+/oR46sH1iUJTew== hadoop@hadoop001 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAykZ7nWRo+dmiMuaTALybK1S7XI/pgZgbpTmQAw3IIC1CwFWVZIRuF8eSCL4wgj16pKbKcfczN/9aYhOq0zsUgaa8LlzI6D2DKU1hzak43dCFcnNM/lBkF3QrkE0m9jfM6wmVozdflvRiM+GygEhydfbWSpJcMmPCmV+scRUFjRuH0AuWlwm7sRBxXbK3w4PpWfMF0ie4ZEbviO4PK+E3BxL4xT93N3fELF0s1ayK0mHOfDGBEkFBRp5vIVU//puFU0pW/2/db/laiA8xO1kHLPaFRwVl/I17yNkGUJjF0goeavtVMkxwckd5FsqFIdVecPZ5ReyObbasjbQlvL4uFQ== hadoop@hadoop002 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAw5v6nMHGJmzVHgC1gg/3QbP8qT2ljoBYcS9WaMdSNUjG/WVfvcRWSA1KlACwjG+8RmlHZkR4OTVAIBlMPMDObhjXK6J4hKGicINNsfB+E0etPczDneFCxZHwf9UQ/7J8g/KoAdmE+ROUWKzdw+q2QOcY5Yhbn7FSzF28CK826HPi5L6WXQlBolvlI4x6hn7vscwqpI7cu2YFLkp2bk5lEoatXShSxHi2MTxoyqrtuSpYZhybuExfDjDOPOXX0zpP/Gj7cUHTRuJrUtqiq+G71L+BhmD5cIsTwguBEXrWF+lsXOXTx2TyBXtc7kbvArE6XKee2sjshE52Kn7ko6ZhtQ== hadoop@hadoop003 [hadoop@hadoop001 .ssh]$ rm -rf id_rsa2 id_rsa3 [hadoop@hadoop001 .ssh]$ scp -r ~/.ssh/authorized_keys root@172.19.121.241:/home/hadoop/.ssh/ [hadoop@hadoop001 .ssh]$ scp -r ~/.ssh/authorized_keys root@172.19.121.242:/home/hadoop/.ssh/ #很重要，若authorized_keys属于非root用户必须将权限设置为600 [hadoop@hadoop001 ~]$ chmod 600 ./.ssh/authorized_keys 互相ssh免秘钥测试，用户第一次ssh会有确认选项 规则：ssh 远程机器执行date命令，不需要输入密码则，则ssh免密码配置成功 [hadoop@hadoop001 ~]$ ssh hadoop001 date [hadoop@hadoop001 ~]$ ssh hadoop002 date [hadoop@hadoop001 ~]$ ssh hadoop003 date [hadoop@hadoop002 ~]$ ssh hadoop001 date [hadoop@hadoop002 ~]$ ssh hadoop002 date [hadoop@hadoop002 ~]$ ssh hadoop003 date [hadoop@hadoop003 ~]$ ssh hadoop001 date [hadoop@hadoop003 ~]$ ssh hadoop002 date [hadoop@hadoop003 ~]$ ssh hadoop003 date 3.5 部署JDK 三台机器同时执行如下命令 #采坑1: 必须为/usr/java/，该目录是cdh默认的jdk目录，若不为该目录，后面一定会采坑。 [root@hadoop003 ~]# mkdir /usr/java/ [root@hadoop001 ~]# tar -zxvf /home/hadoop/soft/jdk-8u45-linux-x64.gz -C /usr/java/ #采坑2:权限必须变更，jdk解压的所属用户很奇怪，后续使用中可能会报类找不到错误 [root@hadoop001 ~]# chown -R root:root /usr/java 配置JDK环境变量 [root@hadoop001 ~]# vim /etc/profile #追加如下两行配置 export JAVA_HOME=/usr/java/jdk1.8.0_45 export PATH=$JAVA_HOME/bin:$PATH [root@hadoop001 ~]# source /etc/profile #更新环境变量文件 [root@hadoop001 ~]# java -version java version &quot;1.8.0_45&quot; Java(TM) SE Runtime Environment (build 1.8.0_45-b14) Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode) [root@hadoop001 ~]# which java /usr/java/jdk1.8.0_45/bin/java 3.6 部署ZK集群 解压ZK安装包 [root@hadoop001 ~]$ su -hadoop [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/zookeeper-3.4.6.tar.gz -C ~/app/ [hadoop@hadoop001 ~]$ ln -s ~/app/zookeeper-3.4.6 ~/app/zookeeper 添加环境变量 #编辑hadoop用户环境变量文件添加如下内容 [hadoop@hadoop001 bin]$ vim ~/.bash_profile export ZOOKEEPER_HOME=/home/hadoop/app/zookeeper export PATH=$ZOOKEEPER_HOME/bin:$PATH [hadoop@hadoop001 bin]$ source ~/.bash_profile [hadoop@hadoop001 bin]$ which zkServer.sh ~/app/zookeeper/bin/zkServer.sh 修改zookeeper配置 [hadoop@hadoop001 conf]$ mkdir ~/data/zkdata/data [hadoop@hadoop001 app]$ cd ~/app/zookeeper/conf/ [hadoop@hadoop001 conf]$ cp zoo_sample.cfg zoo.cfg #添加或修改如下配置 [hadoop@hadoop001 conf]$ vim zoo.cfg dataDir=/home/hadoop/data/zkdata/data server.1=hadoop001:2888:3888 server.2=hadoop002:2888:3888 server.3=hadoop003:2888:3888 #在数据目录创建myid文件，并将标识1传入 [hadoop@hadoop001 conf]$ cd ~/data/zkdata/data/ [hadoop@hadoop001 data]$ echo 1 &gt;myid #将配置文件复制一份到hadoop002、hadoop003 [hadoop@hadoop001 data]$ scp ~/app/zookeeper/conf/zoo.cfg hadoop002:~/app/zookeeper/conf/ [hadoop@hadoop001 data]$ scp ~/app/zookeeper/conf/zoo.cfg hadoop003:~/app/zookeeper/conf/ [hadoop@hadoop001 data]$ scp ~/data/zkdata/data/myid hadoop002:~/data/zkdata/data/ [hadoop@hadoop001 data]$ scp ~/data/zkdata/data/myid hadoop003:~/data/zkdata/data/ #更改hadoop002、hadoop003的myid文件，将标识改为如下内容 [hadoop@hadoop002 ~]$ cat ~/data/zkdata/data/myid 2 [hadoop@hadoop003 ~]$ cat ~/data/zkdata/data/myid 3 启动zk集群，三台集群都需要执行如下命令： [hadoop@hadoop001 data]$ cd ~/app/zookeeper/bin [hadoop@hadoop001 bin]$ ./zkServer.sh start 查询ZK集群状态： #查询zk节点状态 [hadoop@hadoop003 bin]$ ./zkServer.sh status #查看QuorumPeerMain进程是否启动 [hadoop@hadoop002 bin]$ jps -l 3026 org.apache.zookeeper.server.quorum.QuorumPeerMain 若发现集群状态异常，异常的报错以及解决方法如下： #异常信息 [hadoop@hadoop003 bin]$ ./zkServer.sh status JMX enabled by default Using config: /home/hadoop/app/zookeeper/bin/../conf/status grep: /home/hadoop/app/zookeeper/bin/../conf/status: No such file or directory mkdir: cannot create directory `&#39;: No such file or directory Starting zookeeper ... ./zkServer.sh: line 113: /zookeeper_server.pid: Permission denied FAILED TO WRITE PID ###查询日志，观察详细的错误信息 #寻找日志文件，日志文件名称是通过搜索启动脚本发现的 [hadoop@hadoop001 bin]$ find /home/hadoop -name &quot;zookeeper.out&quot; /home/hadoop/app/zookeeper-3.4.6/bin/zookeeper.out [hadoop@hadoop001 bin]$ vim /home/hadoop/app/zookeeper-3.4.6/bin/zookeeper.out 2019-04-03 22:23:55,976 [myid:] - INFO [main:QuorumPeerConfig@103] - Reading configuration from: /home/hadoop/app/zookeeper/bin/../conf/status 2019-04-03 22:23:55,979 [myid:] - ERROR [main:QuorumPeerMain@85] - Invalid config, exiting abnormally org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Error processing /home/hadoop/app/zookeeper/bin/../conf/status at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:123) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:101) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78) Caused by: java.lang.IllegalArgumentException: /home/hadoop/app/zookeeper/bin/../conf/status file is missing at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:107) ... 2 more Invalid config, exiting abnormally #简单分析日志，发现读取的配置文件竟然是/home/hadoop/app/zookeeper/bin/../conf/status文件，很是奇怪（可能是我一开始没有配置环境变量的原因），重启 集群。 发现一切正常。hadoop002 是lead节点，其它为follower节点 [hadoop@hadoop002 bin]$ ./zkServer.sh status JMX enabled by default Using config: /home/hadoop/app/zookeeper/bin/../conf/zoo.cfg Mode: leader 3.6 部署HADOOP HA集群 解压并添加环境变量，三台机器同时执行 [hadoop@hadoop001 bin]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app [hadoop@hadoop001 bin]$ ln -s ~/app/hadoop-2.6.0-cdh5.7.0 ~/app/hadoop [hadoop@hadoop001 bin]$ vim ~/.bash_profile [hadoop@hadoop001 bin]$ cat ~/.bash_profile #添加或修改为如下内容 PATH=$PATH:$HOME/bin export PATH export ZOOKEEPER_HOME=/home/hadoop/app/zookeeper export HADOOP_HOME=/home/hadoop/app/hadoop export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin:$PATH [hadoop@hadoop001 bin]$ source ~/.bash_profile #更新环境变量 创建数据目录，三台机器同时执行 [hadoop@hadoop001 ~]$ mkdir -p ~/app/hadoop-2.6.0-cdh5.7.0/tmp #创建临时目录，由core-site.xml文件配置hadoop.tmp.dir所配置 [hadoop@hadoop003 ~]$ mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name #创建hdfs的namenode数据（fsimage）目录，由hdfs-site.xml的dfs.namenode.name.dir所配置 [hadoop@hadoop003 ~]$ mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/data #创建hdfs的datanode数据目录，由hdfs-site.xm所配置 [hadoop@hadoop003 ~]$ mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/jn #创建hdfs的journalnode数据目录，由hdfs-site.xm所配置 修改五个配置文件，三台机器同时执行 [hadoop@hadoop003 hadoop]$ cd ~/app/hadoop/etc/hadoop [hadoop@hadoop003 hadoop]$ rm -rf core-site.xml hdfs-site.xml yarn-site.xml slaves #删除已有的配置文件 [hadoop@hadoop003 hadoop]$ rz [hadoop@hadoop003 hadoop]$ scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop001:/home/hadoop/app/hadoop/etc/hadoop [hadoop@hadoop003 hadoop]$ scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop002:/home/hadoop/app/hadoop/etc/hadoop [hadoop@hadoop003 hadoop]$ cat slaves #注意 这三行与最后一行并不连在一起，采坑 hadoop001 hadoop002 hadoop003 [hadoop@hadoop003 hadoop]$ 五个配置文件百度网盘链接如下： 链接：https://pan.baidu.com/s/1lQCWc62nccn61gHEztSbyg 提取码：2rgm 复制这段内容后打开百度网盘手机App，操作更方便哦 core-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!--Yarn 需要使用 fs.defaultFS 指定NameNode URI --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ruozeclusterg6&lt;/value&gt; &lt;/property&gt; &lt;!--==============================Trash机制======================================= --&gt; &lt;property&gt; &lt;!--多长时间创建CheckPoint NameNode截点上运行的CheckPointer 从Current文件夹创建CheckPoint;默认：0 由fs.trash.interval项指定 --&gt; &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少分钟.Trash下的CheckPoint目录会被删除,该配置服务器设置优先级大于客户端，默认：0 不删除 --&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;/property&gt; &lt;!--指定hadoop临时目录, hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配 置namenode和datanode的存放位置，默认就放在这&gt;个路径中 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!--指定ZooKeeper超时间隔，单位毫秒 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;!--使用hadoop用户以及用户组代理集群上所有的用户用户组，注意必须是进程启动用户 --&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;!--设置支持的压缩格式，若不支持，若组件不支持任何压缩格式，应当注销本配置 --&gt; &lt;!--&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec &lt;/value&gt; &lt;/property&gt;--&gt; &lt;/configuration&gt; hdfs-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!--HDFS超级用户，必须是启动用户 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt; &lt;/property&gt; &lt;!--开启web hdfs --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name&lt;/value&gt; &lt;description&gt; namenode 存放name table(fsimage)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;${dfs.namenode.name.dir}&lt;/value&gt; &lt;description&gt;namenode粗放 transaction file(edits)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/data&lt;/value&gt; &lt;description&gt;datanode存放block本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 块大小256M （默认128M） --&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;268435456&lt;/value&gt; &lt;/property&gt; &lt;!--======================================================================= --&gt; &lt;!--HDFS高可用配置 --&gt; &lt;!--指定hdfs的nameservice为ruozeclusterg6,需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ruozeclusterg6&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置NameNode IDs 此版本最大只支持两个NameNode --&gt; &lt;name&gt;dfs.ha.namenodes.ruozeclusterg6&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.rpc-address.[nameservice ID] rpc 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ruozeclusterg6.nn1&lt;/name&gt; &lt;value&gt;hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ruozeclusterg6.nn2&lt;/name&gt; &lt;value&gt;hadoop002:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.http-address.[nameservice ID] http 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ruozeclusterg6.nn1&lt;/name&gt; &lt;value&gt;hadoop001:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ruozeclusterg6.nn2&lt;/name&gt; &lt;value&gt;hadoop002:50070&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode editlog同步 ============================================ --&gt; &lt;!--保证数据恢复 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.http-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8480&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.rpc-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8485&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置JournalNode服务器地址，QuorumJournalManager 用于存储editlog --&gt; &lt;!--格式：qjournal://&lt;host1:port1&gt;;&lt;host2:port2&gt;;&lt;host3:port3&gt;/&lt;journalId&gt; 端口同journalnode.rpc-address --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/ruozeclusterg6&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--JournalNode存放数据地址 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/jn&lt;/value&gt; &lt;/property&gt; &lt;!--==================DataNode editlog同步 ============================================ --&gt; &lt;property&gt; &lt;!--DataNode,Client连接Namenode识别选择Active NameNode策略 --&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ruozeclusterg6&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode fencing：=============================================== --&gt; &lt;!--Failover后防止停掉的Namenode启动，造成两个服务 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少milliseconds 认为fencing失败 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;!--==================NameNode auto failover base ZKFC and Zookeeper====================== --&gt; &lt;!--开启基于Zookeeper --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--动态许可datanode连接namenode列表 --&gt; &lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/slaves&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!-- 配置 MapReduce Applications --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- JobHistory Server ============================================================== --&gt; &lt;!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop001:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop001:19888&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 Map段输出的压缩,snappy，注意若，为hadoop为编译集成压缩格式，应注销本配置--&gt; &lt;!-- &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;/property&gt;--&gt; &lt;/configuration&gt; yarn-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!-- nodemanager 配置 ================================================= --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.localizer.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23344&lt;/value&gt; &lt;description&gt;Address where the localizer IPC is.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.webapp.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23999&lt;/value&gt; &lt;description&gt;NM Webapp address.&lt;/description&gt; &lt;/property&gt; &lt;!-- HA 配置 =============================================================== --&gt; &lt;!-- Resource Manager Configs --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.connect.retry-interval.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 使嵌入式自动故障转移。HA环境启动，与 ZKRMStateStore 配合 处理fencing --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.embedded&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群名称，确保HA选举时对应的集群 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarn-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!--这里RM主备结点需要单独指定,（可选） &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm2&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms&lt;/name&gt; &lt;value&gt;5000&lt;/value&gt; &lt;/property&gt; &lt;!-- ZKRMStateStore 配置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk.state-store.address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- Client访问RM的RPC地址 (applications manager interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23140&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23140&lt;/value&gt; &lt;/property&gt; &lt;!-- AM访问RM的RPC地址(scheduler interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23130&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23130&lt;/value&gt; &lt;/property&gt; &lt;!-- RM admin interface --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23141&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23141&lt;/value&gt; &lt;/property&gt; &lt;!--NM访问RM的RPC端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23125&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23125&lt;/value&gt; &lt;/property&gt; &lt;!-- RM web application 地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop001:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;discription&gt;单个任务可申请最少内存，默认1024MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;discription&gt;单个任务可申请最大内存，默认8192MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; slaves文件如下： hadoop001 hadoop002 hadoop003 设置JDK的绝对路径(采坑)。三台都需要设置 [hadoop@hadoop001 hadoop]$ cat hadoop-env.sh |grep JAVA #如下 已设置jdk的绝对路径 # The only required environment variable is JAVA_HOME. All others are # set JAVA_HOME in this file, so that it is correctly defined on export JAVA_HOME=/usr/java/jdk1.8.0_45 #HADOOP_JAVA_PLATFORM_OPTS=&quot;-XX:-UsePerfData $HADOOP_JAVA_PLATFORM_OPTS&quot; 启动HA集群： #确保zk集群是启动的 [hadoop@hadoop003 hadoop]$ zkServer.sh status JMX enabled by default Using config: /home/hadoop/app/zookeeper/bin/../conf/zoo.cfg Mode: leader #启动journalNode守护进程，三台同时执行 [hadoop@hadoop002 sbin]$ cd ~/app/hadoop/bin #删除所有的windows命令 [hadoop@hadoop002 sbin]$ rm -rf *.cmd [hadoop@hadoop002 sbin]$ cd ~/app/hadoop/sbin [hadoop@hadoop002 sbin]$ rm -rf *.cmd [hadoop@hadoop002 sbin]$ ./hadoop-daemon.sh start journalnode [hadoop@hadoop003 sbin]$ jps 1868 JournalNode 1725 QuorumPeerMain 1919 Jps #格式化namenode，注意只要hadoop001格式化即可，格式化成功标志，日志输出successfully formatted信息如下 [hadoop@hadoop001 sbin]$ hadoop namenode -format ...... : Storage directory /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name has been successfully formatted. 19/04/06 19:50:08 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0 19/04/06 19:50:08 INFO util.ExitUtil: Exiting with status 0 19/04/06 19:50:08 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at hadoop001/172.19.121.243 ************************************************************/ [hadoop@hadoop001 sbin]$ scp -r ~/app/hadoop/data/ hadoop002:/home/hadoop/app/hadoop/ #将nn的数据发一份到hadoop002 #格式化zkfc，只要hadoop001执行即可,成功后会在zk的创建hadoop-ha/ruozeclusterg6，如下信息： [hadoop@hadoop001 sbin]$ hdfs zkfc -formatZK .... 19/04/06 20:03:02 INFO ha.ActiveStandbyElector: Session connected. 19/04/06 20:03:02 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/ruozeclusterg6 in ZK. #启动hdfs，只要hadoop001执行即可 [hadoop@hadoop001 sbin]$ start-dfs.sh #若出现如下错误，且jps发现datanode进程未启动，原因是slaves文件被污染，删除，重新编辑一份。 ····· : Name or service not knownstname hadoop003 : Name or service not knownstname hadoop001 : Name or service not knownstname hadoop002 [hadoop@hadoop002 current]$ rm -rf ~/app/hadoop/etc/hadoop/slaves [hadoop@hadoop002 current]$ vim ~/app/hadoop/etc/hadoop/slaves #添加DN节点信息 hadoop001 had00p002 hadoop003 ····· #重新启动hdfs，会共启动NN、DN、JN、ZKFC四个守护进程，停止hdfs，stop--dfs.sh [hadoop@hadoop001 sbin]$ start-dfs.sh 19/04/06 20:51:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [hadoop001 hadoop002] hadoop001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop001.out hadoop002: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop002.out hadoop002: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop002.out hadoop003: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop003.out hadoop001: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop001.out Starting journal nodes [hadoop001 hadoop002 hadoop003] hadoop001: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop001.out hadoop003: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop003.out hadoop002: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop002.out 19/04/06 20:52:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting ZK Failover Controllers on NN hosts [hadoop001 hadoop002] hadoop002: starting zkfc, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-zkfc-hadoop002.out hadoop001: starting zkfc, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-zkfc-hadoop001.out [hadoop@hadoop001 sbin]$ jps 5504 NameNode 5797 JournalNode 5606 DataNode 6054 Jps 1625 QuorumPeerMain 5983 DFSZKFailoverController #启动yarn，首先在hadoop001执行即可，此时从日志中可以看出只启动了一台RM， #另一个RM需手动前往hadoop002去启动 [hadoop@hadoop001 sbin]$ start-yarn.sh starting yarn daemons starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop001.out hadoop001: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop001.out hadoop002: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop002.out hadoop003: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop003.out [hadoop@hadoop002 current]$ yarn-daemon.sh start resourcemanager starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop002.out #启动jobhistory服务，在hadoop001上执行即可 [hadoop@hadoop001 sbin]$ mr-jobhistory-daemon.sh start historyserver starting historyserver, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/mapred-hadoop-historyserver-hadoop001.out [hadoop@hadoop001 sbin]$ jps 5504 NameNode 6211 NodeManager 6116 ResourceManager 5797 JournalNode 5606 DataNode 1625 QuorumPeerMain 7037 JobHistoryServer 7118 Jps 5983 DFSZKFailoverController 3.7测试集群是否部署成功 通过命令空间操作hdfs文件 [hadoop@hadoop002 current]$ hdfs dfs -ls hdfs://ruozeclusterg6/ [hadoop@hadoop002 current]$ hdfs dfs -put ~/app/hadoop/README.txt hdfs://ruozeclusterg6/ 19/04/06 21:08:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable [hadoop@hadoop002 current]$ hdfs dfs -ls hdfs://ruozeclusterg6/ 19/04/06 21:08:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Found 1 items -rw-r--r-- 3 hadoop hadoop 1366 2019-04-06 21:08 hdfs://ruozeclusterg6/README.txt web界面访问 配置阿里云安全组规则,出入方向放行所有端口 配置windos的hosts文件 web访问hadoop001的hdfs页面，具体谁是active有ZK决定 web访问hadoop001的hdfs页面，具体谁是standby有ZK决定 web访问hadoop001的yarn active界面 web访问hadoop002的yarn standby界面直接访问hadoop002:8088地址会被强制跳转hadoop001的地址。应通过如下地址（ip:8088/cluster/cluster）访问 web访问jobhistroy页面，我启动在hadoop001，故访问地址为hadoop001，端口通过netstat进程可查询到， 测试MR代码,此时可从yarn以及jobhistory的web界面上看到任务情况 [hadoop@hadoop001 sbin]$ find ~/app/hadoop/* -name &#39;*example*.jar&#39; [hadoop@hadoop001 sbin]$ hadoop jar /home/hadoop/app/hadoop/share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 5 10 4.卸载HADOOP HA集群 停止hadoop的守护进程 stop-all.sh mr-jobhistory-daemon.sh stop historyserver #执行停止脚本后，查询是否还有hadoop相关进程，若有，直接kill -9 [hadoop@hadoop001 sbin]$ ps -ef | grep hadoop 删除zk上所有关于hadoop的信息 [hadoop@hadoop001 sbin]$ zkCli.sh #进入zk客户端，删除所有hadoop的配置 [zk: localhost:2181(CONNECTED) 0] ls / [zookeeper, hadoop-ha] [zk: localhost:2181(CONNECTED) 1] rmr /hadoop-ha [zk: localhost:2181(CONNECTED) 1] quit Quitting... 清空data数据目录 rm -rf ~/app/hadoop/data/* 扩展1：生产中若遇到两个节点同为stand by状态时（无法HA），通常是ZK夯住了，需检查ZK状态。 扩展2：生产中若某台机器秘钥文件发生变更，不要傻傻的将known_hosts的文件清空，只要找到变更的机器所属的信息，删除即可。清空会影响其他应用登录，正产使用（若known_hosts无改机器登录信息，第一次需要输入yes，写一份信息在known_hosts上），要背锅的。 扩展3：生产中若遇到异常，首先检查错误信息，再检查配置、其次分析运行日志。若是启动或关闭报错，可debug 启动的脚本。sh -x XXX.sh 方式来debug脚本。 注意没有+表示脚本的输出内容，一个+表示当前行执行语法执行结果，两个++表示当前行某部分语法的执行结果。 扩展4：hadoop chechnative 命令可检测hadoop支持的压缩格式，false表示不支持，CDH版本的hadoop不支持压缩，身产中需要编译支持压缩。map阶段通常选择snappy格式压缩，因为snappy压缩速度最快（快速输出，当然压缩比最低），reduce阶段通常选择gzip或bzip2(压缩比最大，占最小磁盘空间，当然压缩解压时间最久) 扩展5 可通过，start-all.sh 或者stop-all.sh，来启动关闭hadoop集群 扩展6 cat * |grep xxx 命令查找当前文件夹下所有的文件内容" />
<meta property="og:description" content="摘要：本文详细记载hadoop-2.6.0-cdh5.7.0在生产中HA集群部署流程，可用于学习以及生产环境部署借鉴参考。 文章目录 1.环境需求以及部署规划 1.1 硬件环境 1.2 软件环境： 1.3 进程部署规划图： 2.Hadoop Ha架构剖析 2.1 HDFS HA架构详解 2.2 YARN HA架构详解 3.HA部署流程 3.1 上传相关安装包 3.2 关闭防火墙 3.3 配置host文件 3.4 配置SSH免密码通信 3.5 部署JDK 3.6 部署ZK集群 3.6 部署HADOOP HA集群 3.7测试集群是否部署成功 4.卸载HADOOP HA集群 1.环境需求以及部署规划 1.1 硬件环境 三台阿里云主机、每台2vcore、4G内存。 1.2 软件环境： 组件名称 组件版本 Hadoop Hadoop-2.6.0-cdh5.7.0 Zookeeper Zookeeper-3.4.5 jdk Jdk-8u45-linux-x64 1.3 进程部署规划图： 主机名称 ZK NN ZkFC JN DN RM(ZKFC) NM Hadoop001 1 1 1 1 1 1 1 Hadoop002 1 1 1 1 1 1 1 Hadoop003 1 0 0 1 1 0 1 注意：1.、1表示部署在该主机上部署相应的进程，0表示不部署 2.Hadoop Ha架构剖析 2.1 HDFS HA架构详解 请参考：https://blog.csdn.net/qq_32641659/article/details/88964464 2.2 YARN HA架构详解 请参考：https://blog.csdn.net/qq_32641659/article/details/88965006 3.HA部署流程 3.1 上传相关安装包 安装包百度网盘地址： 安装包百度网盘地址： 链接：https://pan.baidu.com/s/1NfOv2ODV9ktKXM8zfaofzQ 提取码：mgwr 复制这段内容后打开百度网盘手机App，操作更方便哦 添加用户以及上传安装包： #####三台机器时执行如下命令######## useradd hadoop su - hadoop mkdir app soft lib source data exit yum install -y lrzsz #安装lrzsz软件 su - hadoop cd ~/soft/ rz #上传安装包，先上传到hadoop001，个人测试xftp传输速度大于rz #scp，将安装包传到另外两台机器，注意使用是内网ip scp -r ~/soft/* root@172.19.121.241:/home/hadoop/soft scp -r ~/soft/* root@172.19.121.242:/home/hadoop/soft [hadoop@hadoop001 soft]$ ll total 490792 -rw-r--r-- 1 root root 311585484 Apr 3 15:52 hadoop-2.6.0-cdh5.7.0.tar.gz -rw-r--r-- 1 root root 173271626 Apr 3 15:49 jdk-8u45-linux-x64.gz -rw-r--r-- 1 root root 17699306 Apr 3 15:50 zookeeper-3.4.6.tar.gz 3.2 关闭防火墙 ##三台机器都需要执行如下命令 #清空防火墙规则 [root@hadoop001 ~]# iptables -F [root@hadoop001 ~]# iptables -L #永久关闭防火墙 [root@hadoop001 ~]# service iptables stop [root@hadoop001 ~]# chkconfig iptables off [root@hadoop001 ~]# service iptables status iptables: Firewall is not running. 3.3 配置host文件 三台机器配置相同的host文件，如下（只列举了hadoop001）： #采坑1：第一第二行的内容永远不要自作聪明去改动，不然后面会遇坑的 [root@hadoop001 ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.19.121.243 hadoop001 hadoop001 172.19.121.241 hadoop002 hadoop002 172.19.121.242 hadoop003 hadoop003 [root@hadoop001 ~]# ping hadoop001 [root@hadoop001 ~]# ping hadoop002 [root@hadoop001 ~]# ping hadoop003 3.4 配置SSH免密码通信 三台机器各自生成秘钥： [root@hadoop001 ~]# su - hadoop [hadoop@hadoop001 ~]$ rm -rf ./.ssh [hadoop@hadoop001 ~]$ ssh-keygen #连续生产四个回车 [hadoop@hadoop001 ~]$ cd ~/.ssh [hadoop@hadoop001 .ssh]$ ll total 8 -rw------- 1 hadoop hadoop 1675 Apr 3 16:26 id_rsa -rw-r--r-- 1 hadoop hadoop 398 Apr 3 16:26 id_rsa.pub 合成公钥**(注意命令操作的机器)**： [hadoop@hadoop001 .ssh]$ cat id_rsa.pub &gt;&gt;authorized_keys [hadoop@hadoop002 .ssh]$ scp -r ~/.ssh/id_rsa.pub root@172.19.121.243:/home/hadoop/.ssh/id_rsa2 [hadoop@hadoop003 .ssh]$ scp -r ~/.ssh/id_rsa.pub root@172.19.121.243:/home/hadoop/.ssh/id_rsa3 [hadoop@hadoop001 .ssh]$ ll total 20 -rw-rw-r-- 1 hadoop hadoop 398 Apr 3 16:37 authorized_keys -rw------- 1 hadoop hadoop 1675 Apr 3 16:37 id_rsa -rw-r--r-- 1 root root 398 Apr 3 16:38 id_rsa2 -rw-r--r-- 1 root root 398 Apr 3 16:38 id_rsa3 -rw-r--r-- 1 hadoop hadoop 398 Apr 3 16:37 id_rsa.pub [hadoop@hadoop001 .ssh]$ cat ./id_rsa2 &gt;&gt; authorized_keys [hadoop@hadoop001 .ssh]$ cat ./id_rsa3 &gt;&gt; authorized_keys [hadoop@hadoop001 .ssh]$ cat authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAwZuESml5aeRFyAmZPhzh0WG3waHqGChV4SHWBkjHrkcisLpqpXXotEn0Ap1yWuPYCUKNLIgyLD8tSubnLyj5nNdOXPYnzSyTw0NVIKzKkhLqrYMnpTrckodGjwkhSlaZbIRngBHGB7cUOW8AaWeA79UzEydr1/8Q/arizt82R/K8+t0SAIsk1MUu7+oUGJAzPXpNU76pq69ARb/hJUs0xRMMjOFetqrp8dh8pHoBjgcgUX+fyc5FB/dqJlaCXNJDmNtWclOo8flprB27qj4+1jfCs78wU6AAfewQqo4jJ/2NoD527Vu/SDGysQdlsKpSYBygLB1+/oR46sH1iUJTew== hadoop@hadoop001 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAykZ7nWRo+dmiMuaTALybK1S7XI/pgZgbpTmQAw3IIC1CwFWVZIRuF8eSCL4wgj16pKbKcfczN/9aYhOq0zsUgaa8LlzI6D2DKU1hzak43dCFcnNM/lBkF3QrkE0m9jfM6wmVozdflvRiM+GygEhydfbWSpJcMmPCmV+scRUFjRuH0AuWlwm7sRBxXbK3w4PpWfMF0ie4ZEbviO4PK+E3BxL4xT93N3fELF0s1ayK0mHOfDGBEkFBRp5vIVU//puFU0pW/2/db/laiA8xO1kHLPaFRwVl/I17yNkGUJjF0goeavtVMkxwckd5FsqFIdVecPZ5ReyObbasjbQlvL4uFQ== hadoop@hadoop002 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAw5v6nMHGJmzVHgC1gg/3QbP8qT2ljoBYcS9WaMdSNUjG/WVfvcRWSA1KlACwjG+8RmlHZkR4OTVAIBlMPMDObhjXK6J4hKGicINNsfB+E0etPczDneFCxZHwf9UQ/7J8g/KoAdmE+ROUWKzdw+q2QOcY5Yhbn7FSzF28CK826HPi5L6WXQlBolvlI4x6hn7vscwqpI7cu2YFLkp2bk5lEoatXShSxHi2MTxoyqrtuSpYZhybuExfDjDOPOXX0zpP/Gj7cUHTRuJrUtqiq+G71L+BhmD5cIsTwguBEXrWF+lsXOXTx2TyBXtc7kbvArE6XKee2sjshE52Kn7ko6ZhtQ== hadoop@hadoop003 [hadoop@hadoop001 .ssh]$ rm -rf id_rsa2 id_rsa3 [hadoop@hadoop001 .ssh]$ scp -r ~/.ssh/authorized_keys root@172.19.121.241:/home/hadoop/.ssh/ [hadoop@hadoop001 .ssh]$ scp -r ~/.ssh/authorized_keys root@172.19.121.242:/home/hadoop/.ssh/ #很重要，若authorized_keys属于非root用户必须将权限设置为600 [hadoop@hadoop001 ~]$ chmod 600 ./.ssh/authorized_keys 互相ssh免秘钥测试，用户第一次ssh会有确认选项 规则：ssh 远程机器执行date命令，不需要输入密码则，则ssh免密码配置成功 [hadoop@hadoop001 ~]$ ssh hadoop001 date [hadoop@hadoop001 ~]$ ssh hadoop002 date [hadoop@hadoop001 ~]$ ssh hadoop003 date [hadoop@hadoop002 ~]$ ssh hadoop001 date [hadoop@hadoop002 ~]$ ssh hadoop002 date [hadoop@hadoop002 ~]$ ssh hadoop003 date [hadoop@hadoop003 ~]$ ssh hadoop001 date [hadoop@hadoop003 ~]$ ssh hadoop002 date [hadoop@hadoop003 ~]$ ssh hadoop003 date 3.5 部署JDK 三台机器同时执行如下命令 #采坑1: 必须为/usr/java/，该目录是cdh默认的jdk目录，若不为该目录，后面一定会采坑。 [root@hadoop003 ~]# mkdir /usr/java/ [root@hadoop001 ~]# tar -zxvf /home/hadoop/soft/jdk-8u45-linux-x64.gz -C /usr/java/ #采坑2:权限必须变更，jdk解压的所属用户很奇怪，后续使用中可能会报类找不到错误 [root@hadoop001 ~]# chown -R root:root /usr/java 配置JDK环境变量 [root@hadoop001 ~]# vim /etc/profile #追加如下两行配置 export JAVA_HOME=/usr/java/jdk1.8.0_45 export PATH=$JAVA_HOME/bin:$PATH [root@hadoop001 ~]# source /etc/profile #更新环境变量文件 [root@hadoop001 ~]# java -version java version &quot;1.8.0_45&quot; Java(TM) SE Runtime Environment (build 1.8.0_45-b14) Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode) [root@hadoop001 ~]# which java /usr/java/jdk1.8.0_45/bin/java 3.6 部署ZK集群 解压ZK安装包 [root@hadoop001 ~]$ su -hadoop [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/zookeeper-3.4.6.tar.gz -C ~/app/ [hadoop@hadoop001 ~]$ ln -s ~/app/zookeeper-3.4.6 ~/app/zookeeper 添加环境变量 #编辑hadoop用户环境变量文件添加如下内容 [hadoop@hadoop001 bin]$ vim ~/.bash_profile export ZOOKEEPER_HOME=/home/hadoop/app/zookeeper export PATH=$ZOOKEEPER_HOME/bin:$PATH [hadoop@hadoop001 bin]$ source ~/.bash_profile [hadoop@hadoop001 bin]$ which zkServer.sh ~/app/zookeeper/bin/zkServer.sh 修改zookeeper配置 [hadoop@hadoop001 conf]$ mkdir ~/data/zkdata/data [hadoop@hadoop001 app]$ cd ~/app/zookeeper/conf/ [hadoop@hadoop001 conf]$ cp zoo_sample.cfg zoo.cfg #添加或修改如下配置 [hadoop@hadoop001 conf]$ vim zoo.cfg dataDir=/home/hadoop/data/zkdata/data server.1=hadoop001:2888:3888 server.2=hadoop002:2888:3888 server.3=hadoop003:2888:3888 #在数据目录创建myid文件，并将标识1传入 [hadoop@hadoop001 conf]$ cd ~/data/zkdata/data/ [hadoop@hadoop001 data]$ echo 1 &gt;myid #将配置文件复制一份到hadoop002、hadoop003 [hadoop@hadoop001 data]$ scp ~/app/zookeeper/conf/zoo.cfg hadoop002:~/app/zookeeper/conf/ [hadoop@hadoop001 data]$ scp ~/app/zookeeper/conf/zoo.cfg hadoop003:~/app/zookeeper/conf/ [hadoop@hadoop001 data]$ scp ~/data/zkdata/data/myid hadoop002:~/data/zkdata/data/ [hadoop@hadoop001 data]$ scp ~/data/zkdata/data/myid hadoop003:~/data/zkdata/data/ #更改hadoop002、hadoop003的myid文件，将标识改为如下内容 [hadoop@hadoop002 ~]$ cat ~/data/zkdata/data/myid 2 [hadoop@hadoop003 ~]$ cat ~/data/zkdata/data/myid 3 启动zk集群，三台集群都需要执行如下命令： [hadoop@hadoop001 data]$ cd ~/app/zookeeper/bin [hadoop@hadoop001 bin]$ ./zkServer.sh start 查询ZK集群状态： #查询zk节点状态 [hadoop@hadoop003 bin]$ ./zkServer.sh status #查看QuorumPeerMain进程是否启动 [hadoop@hadoop002 bin]$ jps -l 3026 org.apache.zookeeper.server.quorum.QuorumPeerMain 若发现集群状态异常，异常的报错以及解决方法如下： #异常信息 [hadoop@hadoop003 bin]$ ./zkServer.sh status JMX enabled by default Using config: /home/hadoop/app/zookeeper/bin/../conf/status grep: /home/hadoop/app/zookeeper/bin/../conf/status: No such file or directory mkdir: cannot create directory `&#39;: No such file or directory Starting zookeeper ... ./zkServer.sh: line 113: /zookeeper_server.pid: Permission denied FAILED TO WRITE PID ###查询日志，观察详细的错误信息 #寻找日志文件，日志文件名称是通过搜索启动脚本发现的 [hadoop@hadoop001 bin]$ find /home/hadoop -name &quot;zookeeper.out&quot; /home/hadoop/app/zookeeper-3.4.6/bin/zookeeper.out [hadoop@hadoop001 bin]$ vim /home/hadoop/app/zookeeper-3.4.6/bin/zookeeper.out 2019-04-03 22:23:55,976 [myid:] - INFO [main:QuorumPeerConfig@103] - Reading configuration from: /home/hadoop/app/zookeeper/bin/../conf/status 2019-04-03 22:23:55,979 [myid:] - ERROR [main:QuorumPeerMain@85] - Invalid config, exiting abnormally org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Error processing /home/hadoop/app/zookeeper/bin/../conf/status at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:123) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:101) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78) Caused by: java.lang.IllegalArgumentException: /home/hadoop/app/zookeeper/bin/../conf/status file is missing at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:107) ... 2 more Invalid config, exiting abnormally #简单分析日志，发现读取的配置文件竟然是/home/hadoop/app/zookeeper/bin/../conf/status文件，很是奇怪（可能是我一开始没有配置环境变量的原因），重启 集群。 发现一切正常。hadoop002 是lead节点，其它为follower节点 [hadoop@hadoop002 bin]$ ./zkServer.sh status JMX enabled by default Using config: /home/hadoop/app/zookeeper/bin/../conf/zoo.cfg Mode: leader 3.6 部署HADOOP HA集群 解压并添加环境变量，三台机器同时执行 [hadoop@hadoop001 bin]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app [hadoop@hadoop001 bin]$ ln -s ~/app/hadoop-2.6.0-cdh5.7.0 ~/app/hadoop [hadoop@hadoop001 bin]$ vim ~/.bash_profile [hadoop@hadoop001 bin]$ cat ~/.bash_profile #添加或修改为如下内容 PATH=$PATH:$HOME/bin export PATH export ZOOKEEPER_HOME=/home/hadoop/app/zookeeper export HADOOP_HOME=/home/hadoop/app/hadoop export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin:$PATH [hadoop@hadoop001 bin]$ source ~/.bash_profile #更新环境变量 创建数据目录，三台机器同时执行 [hadoop@hadoop001 ~]$ mkdir -p ~/app/hadoop-2.6.0-cdh5.7.0/tmp #创建临时目录，由core-site.xml文件配置hadoop.tmp.dir所配置 [hadoop@hadoop003 ~]$ mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name #创建hdfs的namenode数据（fsimage）目录，由hdfs-site.xml的dfs.namenode.name.dir所配置 [hadoop@hadoop003 ~]$ mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/data #创建hdfs的datanode数据目录，由hdfs-site.xm所配置 [hadoop@hadoop003 ~]$ mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/jn #创建hdfs的journalnode数据目录，由hdfs-site.xm所配置 修改五个配置文件，三台机器同时执行 [hadoop@hadoop003 hadoop]$ cd ~/app/hadoop/etc/hadoop [hadoop@hadoop003 hadoop]$ rm -rf core-site.xml hdfs-site.xml yarn-site.xml slaves #删除已有的配置文件 [hadoop@hadoop003 hadoop]$ rz [hadoop@hadoop003 hadoop]$ scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop001:/home/hadoop/app/hadoop/etc/hadoop [hadoop@hadoop003 hadoop]$ scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop002:/home/hadoop/app/hadoop/etc/hadoop [hadoop@hadoop003 hadoop]$ cat slaves #注意 这三行与最后一行并不连在一起，采坑 hadoop001 hadoop002 hadoop003 [hadoop@hadoop003 hadoop]$ 五个配置文件百度网盘链接如下： 链接：https://pan.baidu.com/s/1lQCWc62nccn61gHEztSbyg 提取码：2rgm 复制这段内容后打开百度网盘手机App，操作更方便哦 core-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!--Yarn 需要使用 fs.defaultFS 指定NameNode URI --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ruozeclusterg6&lt;/value&gt; &lt;/property&gt; &lt;!--==============================Trash机制======================================= --&gt; &lt;property&gt; &lt;!--多长时间创建CheckPoint NameNode截点上运行的CheckPointer 从Current文件夹创建CheckPoint;默认：0 由fs.trash.interval项指定 --&gt; &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少分钟.Trash下的CheckPoint目录会被删除,该配置服务器设置优先级大于客户端，默认：0 不删除 --&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;/property&gt; &lt;!--指定hadoop临时目录, hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配 置namenode和datanode的存放位置，默认就放在这&gt;个路径中 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!--指定ZooKeeper超时间隔，单位毫秒 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;!--使用hadoop用户以及用户组代理集群上所有的用户用户组，注意必须是进程启动用户 --&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;!--设置支持的压缩格式，若不支持，若组件不支持任何压缩格式，应当注销本配置 --&gt; &lt;!--&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec &lt;/value&gt; &lt;/property&gt;--&gt; &lt;/configuration&gt; hdfs-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!--HDFS超级用户，必须是启动用户 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt; &lt;/property&gt; &lt;!--开启web hdfs --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name&lt;/value&gt; &lt;description&gt; namenode 存放name table(fsimage)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;${dfs.namenode.name.dir}&lt;/value&gt; &lt;description&gt;namenode粗放 transaction file(edits)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/data&lt;/value&gt; &lt;description&gt;datanode存放block本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 块大小256M （默认128M） --&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;268435456&lt;/value&gt; &lt;/property&gt; &lt;!--======================================================================= --&gt; &lt;!--HDFS高可用配置 --&gt; &lt;!--指定hdfs的nameservice为ruozeclusterg6,需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ruozeclusterg6&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置NameNode IDs 此版本最大只支持两个NameNode --&gt; &lt;name&gt;dfs.ha.namenodes.ruozeclusterg6&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.rpc-address.[nameservice ID] rpc 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ruozeclusterg6.nn1&lt;/name&gt; &lt;value&gt;hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ruozeclusterg6.nn2&lt;/name&gt; &lt;value&gt;hadoop002:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.http-address.[nameservice ID] http 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ruozeclusterg6.nn1&lt;/name&gt; &lt;value&gt;hadoop001:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ruozeclusterg6.nn2&lt;/name&gt; &lt;value&gt;hadoop002:50070&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode editlog同步 ============================================ --&gt; &lt;!--保证数据恢复 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.http-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8480&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.rpc-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8485&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置JournalNode服务器地址，QuorumJournalManager 用于存储editlog --&gt; &lt;!--格式：qjournal://&lt;host1:port1&gt;;&lt;host2:port2&gt;;&lt;host3:port3&gt;/&lt;journalId&gt; 端口同journalnode.rpc-address --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/ruozeclusterg6&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--JournalNode存放数据地址 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/jn&lt;/value&gt; &lt;/property&gt; &lt;!--==================DataNode editlog同步 ============================================ --&gt; &lt;property&gt; &lt;!--DataNode,Client连接Namenode识别选择Active NameNode策略 --&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ruozeclusterg6&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode fencing：=============================================== --&gt; &lt;!--Failover后防止停掉的Namenode启动，造成两个服务 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少milliseconds 认为fencing失败 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;!--==================NameNode auto failover base ZKFC and Zookeeper====================== --&gt; &lt;!--开启基于Zookeeper --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--动态许可datanode连接namenode列表 --&gt; &lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/slaves&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!-- 配置 MapReduce Applications --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- JobHistory Server ============================================================== --&gt; &lt;!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop001:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop001:19888&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 Map段输出的压缩,snappy，注意若，为hadoop为编译集成压缩格式，应注销本配置--&gt; &lt;!-- &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;/property&gt;--&gt; &lt;/configuration&gt; yarn-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!-- nodemanager 配置 ================================================= --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.localizer.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23344&lt;/value&gt; &lt;description&gt;Address where the localizer IPC is.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.webapp.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23999&lt;/value&gt; &lt;description&gt;NM Webapp address.&lt;/description&gt; &lt;/property&gt; &lt;!-- HA 配置 =============================================================== --&gt; &lt;!-- Resource Manager Configs --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.connect.retry-interval.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 使嵌入式自动故障转移。HA环境启动，与 ZKRMStateStore 配合 处理fencing --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.embedded&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群名称，确保HA选举时对应的集群 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarn-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!--这里RM主备结点需要单独指定,（可选） &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm2&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms&lt;/name&gt; &lt;value&gt;5000&lt;/value&gt; &lt;/property&gt; &lt;!-- ZKRMStateStore 配置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk.state-store.address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- Client访问RM的RPC地址 (applications manager interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23140&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23140&lt;/value&gt; &lt;/property&gt; &lt;!-- AM访问RM的RPC地址(scheduler interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23130&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23130&lt;/value&gt; &lt;/property&gt; &lt;!-- RM admin interface --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23141&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23141&lt;/value&gt; &lt;/property&gt; &lt;!--NM访问RM的RPC端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23125&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23125&lt;/value&gt; &lt;/property&gt; &lt;!-- RM web application 地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop001:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;discription&gt;单个任务可申请最少内存，默认1024MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;discription&gt;单个任务可申请最大内存，默认8192MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; slaves文件如下： hadoop001 hadoop002 hadoop003 设置JDK的绝对路径(采坑)。三台都需要设置 [hadoop@hadoop001 hadoop]$ cat hadoop-env.sh |grep JAVA #如下 已设置jdk的绝对路径 # The only required environment variable is JAVA_HOME. All others are # set JAVA_HOME in this file, so that it is correctly defined on export JAVA_HOME=/usr/java/jdk1.8.0_45 #HADOOP_JAVA_PLATFORM_OPTS=&quot;-XX:-UsePerfData $HADOOP_JAVA_PLATFORM_OPTS&quot; 启动HA集群： #确保zk集群是启动的 [hadoop@hadoop003 hadoop]$ zkServer.sh status JMX enabled by default Using config: /home/hadoop/app/zookeeper/bin/../conf/zoo.cfg Mode: leader #启动journalNode守护进程，三台同时执行 [hadoop@hadoop002 sbin]$ cd ~/app/hadoop/bin #删除所有的windows命令 [hadoop@hadoop002 sbin]$ rm -rf *.cmd [hadoop@hadoop002 sbin]$ cd ~/app/hadoop/sbin [hadoop@hadoop002 sbin]$ rm -rf *.cmd [hadoop@hadoop002 sbin]$ ./hadoop-daemon.sh start journalnode [hadoop@hadoop003 sbin]$ jps 1868 JournalNode 1725 QuorumPeerMain 1919 Jps #格式化namenode，注意只要hadoop001格式化即可，格式化成功标志，日志输出successfully formatted信息如下 [hadoop@hadoop001 sbin]$ hadoop namenode -format ...... : Storage directory /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name has been successfully formatted. 19/04/06 19:50:08 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0 19/04/06 19:50:08 INFO util.ExitUtil: Exiting with status 0 19/04/06 19:50:08 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at hadoop001/172.19.121.243 ************************************************************/ [hadoop@hadoop001 sbin]$ scp -r ~/app/hadoop/data/ hadoop002:/home/hadoop/app/hadoop/ #将nn的数据发一份到hadoop002 #格式化zkfc，只要hadoop001执行即可,成功后会在zk的创建hadoop-ha/ruozeclusterg6，如下信息： [hadoop@hadoop001 sbin]$ hdfs zkfc -formatZK .... 19/04/06 20:03:02 INFO ha.ActiveStandbyElector: Session connected. 19/04/06 20:03:02 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/ruozeclusterg6 in ZK. #启动hdfs，只要hadoop001执行即可 [hadoop@hadoop001 sbin]$ start-dfs.sh #若出现如下错误，且jps发现datanode进程未启动，原因是slaves文件被污染，删除，重新编辑一份。 ····· : Name or service not knownstname hadoop003 : Name or service not knownstname hadoop001 : Name or service not knownstname hadoop002 [hadoop@hadoop002 current]$ rm -rf ~/app/hadoop/etc/hadoop/slaves [hadoop@hadoop002 current]$ vim ~/app/hadoop/etc/hadoop/slaves #添加DN节点信息 hadoop001 had00p002 hadoop003 ····· #重新启动hdfs，会共启动NN、DN、JN、ZKFC四个守护进程，停止hdfs，stop--dfs.sh [hadoop@hadoop001 sbin]$ start-dfs.sh 19/04/06 20:51:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [hadoop001 hadoop002] hadoop001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop001.out hadoop002: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop002.out hadoop002: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop002.out hadoop003: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop003.out hadoop001: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop001.out Starting journal nodes [hadoop001 hadoop002 hadoop003] hadoop001: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop001.out hadoop003: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop003.out hadoop002: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop002.out 19/04/06 20:52:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting ZK Failover Controllers on NN hosts [hadoop001 hadoop002] hadoop002: starting zkfc, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-zkfc-hadoop002.out hadoop001: starting zkfc, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-zkfc-hadoop001.out [hadoop@hadoop001 sbin]$ jps 5504 NameNode 5797 JournalNode 5606 DataNode 6054 Jps 1625 QuorumPeerMain 5983 DFSZKFailoverController #启动yarn，首先在hadoop001执行即可，此时从日志中可以看出只启动了一台RM， #另一个RM需手动前往hadoop002去启动 [hadoop@hadoop001 sbin]$ start-yarn.sh starting yarn daemons starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop001.out hadoop001: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop001.out hadoop002: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop002.out hadoop003: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop003.out [hadoop@hadoop002 current]$ yarn-daemon.sh start resourcemanager starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop002.out #启动jobhistory服务，在hadoop001上执行即可 [hadoop@hadoop001 sbin]$ mr-jobhistory-daemon.sh start historyserver starting historyserver, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/mapred-hadoop-historyserver-hadoop001.out [hadoop@hadoop001 sbin]$ jps 5504 NameNode 6211 NodeManager 6116 ResourceManager 5797 JournalNode 5606 DataNode 1625 QuorumPeerMain 7037 JobHistoryServer 7118 Jps 5983 DFSZKFailoverController 3.7测试集群是否部署成功 通过命令空间操作hdfs文件 [hadoop@hadoop002 current]$ hdfs dfs -ls hdfs://ruozeclusterg6/ [hadoop@hadoop002 current]$ hdfs dfs -put ~/app/hadoop/README.txt hdfs://ruozeclusterg6/ 19/04/06 21:08:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable [hadoop@hadoop002 current]$ hdfs dfs -ls hdfs://ruozeclusterg6/ 19/04/06 21:08:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Found 1 items -rw-r--r-- 3 hadoop hadoop 1366 2019-04-06 21:08 hdfs://ruozeclusterg6/README.txt web界面访问 配置阿里云安全组规则,出入方向放行所有端口 配置windos的hosts文件 web访问hadoop001的hdfs页面，具体谁是active有ZK决定 web访问hadoop001的hdfs页面，具体谁是standby有ZK决定 web访问hadoop001的yarn active界面 web访问hadoop002的yarn standby界面直接访问hadoop002:8088地址会被强制跳转hadoop001的地址。应通过如下地址（ip:8088/cluster/cluster）访问 web访问jobhistroy页面，我启动在hadoop001，故访问地址为hadoop001，端口通过netstat进程可查询到， 测试MR代码,此时可从yarn以及jobhistory的web界面上看到任务情况 [hadoop@hadoop001 sbin]$ find ~/app/hadoop/* -name &#39;*example*.jar&#39; [hadoop@hadoop001 sbin]$ hadoop jar /home/hadoop/app/hadoop/share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 5 10 4.卸载HADOOP HA集群 停止hadoop的守护进程 stop-all.sh mr-jobhistory-daemon.sh stop historyserver #执行停止脚本后，查询是否还有hadoop相关进程，若有，直接kill -9 [hadoop@hadoop001 sbin]$ ps -ef | grep hadoop 删除zk上所有关于hadoop的信息 [hadoop@hadoop001 sbin]$ zkCli.sh #进入zk客户端，删除所有hadoop的配置 [zk: localhost:2181(CONNECTED) 0] ls / [zookeeper, hadoop-ha] [zk: localhost:2181(CONNECTED) 1] rmr /hadoop-ha [zk: localhost:2181(CONNECTED) 1] quit Quitting... 清空data数据目录 rm -rf ~/app/hadoop/data/* 扩展1：生产中若遇到两个节点同为stand by状态时（无法HA），通常是ZK夯住了，需检查ZK状态。 扩展2：生产中若某台机器秘钥文件发生变更，不要傻傻的将known_hosts的文件清空，只要找到变更的机器所属的信息，删除即可。清空会影响其他应用登录，正产使用（若known_hosts无改机器登录信息，第一次需要输入yes，写一份信息在known_hosts上），要背锅的。 扩展3：生产中若遇到异常，首先检查错误信息，再检查配置、其次分析运行日志。若是启动或关闭报错，可debug 启动的脚本。sh -x XXX.sh 方式来debug脚本。 注意没有+表示脚本的输出内容，一个+表示当前行执行语法执行结果，两个++表示当前行某部分语法的执行结果。 扩展4：hadoop chechnative 命令可检测hadoop支持的压缩格式，false表示不支持，CDH版本的hadoop不支持压缩，身产中需要编译支持压缩。map阶段通常选择snappy格式压缩，因为snappy压缩速度最快（快速输出，当然压缩比最低），reduce阶段通常选择gzip或bzip2(压缩比最大，占最小磁盘空间，当然压缩解压时间最久) 扩展5 可通过，start-all.sh 或者stop-all.sh，来启动关闭hadoop集群 扩展6 cat * |grep xxx 命令查找当前文件夹下所有的文件内容" />
<link rel="canonical" href="https://mlh.app/2019/04/07/728708.html" />
<meta property="og:url" content="https://mlh.app/2019/04/07/728708.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-07T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"摘要：本文详细记载hadoop-2.6.0-cdh5.7.0在生产中HA集群部署流程，可用于学习以及生产环境部署借鉴参考。 文章目录 1.环境需求以及部署规划 1.1 硬件环境 1.2 软件环境： 1.3 进程部署规划图： 2.Hadoop Ha架构剖析 2.1 HDFS HA架构详解 2.2 YARN HA架构详解 3.HA部署流程 3.1 上传相关安装包 3.2 关闭防火墙 3.3 配置host文件 3.4 配置SSH免密码通信 3.5 部署JDK 3.6 部署ZK集群 3.6 部署HADOOP HA集群 3.7测试集群是否部署成功 4.卸载HADOOP HA集群 1.环境需求以及部署规划 1.1 硬件环境 三台阿里云主机、每台2vcore、4G内存。 1.2 软件环境： 组件名称 组件版本 Hadoop Hadoop-2.6.0-cdh5.7.0 Zookeeper Zookeeper-3.4.5 jdk Jdk-8u45-linux-x64 1.3 进程部署规划图： 主机名称 ZK NN ZkFC JN DN RM(ZKFC) NM Hadoop001 1 1 1 1 1 1 1 Hadoop002 1 1 1 1 1 1 1 Hadoop003 1 0 0 1 1 0 1 注意：1.、1表示部署在该主机上部署相应的进程，0表示不部署 2.Hadoop Ha架构剖析 2.1 HDFS HA架构详解 请参考：https://blog.csdn.net/qq_32641659/article/details/88964464 2.2 YARN HA架构详解 请参考：https://blog.csdn.net/qq_32641659/article/details/88965006 3.HA部署流程 3.1 上传相关安装包 安装包百度网盘地址： 安装包百度网盘地址： 链接：https://pan.baidu.com/s/1NfOv2ODV9ktKXM8zfaofzQ 提取码：mgwr 复制这段内容后打开百度网盘手机App，操作更方便哦 添加用户以及上传安装包： #####三台机器时执行如下命令######## useradd hadoop su - hadoop mkdir app soft lib source data exit yum install -y lrzsz #安装lrzsz软件 su - hadoop cd ~/soft/ rz #上传安装包，先上传到hadoop001，个人测试xftp传输速度大于rz #scp，将安装包传到另外两台机器，注意使用是内网ip scp -r ~/soft/* root@172.19.121.241:/home/hadoop/soft scp -r ~/soft/* root@172.19.121.242:/home/hadoop/soft [hadoop@hadoop001 soft]$ ll total 490792 -rw-r--r-- 1 root root 311585484 Apr 3 15:52 hadoop-2.6.0-cdh5.7.0.tar.gz -rw-r--r-- 1 root root 173271626 Apr 3 15:49 jdk-8u45-linux-x64.gz -rw-r--r-- 1 root root 17699306 Apr 3 15:50 zookeeper-3.4.6.tar.gz 3.2 关闭防火墙 ##三台机器都需要执行如下命令 #清空防火墙规则 [root@hadoop001 ~]# iptables -F [root@hadoop001 ~]# iptables -L #永久关闭防火墙 [root@hadoop001 ~]# service iptables stop [root@hadoop001 ~]# chkconfig iptables off [root@hadoop001 ~]# service iptables status iptables: Firewall is not running. 3.3 配置host文件 三台机器配置相同的host文件，如下（只列举了hadoop001）： #采坑1：第一第二行的内容永远不要自作聪明去改动，不然后面会遇坑的 [root@hadoop001 ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 172.19.121.243 hadoop001 hadoop001 172.19.121.241 hadoop002 hadoop002 172.19.121.242 hadoop003 hadoop003 [root@hadoop001 ~]# ping hadoop001 [root@hadoop001 ~]# ping hadoop002 [root@hadoop001 ~]# ping hadoop003 3.4 配置SSH免密码通信 三台机器各自生成秘钥： [root@hadoop001 ~]# su - hadoop [hadoop@hadoop001 ~]$ rm -rf ./.ssh [hadoop@hadoop001 ~]$ ssh-keygen #连续生产四个回车 [hadoop@hadoop001 ~]$ cd ~/.ssh [hadoop@hadoop001 .ssh]$ ll total 8 -rw------- 1 hadoop hadoop 1675 Apr 3 16:26 id_rsa -rw-r--r-- 1 hadoop hadoop 398 Apr 3 16:26 id_rsa.pub 合成公钥**(注意命令操作的机器)**： [hadoop@hadoop001 .ssh]$ cat id_rsa.pub &gt;&gt;authorized_keys [hadoop@hadoop002 .ssh]$ scp -r ~/.ssh/id_rsa.pub root@172.19.121.243:/home/hadoop/.ssh/id_rsa2 [hadoop@hadoop003 .ssh]$ scp -r ~/.ssh/id_rsa.pub root@172.19.121.243:/home/hadoop/.ssh/id_rsa3 [hadoop@hadoop001 .ssh]$ ll total 20 -rw-rw-r-- 1 hadoop hadoop 398 Apr 3 16:37 authorized_keys -rw------- 1 hadoop hadoop 1675 Apr 3 16:37 id_rsa -rw-r--r-- 1 root root 398 Apr 3 16:38 id_rsa2 -rw-r--r-- 1 root root 398 Apr 3 16:38 id_rsa3 -rw-r--r-- 1 hadoop hadoop 398 Apr 3 16:37 id_rsa.pub [hadoop@hadoop001 .ssh]$ cat ./id_rsa2 &gt;&gt; authorized_keys [hadoop@hadoop001 .ssh]$ cat ./id_rsa3 &gt;&gt; authorized_keys [hadoop@hadoop001 .ssh]$ cat authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAwZuESml5aeRFyAmZPhzh0WG3waHqGChV4SHWBkjHrkcisLpqpXXotEn0Ap1yWuPYCUKNLIgyLD8tSubnLyj5nNdOXPYnzSyTw0NVIKzKkhLqrYMnpTrckodGjwkhSlaZbIRngBHGB7cUOW8AaWeA79UzEydr1/8Q/arizt82R/K8+t0SAIsk1MUu7+oUGJAzPXpNU76pq69ARb/hJUs0xRMMjOFetqrp8dh8pHoBjgcgUX+fyc5FB/dqJlaCXNJDmNtWclOo8flprB27qj4+1jfCs78wU6AAfewQqo4jJ/2NoD527Vu/SDGysQdlsKpSYBygLB1+/oR46sH1iUJTew== hadoop@hadoop001 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAykZ7nWRo+dmiMuaTALybK1S7XI/pgZgbpTmQAw3IIC1CwFWVZIRuF8eSCL4wgj16pKbKcfczN/9aYhOq0zsUgaa8LlzI6D2DKU1hzak43dCFcnNM/lBkF3QrkE0m9jfM6wmVozdflvRiM+GygEhydfbWSpJcMmPCmV+scRUFjRuH0AuWlwm7sRBxXbK3w4PpWfMF0ie4ZEbviO4PK+E3BxL4xT93N3fELF0s1ayK0mHOfDGBEkFBRp5vIVU//puFU0pW/2/db/laiA8xO1kHLPaFRwVl/I17yNkGUJjF0goeavtVMkxwckd5FsqFIdVecPZ5ReyObbasjbQlvL4uFQ== hadoop@hadoop002 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAw5v6nMHGJmzVHgC1gg/3QbP8qT2ljoBYcS9WaMdSNUjG/WVfvcRWSA1KlACwjG+8RmlHZkR4OTVAIBlMPMDObhjXK6J4hKGicINNsfB+E0etPczDneFCxZHwf9UQ/7J8g/KoAdmE+ROUWKzdw+q2QOcY5Yhbn7FSzF28CK826HPi5L6WXQlBolvlI4x6hn7vscwqpI7cu2YFLkp2bk5lEoatXShSxHi2MTxoyqrtuSpYZhybuExfDjDOPOXX0zpP/Gj7cUHTRuJrUtqiq+G71L+BhmD5cIsTwguBEXrWF+lsXOXTx2TyBXtc7kbvArE6XKee2sjshE52Kn7ko6ZhtQ== hadoop@hadoop003 [hadoop@hadoop001 .ssh]$ rm -rf id_rsa2 id_rsa3 [hadoop@hadoop001 .ssh]$ scp -r ~/.ssh/authorized_keys root@172.19.121.241:/home/hadoop/.ssh/ [hadoop@hadoop001 .ssh]$ scp -r ~/.ssh/authorized_keys root@172.19.121.242:/home/hadoop/.ssh/ #很重要，若authorized_keys属于非root用户必须将权限设置为600 [hadoop@hadoop001 ~]$ chmod 600 ./.ssh/authorized_keys 互相ssh免秘钥测试，用户第一次ssh会有确认选项 规则：ssh 远程机器执行date命令，不需要输入密码则，则ssh免密码配置成功 [hadoop@hadoop001 ~]$ ssh hadoop001 date [hadoop@hadoop001 ~]$ ssh hadoop002 date [hadoop@hadoop001 ~]$ ssh hadoop003 date [hadoop@hadoop002 ~]$ ssh hadoop001 date [hadoop@hadoop002 ~]$ ssh hadoop002 date [hadoop@hadoop002 ~]$ ssh hadoop003 date [hadoop@hadoop003 ~]$ ssh hadoop001 date [hadoop@hadoop003 ~]$ ssh hadoop002 date [hadoop@hadoop003 ~]$ ssh hadoop003 date 3.5 部署JDK 三台机器同时执行如下命令 #采坑1: 必须为/usr/java/，该目录是cdh默认的jdk目录，若不为该目录，后面一定会采坑。 [root@hadoop003 ~]# mkdir /usr/java/ [root@hadoop001 ~]# tar -zxvf /home/hadoop/soft/jdk-8u45-linux-x64.gz -C /usr/java/ #采坑2:权限必须变更，jdk解压的所属用户很奇怪，后续使用中可能会报类找不到错误 [root@hadoop001 ~]# chown -R root:root /usr/java 配置JDK环境变量 [root@hadoop001 ~]# vim /etc/profile #追加如下两行配置 export JAVA_HOME=/usr/java/jdk1.8.0_45 export PATH=$JAVA_HOME/bin:$PATH [root@hadoop001 ~]# source /etc/profile #更新环境变量文件 [root@hadoop001 ~]# java -version java version &quot;1.8.0_45&quot; Java(TM) SE Runtime Environment (build 1.8.0_45-b14) Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode) [root@hadoop001 ~]# which java /usr/java/jdk1.8.0_45/bin/java 3.6 部署ZK集群 解压ZK安装包 [root@hadoop001 ~]$ su -hadoop [hadoop@hadoop001 ~]$ tar -zxvf ~/soft/zookeeper-3.4.6.tar.gz -C ~/app/ [hadoop@hadoop001 ~]$ ln -s ~/app/zookeeper-3.4.6 ~/app/zookeeper 添加环境变量 #编辑hadoop用户环境变量文件添加如下内容 [hadoop@hadoop001 bin]$ vim ~/.bash_profile export ZOOKEEPER_HOME=/home/hadoop/app/zookeeper export PATH=$ZOOKEEPER_HOME/bin:$PATH [hadoop@hadoop001 bin]$ source ~/.bash_profile [hadoop@hadoop001 bin]$ which zkServer.sh ~/app/zookeeper/bin/zkServer.sh 修改zookeeper配置 [hadoop@hadoop001 conf]$ mkdir ~/data/zkdata/data [hadoop@hadoop001 app]$ cd ~/app/zookeeper/conf/ [hadoop@hadoop001 conf]$ cp zoo_sample.cfg zoo.cfg #添加或修改如下配置 [hadoop@hadoop001 conf]$ vim zoo.cfg dataDir=/home/hadoop/data/zkdata/data server.1=hadoop001:2888:3888 server.2=hadoop002:2888:3888 server.3=hadoop003:2888:3888 #在数据目录创建myid文件，并将标识1传入 [hadoop@hadoop001 conf]$ cd ~/data/zkdata/data/ [hadoop@hadoop001 data]$ echo 1 &gt;myid #将配置文件复制一份到hadoop002、hadoop003 [hadoop@hadoop001 data]$ scp ~/app/zookeeper/conf/zoo.cfg hadoop002:~/app/zookeeper/conf/ [hadoop@hadoop001 data]$ scp ~/app/zookeeper/conf/zoo.cfg hadoop003:~/app/zookeeper/conf/ [hadoop@hadoop001 data]$ scp ~/data/zkdata/data/myid hadoop002:~/data/zkdata/data/ [hadoop@hadoop001 data]$ scp ~/data/zkdata/data/myid hadoop003:~/data/zkdata/data/ #更改hadoop002、hadoop003的myid文件，将标识改为如下内容 [hadoop@hadoop002 ~]$ cat ~/data/zkdata/data/myid 2 [hadoop@hadoop003 ~]$ cat ~/data/zkdata/data/myid 3 启动zk集群，三台集群都需要执行如下命令： [hadoop@hadoop001 data]$ cd ~/app/zookeeper/bin [hadoop@hadoop001 bin]$ ./zkServer.sh start 查询ZK集群状态： #查询zk节点状态 [hadoop@hadoop003 bin]$ ./zkServer.sh status #查看QuorumPeerMain进程是否启动 [hadoop@hadoop002 bin]$ jps -l 3026 org.apache.zookeeper.server.quorum.QuorumPeerMain 若发现集群状态异常，异常的报错以及解决方法如下： #异常信息 [hadoop@hadoop003 bin]$ ./zkServer.sh status JMX enabled by default Using config: /home/hadoop/app/zookeeper/bin/../conf/status grep: /home/hadoop/app/zookeeper/bin/../conf/status: No such file or directory mkdir: cannot create directory `&#39;: No such file or directory Starting zookeeper ... ./zkServer.sh: line 113: /zookeeper_server.pid: Permission denied FAILED TO WRITE PID ###查询日志，观察详细的错误信息 #寻找日志文件，日志文件名称是通过搜索启动脚本发现的 [hadoop@hadoop001 bin]$ find /home/hadoop -name &quot;zookeeper.out&quot; /home/hadoop/app/zookeeper-3.4.6/bin/zookeeper.out [hadoop@hadoop001 bin]$ vim /home/hadoop/app/zookeeper-3.4.6/bin/zookeeper.out 2019-04-03 22:23:55,976 [myid:] - INFO [main:QuorumPeerConfig@103] - Reading configuration from: /home/hadoop/app/zookeeper/bin/../conf/status 2019-04-03 22:23:55,979 [myid:] - ERROR [main:QuorumPeerMain@85] - Invalid config, exiting abnormally org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Error processing /home/hadoop/app/zookeeper/bin/../conf/status at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:123) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:101) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78) Caused by: java.lang.IllegalArgumentException: /home/hadoop/app/zookeeper/bin/../conf/status file is missing at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:107) ... 2 more Invalid config, exiting abnormally #简单分析日志，发现读取的配置文件竟然是/home/hadoop/app/zookeeper/bin/../conf/status文件，很是奇怪（可能是我一开始没有配置环境变量的原因），重启 集群。 发现一切正常。hadoop002 是lead节点，其它为follower节点 [hadoop@hadoop002 bin]$ ./zkServer.sh status JMX enabled by default Using config: /home/hadoop/app/zookeeper/bin/../conf/zoo.cfg Mode: leader 3.6 部署HADOOP HA集群 解压并添加环境变量，三台机器同时执行 [hadoop@hadoop001 bin]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app [hadoop@hadoop001 bin]$ ln -s ~/app/hadoop-2.6.0-cdh5.7.0 ~/app/hadoop [hadoop@hadoop001 bin]$ vim ~/.bash_profile [hadoop@hadoop001 bin]$ cat ~/.bash_profile #添加或修改为如下内容 PATH=$PATH:$HOME/bin export PATH export ZOOKEEPER_HOME=/home/hadoop/app/zookeeper export HADOOP_HOME=/home/hadoop/app/hadoop export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin:$PATH [hadoop@hadoop001 bin]$ source ~/.bash_profile #更新环境变量 创建数据目录，三台机器同时执行 [hadoop@hadoop001 ~]$ mkdir -p ~/app/hadoop-2.6.0-cdh5.7.0/tmp #创建临时目录，由core-site.xml文件配置hadoop.tmp.dir所配置 [hadoop@hadoop003 ~]$ mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name #创建hdfs的namenode数据（fsimage）目录，由hdfs-site.xml的dfs.namenode.name.dir所配置 [hadoop@hadoop003 ~]$ mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/data #创建hdfs的datanode数据目录，由hdfs-site.xm所配置 [hadoop@hadoop003 ~]$ mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/jn #创建hdfs的journalnode数据目录，由hdfs-site.xm所配置 修改五个配置文件，三台机器同时执行 [hadoop@hadoop003 hadoop]$ cd ~/app/hadoop/etc/hadoop [hadoop@hadoop003 hadoop]$ rm -rf core-site.xml hdfs-site.xml yarn-site.xml slaves #删除已有的配置文件 [hadoop@hadoop003 hadoop]$ rz [hadoop@hadoop003 hadoop]$ scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop001:/home/hadoop/app/hadoop/etc/hadoop [hadoop@hadoop003 hadoop]$ scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop002:/home/hadoop/app/hadoop/etc/hadoop [hadoop@hadoop003 hadoop]$ cat slaves #注意 这三行与最后一行并不连在一起，采坑 hadoop001 hadoop002 hadoop003 [hadoop@hadoop003 hadoop]$ 五个配置文件百度网盘链接如下： 链接：https://pan.baidu.com/s/1lQCWc62nccn61gHEztSbyg 提取码：2rgm 复制这段内容后打开百度网盘手机App，操作更方便哦 core-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!--Yarn 需要使用 fs.defaultFS 指定NameNode URI --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ruozeclusterg6&lt;/value&gt; &lt;/property&gt; &lt;!--==============================Trash机制======================================= --&gt; &lt;property&gt; &lt;!--多长时间创建CheckPoint NameNode截点上运行的CheckPointer 从Current文件夹创建CheckPoint;默认：0 由fs.trash.interval项指定 --&gt; &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少分钟.Trash下的CheckPoint目录会被删除,该配置服务器设置优先级大于客户端，默认：0 不删除 --&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt; &lt;/property&gt; &lt;!--指定hadoop临时目录, hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配 置namenode和datanode的存放位置，默认就放在这&gt;个路径中 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!--指定ZooKeeper超时间隔，单位毫秒 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;!--使用hadoop用户以及用户组代理集群上所有的用户用户组，注意必须是进程启动用户 --&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;!--设置支持的压缩格式，若不支持，若组件不支持任何压缩格式，应当注销本配置 --&gt; &lt;!--&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec &lt;/value&gt; &lt;/property&gt;--&gt; &lt;/configuration&gt; hdfs-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!--HDFS超级用户，必须是启动用户 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt; &lt;/property&gt; &lt;!--开启web hdfs --&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name&lt;/value&gt; &lt;description&gt; namenode 存放name table(fsimage)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;${dfs.namenode.name.dir}&lt;/value&gt; &lt;description&gt;namenode粗放 transaction file(edits)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/data&lt;/value&gt; &lt;description&gt;datanode存放block本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 块大小256M （默认128M） --&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;268435456&lt;/value&gt; &lt;/property&gt; &lt;!--======================================================================= --&gt; &lt;!--HDFS高可用配置 --&gt; &lt;!--指定hdfs的nameservice为ruozeclusterg6,需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ruozeclusterg6&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置NameNode IDs 此版本最大只支持两个NameNode --&gt; &lt;name&gt;dfs.ha.namenodes.ruozeclusterg6&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.rpc-address.[nameservice ID] rpc 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ruozeclusterg6.nn1&lt;/name&gt; &lt;value&gt;hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ruozeclusterg6.nn2&lt;/name&gt; &lt;value&gt;hadoop002:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- Hdfs HA: dfs.namenode.http-address.[nameservice ID] http 通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ruozeclusterg6.nn1&lt;/name&gt; &lt;value&gt;hadoop001:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ruozeclusterg6.nn2&lt;/name&gt; &lt;value&gt;hadoop002:50070&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode editlog同步 ============================================ --&gt; &lt;!--保证数据恢复 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.http-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8480&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.rpc-address&lt;/name&gt; &lt;value&gt;0.0.0.0:8485&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--设置JournalNode服务器地址，QuorumJournalManager 用于存储editlog --&gt; &lt;!--格式：qjournal://&lt;host1:port1&gt;;&lt;host2:port2&gt;;&lt;host3:port3&gt;/&lt;journalId&gt; 端口同journalnode.rpc-address --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/ruozeclusterg6&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--JournalNode存放数据地址 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/jn&lt;/value&gt; &lt;/property&gt; &lt;!--==================DataNode editlog同步 ============================================ --&gt; &lt;property&gt; &lt;!--DataNode,Client连接Namenode识别选择Active NameNode策略 --&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ruozeclusterg6&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!--==================Namenode fencing：=============================================== --&gt; &lt;!--Failover后防止停掉的Namenode启动，造成两个服务 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--多少milliseconds 认为fencing失败 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;!--==================NameNode auto failover base ZKFC and Zookeeper====================== --&gt; &lt;!--开启基于Zookeeper --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--动态许可datanode连接namenode列表 --&gt; &lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/slaves&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!-- 配置 MapReduce Applications --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- JobHistory Server ============================================================== --&gt; &lt;!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;hadoop001:10020&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop001:19888&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 Map段输出的压缩,snappy，注意若，为hadoop为编译集成压缩格式，应注销本配置--&gt; &lt;!-- &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;/property&gt;--&gt; &lt;/configuration&gt; yarn-site.xml配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;!-- nodemanager 配置 ================================================= --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.localizer.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23344&lt;/value&gt; &lt;description&gt;Address where the localizer IPC is.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.webapp.address&lt;/name&gt; &lt;value&gt;0.0.0.0:23999&lt;/value&gt; &lt;description&gt;NM Webapp address.&lt;/description&gt; &lt;/property&gt; &lt;!-- HA 配置 =============================================================== --&gt; &lt;!-- Resource Manager Configs --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.connect.retry-interval.ms&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 使嵌入式自动故障转移。HA环境启动，与 ZKRMStateStore 配合 处理fencing --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.automatic-failover.embedded&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群名称，确保HA选举时对应的集群 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarn-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!--这里RM主备结点需要单独指定,（可选） &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt; &lt;value&gt;rm2&lt;/value&gt; &lt;/property&gt; --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms&lt;/name&gt; &lt;value&gt;5000&lt;/value&gt; &lt;/property&gt; &lt;!-- ZKRMStateStore 配置 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk.state-store.address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- Client访问RM的RPC地址 (applications manager interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23140&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23140&lt;/value&gt; &lt;/property&gt; &lt;!-- AM访问RM的RPC地址(scheduler interface) --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23130&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23130&lt;/value&gt; &lt;/property&gt; &lt;!-- RM admin interface --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23141&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23141&lt;/value&gt; &lt;/property&gt; &lt;!--NM访问RM的RPC端口 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23125&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23125&lt;/value&gt; &lt;/property&gt; &lt;!-- RM web application 地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm1&lt;/name&gt; &lt;value&gt;hadoop001:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address.rm2&lt;/name&gt; &lt;value&gt;hadoop002:23189&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log.server.url&lt;/name&gt; &lt;value&gt;http://hadoop001:19888/jobhistory/logs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;discription&gt;单个任务可申请最少内存，默认1024MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;discription&gt;单个任务可申请最大内存，默认8192MB&lt;/discription&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; slaves文件如下： hadoop001 hadoop002 hadoop003 设置JDK的绝对路径(采坑)。三台都需要设置 [hadoop@hadoop001 hadoop]$ cat hadoop-env.sh |grep JAVA #如下 已设置jdk的绝对路径 # The only required environment variable is JAVA_HOME. All others are # set JAVA_HOME in this file, so that it is correctly defined on export JAVA_HOME=/usr/java/jdk1.8.0_45 #HADOOP_JAVA_PLATFORM_OPTS=&quot;-XX:-UsePerfData $HADOOP_JAVA_PLATFORM_OPTS&quot; 启动HA集群： #确保zk集群是启动的 [hadoop@hadoop003 hadoop]$ zkServer.sh status JMX enabled by default Using config: /home/hadoop/app/zookeeper/bin/../conf/zoo.cfg Mode: leader #启动journalNode守护进程，三台同时执行 [hadoop@hadoop002 sbin]$ cd ~/app/hadoop/bin #删除所有的windows命令 [hadoop@hadoop002 sbin]$ rm -rf *.cmd [hadoop@hadoop002 sbin]$ cd ~/app/hadoop/sbin [hadoop@hadoop002 sbin]$ rm -rf *.cmd [hadoop@hadoop002 sbin]$ ./hadoop-daemon.sh start journalnode [hadoop@hadoop003 sbin]$ jps 1868 JournalNode 1725 QuorumPeerMain 1919 Jps #格式化namenode，注意只要hadoop001格式化即可，格式化成功标志，日志输出successfully formatted信息如下 [hadoop@hadoop001 sbin]$ hadoop namenode -format ...... : Storage directory /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name has been successfully formatted. 19/04/06 19:50:08 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0 19/04/06 19:50:08 INFO util.ExitUtil: Exiting with status 0 19/04/06 19:50:08 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************ SHUTDOWN_MSG: Shutting down NameNode at hadoop001/172.19.121.243 ************************************************************/ [hadoop@hadoop001 sbin]$ scp -r ~/app/hadoop/data/ hadoop002:/home/hadoop/app/hadoop/ #将nn的数据发一份到hadoop002 #格式化zkfc，只要hadoop001执行即可,成功后会在zk的创建hadoop-ha/ruozeclusterg6，如下信息： [hadoop@hadoop001 sbin]$ hdfs zkfc -formatZK .... 19/04/06 20:03:02 INFO ha.ActiveStandbyElector: Session connected. 19/04/06 20:03:02 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/ruozeclusterg6 in ZK. #启动hdfs，只要hadoop001执行即可 [hadoop@hadoop001 sbin]$ start-dfs.sh #若出现如下错误，且jps发现datanode进程未启动，原因是slaves文件被污染，删除，重新编辑一份。 ····· : Name or service not knownstname hadoop003 : Name or service not knownstname hadoop001 : Name or service not knownstname hadoop002 [hadoop@hadoop002 current]$ rm -rf ~/app/hadoop/etc/hadoop/slaves [hadoop@hadoop002 current]$ vim ~/app/hadoop/etc/hadoop/slaves #添加DN节点信息 hadoop001 had00p002 hadoop003 ····· #重新启动hdfs，会共启动NN、DN、JN、ZKFC四个守护进程，停止hdfs，stop--dfs.sh [hadoop@hadoop001 sbin]$ start-dfs.sh 19/04/06 20:51:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting namenodes on [hadoop001 hadoop002] hadoop001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop001.out hadoop002: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop002.out hadoop002: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop002.out hadoop003: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop003.out hadoop001: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop001.out Starting journal nodes [hadoop001 hadoop002 hadoop003] hadoop001: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop001.out hadoop003: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop003.out hadoop002: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop002.out 19/04/06 20:52:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Starting ZK Failover Controllers on NN hosts [hadoop001 hadoop002] hadoop002: starting zkfc, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-zkfc-hadoop002.out hadoop001: starting zkfc, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-zkfc-hadoop001.out [hadoop@hadoop001 sbin]$ jps 5504 NameNode 5797 JournalNode 5606 DataNode 6054 Jps 1625 QuorumPeerMain 5983 DFSZKFailoverController #启动yarn，首先在hadoop001执行即可，此时从日志中可以看出只启动了一台RM， #另一个RM需手动前往hadoop002去启动 [hadoop@hadoop001 sbin]$ start-yarn.sh starting yarn daemons starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop001.out hadoop001: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop001.out hadoop002: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop002.out hadoop003: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop003.out [hadoop@hadoop002 current]$ yarn-daemon.sh start resourcemanager starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop002.out #启动jobhistory服务，在hadoop001上执行即可 [hadoop@hadoop001 sbin]$ mr-jobhistory-daemon.sh start historyserver starting historyserver, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/mapred-hadoop-historyserver-hadoop001.out [hadoop@hadoop001 sbin]$ jps 5504 NameNode 6211 NodeManager 6116 ResourceManager 5797 JournalNode 5606 DataNode 1625 QuorumPeerMain 7037 JobHistoryServer 7118 Jps 5983 DFSZKFailoverController 3.7测试集群是否部署成功 通过命令空间操作hdfs文件 [hadoop@hadoop002 current]$ hdfs dfs -ls hdfs://ruozeclusterg6/ [hadoop@hadoop002 current]$ hdfs dfs -put ~/app/hadoop/README.txt hdfs://ruozeclusterg6/ 19/04/06 21:08:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable [hadoop@hadoop002 current]$ hdfs dfs -ls hdfs://ruozeclusterg6/ 19/04/06 21:08:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Found 1 items -rw-r--r-- 3 hadoop hadoop 1366 2019-04-06 21:08 hdfs://ruozeclusterg6/README.txt web界面访问 配置阿里云安全组规则,出入方向放行所有端口 配置windos的hosts文件 web访问hadoop001的hdfs页面，具体谁是active有ZK决定 web访问hadoop001的hdfs页面，具体谁是standby有ZK决定 web访问hadoop001的yarn active界面 web访问hadoop002的yarn standby界面直接访问hadoop002:8088地址会被强制跳转hadoop001的地址。应通过如下地址（ip:8088/cluster/cluster）访问 web访问jobhistroy页面，我启动在hadoop001，故访问地址为hadoop001，端口通过netstat进程可查询到， 测试MR代码,此时可从yarn以及jobhistory的web界面上看到任务情况 [hadoop@hadoop001 sbin]$ find ~/app/hadoop/* -name &#39;*example*.jar&#39; [hadoop@hadoop001 sbin]$ hadoop jar /home/hadoop/app/hadoop/share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 5 10 4.卸载HADOOP HA集群 停止hadoop的守护进程 stop-all.sh mr-jobhistory-daemon.sh stop historyserver #执行停止脚本后，查询是否还有hadoop相关进程，若有，直接kill -9 [hadoop@hadoop001 sbin]$ ps -ef | grep hadoop 删除zk上所有关于hadoop的信息 [hadoop@hadoop001 sbin]$ zkCli.sh #进入zk客户端，删除所有hadoop的配置 [zk: localhost:2181(CONNECTED) 0] ls / [zookeeper, hadoop-ha] [zk: localhost:2181(CONNECTED) 1] rmr /hadoop-ha [zk: localhost:2181(CONNECTED) 1] quit Quitting... 清空data数据目录 rm -rf ~/app/hadoop/data/* 扩展1：生产中若遇到两个节点同为stand by状态时（无法HA），通常是ZK夯住了，需检查ZK状态。 扩展2：生产中若某台机器秘钥文件发生变更，不要傻傻的将known_hosts的文件清空，只要找到变更的机器所属的信息，删除即可。清空会影响其他应用登录，正产使用（若known_hosts无改机器登录信息，第一次需要输入yes，写一份信息在known_hosts上），要背锅的。 扩展3：生产中若遇到异常，首先检查错误信息，再检查配置、其次分析运行日志。若是启动或关闭报错，可debug 启动的脚本。sh -x XXX.sh 方式来debug脚本。 注意没有+表示脚本的输出内容，一个+表示当前行执行语法执行结果，两个++表示当前行某部分语法的执行结果。 扩展4：hadoop chechnative 命令可检测hadoop支持的压缩格式，false表示不支持，CDH版本的hadoop不支持压缩，身产中需要编译支持压缩。map阶段通常选择snappy格式压缩，因为snappy压缩速度最快（快速输出，当然压缩比最低），reduce阶段通常选择gzip或bzip2(压缩比最大，占最小磁盘空间，当然压缩解压时间最久) 扩展5 可通过，start-all.sh 或者stop-all.sh，来启动关闭hadoop集群 扩展6 cat * |grep xxx 命令查找当前文件夹下所有的文件内容","@type":"BlogPosting","url":"https://mlh.app/2019/04/07/728708.html","headline":"hadoop之HA生产集群部署","dateModified":"2019-04-07T00:00:00+08:00","datePublished":"2019-04-07T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/04/07/728708.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>hadoop之HA生产集群部署</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-tomorrow-night-eighties"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p><strong>摘要：本文详细记载hadoop-2.6.0-cdh5.7.0在生产中HA集群部署流程，可用于学习以及生产环境部署借鉴参考</strong>。</p> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <ul>
     <ul>
      <li><a href="#1_3" rel="nofollow">1.环境需求以及部署规划</a></li>
      <ul>
       <li><a href="#11__4" rel="nofollow">1.1 硬件环境</a></li>
       <li><a href="#12__6" rel="nofollow">1.2 软件环境：</a></li>
       <li><a href="#13__14" rel="nofollow">1.3 进程部署规划图：</a></li>
      </ul>
      <li><a href="#2Hadoop_Ha_22" rel="nofollow">2.Hadoop Ha架构剖析</a></li>
      <ul>
       <li><a href="#21_HDFS_HA_23" rel="nofollow">2.1 HDFS HA架构详解</a></li>
       <li><a href="#22_YARN_HA_25" rel="nofollow">2.2 YARN HA架构详解</a></li>
      </ul>
      <li><a href="#3HA_27" rel="nofollow">3.HA部署流程</a></li>
      <ul>
       <li><a href="#31__29" rel="nofollow">3.1 上传相关安装包</a></li>
       <li><a href="#32__59" rel="nofollow">3.2 关闭防火墙</a></li>
       <li><a href="#33_host_73" rel="nofollow">3.3 配置host文件</a></li>
       <li><a href="#34_SSH_87" rel="nofollow">3.4 配置SSH免密码通信</a></li>
       <li><a href="#35_JDK_143" rel="nofollow">3.5 部署JDK</a></li>
       <li><a href="#36_ZK_168" rel="nofollow">3.6 部署ZK集群</a></li>
       <li><a href="#36_HADOOP_HA_264" rel="nofollow">3.6 部署HADOOP HA集群</a></li>
       <li><a href="#37_836" rel="nofollow">3.7测试集群是否部署成功</a></li>
      </ul>
      <li><a href="#4HADOOP_HA_877" rel="nofollow">4.卸载HADOOP HA集群</a></li>
     </ul>
    </ul>
   </ul>
  </div>
  <p></p> 
  <h3><a id="1_3"></a>1.环境需求以及部署规划</h3> 
  <h4><a id="11__4"></a>1.1 硬件环境</h4> 
  <p>三台阿里云主机、每台2vcore、4G内存。</p> 
  <h4><a id="12__6"></a>1.2 软件环境：</h4> 
  <table> 
   <thead> 
    <tr> 
     <th align="left">组件名称</th> 
     <th align="left">组件版本</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td align="left">Hadoop</td> 
     <td align="left">Hadoop-2.6.0-cdh5.7.0</td> 
    </tr> 
    <tr> 
     <td align="left">Zookeeper</td> 
     <td align="left">Zookeeper-3.4.5</td> 
    </tr> 
    <tr> 
     <td align="left">jdk</td> 
     <td align="left">Jdk-8u45-linux-x64</td> 
    </tr> 
   </tbody> 
  </table>
  <h4><a id="13__14"></a>1.3 进程部署规划图：</h4> 
  <table> 
   <thead> 
    <tr> 
     <th align="left">主机名称</th> 
     <th align="left">ZK</th> 
     <th align="left">NN</th> 
     <th align="left">ZkFC</th> 
     <th align="left">JN</th> 
     <th align="left">DN</th> 
     <th align="left">RM(ZKFC)</th> 
     <th align="left">NM</th> 
    </tr> 
   </thead> 
   <tbody> 
    <tr> 
     <td align="left">Hadoop001</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
    </tr> 
    <tr> 
     <td align="left">Hadoop002</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
    </tr> 
    <tr> 
     <td align="left">Hadoop003</td> 
     <td align="left">1</td> 
     <td align="left">0</td> 
     <td align="left">0</td> 
     <td align="left">1</td> 
     <td align="left">1</td> 
     <td align="left">0</td> 
     <td align="left">1</td> 
    </tr> 
   </tbody> 
  </table>
  <p><strong>注意</strong>：1.、1表示部署在该主机上部署相应的进程，0表示不部署</p> 
  <h3><a id="2Hadoop_Ha_22"></a>2.Hadoop Ha架构剖析</h3> 
  <h4><a id="21_HDFS_HA_23"></a>2.1 HDFS HA架构详解</h4> 
  <p>请参考：<a href="https://blog.csdn.net/qq_32641659/article/details/88964464" rel="nofollow">https://blog.csdn.net/qq_32641659/article/details/88964464</a></p> 
  <h4><a id="22_YARN_HA_25"></a>2.2 YARN HA架构详解</h4> 
  <p>请参考：<a href="https://blog.csdn.net/qq_32641659/article/details/88965006" rel="nofollow">https://blog.csdn.net/qq_32641659/article/details/88965006</a></p> 
  <h3><a id="3HA_27"></a>3.HA部署流程</h3> 
  <h4><a id="31__29"></a>3.1 上传相关安装包</h4> 
  <p>安装包百度网盘地址：</p> 
  <pre><code>安装包百度网盘地址：
链接：https://pan.baidu.com/s/1NfOv2ODV9ktKXM8zfaofzQ 
提取码：mgwr 
复制这段内容后打开百度网盘手机App，操作更方便哦
</code></pre> 
  <p>添加用户以及上传安装包：</p> 
  <pre><code>#####三台机器时执行如下命令########
useradd hadoop
su - hadoop
mkdir app soft lib source data
exit
yum install -y lrzsz  #安装lrzsz软件
su - hadoop
cd ~/soft/
rz  #上传安装包，先上传到hadoop001，个人测试xftp传输速度大于rz
#scp，将安装包传到另外两台机器，注意使用是内网ip
scp -r  ~/soft/* root@172.19.121.241:/home/hadoop/soft
scp -r  ~/soft/* root@172.19.121.242:/home/hadoop/soft

[hadoop@hadoop001 soft]$ ll
total 490792
-rw-r--r-- 1 root root 311585484 Apr  3 15:52 hadoop-2.6.0-cdh5.7.0.tar.gz
-rw-r--r-- 1 root root 173271626 Apr  3 15:49 jdk-8u45-linux-x64.gz
-rw-r--r-- 1 root root  17699306 Apr  3 15:50 zookeeper-3.4.6.tar.gz
</code></pre> 
  <h4><a id="32__59"></a>3.2 关闭防火墙</h4> 
  <pre><code>##三台机器都需要执行如下命令

#清空防火墙规则
[root@hadoop001 ~]# iptables -F
[root@hadoop001 ~]# iptables -L

#永久关闭防火墙
[root@hadoop001 ~]# service iptables stop
[root@hadoop001 ~]# chkconfig iptables off
[root@hadoop001 ~]# service iptables status
iptables: Firewall is not running.
</code></pre> 
  <h4><a id="33_host_73"></a>3.3 配置host文件</h4> 
  <p>三台机器配置相同的host文件，如下（只列举了hadoop001）：</p> 
  <pre><code>#采坑1：第一第二行的内容永远不要自作聪明去改动，不然后面会遇坑的
[root@hadoop001 ~]# cat /etc/hosts
127.0.0.1	localhost	localhost.localdomain	localhost4	localhost4.localdomain4
::1	localhost	localhost.localdomain	localhost6	localhost6.localdomain6
172.19.121.243	hadoop001	hadoop001
172.19.121.241  hadoop002       hadoop002
172.19.121.242  hadoop003       hadoop003
[root@hadoop001 ~]# ping hadoop001
[root@hadoop001 ~]# ping hadoop002
[root@hadoop001 ~]# ping hadoop003
</code></pre> 
  <h4><a id="34_SSH_87"></a>3.4 配置SSH免密码通信</h4> 
  <p><strong>三台机器</strong>各自生成秘钥：</p> 
  <pre><code>[root@hadoop001 ~]# su - hadoop
[hadoop@hadoop001 ~]$ rm -rf ./.ssh
[hadoop@hadoop001 ~]$ ssh-keygen  #连续生产四个回车
[hadoop@hadoop001 ~]$ cd ~/.ssh
[hadoop@hadoop001 .ssh]$ ll
total 8
-rw------- 1 hadoop hadoop 1675 Apr  3 16:26 id_rsa
-rw-r--r-- 1 hadoop hadoop  398 Apr  3 16:26 id_rsa.pub
</code></pre> 
  <p>合成公钥**(注意命令操作的机器)**：</p> 
  <pre><code>[hadoop@hadoop001 .ssh]$ cat id_rsa.pub &gt;&gt;authorized_keys
[hadoop@hadoop002 .ssh]$ scp -r  ~/.ssh/id_rsa.pub root@172.19.121.243:/home/hadoop/.ssh/id_rsa2
[hadoop@hadoop003 .ssh]$ scp -r  ~/.ssh/id_rsa.pub root@172.19.121.243:/home/hadoop/.ssh/id_rsa3
[hadoop@hadoop001 .ssh]$ ll
total 20
-rw-rw-r-- 1 hadoop hadoop  398 Apr  3 16:37 authorized_keys
-rw------- 1 hadoop hadoop 1675 Apr  3 16:37 id_rsa
-rw-r--r-- 1 root   root    398 Apr  3 16:38 id_rsa2
-rw-r--r-- 1 root   root    398 Apr  3 16:38 id_rsa3
-rw-r--r-- 1 hadoop hadoop  398 Apr  3 16:37 id_rsa.pub
[hadoop@hadoop001 .ssh]$ cat ./id_rsa2 &gt;&gt; authorized_keys
[hadoop@hadoop001 .ssh]$ cat ./id_rsa3 &gt;&gt; authorized_keys
[hadoop@hadoop001 .ssh]$ cat authorized_keys
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAwZuESml5aeRFyAmZPhzh0WG3waHqGChV4SHWBkjHrkcisLpqpXXotEn0Ap1yWuPYCUKNLIgyLD8tSubnLyj5nNdOXPYnzSyTw0NVIKzKkhLqrYMnpTrckodGjwkhSlaZbIRngBHGB7cUOW8AaWeA79UzEydr1/8Q/arizt82R/K8+t0SAIsk1MUu7+oUGJAzPXpNU76pq69ARb/hJUs0xRMMjOFetqrp8dh8pHoBjgcgUX+fyc5FB/dqJlaCXNJDmNtWclOo8flprB27qj4+1jfCs78wU6AAfewQqo4jJ/2NoD527Vu/SDGysQdlsKpSYBygLB1+/oR46sH1iUJTew== hadoop@hadoop001
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAykZ7nWRo+dmiMuaTALybK1S7XI/pgZgbpTmQAw3IIC1CwFWVZIRuF8eSCL4wgj16pKbKcfczN/9aYhOq0zsUgaa8LlzI6D2DKU1hzak43dCFcnNM/lBkF3QrkE0m9jfM6wmVozdflvRiM+GygEhydfbWSpJcMmPCmV+scRUFjRuH0AuWlwm7sRBxXbK3w4PpWfMF0ie4ZEbviO4PK+E3BxL4xT93N3fELF0s1ayK0mHOfDGBEkFBRp5vIVU//puFU0pW/2/db/laiA8xO1kHLPaFRwVl/I17yNkGUJjF0goeavtVMkxwckd5FsqFIdVecPZ5ReyObbasjbQlvL4uFQ== hadoop@hadoop002
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAw5v6nMHGJmzVHgC1gg/3QbP8qT2ljoBYcS9WaMdSNUjG/WVfvcRWSA1KlACwjG+8RmlHZkR4OTVAIBlMPMDObhjXK6J4hKGicINNsfB+E0etPczDneFCxZHwf9UQ/7J8g/KoAdmE+ROUWKzdw+q2QOcY5Yhbn7FSzF28CK826HPi5L6WXQlBolvlI4x6hn7vscwqpI7cu2YFLkp2bk5lEoatXShSxHi2MTxoyqrtuSpYZhybuExfDjDOPOXX0zpP/Gj7cUHTRuJrUtqiq+G71L+BhmD5cIsTwguBEXrWF+lsXOXTx2TyBXtc7kbvArE6XKee2sjshE52Kn7ko6ZhtQ== hadoop@hadoop003
[hadoop@hadoop001 .ssh]$ rm -rf id_rsa2 id_rsa3
[hadoop@hadoop001 .ssh]$ scp -r  ~/.ssh/authorized_keys root@172.19.121.241:/home/hadoop/.ssh/
[hadoop@hadoop001 .ssh]$ scp -r  ~/.ssh/authorized_keys root@172.19.121.242:/home/hadoop/.ssh/

#很重要，若authorized_keys属于非root用户必须将权限设置为600
[hadoop@hadoop001 ~]$ chmod 600 ./.ssh/authorized_keys

</code></pre> 
  <p>互相ssh免秘钥测试，用户第一次ssh会有确认选项</p> 
  <pre><code>规则：ssh 远程机器执行date命令，不需要输入密码则，则ssh免密码配置成功
[hadoop@hadoop001 ~]$ ssh hadoop001 date
[hadoop@hadoop001 ~]$ ssh hadoop002 date
[hadoop@hadoop001 ~]$ ssh hadoop003 date

[hadoop@hadoop002 ~]$ ssh hadoop001 date
[hadoop@hadoop002 ~]$ ssh hadoop002 date
[hadoop@hadoop002 ~]$ ssh hadoop003 date

[hadoop@hadoop003 ~]$ ssh hadoop001 date
[hadoop@hadoop003 ~]$ ssh hadoop002 date
[hadoop@hadoop003 ~]$ ssh hadoop003 date
</code></pre> 
  <h4><a id="35_JDK_143"></a>3.5 部署JDK</h4> 
  <p>三台机器同时执行如下命令</p> 
  <pre><code>#采坑1: 必须为/usr/java/，该目录是cdh默认的jdk目录，若不为该目录，后面一定会采坑。
[root@hadoop003 ~]# mkdir /usr/java/
[root@hadoop001 ~]# tar -zxvf /home/hadoop/soft/jdk-8u45-linux-x64.gz -C /usr/java/
#采坑2:权限必须变更，jdk解压的所属用户很奇怪，后续使用中可能会报类找不到错误
[root@hadoop001 ~]# chown -R root:root /usr/java
</code></pre> 
  <p>配置JDK环境变量</p> 
  <pre><code>[root@hadoop001 ~]# vim /etc/profile        #追加如下两行配置
export JAVA_HOME=/usr/java/jdk1.8.0_45
export PATH=$JAVA_HOME/bin:$PATH

[root@hadoop001 ~]# source /etc/profile 	#更新环境变量文件

[root@hadoop001 ~]# java -version
java version "1.8.0_45"
Java(TM) SE Runtime Environment (build 1.8.0_45-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)
[root@hadoop001 ~]# which java
/usr/java/jdk1.8.0_45/bin/java
</code></pre> 
  <h4><a id="36_ZK_168"></a>3.6 部署ZK集群</h4> 
  <p>解压ZK安装包</p> 
  <pre><code>[root@hadoop001 ~]$ su -hadoop
[hadoop@hadoop001 ~]$ tar -zxvf ~/soft/zookeeper-3.4.6.tar.gz -C ~/app/
[hadoop@hadoop001 ~]$ ln -s ~/app/zookeeper-3.4.6 ~/app/zookeeper
</code></pre> 
  <p>添加环境变量</p> 
  <pre><code>#编辑hadoop用户环境变量文件添加如下内容
[hadoop@hadoop001 bin]$ vim ~/.bash_profile 
export ZOOKEEPER_HOME=/home/hadoop/app/zookeeper
export PATH=$ZOOKEEPER_HOME/bin:$PATH
[hadoop@hadoop001 bin]$ source ~/.bash_profile
[hadoop@hadoop001 bin]$ which zkServer.sh
~/app/zookeeper/bin/zkServer.sh

</code></pre> 
  <p>修改zookeeper配置</p> 
  <pre><code>[hadoop@hadoop001 conf]$ mkdir ~/data/zkdata/data
[hadoop@hadoop001 app]$ cd ~/app/zookeeper/conf/
[hadoop@hadoop001 conf]$ cp zoo_sample.cfg  zoo.cfg

#添加或修改如下配置
[hadoop@hadoop001 conf]$ vim zoo.cfg    
dataDir=/home/hadoop/data/zkdata/data
server.1=hadoop001:2888:3888
server.2=hadoop002:2888:3888
server.3=hadoop003:2888:3888

#在数据目录创建myid文件，并将标识1传入
[hadoop@hadoop001 conf]$ cd ~/data/zkdata/data/
[hadoop@hadoop001 data]$ echo 1 &gt;myid

#将配置文件复制一份到hadoop002、hadoop003
[hadoop@hadoop001 data]$ scp ~/app/zookeeper/conf/zoo.cfg hadoop002:~/app/zookeeper/conf/
[hadoop@hadoop001 data]$ scp ~/app/zookeeper/conf/zoo.cfg hadoop003:~/app/zookeeper/conf/
[hadoop@hadoop001 data]$ scp ~/data/zkdata/data/myid hadoop002:~/data/zkdata/data/
[hadoop@hadoop001 data]$ scp ~/data/zkdata/data/myid hadoop003:~/data/zkdata/data/

#更改hadoop002、hadoop003的myid文件，将标识改为如下内容
[hadoop@hadoop002 ~]$ cat ~/data/zkdata/data/myid
2
[hadoop@hadoop003 ~]$ cat ~/data/zkdata/data/myid
3
</code></pre> 
  <p>启动zk集群，三台集群都需要执行如下命令：</p> 
  <pre><code>[hadoop@hadoop001 data]$ cd ~/app/zookeeper/bin
[hadoop@hadoop001 bin]$ ./zkServer.sh start
</code></pre> 
  <p>查询ZK集群状态：</p> 
  <pre><code>#查询zk节点状态
[hadoop@hadoop003 bin]$  ./zkServer.sh status
#查看QuorumPeerMain进程是否启动
[hadoop@hadoop002 bin]$ jps -l
3026 org.apache.zookeeper.server.quorum.QuorumPeerMain
</code></pre> 
  <p>若发现集群状态异常，异常的报错以及解决方法如下：</p> 
  <pre><code>#异常信息
[hadoop@hadoop003 bin]$  ./zkServer.sh status
JMX enabled by default
Using config: /home/hadoop/app/zookeeper/bin/../conf/status
grep: /home/hadoop/app/zookeeper/bin/../conf/status: No such file or directory
mkdir: cannot create directory `': No such file or directory
Starting zookeeper ... ./zkServer.sh: line 113: /zookeeper_server.pid: Permission denied
FAILED TO WRITE PID

###查询日志，观察详细的错误信息
#寻找日志文件，日志文件名称是通过搜索启动脚本发现的
[hadoop@hadoop001 bin]$ find /home/hadoop -name "zookeeper.out"  
/home/hadoop/app/zookeeper-3.4.6/bin/zookeeper.out
[hadoop@hadoop001 bin]$ vim /home/hadoop/app/zookeeper-3.4.6/bin/zookeeper.out
2019-04-03 22:23:55,976 [myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /home/hadoop/app/zookeeper/bin/../conf/status
2019-04-03 22:23:55,979 [myid:] - ERROR [main:QuorumPeerMain@85] - Invalid config, exiting abnormally
org.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Error processing /home/hadoop/app/zookeeper/bin/../conf/status
        at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:123)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:101)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)
Caused by: java.lang.IllegalArgumentException: /home/hadoop/app/zookeeper/bin/../conf/status file is missing
        at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:107)
        ... 2 more
Invalid config, exiting abnormally

#简单分析日志，发现读取的配置文件竟然是/home/hadoop/app/zookeeper/bin/../conf/status文件，很是奇怪（可能是我一开始没有配置环境变量的原因），重启 集群。 发现一切正常。hadoop002 是lead节点，其它为follower节点
[hadoop@hadoop002 bin]$ ./zkServer.sh status
JMX enabled by default
Using config: /home/hadoop/app/zookeeper/bin/../conf/zoo.cfg
Mode: leader
</code></pre> 
  <h4><a id="36_HADOOP_HA_264"></a>3.6 部署HADOOP HA集群</h4> 
  <p>解压并添加环境变量，三台机器同时执行</p> 
  <pre><code>[hadoop@hadoop001 bin]$ tar -zxvf ~/soft/hadoop-2.6.0-cdh5.7.0.tar.gz -C ~/app
[hadoop@hadoop001 bin]$ ln -s ~/app/hadoop-2.6.0-cdh5.7.0  ~/app/hadoop

[hadoop@hadoop001 bin]$ vim ~/.bash_profile

[hadoop@hadoop001 bin]$ cat ~/.bash_profile  #添加或修改为如下内容
PATH=$PATH:$HOME/bin

export PATH

export ZOOKEEPER_HOME=/home/hadoop/app/zookeeper
export HADOOP_HOME=/home/hadoop/app/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin:$PATH
[hadoop@hadoop001 bin]$ source ~/.bash_profile  #更新环境变量
</code></pre> 
  <p>创建数据目录，三台机器同时执行</p> 
  <pre><code>[hadoop@hadoop001 ~]$ mkdir -p ~/app/hadoop-2.6.0-cdh5.7.0/tmp #创建临时目录，由core-site.xml文件配置hadoop.tmp.dir所配置
[hadoop@hadoop003 ~]$ mkdir -p  /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name  #创建hdfs的namenode数据（fsimage）目录，由hdfs-site.xml的dfs.namenode.name.dir所配置
[hadoop@hadoop003 ~]$ mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/data #创建hdfs的datanode数据目录，由hdfs-site.xm所配置
[hadoop@hadoop003 ~]$ mkdir -p /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/jn  #创建hdfs的journalnode数据目录，由hdfs-site.xm所配置
</code></pre> 
  <p>修改五个配置文件，三台机器同时执行</p> 
  <pre><code>[hadoop@hadoop003 hadoop]$ cd ~/app/hadoop/etc/hadoop 
[hadoop@hadoop003 hadoop]$ rm -rf core-site.xml hdfs-site.xml yarn-site.xml slaves  #删除已有的配置文件
[hadoop@hadoop003 hadoop]$ rz 
[hadoop@hadoop003 hadoop]$ scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop001:/home/hadoop/app/hadoop/etc/hadoop
[hadoop@hadoop003 hadoop]$ scp core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml slaves hadoop002:/home/hadoop/app/hadoop/etc/hadoop
[hadoop@hadoop003 hadoop]$ cat slaves   #注意 这三行与最后一行并不连在一起，采坑
hadoop001
hadoop002
hadoop003
[hadoop@hadoop003 hadoop]$
</code></pre> 
  <p>五个配置文件百度网盘链接如下：</p> 
  <pre><code>链接：https://pan.baidu.com/s/1lQCWc62nccn61gHEztSbyg 
提取码：2rgm 
复制这段内容后打开百度网盘手机App，操作更方便哦
</code></pre> 
  <p>core-site.xml配置如下：</p> 
  <pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
	&lt;!--Yarn 需要使用 fs.defaultFS 指定NameNode URI --&gt;
        &lt;property&gt;
                &lt;name&gt;fs.defaultFS&lt;/name&gt;
                &lt;value&gt;hdfs://ruozeclusterg6&lt;/value&gt;
        &lt;/property&gt;
        &lt;!--==============================Trash机制======================================= --&gt;
        &lt;property&gt;
                &lt;!--多长时间创建CheckPoint NameNode截点上运行的CheckPointer 从Current文件夹创建CheckPoint;默认：0 由fs.trash.interval项指定 --&gt;
                &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;
                &lt;value&gt;0&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
                &lt;!--多少分钟.Trash下的CheckPoint目录会被删除,该配置服务器设置优先级大于客户端，默认：0 不删除 --&gt;
                &lt;name&gt;fs.trash.interval&lt;/name&gt;
                &lt;value&gt;1440&lt;/value&gt;
        &lt;/property&gt;

         &lt;!--指定hadoop临时目录, hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配 置namenode和datanode的存放位置，默认就放在这&gt;个路径中 --&gt;
        &lt;property&gt;   
                &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
                &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/tmp&lt;/value&gt;
        &lt;/property&gt;

         &lt;!-- 指定zookeeper地址 --&gt;
        &lt;property&gt;
                &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;
                &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt;
        &lt;/property&gt;
         &lt;!--指定ZooKeeper超时间隔，单位毫秒 --&gt;
        &lt;property&gt;
                &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt;
                &lt;value&gt;2000&lt;/value&gt;
        &lt;/property&gt;

		&lt;!--使用hadoop用户以及用户组代理集群上所有的用户用户组，注意必须是进程启动用户 --&gt;
        &lt;property&gt;
           &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;
           &lt;value&gt;*&lt;/value&gt; 
        &lt;/property&gt; 
        &lt;property&gt; 
            &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt; 
            &lt;value&gt;*&lt;/value&gt; 
       &lt;/property&gt; 

		&lt;!--设置支持的压缩格式，若不支持，若组件不支持任何压缩格式，应当注销本配置 --&gt;
      &lt;!--&lt;property&gt;
		  &lt;name&gt;io.compression.codecs&lt;/name&gt;
		  &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,
			org.apache.hadoop.io.compress.DefaultCodec,
			org.apache.hadoop.io.compress.BZip2Codec,
			org.apache.hadoop.io.compress.SnappyCodec
		  &lt;/value&gt;
      &lt;/property&gt;--&gt;
	  
&lt;/configuration&gt;

</code></pre> 
  <p>hdfs-site.xml配置如下：</p> 
  <pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
	&lt;!--HDFS超级用户，必须是启动用户 --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt;
		&lt;value&gt;hadoop&lt;/value&gt;
	&lt;/property&gt;

	&lt;!--开启web hdfs --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
		&lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
		&lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name&lt;/value&gt;
		&lt;description&gt; namenode 存放name table(fsimage)本地目录（需要修改）&lt;/description&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt;
		&lt;value&gt;${dfs.namenode.name.dir}&lt;/value&gt;
		&lt;description&gt;namenode粗放 transaction file(edits)本地目录（需要修改）&lt;/description&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
		&lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/data&lt;/value&gt;
		&lt;description&gt;datanode存放block本地目录（需要修改）&lt;/description&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.replication&lt;/name&gt;
		&lt;value&gt;3&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- 块大小256M （默认128M） --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.blocksize&lt;/name&gt;
		&lt;value&gt;268435456&lt;/value&gt;
	&lt;/property&gt;
	&lt;!--======================================================================= --&gt;
	&lt;!--HDFS高可用配置 --&gt;
	&lt;!--指定hdfs的nameservice为ruozeclusterg6,需要和core-site.xml中的保持一致 --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.nameservices&lt;/name&gt;
		&lt;value&gt;ruozeclusterg6&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;!--设置NameNode IDs 此版本最大只支持两个NameNode --&gt;
		&lt;name&gt;dfs.ha.namenodes.ruozeclusterg6&lt;/name&gt;
		&lt;value&gt;nn1,nn2&lt;/value&gt;
	&lt;/property&gt;

	&lt;!-- Hdfs HA: dfs.namenode.rpc-address.[nameservice ID] rpc 通信地址 --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.rpc-address.ruozeclusterg6.nn1&lt;/name&gt;
		&lt;value&gt;hadoop001:8020&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.rpc-address.ruozeclusterg6.nn2&lt;/name&gt;
		&lt;value&gt;hadoop002:8020&lt;/value&gt;
	&lt;/property&gt;

	&lt;!-- Hdfs HA: dfs.namenode.http-address.[nameservice ID] http 通信地址 --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.http-address.ruozeclusterg6.nn1&lt;/name&gt;
		&lt;value&gt;hadoop001:50070&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.http-address.ruozeclusterg6.nn2&lt;/name&gt;
		&lt;value&gt;hadoop002:50070&lt;/value&gt;
	&lt;/property&gt;

	&lt;!--==================Namenode editlog同步 ============================================ --&gt;
	&lt;!--保证数据恢复 --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.journalnode.http-address&lt;/name&gt;
		&lt;value&gt;0.0.0.0:8480&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.journalnode.rpc-address&lt;/name&gt;
		&lt;value&gt;0.0.0.0:8485&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;!--设置JournalNode服务器地址，QuorumJournalManager 用于存储editlog --&gt;
		&lt;!--格式：qjournal://&lt;host1:port1&gt;;&lt;host2:port2&gt;;&lt;host3:port3&gt;/&lt;journalId&gt; 端口同journalnode.rpc-address --&gt;
		&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;
		&lt;value&gt;qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/ruozeclusterg6&lt;/value&gt;
	&lt;/property&gt;

	&lt;property&gt;
		&lt;!--JournalNode存放数据地址 --&gt;
		&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;
		&lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/jn&lt;/value&gt;
	&lt;/property&gt;
	&lt;!--==================DataNode editlog同步 ============================================ --&gt;
	&lt;property&gt;
		&lt;!--DataNode,Client连接Namenode识别选择Active NameNode策略 --&gt;
                             &lt;!-- 配置失败自动切换实现方式 --&gt;
		&lt;name&gt;dfs.client.failover.proxy.provider.ruozeclusterg6&lt;/name&gt;
		&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;
	&lt;/property&gt;
	&lt;!--==================Namenode fencing：=============================================== --&gt;
	&lt;!--Failover后防止停掉的Namenode启动，造成两个服务 --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;
		&lt;value&gt;sshfence&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;
		&lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;!--多少milliseconds 认为fencing失败 --&gt;
		&lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;
		&lt;value&gt;30000&lt;/value&gt;
	&lt;/property&gt;

	&lt;!--==================NameNode auto failover base ZKFC and Zookeeper====================== --&gt;
	&lt;!--开启基于Zookeeper  --&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;
		&lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;
	&lt;!--动态许可datanode连接namenode列表 --&gt;
	 &lt;property&gt;
	   &lt;name&gt;dfs.hosts&lt;/name&gt;
	   &lt;value&gt;/home/hadoop/app/hadoop-2.6.0-cdh5.7.0/etc/hadoop/slaves&lt;/value&gt;
	 &lt;/property&gt;
&lt;/configuration&gt;

</code></pre> 
  <p>mapred-site.xml配置如下：</p> 
  <pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
	&lt;!-- 配置 MapReduce Applications --&gt;
	&lt;property&gt;
		&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
		&lt;value&gt;yarn&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- JobHistory Server ============================================================== --&gt;
	&lt;!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 --&gt;
	&lt;property&gt;
		&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
		&lt;value&gt;hadoop001:10020&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 --&gt;
	&lt;property&gt;
		&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
		&lt;value&gt;hadoop001:19888&lt;/value&gt;
	&lt;/property&gt;

&lt;!-- 配置 Map段输出的压缩,snappy，注意若，为hadoop为编译集成压缩格式，应注销本配置--&gt;
 &lt;!--  &lt;property&gt;
      &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; 
      &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
              
  &lt;property&gt;
      &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; 
      &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
   &lt;/property&gt;--&gt;

&lt;/configuration&gt;

</code></pre> 
  <p>yarn-site.xml配置如下：</p> 
  <pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
	&lt;!-- nodemanager 配置 ================================================= --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
		&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
		&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.nodemanager.localizer.address&lt;/name&gt;
		&lt;value&gt;0.0.0.0:23344&lt;/value&gt;
		&lt;description&gt;Address where the localizer IPC is.&lt;/description&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.nodemanager.webapp.address&lt;/name&gt;
		&lt;value&gt;0.0.0.0:23999&lt;/value&gt;
		&lt;description&gt;NM Webapp address.&lt;/description&gt;
	&lt;/property&gt;

	&lt;!-- HA 配置 =============================================================== --&gt;
	&lt;!-- Resource Manager Configs --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.connect.retry-interval.ms&lt;/name&gt;
		&lt;value&gt;2000&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;
		&lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.ha.automatic-failover.enabled&lt;/name&gt;
		&lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- 使嵌入式自动故障转移。HA环境启动，与 ZKRMStateStore 配合 处理fencing --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.ha.automatic-failover.embedded&lt;/name&gt;
		&lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- 集群名称，确保HA选举时对应的集群 --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;
		&lt;value&gt;yarn-cluster&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;
		&lt;value&gt;rm1,rm2&lt;/value&gt;
	&lt;/property&gt;


    &lt;!--这里RM主备结点需要单独指定,（可选）
	&lt;property&gt;
		 &lt;name&gt;yarn.resourcemanager.ha.id&lt;/name&gt;
		 &lt;value&gt;rm2&lt;/value&gt;
	 &lt;/property&gt;
	 --&gt;

	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;
		&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;
		&lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms&lt;/name&gt;
		&lt;value&gt;5000&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- ZKRMStateStore 配置 --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;
		&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;
		&lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.zk.state-store.address&lt;/name&gt;
		&lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- Client访问RM的RPC地址 (applications manager interface) --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt;
		&lt;value&gt;hadoop001:23140&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt;
		&lt;value&gt;hadoop002:23140&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- AM访问RM的RPC地址(scheduler interface) --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt;
		&lt;value&gt;hadoop001:23130&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt;
		&lt;value&gt;hadoop002:23130&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- RM admin interface --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.admin.address.rm1&lt;/name&gt;
		&lt;value&gt;hadoop001:23141&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.admin.address.rm2&lt;/name&gt;
		&lt;value&gt;hadoop002:23141&lt;/value&gt;
	&lt;/property&gt;
	&lt;!--NM访问RM的RPC端口 --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt;
		&lt;value&gt;hadoop001:23125&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt;
		&lt;value&gt;hadoop002:23125&lt;/value&gt;
	&lt;/property&gt;
	&lt;!-- RM web application 地址 --&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;
		&lt;value&gt;hadoop001:8088&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;
		&lt;value&gt;hadoop002:8088&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.webapp.https.address.rm1&lt;/name&gt;
		&lt;value&gt;hadoop001:23189&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.resourcemanager.webapp.https.address.rm2&lt;/name&gt;
		&lt;value&gt;hadoop002:23189&lt;/value&gt;
	&lt;/property&gt;

	&lt;property&gt;
	   &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
	   &lt;value&gt;true&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		 &lt;name&gt;yarn.log.server.url&lt;/name&gt;
		 &lt;value&gt;http://hadoop001:19888/jobhistory/logs&lt;/value&gt;
	&lt;/property&gt;


	&lt;property&gt;
		&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;
		&lt;value&gt;2048&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;
		&lt;value&gt;1024&lt;/value&gt;
		&lt;discription&gt;单个任务可申请最少内存，默认1024MB&lt;/discription&gt;
	 &lt;/property&gt;

  
  &lt;property&gt;
	&lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;
	&lt;value&gt;2048&lt;/value&gt;
	&lt;discription&gt;单个任务可申请最大内存，默认8192MB&lt;/discription&gt;
  &lt;/property&gt;

   &lt;property&gt;
       &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;
       &lt;value&gt;2&lt;/value&gt;
    &lt;/property&gt;

&lt;/configuration&gt;


</code></pre> 
  <p>slaves文件如下：</p> 
  <pre><code>hadoop001
hadoop002
hadoop003
</code></pre> 
  <p>设置JDK的绝对路径(采坑)。三台都需要设置</p> 
  <pre><code>[hadoop@hadoop001 hadoop]$ cat hadoop-env.sh |grep JAVA  #如下 已设置jdk的绝对路径
# The only required environment variable is JAVA_HOME.  All others are
# set JAVA_HOME in this file, so that it is correctly defined on
export JAVA_HOME=/usr/java/jdk1.8.0_45
#HADOOP_JAVA_PLATFORM_OPTS="-XX:-UsePerfData $HADOOP_JAVA_PLATFORM_OPTS"
</code></pre> 
  <p>启动HA集群：</p> 
  <pre><code>#确保zk集群是启动的
[hadoop@hadoop003 hadoop]$ zkServer.sh status  
JMX enabled by default
Using config: /home/hadoop/app/zookeeper/bin/../conf/zoo.cfg
Mode: leader

#启动journalNode守护进程，三台同时执行
[hadoop@hadoop002 sbin]$ cd ~/app/hadoop/bin #删除所有的windows命令
[hadoop@hadoop002 sbin]$ rm -rf *.cmd
[hadoop@hadoop002 sbin]$ cd ~/app/hadoop/sbin
[hadoop@hadoop002 sbin]$ rm -rf *.cmd
[hadoop@hadoop002 sbin]$ ./hadoop-daemon.sh start journalnode
[hadoop@hadoop003 sbin]$ jps
1868 JournalNode
1725 QuorumPeerMain
1919 Jps

#格式化namenode，注意只要hadoop001格式化即可，格式化成功标志，日志输出successfully formatted信息如下
[hadoop@hadoop001 sbin]$ hadoop namenode -format
......
: Storage directory /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/data/dfs/name has been successfully formatted.
19/04/06 19:50:08 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0
19/04/06 19:50:08 INFO util.ExitUtil: Exiting with status 0
19/04/06 19:50:08 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at hadoop001/172.19.121.243
************************************************************/
[hadoop@hadoop001 sbin]$ scp -r ~/app/hadoop/data/ hadoop002:/home/hadoop/app/hadoop/  #将nn的数据发一份到hadoop002

#格式化zkfc，只要hadoop001执行即可,成功后会在zk的创建hadoop-ha/ruozeclusterg6，如下信息：
[hadoop@hadoop001 sbin]$ hdfs zkfc -formatZK
....
19/04/06 20:03:02 INFO ha.ActiveStandbyElector: Session connected.
19/04/06 20:03:02 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/ruozeclusterg6 in ZK.

#启动hdfs，只要hadoop001执行即可
[hadoop@hadoop001 sbin]$ start-dfs.sh   #若出现如下错误，且jps发现datanode进程未启动，原因是slaves文件被污染，删除，重新编辑一份。
·····
: Name or service not knownstname hadoop003
: Name or service not knownstname hadoop001
: Name or service not knownstname hadoop002
[hadoop@hadoop002 current]$ rm -rf ~/app/hadoop/etc/hadoop/slaves
[hadoop@hadoop002 current]$ vim ~/app/hadoop/etc/hadoop/slaves #添加DN节点信息
hadoop001 
had00p002
hadoop003

·····

#重新启动hdfs，会共启动NN、DN、JN、ZKFC四个守护进程，停止hdfs，stop--dfs.sh
[hadoop@hadoop001 sbin]$ start-dfs.sh
19/04/06 20:51:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting namenodes on [hadoop001 hadoop002]
hadoop001: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop001.out
hadoop002: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop002.out
hadoop002: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop002.out
hadoop003: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop003.out
hadoop001: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop001.out
Starting journal nodes [hadoop001 hadoop002 hadoop003]
hadoop001: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop001.out
hadoop003: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop003.out
hadoop002: starting journalnode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-journalnode-hadoop002.out
19/04/06 20:52:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting ZK Failover Controllers on NN hosts [hadoop001 hadoop002]
hadoop002: starting zkfc, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-zkfc-hadoop002.out
hadoop001: starting zkfc, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-zkfc-hadoop001.out
[hadoop@hadoop001 sbin]$ jps
5504 NameNode
5797 JournalNode
5606 DataNode
6054 Jps
1625 QuorumPeerMain
5983 DFSZKFailoverController


#启动yarn，首先在hadoop001执行即可，此时从日志中可以看出只启动了一台RM，
#另一个RM需手动前往hadoop002去启动
[hadoop@hadoop001 sbin]$ start-yarn.sh 
starting yarn daemons
starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop001.out
hadoop001: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop001.out
hadoop002: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop002.out
hadoop003: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-nodemanager-hadoop003.out

[hadoop@hadoop002 current]$ yarn-daemon.sh start resourcemanager
starting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/yarn-hadoop-resourcemanager-hadoop002.out

#启动jobhistory服务，在hadoop001上执行即可
[hadoop@hadoop001 sbin]$ mr-jobhistory-daemon.sh start historyserver
starting historyserver, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/mapred-hadoop-historyserver-hadoop001.out
[hadoop@hadoop001 sbin]$ jps
5504 NameNode
6211 NodeManager
6116 ResourceManager
5797 JournalNode
5606 DataNode
1625 QuorumPeerMain
7037 JobHistoryServer
7118 Jps
5983 DFSZKFailoverController

</code></pre> 
  <h4><a id="37_836"></a>3.7测试集群是否部署成功</h4> 
  <p>通过命令空间操作hdfs文件</p> 
  <pre><code>[hadoop@hadoop002 current]$ hdfs dfs -ls hdfs://ruozeclusterg6/
[hadoop@hadoop002 current]$ hdfs dfs -put  ~/app/hadoop/README.txt hdfs://ruozeclusterg6/
19/04/06 21:08:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[hadoop@hadoop002 current]$ hdfs dfs -ls hdfs://ruozeclusterg6/
19/04/06 21:08:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
-rw-r--r--   3 hadoop hadoop       1366 2019-04-06 21:08 hdfs://ruozeclusterg6/README.txt

</code></pre> 
  <p>web界面访问</p> 
  <ul> 
   <li>配置阿里云安全组规则,出入方向放行所有端口<br> <img src="https://s2.ax1x.com/2019/04/07/Aflfln.md.png" alt="图1"><br> <img src="https://s2.ax1x.com/2019/04/07/Af1S0K.md.png" alt="图2"><br> <img src="https://s2.ax1x.com/2019/04/07/Af1kpd.md.png" alt="图3"><br> <img src="https://s2.ax1x.com/2019/04/07/Af1VXt.md.png" alt="图4"><br> <img src="https://s2.ax1x.com/2019/04/07/Af1m0f.md.png" alt="图5"></li> 
   <li>配置windos的hosts文件<br> <img src="https://s2.ax1x.com/2019/04/07/Af1RAO.md.png" alt="图-1"><br> <img src="https://s2.ax1x.com/2019/04/07/Af1A1A.png" alt="图0"></li> 
   <li>web访问hadoop001的hdfs页面，具体谁是active有ZK决定<br> <img src="https://s2.ax1x.com/2019/04/07/Af1Mtg.md.png" alt="图6"></li> 
   <li>web访问hadoop001的hdfs页面，具体谁是standby有ZK决定<br> <img src="https://s2.ax1x.com/2019/04/07/Af131s.md.png" alt="图7"></li> 
   <li>web访问hadoop001的yarn active界面<br> <img src="https://s2.ax1x.com/2019/04/07/Af11pj.md.png" alt="图8"></li> 
   <li>web访问hadoop002的yarn standby界面直接访问hadoop002:8088地址会被强制跳转hadoop001的地址。应通过如下地址（ip:8088/cluster/cluster）访问<br> <img src="https://s2.ax1x.com/2019/04/07/Af18cn.md.png" alt="图9"></li> 
   <li>web访问jobhistroy页面，我启动在hadoop001，故访问地址为hadoop001，端口通过netstat进程可查询到，<br> <img src="https://s2.ax1x.com/2019/04/07/Af1GXq.md.png" alt="图10"></li> 
  </ul> 
  <p>测试MR代码,此时可从yarn以及jobhistory的web界面上看到任务情况</p> 
  <pre><code>[hadoop@hadoop001 sbin]$ find ~/app/hadoop/* -name '*example*.jar'
[hadoop@hadoop001 sbin]$ hadoop jar /home/hadoop/app/hadoop/share/hadoop/mapreduce2/hadoop-mapreduce-examples-2.6.0-cdh5.7.0.jar pi 5 10

</code></pre> 
  <h3><a id="4HADOOP_HA_877"></a>4.卸载HADOOP HA集群</h3> 
  <p>停止hadoop的守护进程</p> 
  <pre><code>stop-all.sh
mr-jobhistory-daemon.sh stop historyserver
#执行停止脚本后，查询是否还有hadoop相关进程，若有，直接kill -9 
[hadoop@hadoop001 sbin]$ ps -ef | grep hadoop

</code></pre> 
  <p>删除zk上所有关于hadoop的信息</p> 
  <pre><code>[hadoop@hadoop001 sbin]$ zkCli.sh  #进入zk客户端，删除所有hadoop的配置
[zk: localhost:2181(CONNECTED) 0] ls /
[zookeeper, hadoop-ha]
[zk: localhost:2181(CONNECTED) 1] rmr /hadoop-ha
[zk: localhost:2181(CONNECTED) 1] quit          
Quitting...

</code></pre> 
  <p>清空data数据目录</p> 
  <pre><code>rm -rf ~/app/hadoop/data/*
</code></pre> 
  <p><strong>扩展1</strong>：生产中若遇到两个节点同为<strong>stand by</strong>状态时（无法HA），通常是ZK夯住了，需检查ZK状态。</p> 
  <p><strong>扩展2</strong>：生产中若某台机器秘钥文件发生变更，不要傻傻的将known_hosts的文件清空，只要找到变更的机器所属的信息，删除即可。清空会影响其他应用登录，正产使用（若known_hosts无改机器登录信息，第一次需要输入yes，写一份信息在known_hosts上），要背锅的。</p> 
  <p><strong>扩展3</strong>：生产中若遇到异常，首先检查错误信息，再检查配置、其次分析运行日志。若是启动或关闭报错，可debug 启动的脚本。sh -x <a href="http://XXX.sh" rel="nofollow">XXX.sh</a> 方式来debug脚本。 注意没有+表示脚本的输出内容，一个+表示当前行执行语法执行结果，两个++表示当前行某部分语法的执行结果。</p> 
  <p><strong>扩展4</strong>：hadoop chechnative 命令可检测hadoop支持的压缩格式，false表示不支持，CDH版本的hadoop不支持压缩，身产中需要编译支持压缩。map阶段通常选择snappy格式压缩，因为snappy压缩速度最快（快速输出，当然压缩比最低），reduce阶段通常选择gzip或bzip2(压缩比最大，占最小磁盘空间，当然压缩解压时间最久)</p> 
  <p><strong>扩展5</strong> 可通过，<a href="http://start-all.sh" rel="nofollow">start-all.sh</a> <a href="http://xn--stop-all-7s7pw80v.sh" rel="nofollow">或者stop-all.sh</a>，来启动关闭hadoop集群</p> 
  <p><strong>扩展6</strong> cat * |grep xxx 命令查找当前文件夹下所有的文件内容</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
