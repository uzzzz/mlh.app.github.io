<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>机器学习算法梳理第二篇–GBDT | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="机器学习算法梳理第二篇–GBDT" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="简介： &nbsp; &nbsp; GBDT是Gradient Boosting Decision Tree，梯度提升决策树，首先gbdt 是通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法。 &nbsp; &nbsp;gbdt的训练过程 &nbsp; &nbsp; &nbsp; &nbsp; 我们通过一张图片说明gbdt的训练过程:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 解释：gbdt要经过多轮迭代，首先弱分类器1进行训练，分类器2在分类器1的残差的基础上进行训练，然后分类器3在分类器2的残差的基础上进行训练，继续迭代训练，直至达到迭代停止条件（达到设置的参数，精度要求等）。 &nbsp;对弱分类器的要求：足够简单，并且是低偏差高方差的。因为boosting的过程是一个不断降低偏差来提高最终分类器的精度的。 弱分类器一般选择为CART，根据上述对分类器的要求，CART的深度不会很深。最终的强分类器要将整个迭代过程产生的弱分类器进行加权求和。强分类器模型可以描述为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 模型一共训练m轮，生成m个弱分类器T(x; )。每个弱分类器的损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 为当前的模型，gbdt通过经验风险极小化来确定下一个弱分类器的参数。 在传统机器学习算法中， GBDT算的上top3的算法。想要真正理解GBDT的真正意义，就必须理解GBDT中的Gradient Boosting和Decison Tree分别是什么！ &nbsp; 一. Decision Tree:CART回归树 &nbsp; GBDT使用的决策树是CART回归树，无论是处理回归问题还是处理二分类，多分类问题，GBDT使用的决策树都是CART回归树。为啥不用CART分类树？因为CART分类树一般处理标称型数据，划分节点的最佳判别标准是某一特征对应的信息增益最大，但在回归树中的样本一般是连续数值，所以用信息增益作为判别标准已经不合适了，通常用最小误差平方和所对应的特征值作为节点的划分依据。下面通过具体代码详解CART回归树 CART回归树算法： 输入：数据集，叶节点中值的类型，划分节点时误差评判方法，允许的误差下降值，划分节点的最小样本数 输出：回归树 整体算法： def createTree(dataSet, leaf_type=regLeaf, err_type=regErr, ops=(1, 4)): &nbsp; #选择最佳划分特征及对应的特征值bestFeature, bestFeatureSplitValue bestFeature, bestFeatureSplitValue = chooseBestFeatureSplit(dataSet, leaf_type, err_type, ops) &nbsp; #根据最佳划分特征及对应的特征值将数据集分成两部分lset, rset lset, rset = binSplitDataset(dataSet, bestFeature, bestFeatureSplitValue) &nbsp; #将前面得到的两部分数据作为递归的输入，开始两次递归，创建整个决策树 ltree = createTree(lset, leaf_type, err_type, ops) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; rtree = createTree(rset, leaf_type, err_type, ops) #记录左右子树的信息 regTree={} regTree[&#39;splitFIdx&#39;] = bestFeature &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; regTree[&#39;splitFVal&#39;] = bestFeatureSplitValue regTree[&#39;left&#39;] = ltree regTree[&#39;right&#39;] =&nbsp; rtree &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; #&nbsp; 返回决策树 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return regTree 各个击破： #按照特征及对应的特征值把整个数据集划分为两部分：一部分大于该特征值，一部分小于等于该特征值 def binSplitDataset(dataset, feature, feature_value): mat0 = dataset[nonzero(dataset[:, feature] &gt; feature_value)[0], :] mat1 = dataset[nonzero(dataset[:, feature] &lt;= feature_value)[0], :] return mat0, mat1 def regErr(dataSet): #计算目标的总误差， return var(dataSet[:, -1])*shape(dataSet)[0] def regLeaf(dataSet): #生成叶节点，在回归树中是目标变量特征的均值 return mean(dataSet[:, -1]) #选择最佳划分特征及特征值 def chooseBestFeatureSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)): print(&#39;hello, begin to choose best feature split&#39;) tolS = ops[0] #由用户指定的参数，用于控制函数的停止,实际允许的误差下降值 tolN = ops[1] #切分的最小样本数 if len(set(dataSet[:, -1].T.tolist()[0])) == 1: #当输入数据集所有标签都相同时，说明节点纯度100%，函数返回 return None, leafType(dataSet) minErr = 1e6 bestFeature = 0 #最佳划分特征对应的Id bestFeatureSplitValue = 0.0 #最佳划分特征值 origin_error = errType(dataSet) #求未划分之前初始数据的总方差 num_sample, n = shape(dataSet) #获取样本数量及维度 for feature_idx in range(n - 1): #数据最后一列对应的目标变量不在遍历范围内 for split_value in dataSet[:, feature_idx]: #遍历特征Id为feature_idx的所有特征值 #feature_value作为拆分的阈值 l_data, r_data = binSplitDataset(dataSet, feature_idx, split_value) if shape(l_data)[0] &lt; tolN or shape(r_data)[0] &lt; tolN: #划分出的两数据集大小不满足要求, 继续下一次划分 continue splitErr = errType(l_data) + errType(r_data) #计算节点划分过程产生的误差 if minErr &gt; splitErr: #寻找最小划分误差及对应的特征id,特征值 minErr = splitErr bestFeature = feature_idx bestFeatureSplitValue = split_value if origin_error - minErr &lt; tolS: #切分数据集后效果提升不够大， 则不应该进行切分工作而直接将该节点作为叶节点 return None, leafType(dataSet) l_data, r_data = binSplitDataset(dataSet, bestFeature, bestFeatureSplitValue) #使用最佳特征值进行划分 # plt.subplot(121) # plt.plot(l_data[:, 0], l_data[:, 1], &#39;ro&#39;) # plt.plot(r_data[:, 0], r_data[:, 1], &#39;go&#39;) # plt.show() if shape(l_data)[0] &lt; tolN or shape(r_data)[0] &lt; tolN: #最佳特征值划分得到的结果不满足用户要求，则返回输入数据 return None, leafType(dataSet) 目标变量的均值 return bestFeature, bestFeatureSplitValue #返回最佳划分特征Id及特征值 至此一棵决策树就建立起来了，可利用下面的方法存储和载入模型 def storeTree(intree, filename): import pickle fw = open(filename, &#39;wb+&#39;) pickle.dump(intree, fw) fw.close() def grabTree(filename): import pickle fr = open(filename, &#39;rb+&#39;) return pickle.load(fr) CART回归树如何避免过拟合： 预剪枝：实际上createTree函数中设置输入参数ops（停止条件）就是进行预剪枝的一种方法，当按某一特征值划分数据集时，如果得到的两个子集的大小不满足ops的要求，停止此次划分；数据集划分前和划分后的误差未达到用户要求时，此次划分无效。该方法有一定的局限性：ops对误差的数量级非常敏感，而且事实上我们并不知道ops该设置为多少 后剪枝：由于预剪枝的局限性，我们考虑后剪枝技术，即从上到下找到叶节点，用测试集来判断叶节点合并能否降低测试误差， 如果能降低误差则合并叶节点。一定注意使用的是测试集来进行后剪枝操作 伪代码如下： 基于已有的树切分测试数据： &nbsp; &nbsp;&nbsp; 如果任意子集依然是一棵树，则递归该过程 &nbsp; &nbsp; 当左右两个子集都不是树时，计算当前两个叶节点合并后误差 &nbsp; &nbsp; 计算合并前的误差 &nbsp; &nbsp; 如果合并后的误差小于合并前的误差则合并 def prune(tree, testData): #没有测试数据则对树进行塌陷处理，即返回树平均值 if shape(testData)[0] == 0: return getMean(tree) # if isTree(tree): #如果输入tree的左节点或右节点依然是一个子树，则划分当前数据集，并进入子树递归 if ((type(tree[&#39;left&#39;])).__name__ == &#39;dict&#39; or (type(tree[&#39;right&#39;])).__name__ == &#39;dict&#39;): l_data, r_data = binSplitDataset(testData, tree[&#39;splitFIdx&#39;], tree[&#39;splitFVal&#39;]) if isTree(tree[&#39;left&#39;]): tree[&#39;left&#39;] = prune(tree[&#39;left&#39;], l_data) if isTree(tree[&#39;right&#39;]): tree[&#39;right&#39;] = prune(tree[&#39;right&#39;], r_data) #当左分支和右分支都是叶节点时 if not isTree(tree[&#39;left&#39;]) and not isTree(tree[&#39;right&#39;]): ldata, rdata = binSplitDataset(testData, tree[&#39;splitFIdx&#39;], tree[&#39;splitFVal&#39;]) treeMean = (tree[&#39;left&#39;]+tree[&#39;right&#39;])/2.0 #计算两个叶节点的均值，即两叶节点合并后的均值，与testData对应 #当 if shape(ldata)[0] == 0 and shape(rdata)[0] != 0 or shape(ldata)[0] != 0 and shape(rdata)[0] == 0: return treeMean elif shape(ldata)[0] != 0 and shape(rdata)[0] != 0: no_combine_err = sum(power(ldata[:, -1] - tree[&#39;left&#39;], 2)) + sum(power(rdata[:, -1] - tree[&#39;right&#39;], 2)) combineErr = sum(power(testData[:, -1] - treeMean, 2)) if combineErr &lt; no_combine_err: //合并后的误差小于合并前的误差则进行合并 return treeMean #进行合并 else: return tree #否则返回输入的tree else: return tree 使用CART回归树进行预测 def regTreeEval(tree, input): return float(tree) def treeForeCast(tree, testdata, modelType=regTreeEval): # print(&#39;tree[splitFIdx], tree[splitFVal]&#39;, tree[&#39;splitFIdx&#39;], tree[&#39;splitFVal&#39;]) # print(testdata[:, tree[&#39;splitFIdx&#39;]]) #递归停止条件：当输入tree不是子树，而是叶节点时，返回叶节点的均值 if (type(tree)).__name__ != &#39;dict&#39;: return modelType(tree, testdata) #测试数据在最佳划分特征下的数值大于最佳划分阈值则进入左子树进行递归，否则进入右子树进行递归 if(testdata[:, tree[&#39;splitFIdx&#39;]] &gt; tree[&#39;splitFVal&#39;]): if (type(tree[&#39;left&#39;])).__name__ == &#39;dict&#39;: return treeForeCast(tree[&#39;left&#39;], testdata, modelType) else: return modelType(tree[&#39;left&#39;], testdata) else: if (type(tree[&#39;right&#39;])).__name__ == &#39;dict&#39;: return treeForeCast(tree[&#39;right&#39;], testdata, modelType) else: return modelType(tree[&#39;right&#39;], testdata def createForeCast(tree, testdata, modelType=regTreeEval): m = len(testdata) h_cat = mat(zeros((m, 1))) for i in range(m): #对每个测试数据分别进行预测 h_cat[i, 0] = treeForeCast(tree, mat(testdata[i]), modelType) return h_cat 上面是对CART树的较为全面的解释 二、前向分布算法与Gradient Boosting 1. 前向分步算法（考虑加法模型） 要理解GBDT算法，得先来了解一下什么是前向分步算法。下面一起来瞧瞧。 加法模型是这样的： &nbsp;（就是基学习器的一种线性组合） 其中，&nbsp;&nbsp;为基函数，&nbsp;&nbsp;为基函数的参数，&nbsp;&nbsp;为基函数的系数。 在给定训练数据及损失函数&nbsp;&nbsp;的条件下，学习加法模型成为损失函数极小化问题： 前向分步算法求解这一优化问题的思路：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步去逼近上述的目标函数式，就可简化优化的复杂度，每一步只需优化如下损失函数： &nbsp;（每步学习一个基函数和系数） 前向分步算法流程： -------------------------------------------------------------------------------------------- 输入：训练数据集T=({&nbsp;&nbsp;})；损失函数L(y,f(x))；基函数集{&nbsp;&nbsp;}； 输出：加法模型f(x) (1) 初始化&nbsp; (2) 对m=1,2,...,M (a) 极小化损失函数 得到参数&nbsp; (b)更新 (3)得到加法模型 ------------------------------------------------------------------------------------------ 可见，前向分步算法将同时求解从m=1到M所有参数&nbsp;&nbsp;的优化问题简化成逐步求解各个&nbsp;&nbsp;的优化问题了。 &nbsp; 2. Gradient Boosting : 拟合负梯度 梯度提升树（Gradient Boosting）是提升树（Boosting Tree）的改进算法，故先讲一下boosting tree 先来个通俗理解：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。&nbsp; 提升树算法： （1）初始化 &nbsp; (2)&nbsp; 对m为1，2，3，...， M，其中m代表第几轮迭代，有如下步骤： &nbsp; 计算残差 &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; i=1,2, 3, ...N &nbsp;拟合残差学习一个回归树，得到 &nbsp;更新 &nbsp; (3) 得到回归问题提升树 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 上面伪代码中的残差是什么？ 在提升算法树中，假设上一轮得到强学习器:&nbsp; 损失函数为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 本轮迭代的目标是找到一个弱学习器： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 最小化本轮的损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 当采用平方损失函数时： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 这里： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 是当前模型拟合数据的残差（residual）所以，对于提升树来说只需要简单地拟合当前模型的残差。   回到我们上面讲的那个通俗易懂的例子中，第一次迭代的残差是10岁，第二 次残差4岁…… 当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Freidman提出了梯度提升树算法，这是利用最速下降的近似方法，其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。 那么负梯度长什么样呢？ 第t轮的第i个样本的损失函数的负梯度为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 此时不同的损失函数将会得到不同的负梯度，如果选择平方损失 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 负梯度为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 此时我们发现GBDT的负梯度就是残差，所以说对于回归问题，我们要拟合的就是残差。 GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。 &nbsp; 三、损失函数 在GBDT算法中，损失函数的选择十分重要。针对不同的问题，损失函数有不同的选择。 1.对于分类算法，其损失函数一般由对数损失函数和指数损失函数两种。 (1)指数损失函数表达式： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 其负梯度计算和叶子节点的最佳残差拟合参见Adaboost原理篇。 (2)如果是对数损失函数，分为二元分类和多元分类两种， 对于二元分类 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; L(y,h(x)) = log(1+exp(−yh(x))) &nbsp; 2.对于回归算法，常用损失函数有如下4种。 (1)平方损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (2)绝对损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 对应负梯度误差为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (3)Huber损失，它是均方差和绝对损失的折中产物，对于远离中心的异常点，采用绝对损失误差，而对于靠近中心的点则采用平方损失。这个界限一般用分位数点度量。损失函数如下 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; （4）分位数损失。它对应的是分位数回归的损失函数，表达式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 其中&nbsp;&nbsp;为分位数，需要我们在回归之前指定。对应的负梯度误差为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。 &nbsp; &nbsp; 四、回归问题 梯度提升算法（回归问题）流程： -------------------------------------------------------------------------------------------- 输入：训练数据集T={&nbsp;&nbsp;}，&nbsp;；损失函数L(y,f(x))； 输出：回归树&nbsp; (1)初始化 &nbsp;注：估计使损失函数极小化的常数值，它是只有一个根结点的树 (2)对m=1,2,...,M (a)对i=1,2,...N，计算 &nbsp;注：&nbsp;计算损失函数在当前模型的值，作为残差的估计 (b)对&nbsp;&nbsp;拟合一个回归树，得到第m棵树的叶结点区域&nbsp;&nbsp;,j=1,2,...,J (c)对j=1,2,...,J，计算 &nbsp;注：在损失函数极小化条件下，估计出相应叶结点区域的值 (d)更新 (3)得到回归树 -------------------------------------------------------------------------------------------- 五、分类问题（二分类与多分类） 这里看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合输出类别的误差。 为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法用类似逻辑回归的对数似然函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。此处仅讨论用对数似然函数的GBDT分类。对于对数似然损失函数，我们有又有二元分类和的多元分类的区别。 1.二分类GBDT算法 　　对于二分类GBDT，如果用类似逻辑回归的对数似然损失函数，则损失函数为： 　　 　　其中&nbsp;&nbsp;{-1,1}。此时的负梯度误差为： 　　 　　对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为 　　 　　由于上式比较难优化，我们一般使用近似值代替 　　 　　除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，二分类GBDT与GBDT回归算法过程相同。 2.多分类GBDT算法 　　多分类GBDT比二分类GBDT复杂些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为： 　　 　　其中如果样本输出类别为k，则&nbsp;&nbsp;=1.第k类的概率&nbsp;&nbsp;的表达式为： 　　 　　集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为： 　　 　　观察上式可以看出，其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。 　　对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为： 　　 　　由于上式比较难优化，我们一般使用近似值代替 　　 　　除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多分类GBDT与二分类GBDT以及GBDT回归算法过程相同。 六、正则化 对GBDT进行正则化来防止过拟合，主要有三种形式。 　　　　1.给每棵数的输出结果乘上一个步长a（learning rate）。 　　　　　　对于前面的弱学习器的迭代： 　　　　　　 　　　　　　加上正则化项，则有 　　　　　　 　　　　　　此处，a的取值范围为(0,1]。对于同样的训练集学习效果，较小的a意味着需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起决定算法的拟合效果。 　　　　2.第二种正则化的方式就是通过子采样比例(subsample)。取值范围为(0,1]。 　　　　　　GBDT这里的做法是在每一轮建树时，样本是从原始训练集中采用无放回随机抽样的方式产生，与随机森立的有放回抽样产生采样集的方式不同。若取值为1，则采用全部样本进行训练，若取值小于1，则不选取全部样本进行训练。选择小于1的比例可以减少方差，防止过拟合，但可能会增加样本拟合的偏差。取值要适中，推荐[0.5,0.8]。 　　　　3.第三种是对弱学习器即CART回归树进行正则化剪枝。（如控制树的最大深度、节点的最少样本数、最大叶子节点数、节点分支的最小样本数等） 七、GBDT优缺点 1.GBDT优点 可以灵活处理各种类型的数据，包括连续值和离散值。 在相对较少的调参时间情况下，预测的准确率也比较高，相对SVM而言。 在使用一些健壮的损失函数，对异常值得鲁棒性非常强。比如Huber损失函数和Quantile损失函数。 2.GBDT缺点 由于弱学习器之间存在较强依赖关系，难以并行训练。可以通过自采样的SGBT来达到部分并行。 八、sklearn参数 在scikit-learning中，GradientBoostingClassifier对应GBDT的分类算法，GradientBoostingRegressor对应GBDT的回归算法。 具体算法参数情况如下： GradientBoostingRegressor(loss=’ls’, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=’friedman_mse’, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort=’auto’, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001) 参数说明： n_estimators：弱学习器的最大迭代次数，也就是最大弱学习器的个数。 learning_rate：步长，即每个学习器的权重缩减系数a，属于GBDT正则化方化手段之一。 subsample：子采样，取值(0,1]。决定是否对原始数据集进行采样以及采样的比例，也是GBDT正则化手段之一。 init：我们初始化的时候的弱学习器。若不设置，则使用默认的。 loss：损失函数，可选{&#39;ls&#39;-平方损失函数，&#39;lad&#39;绝对损失函数-,&#39;huber&#39;-huber损失函数,&#39;quantile&#39;-分位数损失函数}，默认&#39;ls&#39;。 alpha：当我们在使用Huber损失&quot;Huber&quot;和分位数损失&quot;quantile&quot;时，需要指定相应的值。默认是0.9，若噪声点比较多，可适当降低这个分位数值。 criterion：决策树节搜索最优分割点的准则，默认是&quot;friedman_mse&quot;，可选&quot;mse&quot;-均方误差与&#39;mae&quot;-绝对误差。 max_features：划分时考虑的最大特征数，就是特征抽样的意思，默认考虑全部特征。 max_depth：树的最大深度。 min_samples_split：内部节点再划分所需最小样本数。 min_samples_leaf：叶子节点最少样本数。 max_leaf_nodes：最大叶子节点数。 min_impurity_split：节点划分最小不纯度。 presort：是否预先对数据进行排序以加快最优分割点搜索的速度。默认是预先排序，若是稀疏数据，则不会预先排序，另外，稀疏数据不能设置为True。 validationfraction：为提前停止而预留的验证数据比例。当n_iter_no_change设置时才能用。 n_iter_no_change：当验证分数没有提高时，用于决定是否使用早期停止来终止训练。 八、GBDT应用场景 GBDT几乎可以用于所有回归问题（线性/非线性），相对loigstic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于分类问题。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<meta property="og:description" content="简介： &nbsp; &nbsp; GBDT是Gradient Boosting Decision Tree，梯度提升决策树，首先gbdt 是通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法。 &nbsp; &nbsp;gbdt的训练过程 &nbsp; &nbsp; &nbsp; &nbsp; 我们通过一张图片说明gbdt的训练过程:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 解释：gbdt要经过多轮迭代，首先弱分类器1进行训练，分类器2在分类器1的残差的基础上进行训练，然后分类器3在分类器2的残差的基础上进行训练，继续迭代训练，直至达到迭代停止条件（达到设置的参数，精度要求等）。 &nbsp;对弱分类器的要求：足够简单，并且是低偏差高方差的。因为boosting的过程是一个不断降低偏差来提高最终分类器的精度的。 弱分类器一般选择为CART，根据上述对分类器的要求，CART的深度不会很深。最终的强分类器要将整个迭代过程产生的弱分类器进行加权求和。强分类器模型可以描述为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 模型一共训练m轮，生成m个弱分类器T(x; )。每个弱分类器的损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 为当前的模型，gbdt通过经验风险极小化来确定下一个弱分类器的参数。 在传统机器学习算法中， GBDT算的上top3的算法。想要真正理解GBDT的真正意义，就必须理解GBDT中的Gradient Boosting和Decison Tree分别是什么！ &nbsp; 一. Decision Tree:CART回归树 &nbsp; GBDT使用的决策树是CART回归树，无论是处理回归问题还是处理二分类，多分类问题，GBDT使用的决策树都是CART回归树。为啥不用CART分类树？因为CART分类树一般处理标称型数据，划分节点的最佳判别标准是某一特征对应的信息增益最大，但在回归树中的样本一般是连续数值，所以用信息增益作为判别标准已经不合适了，通常用最小误差平方和所对应的特征值作为节点的划分依据。下面通过具体代码详解CART回归树 CART回归树算法： 输入：数据集，叶节点中值的类型，划分节点时误差评判方法，允许的误差下降值，划分节点的最小样本数 输出：回归树 整体算法： def createTree(dataSet, leaf_type=regLeaf, err_type=regErr, ops=(1, 4)): &nbsp; #选择最佳划分特征及对应的特征值bestFeature, bestFeatureSplitValue bestFeature, bestFeatureSplitValue = chooseBestFeatureSplit(dataSet, leaf_type, err_type, ops) &nbsp; #根据最佳划分特征及对应的特征值将数据集分成两部分lset, rset lset, rset = binSplitDataset(dataSet, bestFeature, bestFeatureSplitValue) &nbsp; #将前面得到的两部分数据作为递归的输入，开始两次递归，创建整个决策树 ltree = createTree(lset, leaf_type, err_type, ops) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; rtree = createTree(rset, leaf_type, err_type, ops) #记录左右子树的信息 regTree={} regTree[&#39;splitFIdx&#39;] = bestFeature &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; regTree[&#39;splitFVal&#39;] = bestFeatureSplitValue regTree[&#39;left&#39;] = ltree regTree[&#39;right&#39;] =&nbsp; rtree &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; #&nbsp; 返回决策树 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return regTree 各个击破： #按照特征及对应的特征值把整个数据集划分为两部分：一部分大于该特征值，一部分小于等于该特征值 def binSplitDataset(dataset, feature, feature_value): mat0 = dataset[nonzero(dataset[:, feature] &gt; feature_value)[0], :] mat1 = dataset[nonzero(dataset[:, feature] &lt;= feature_value)[0], :] return mat0, mat1 def regErr(dataSet): #计算目标的总误差， return var(dataSet[:, -1])*shape(dataSet)[0] def regLeaf(dataSet): #生成叶节点，在回归树中是目标变量特征的均值 return mean(dataSet[:, -1]) #选择最佳划分特征及特征值 def chooseBestFeatureSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)): print(&#39;hello, begin to choose best feature split&#39;) tolS = ops[0] #由用户指定的参数，用于控制函数的停止,实际允许的误差下降值 tolN = ops[1] #切分的最小样本数 if len(set(dataSet[:, -1].T.tolist()[0])) == 1: #当输入数据集所有标签都相同时，说明节点纯度100%，函数返回 return None, leafType(dataSet) minErr = 1e6 bestFeature = 0 #最佳划分特征对应的Id bestFeatureSplitValue = 0.0 #最佳划分特征值 origin_error = errType(dataSet) #求未划分之前初始数据的总方差 num_sample, n = shape(dataSet) #获取样本数量及维度 for feature_idx in range(n - 1): #数据最后一列对应的目标变量不在遍历范围内 for split_value in dataSet[:, feature_idx]: #遍历特征Id为feature_idx的所有特征值 #feature_value作为拆分的阈值 l_data, r_data = binSplitDataset(dataSet, feature_idx, split_value) if shape(l_data)[0] &lt; tolN or shape(r_data)[0] &lt; tolN: #划分出的两数据集大小不满足要求, 继续下一次划分 continue splitErr = errType(l_data) + errType(r_data) #计算节点划分过程产生的误差 if minErr &gt; splitErr: #寻找最小划分误差及对应的特征id,特征值 minErr = splitErr bestFeature = feature_idx bestFeatureSplitValue = split_value if origin_error - minErr &lt; tolS: #切分数据集后效果提升不够大， 则不应该进行切分工作而直接将该节点作为叶节点 return None, leafType(dataSet) l_data, r_data = binSplitDataset(dataSet, bestFeature, bestFeatureSplitValue) #使用最佳特征值进行划分 # plt.subplot(121) # plt.plot(l_data[:, 0], l_data[:, 1], &#39;ro&#39;) # plt.plot(r_data[:, 0], r_data[:, 1], &#39;go&#39;) # plt.show() if shape(l_data)[0] &lt; tolN or shape(r_data)[0] &lt; tolN: #最佳特征值划分得到的结果不满足用户要求，则返回输入数据 return None, leafType(dataSet) 目标变量的均值 return bestFeature, bestFeatureSplitValue #返回最佳划分特征Id及特征值 至此一棵决策树就建立起来了，可利用下面的方法存储和载入模型 def storeTree(intree, filename): import pickle fw = open(filename, &#39;wb+&#39;) pickle.dump(intree, fw) fw.close() def grabTree(filename): import pickle fr = open(filename, &#39;rb+&#39;) return pickle.load(fr) CART回归树如何避免过拟合： 预剪枝：实际上createTree函数中设置输入参数ops（停止条件）就是进行预剪枝的一种方法，当按某一特征值划分数据集时，如果得到的两个子集的大小不满足ops的要求，停止此次划分；数据集划分前和划分后的误差未达到用户要求时，此次划分无效。该方法有一定的局限性：ops对误差的数量级非常敏感，而且事实上我们并不知道ops该设置为多少 后剪枝：由于预剪枝的局限性，我们考虑后剪枝技术，即从上到下找到叶节点，用测试集来判断叶节点合并能否降低测试误差， 如果能降低误差则合并叶节点。一定注意使用的是测试集来进行后剪枝操作 伪代码如下： 基于已有的树切分测试数据： &nbsp; &nbsp;&nbsp; 如果任意子集依然是一棵树，则递归该过程 &nbsp; &nbsp; 当左右两个子集都不是树时，计算当前两个叶节点合并后误差 &nbsp; &nbsp; 计算合并前的误差 &nbsp; &nbsp; 如果合并后的误差小于合并前的误差则合并 def prune(tree, testData): #没有测试数据则对树进行塌陷处理，即返回树平均值 if shape(testData)[0] == 0: return getMean(tree) # if isTree(tree): #如果输入tree的左节点或右节点依然是一个子树，则划分当前数据集，并进入子树递归 if ((type(tree[&#39;left&#39;])).__name__ == &#39;dict&#39; or (type(tree[&#39;right&#39;])).__name__ == &#39;dict&#39;): l_data, r_data = binSplitDataset(testData, tree[&#39;splitFIdx&#39;], tree[&#39;splitFVal&#39;]) if isTree(tree[&#39;left&#39;]): tree[&#39;left&#39;] = prune(tree[&#39;left&#39;], l_data) if isTree(tree[&#39;right&#39;]): tree[&#39;right&#39;] = prune(tree[&#39;right&#39;], r_data) #当左分支和右分支都是叶节点时 if not isTree(tree[&#39;left&#39;]) and not isTree(tree[&#39;right&#39;]): ldata, rdata = binSplitDataset(testData, tree[&#39;splitFIdx&#39;], tree[&#39;splitFVal&#39;]) treeMean = (tree[&#39;left&#39;]+tree[&#39;right&#39;])/2.0 #计算两个叶节点的均值，即两叶节点合并后的均值，与testData对应 #当 if shape(ldata)[0] == 0 and shape(rdata)[0] != 0 or shape(ldata)[0] != 0 and shape(rdata)[0] == 0: return treeMean elif shape(ldata)[0] != 0 and shape(rdata)[0] != 0: no_combine_err = sum(power(ldata[:, -1] - tree[&#39;left&#39;], 2)) + sum(power(rdata[:, -1] - tree[&#39;right&#39;], 2)) combineErr = sum(power(testData[:, -1] - treeMean, 2)) if combineErr &lt; no_combine_err: //合并后的误差小于合并前的误差则进行合并 return treeMean #进行合并 else: return tree #否则返回输入的tree else: return tree 使用CART回归树进行预测 def regTreeEval(tree, input): return float(tree) def treeForeCast(tree, testdata, modelType=regTreeEval): # print(&#39;tree[splitFIdx], tree[splitFVal]&#39;, tree[&#39;splitFIdx&#39;], tree[&#39;splitFVal&#39;]) # print(testdata[:, tree[&#39;splitFIdx&#39;]]) #递归停止条件：当输入tree不是子树，而是叶节点时，返回叶节点的均值 if (type(tree)).__name__ != &#39;dict&#39;: return modelType(tree, testdata) #测试数据在最佳划分特征下的数值大于最佳划分阈值则进入左子树进行递归，否则进入右子树进行递归 if(testdata[:, tree[&#39;splitFIdx&#39;]] &gt; tree[&#39;splitFVal&#39;]): if (type(tree[&#39;left&#39;])).__name__ == &#39;dict&#39;: return treeForeCast(tree[&#39;left&#39;], testdata, modelType) else: return modelType(tree[&#39;left&#39;], testdata) else: if (type(tree[&#39;right&#39;])).__name__ == &#39;dict&#39;: return treeForeCast(tree[&#39;right&#39;], testdata, modelType) else: return modelType(tree[&#39;right&#39;], testdata def createForeCast(tree, testdata, modelType=regTreeEval): m = len(testdata) h_cat = mat(zeros((m, 1))) for i in range(m): #对每个测试数据分别进行预测 h_cat[i, 0] = treeForeCast(tree, mat(testdata[i]), modelType) return h_cat 上面是对CART树的较为全面的解释 二、前向分布算法与Gradient Boosting 1. 前向分步算法（考虑加法模型） 要理解GBDT算法，得先来了解一下什么是前向分步算法。下面一起来瞧瞧。 加法模型是这样的： &nbsp;（就是基学习器的一种线性组合） 其中，&nbsp;&nbsp;为基函数，&nbsp;&nbsp;为基函数的参数，&nbsp;&nbsp;为基函数的系数。 在给定训练数据及损失函数&nbsp;&nbsp;的条件下，学习加法模型成为损失函数极小化问题： 前向分步算法求解这一优化问题的思路：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步去逼近上述的目标函数式，就可简化优化的复杂度，每一步只需优化如下损失函数： &nbsp;（每步学习一个基函数和系数） 前向分步算法流程： -------------------------------------------------------------------------------------------- 输入：训练数据集T=({&nbsp;&nbsp;})；损失函数L(y,f(x))；基函数集{&nbsp;&nbsp;}； 输出：加法模型f(x) (1) 初始化&nbsp; (2) 对m=1,2,...,M (a) 极小化损失函数 得到参数&nbsp; (b)更新 (3)得到加法模型 ------------------------------------------------------------------------------------------ 可见，前向分步算法将同时求解从m=1到M所有参数&nbsp;&nbsp;的优化问题简化成逐步求解各个&nbsp;&nbsp;的优化问题了。 &nbsp; 2. Gradient Boosting : 拟合负梯度 梯度提升树（Gradient Boosting）是提升树（Boosting Tree）的改进算法，故先讲一下boosting tree 先来个通俗理解：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。&nbsp; 提升树算法： （1）初始化 &nbsp; (2)&nbsp; 对m为1，2，3，...， M，其中m代表第几轮迭代，有如下步骤： &nbsp; 计算残差 &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; i=1,2, 3, ...N &nbsp;拟合残差学习一个回归树，得到 &nbsp;更新 &nbsp; (3) 得到回归问题提升树 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 上面伪代码中的残差是什么？ 在提升算法树中，假设上一轮得到强学习器:&nbsp; 损失函数为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 本轮迭代的目标是找到一个弱学习器： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 最小化本轮的损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 当采用平方损失函数时： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 这里： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 是当前模型拟合数据的残差（residual）所以，对于提升树来说只需要简单地拟合当前模型的残差。   回到我们上面讲的那个通俗易懂的例子中，第一次迭代的残差是10岁，第二 次残差4岁…… 当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Freidman提出了梯度提升树算法，这是利用最速下降的近似方法，其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。 那么负梯度长什么样呢？ 第t轮的第i个样本的损失函数的负梯度为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 此时不同的损失函数将会得到不同的负梯度，如果选择平方损失 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 负梯度为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 此时我们发现GBDT的负梯度就是残差，所以说对于回归问题，我们要拟合的就是残差。 GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。 &nbsp; 三、损失函数 在GBDT算法中，损失函数的选择十分重要。针对不同的问题，损失函数有不同的选择。 1.对于分类算法，其损失函数一般由对数损失函数和指数损失函数两种。 (1)指数损失函数表达式： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 其负梯度计算和叶子节点的最佳残差拟合参见Adaboost原理篇。 (2)如果是对数损失函数，分为二元分类和多元分类两种， 对于二元分类 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; L(y,h(x)) = log(1+exp(−yh(x))) &nbsp; 2.对于回归算法，常用损失函数有如下4种。 (1)平方损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (2)绝对损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 对应负梯度误差为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (3)Huber损失，它是均方差和绝对损失的折中产物，对于远离中心的异常点，采用绝对损失误差，而对于靠近中心的点则采用平方损失。这个界限一般用分位数点度量。损失函数如下 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; （4）分位数损失。它对应的是分位数回归的损失函数，表达式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 其中&nbsp;&nbsp;为分位数，需要我们在回归之前指定。对应的负梯度误差为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。 &nbsp; &nbsp; 四、回归问题 梯度提升算法（回归问题）流程： -------------------------------------------------------------------------------------------- 输入：训练数据集T={&nbsp;&nbsp;}，&nbsp;；损失函数L(y,f(x))； 输出：回归树&nbsp; (1)初始化 &nbsp;注：估计使损失函数极小化的常数值，它是只有一个根结点的树 (2)对m=1,2,...,M (a)对i=1,2,...N，计算 &nbsp;注：&nbsp;计算损失函数在当前模型的值，作为残差的估计 (b)对&nbsp;&nbsp;拟合一个回归树，得到第m棵树的叶结点区域&nbsp;&nbsp;,j=1,2,...,J (c)对j=1,2,...,J，计算 &nbsp;注：在损失函数极小化条件下，估计出相应叶结点区域的值 (d)更新 (3)得到回归树 -------------------------------------------------------------------------------------------- 五、分类问题（二分类与多分类） 这里看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合输出类别的误差。 为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法用类似逻辑回归的对数似然函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。此处仅讨论用对数似然函数的GBDT分类。对于对数似然损失函数，我们有又有二元分类和的多元分类的区别。 1.二分类GBDT算法 　　对于二分类GBDT，如果用类似逻辑回归的对数似然损失函数，则损失函数为： 　　 　　其中&nbsp;&nbsp;{-1,1}。此时的负梯度误差为： 　　 　　对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为 　　 　　由于上式比较难优化，我们一般使用近似值代替 　　 　　除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，二分类GBDT与GBDT回归算法过程相同。 2.多分类GBDT算法 　　多分类GBDT比二分类GBDT复杂些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为： 　　 　　其中如果样本输出类别为k，则&nbsp;&nbsp;=1.第k类的概率&nbsp;&nbsp;的表达式为： 　　 　　集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为： 　　 　　观察上式可以看出，其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。 　　对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为： 　　 　　由于上式比较难优化，我们一般使用近似值代替 　　 　　除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多分类GBDT与二分类GBDT以及GBDT回归算法过程相同。 六、正则化 对GBDT进行正则化来防止过拟合，主要有三种形式。 　　　　1.给每棵数的输出结果乘上一个步长a（learning rate）。 　　　　　　对于前面的弱学习器的迭代： 　　　　　　 　　　　　　加上正则化项，则有 　　　　　　 　　　　　　此处，a的取值范围为(0,1]。对于同样的训练集学习效果，较小的a意味着需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起决定算法的拟合效果。 　　　　2.第二种正则化的方式就是通过子采样比例(subsample)。取值范围为(0,1]。 　　　　　　GBDT这里的做法是在每一轮建树时，样本是从原始训练集中采用无放回随机抽样的方式产生，与随机森立的有放回抽样产生采样集的方式不同。若取值为1，则采用全部样本进行训练，若取值小于1，则不选取全部样本进行训练。选择小于1的比例可以减少方差，防止过拟合，但可能会增加样本拟合的偏差。取值要适中，推荐[0.5,0.8]。 　　　　3.第三种是对弱学习器即CART回归树进行正则化剪枝。（如控制树的最大深度、节点的最少样本数、最大叶子节点数、节点分支的最小样本数等） 七、GBDT优缺点 1.GBDT优点 可以灵活处理各种类型的数据，包括连续值和离散值。 在相对较少的调参时间情况下，预测的准确率也比较高，相对SVM而言。 在使用一些健壮的损失函数，对异常值得鲁棒性非常强。比如Huber损失函数和Quantile损失函数。 2.GBDT缺点 由于弱学习器之间存在较强依赖关系，难以并行训练。可以通过自采样的SGBT来达到部分并行。 八、sklearn参数 在scikit-learning中，GradientBoostingClassifier对应GBDT的分类算法，GradientBoostingRegressor对应GBDT的回归算法。 具体算法参数情况如下： GradientBoostingRegressor(loss=’ls’, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=’friedman_mse’, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort=’auto’, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001) 参数说明： n_estimators：弱学习器的最大迭代次数，也就是最大弱学习器的个数。 learning_rate：步长，即每个学习器的权重缩减系数a，属于GBDT正则化方化手段之一。 subsample：子采样，取值(0,1]。决定是否对原始数据集进行采样以及采样的比例，也是GBDT正则化手段之一。 init：我们初始化的时候的弱学习器。若不设置，则使用默认的。 loss：损失函数，可选{&#39;ls&#39;-平方损失函数，&#39;lad&#39;绝对损失函数-,&#39;huber&#39;-huber损失函数,&#39;quantile&#39;-分位数损失函数}，默认&#39;ls&#39;。 alpha：当我们在使用Huber损失&quot;Huber&quot;和分位数损失&quot;quantile&quot;时，需要指定相应的值。默认是0.9，若噪声点比较多，可适当降低这个分位数值。 criterion：决策树节搜索最优分割点的准则，默认是&quot;friedman_mse&quot;，可选&quot;mse&quot;-均方误差与&#39;mae&quot;-绝对误差。 max_features：划分时考虑的最大特征数，就是特征抽样的意思，默认考虑全部特征。 max_depth：树的最大深度。 min_samples_split：内部节点再划分所需最小样本数。 min_samples_leaf：叶子节点最少样本数。 max_leaf_nodes：最大叶子节点数。 min_impurity_split：节点划分最小不纯度。 presort：是否预先对数据进行排序以加快最优分割点搜索的速度。默认是预先排序，若是稀疏数据，则不会预先排序，另外，稀疏数据不能设置为True。 validationfraction：为提前停止而预留的验证数据比例。当n_iter_no_change设置时才能用。 n_iter_no_change：当验证分数没有提高时，用于决定是否使用早期停止来终止训练。 八、GBDT应用场景 GBDT几乎可以用于所有回归问题（线性/非线性），相对loigstic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于分类问题。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/04/07/727250.html" />
<meta property="og:url" content="https://mlh.app/2019/04/07/727250.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-07T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"简介： &nbsp; &nbsp; GBDT是Gradient Boosting Decision Tree，梯度提升决策树，首先gbdt 是通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法。 &nbsp; &nbsp;gbdt的训练过程 &nbsp; &nbsp; &nbsp; &nbsp; 我们通过一张图片说明gbdt的训练过程:&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 解释：gbdt要经过多轮迭代，首先弱分类器1进行训练，分类器2在分类器1的残差的基础上进行训练，然后分类器3在分类器2的残差的基础上进行训练，继续迭代训练，直至达到迭代停止条件（达到设置的参数，精度要求等）。 &nbsp;对弱分类器的要求：足够简单，并且是低偏差高方差的。因为boosting的过程是一个不断降低偏差来提高最终分类器的精度的。 弱分类器一般选择为CART，根据上述对分类器的要求，CART的深度不会很深。最终的强分类器要将整个迭代过程产生的弱分类器进行加权求和。强分类器模型可以描述为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 模型一共训练m轮，生成m个弱分类器T(x; )。每个弱分类器的损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 为当前的模型，gbdt通过经验风险极小化来确定下一个弱分类器的参数。 在传统机器学习算法中， GBDT算的上top3的算法。想要真正理解GBDT的真正意义，就必须理解GBDT中的Gradient Boosting和Decison Tree分别是什么！ &nbsp; 一. Decision Tree:CART回归树 &nbsp; GBDT使用的决策树是CART回归树，无论是处理回归问题还是处理二分类，多分类问题，GBDT使用的决策树都是CART回归树。为啥不用CART分类树？因为CART分类树一般处理标称型数据，划分节点的最佳判别标准是某一特征对应的信息增益最大，但在回归树中的样本一般是连续数值，所以用信息增益作为判别标准已经不合适了，通常用最小误差平方和所对应的特征值作为节点的划分依据。下面通过具体代码详解CART回归树 CART回归树算法： 输入：数据集，叶节点中值的类型，划分节点时误差评判方法，允许的误差下降值，划分节点的最小样本数 输出：回归树 整体算法： def createTree(dataSet, leaf_type=regLeaf, err_type=regErr, ops=(1, 4)): &nbsp; #选择最佳划分特征及对应的特征值bestFeature, bestFeatureSplitValue bestFeature, bestFeatureSplitValue = chooseBestFeatureSplit(dataSet, leaf_type, err_type, ops) &nbsp; #根据最佳划分特征及对应的特征值将数据集分成两部分lset, rset lset, rset = binSplitDataset(dataSet, bestFeature, bestFeatureSplitValue) &nbsp; #将前面得到的两部分数据作为递归的输入，开始两次递归，创建整个决策树 ltree = createTree(lset, leaf_type, err_type, ops) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; rtree = createTree(rset, leaf_type, err_type, ops) #记录左右子树的信息 regTree={} regTree[&#39;splitFIdx&#39;] = bestFeature &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; regTree[&#39;splitFVal&#39;] = bestFeatureSplitValue regTree[&#39;left&#39;] = ltree regTree[&#39;right&#39;] =&nbsp; rtree &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; #&nbsp; 返回决策树 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return regTree 各个击破： #按照特征及对应的特征值把整个数据集划分为两部分：一部分大于该特征值，一部分小于等于该特征值 def binSplitDataset(dataset, feature, feature_value): mat0 = dataset[nonzero(dataset[:, feature] &gt; feature_value)[0], :] mat1 = dataset[nonzero(dataset[:, feature] &lt;= feature_value)[0], :] return mat0, mat1 def regErr(dataSet): #计算目标的总误差， return var(dataSet[:, -1])*shape(dataSet)[0] def regLeaf(dataSet): #生成叶节点，在回归树中是目标变量特征的均值 return mean(dataSet[:, -1]) #选择最佳划分特征及特征值 def chooseBestFeatureSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)): print(&#39;hello, begin to choose best feature split&#39;) tolS = ops[0] #由用户指定的参数，用于控制函数的停止,实际允许的误差下降值 tolN = ops[1] #切分的最小样本数 if len(set(dataSet[:, -1].T.tolist()[0])) == 1: #当输入数据集所有标签都相同时，说明节点纯度100%，函数返回 return None, leafType(dataSet) minErr = 1e6 bestFeature = 0 #最佳划分特征对应的Id bestFeatureSplitValue = 0.0 #最佳划分特征值 origin_error = errType(dataSet) #求未划分之前初始数据的总方差 num_sample, n = shape(dataSet) #获取样本数量及维度 for feature_idx in range(n - 1): #数据最后一列对应的目标变量不在遍历范围内 for split_value in dataSet[:, feature_idx]: #遍历特征Id为feature_idx的所有特征值 #feature_value作为拆分的阈值 l_data, r_data = binSplitDataset(dataSet, feature_idx, split_value) if shape(l_data)[0] &lt; tolN or shape(r_data)[0] &lt; tolN: #划分出的两数据集大小不满足要求, 继续下一次划分 continue splitErr = errType(l_data) + errType(r_data) #计算节点划分过程产生的误差 if minErr &gt; splitErr: #寻找最小划分误差及对应的特征id,特征值 minErr = splitErr bestFeature = feature_idx bestFeatureSplitValue = split_value if origin_error - minErr &lt; tolS: #切分数据集后效果提升不够大， 则不应该进行切分工作而直接将该节点作为叶节点 return None, leafType(dataSet) l_data, r_data = binSplitDataset(dataSet, bestFeature, bestFeatureSplitValue) #使用最佳特征值进行划分 # plt.subplot(121) # plt.plot(l_data[:, 0], l_data[:, 1], &#39;ro&#39;) # plt.plot(r_data[:, 0], r_data[:, 1], &#39;go&#39;) # plt.show() if shape(l_data)[0] &lt; tolN or shape(r_data)[0] &lt; tolN: #最佳特征值划分得到的结果不满足用户要求，则返回输入数据 return None, leafType(dataSet) 目标变量的均值 return bestFeature, bestFeatureSplitValue #返回最佳划分特征Id及特征值 至此一棵决策树就建立起来了，可利用下面的方法存储和载入模型 def storeTree(intree, filename): import pickle fw = open(filename, &#39;wb+&#39;) pickle.dump(intree, fw) fw.close() def grabTree(filename): import pickle fr = open(filename, &#39;rb+&#39;) return pickle.load(fr) CART回归树如何避免过拟合： 预剪枝：实际上createTree函数中设置输入参数ops（停止条件）就是进行预剪枝的一种方法，当按某一特征值划分数据集时，如果得到的两个子集的大小不满足ops的要求，停止此次划分；数据集划分前和划分后的误差未达到用户要求时，此次划分无效。该方法有一定的局限性：ops对误差的数量级非常敏感，而且事实上我们并不知道ops该设置为多少 后剪枝：由于预剪枝的局限性，我们考虑后剪枝技术，即从上到下找到叶节点，用测试集来判断叶节点合并能否降低测试误差， 如果能降低误差则合并叶节点。一定注意使用的是测试集来进行后剪枝操作 伪代码如下： 基于已有的树切分测试数据： &nbsp; &nbsp;&nbsp; 如果任意子集依然是一棵树，则递归该过程 &nbsp; &nbsp; 当左右两个子集都不是树时，计算当前两个叶节点合并后误差 &nbsp; &nbsp; 计算合并前的误差 &nbsp; &nbsp; 如果合并后的误差小于合并前的误差则合并 def prune(tree, testData): #没有测试数据则对树进行塌陷处理，即返回树平均值 if shape(testData)[0] == 0: return getMean(tree) # if isTree(tree): #如果输入tree的左节点或右节点依然是一个子树，则划分当前数据集，并进入子树递归 if ((type(tree[&#39;left&#39;])).__name__ == &#39;dict&#39; or (type(tree[&#39;right&#39;])).__name__ == &#39;dict&#39;): l_data, r_data = binSplitDataset(testData, tree[&#39;splitFIdx&#39;], tree[&#39;splitFVal&#39;]) if isTree(tree[&#39;left&#39;]): tree[&#39;left&#39;] = prune(tree[&#39;left&#39;], l_data) if isTree(tree[&#39;right&#39;]): tree[&#39;right&#39;] = prune(tree[&#39;right&#39;], r_data) #当左分支和右分支都是叶节点时 if not isTree(tree[&#39;left&#39;]) and not isTree(tree[&#39;right&#39;]): ldata, rdata = binSplitDataset(testData, tree[&#39;splitFIdx&#39;], tree[&#39;splitFVal&#39;]) treeMean = (tree[&#39;left&#39;]+tree[&#39;right&#39;])/2.0 #计算两个叶节点的均值，即两叶节点合并后的均值，与testData对应 #当 if shape(ldata)[0] == 0 and shape(rdata)[0] != 0 or shape(ldata)[0] != 0 and shape(rdata)[0] == 0: return treeMean elif shape(ldata)[0] != 0 and shape(rdata)[0] != 0: no_combine_err = sum(power(ldata[:, -1] - tree[&#39;left&#39;], 2)) + sum(power(rdata[:, -1] - tree[&#39;right&#39;], 2)) combineErr = sum(power(testData[:, -1] - treeMean, 2)) if combineErr &lt; no_combine_err: //合并后的误差小于合并前的误差则进行合并 return treeMean #进行合并 else: return tree #否则返回输入的tree else: return tree 使用CART回归树进行预测 def regTreeEval(tree, input): return float(tree) def treeForeCast(tree, testdata, modelType=regTreeEval): # print(&#39;tree[splitFIdx], tree[splitFVal]&#39;, tree[&#39;splitFIdx&#39;], tree[&#39;splitFVal&#39;]) # print(testdata[:, tree[&#39;splitFIdx&#39;]]) #递归停止条件：当输入tree不是子树，而是叶节点时，返回叶节点的均值 if (type(tree)).__name__ != &#39;dict&#39;: return modelType(tree, testdata) #测试数据在最佳划分特征下的数值大于最佳划分阈值则进入左子树进行递归，否则进入右子树进行递归 if(testdata[:, tree[&#39;splitFIdx&#39;]] &gt; tree[&#39;splitFVal&#39;]): if (type(tree[&#39;left&#39;])).__name__ == &#39;dict&#39;: return treeForeCast(tree[&#39;left&#39;], testdata, modelType) else: return modelType(tree[&#39;left&#39;], testdata) else: if (type(tree[&#39;right&#39;])).__name__ == &#39;dict&#39;: return treeForeCast(tree[&#39;right&#39;], testdata, modelType) else: return modelType(tree[&#39;right&#39;], testdata def createForeCast(tree, testdata, modelType=regTreeEval): m = len(testdata) h_cat = mat(zeros((m, 1))) for i in range(m): #对每个测试数据分别进行预测 h_cat[i, 0] = treeForeCast(tree, mat(testdata[i]), modelType) return h_cat 上面是对CART树的较为全面的解释 二、前向分布算法与Gradient Boosting 1. 前向分步算法（考虑加法模型） 要理解GBDT算法，得先来了解一下什么是前向分步算法。下面一起来瞧瞧。 加法模型是这样的： &nbsp;（就是基学习器的一种线性组合） 其中，&nbsp;&nbsp;为基函数，&nbsp;&nbsp;为基函数的参数，&nbsp;&nbsp;为基函数的系数。 在给定训练数据及损失函数&nbsp;&nbsp;的条件下，学习加法模型成为损失函数极小化问题： 前向分步算法求解这一优化问题的思路：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步去逼近上述的目标函数式，就可简化优化的复杂度，每一步只需优化如下损失函数： &nbsp;（每步学习一个基函数和系数） 前向分步算法流程： -------------------------------------------------------------------------------------------- 输入：训练数据集T=({&nbsp;&nbsp;})；损失函数L(y,f(x))；基函数集{&nbsp;&nbsp;}； 输出：加法模型f(x) (1) 初始化&nbsp; (2) 对m=1,2,...,M (a) 极小化损失函数 得到参数&nbsp; (b)更新 (3)得到加法模型 ------------------------------------------------------------------------------------------ 可见，前向分步算法将同时求解从m=1到M所有参数&nbsp;&nbsp;的优化问题简化成逐步求解各个&nbsp;&nbsp;的优化问题了。 &nbsp; 2. Gradient Boosting : 拟合负梯度 梯度提升树（Gradient Boosting）是提升树（Boosting Tree）的改进算法，故先讲一下boosting tree 先来个通俗理解：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。&nbsp; 提升树算法： （1）初始化 &nbsp; (2)&nbsp; 对m为1，2，3，...， M，其中m代表第几轮迭代，有如下步骤： &nbsp; 计算残差 &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; i=1,2, 3, ...N &nbsp;拟合残差学习一个回归树，得到 &nbsp;更新 &nbsp; (3) 得到回归问题提升树 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 上面伪代码中的残差是什么？ 在提升算法树中，假设上一轮得到强学习器:&nbsp; 损失函数为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 本轮迭代的目标是找到一个弱学习器： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 最小化本轮的损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 当采用平方损失函数时： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 这里： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 是当前模型拟合数据的残差（residual）所以，对于提升树来说只需要简单地拟合当前模型的残差。   回到我们上面讲的那个通俗易懂的例子中，第一次迭代的残差是10岁，第二 次残差4岁…… 当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Freidman提出了梯度提升树算法，这是利用最速下降的近似方法，其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。 那么负梯度长什么样呢？ 第t轮的第i个样本的损失函数的负梯度为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 此时不同的损失函数将会得到不同的负梯度，如果选择平方损失 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 负梯度为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 此时我们发现GBDT的负梯度就是残差，所以说对于回归问题，我们要拟合的就是残差。 GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。 &nbsp; 三、损失函数 在GBDT算法中，损失函数的选择十分重要。针对不同的问题，损失函数有不同的选择。 1.对于分类算法，其损失函数一般由对数损失函数和指数损失函数两种。 (1)指数损失函数表达式： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 其负梯度计算和叶子节点的最佳残差拟合参见Adaboost原理篇。 (2)如果是对数损失函数，分为二元分类和多元分类两种， 对于二元分类 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; L(y,h(x)) = log(1+exp(−yh(x))) &nbsp; 2.对于回归算法，常用损失函数有如下4种。 (1)平方损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (2)绝对损失函数： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 对应负梯度误差为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (3)Huber损失，它是均方差和绝对损失的折中产物，对于远离中心的异常点，采用绝对损失误差，而对于靠近中心的点则采用平方损失。这个界限一般用分位数点度量。损失函数如下 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; （4）分位数损失。它对应的是分位数回归的损失函数，表达式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 其中&nbsp;&nbsp;为分位数，需要我们在回归之前指定。对应的负梯度误差为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。 &nbsp; &nbsp; 四、回归问题 梯度提升算法（回归问题）流程： -------------------------------------------------------------------------------------------- 输入：训练数据集T={&nbsp;&nbsp;}，&nbsp;；损失函数L(y,f(x))； 输出：回归树&nbsp; (1)初始化 &nbsp;注：估计使损失函数极小化的常数值，它是只有一个根结点的树 (2)对m=1,2,...,M (a)对i=1,2,...N，计算 &nbsp;注：&nbsp;计算损失函数在当前模型的值，作为残差的估计 (b)对&nbsp;&nbsp;拟合一个回归树，得到第m棵树的叶结点区域&nbsp;&nbsp;,j=1,2,...,J (c)对j=1,2,...,J，计算 &nbsp;注：在损失函数极小化条件下，估计出相应叶结点区域的值 (d)更新 (3)得到回归树 -------------------------------------------------------------------------------------------- 五、分类问题（二分类与多分类） 这里看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合输出类别的误差。 为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法用类似逻辑回归的对数似然函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。此处仅讨论用对数似然函数的GBDT分类。对于对数似然损失函数，我们有又有二元分类和的多元分类的区别。 1.二分类GBDT算法 　　对于二分类GBDT，如果用类似逻辑回归的对数似然损失函数，则损失函数为： 　　 　　其中&nbsp;&nbsp;{-1,1}。此时的负梯度误差为： 　　 　　对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为 　　 　　由于上式比较难优化，我们一般使用近似值代替 　　 　　除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，二分类GBDT与GBDT回归算法过程相同。 2.多分类GBDT算法 　　多分类GBDT比二分类GBDT复杂些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为： 　　 　　其中如果样本输出类别为k，则&nbsp;&nbsp;=1.第k类的概率&nbsp;&nbsp;的表达式为： 　　 　　集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为： 　　 　　观察上式可以看出，其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。 　　对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为： 　　 　　由于上式比较难优化，我们一般使用近似值代替 　　 　　除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多分类GBDT与二分类GBDT以及GBDT回归算法过程相同。 六、正则化 对GBDT进行正则化来防止过拟合，主要有三种形式。 　　　　1.给每棵数的输出结果乘上一个步长a（learning rate）。 　　　　　　对于前面的弱学习器的迭代： 　　　　　　 　　　　　　加上正则化项，则有 　　　　　　 　　　　　　此处，a的取值范围为(0,1]。对于同样的训练集学习效果，较小的a意味着需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起决定算法的拟合效果。 　　　　2.第二种正则化的方式就是通过子采样比例(subsample)。取值范围为(0,1]。 　　　　　　GBDT这里的做法是在每一轮建树时，样本是从原始训练集中采用无放回随机抽样的方式产生，与随机森立的有放回抽样产生采样集的方式不同。若取值为1，则采用全部样本进行训练，若取值小于1，则不选取全部样本进行训练。选择小于1的比例可以减少方差，防止过拟合，但可能会增加样本拟合的偏差。取值要适中，推荐[0.5,0.8]。 　　　　3.第三种是对弱学习器即CART回归树进行正则化剪枝。（如控制树的最大深度、节点的最少样本数、最大叶子节点数、节点分支的最小样本数等） 七、GBDT优缺点 1.GBDT优点 可以灵活处理各种类型的数据，包括连续值和离散值。 在相对较少的调参时间情况下，预测的准确率也比较高，相对SVM而言。 在使用一些健壮的损失函数，对异常值得鲁棒性非常强。比如Huber损失函数和Quantile损失函数。 2.GBDT缺点 由于弱学习器之间存在较强依赖关系，难以并行训练。可以通过自采样的SGBT来达到部分并行。 八、sklearn参数 在scikit-learning中，GradientBoostingClassifier对应GBDT的分类算法，GradientBoostingRegressor对应GBDT的回归算法。 具体算法参数情况如下： GradientBoostingRegressor(loss=’ls’, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=’friedman_mse’, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort=’auto’, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001) 参数说明： n_estimators：弱学习器的最大迭代次数，也就是最大弱学习器的个数。 learning_rate：步长，即每个学习器的权重缩减系数a，属于GBDT正则化方化手段之一。 subsample：子采样，取值(0,1]。决定是否对原始数据集进行采样以及采样的比例，也是GBDT正则化手段之一。 init：我们初始化的时候的弱学习器。若不设置，则使用默认的。 loss：损失函数，可选{&#39;ls&#39;-平方损失函数，&#39;lad&#39;绝对损失函数-,&#39;huber&#39;-huber损失函数,&#39;quantile&#39;-分位数损失函数}，默认&#39;ls&#39;。 alpha：当我们在使用Huber损失&quot;Huber&quot;和分位数损失&quot;quantile&quot;时，需要指定相应的值。默认是0.9，若噪声点比较多，可适当降低这个分位数值。 criterion：决策树节搜索最优分割点的准则，默认是&quot;friedman_mse&quot;，可选&quot;mse&quot;-均方误差与&#39;mae&quot;-绝对误差。 max_features：划分时考虑的最大特征数，就是特征抽样的意思，默认考虑全部特征。 max_depth：树的最大深度。 min_samples_split：内部节点再划分所需最小样本数。 min_samples_leaf：叶子节点最少样本数。 max_leaf_nodes：最大叶子节点数。 min_impurity_split：节点划分最小不纯度。 presort：是否预先对数据进行排序以加快最优分割点搜索的速度。默认是预先排序，若是稀疏数据，则不会预先排序，另外，稀疏数据不能设置为True。 validationfraction：为提前停止而预留的验证数据比例。当n_iter_no_change设置时才能用。 n_iter_no_change：当验证分数没有提高时，用于决定是否使用早期停止来终止训练。 八、GBDT应用场景 GBDT几乎可以用于所有回归问题（线性/非线性），相对loigstic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于分类问题。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/04/07/727250.html","headline":"机器学习算法梳理第二篇–GBDT","dateModified":"2019-04-07T00:00:00+08:00","datePublished":"2019-04-07T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/04/07/727250.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>机器学习算法梳理第二篇--GBDT</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h2 id="%E7%AE%80%E4%BB%8B%EF%BC%9A">简介：</h2> 
  <p>&nbsp; &nbsp; GBDT是Gradient Boosting Decision Tree，梯度提升决策树，首先gbdt 是通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法。</p> 
  <p>&nbsp;</p> 
  <ul>
   <li>&nbsp;<strong>gbdt的训练过程</strong></li> 
  </ul>
  <p>&nbsp; &nbsp; &nbsp; &nbsp; 我们通过一张图片说明gbdt的训练过程:&nbsp;</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="" class="has" height="189" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407155400442.png" width="515"></p> 
  <p>解释：gbdt要经过多轮迭代，首先弱分类器1进行训练，分类器2在分类器1的残差的基础上进行训练，然后分类器3在分类器2的残差的基础上进行训练，继续迭代训练，直至达到迭代停止条件（达到设置的参数，精度要求等）。</p> 
  <p>&nbsp;对弱分类器的要求：足够简单，并且是低偏差高方差的。因为boosting的过程是一个不断降低偏差来提高最终分类器的精度的。</p> 
  <p>弱分类器一般选择为CART，根据上述对分类器的要求，CART的深度不会很深。最终的强分类器要将整个迭代过程产生的弱分类器进行加权求和。强分类器模型可以描述为：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img alt="F_{m}(x) = \sum_{m=1}^{M}T(x; \Theta_{m})" class="mathcode" src="https://private.codecogs.com/gif.latex?F_%7Bm%7D%28x%29%20%3D%20%5Csum_%7Bm%3D1%7D%5E%7BM%7DT%28x%3B%20%5CTheta_%7Bm%7D%29"></p> 
  <p>模型一共训练m轮，生成m个弱分类器T(x; <img alt="\Theta _m" class="mathcode" src="https://private.codecogs.com/gif.latex?%5CTheta%20_m">)。每个弱分类器的损失函数<img alt="\Theta _m" class="mathcode" src="https://private.codecogs.com/gif.latex?%5CTheta%20_m">：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="\hat{\Theta }_m = arg min\sum _{i=1}^{N}L(yi, F_{m-1}(xi))+T(xi;\Theta _{m})" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Chat%7B%5CTheta%20%7D_m%20%3D%20arg%20min%5Csum%20_%7Bi%3D1%7D%5E%7BN%7DL%28yi%2C%20F_%7Bm-1%7D%28xi%29%29&amp;plus;T%28xi%3B%5CTheta%20_%7Bm%7D%29"></p> 
  <p><img alt="F_{m-1}(x)" class="mathcode" src="https://private.codecogs.com/gif.latex?F_%7Bm-1%7D%28x%29">为当前的模型，gbdt通过经验风险极小化来确定下一个弱分类器的参数。</p> 
  <p>在传统机器学习算法中， GBDT算的上top3的算法。想要真正理解GBDT的真正意义，就必须理解GBDT中的Gradient Boosting和Decison Tree分别是什么！</p> 
  <p>&nbsp;</p> 
  <h2>一. Decision Tree:CART回归树</h2> 
  <p>&nbsp; GBDT使用的决策树是CART回归树，无论是处理回归问题还是处理二分类，多分类问题，GBDT使用的决策树都是CART回归树。为啥不用CART分类树？因为CART分类树一般处理标称型数据，划分节点的最佳判别标准是某一特征对应的信息增益最大，但在回归树中的样本一般是连续数值，所以用信息增益作为判别标准已经不合适了，通常用最小误差平方和所对应的特征值作为节点的划分依据。下面通过具体代码详解CART回归树</p> 
  <p>CART回归树算法：</p> 
  <p>输入：数据集，叶节点中值的类型，划分节点时误差评判方法，允许的误差下降值，划分节点的最小样本数</p> 
  <p>输出：回归树</p> 
  <p>整体算法：</p> 
  <pre>
<code class="language-html hljs">def createTree(dataSet, leaf_type=regLeaf, err_type=regErr, ops=(1, 4)):</code></pre> 
  <p style="text-indent:50px;">&nbsp; #选择最佳划分特征及对应的特征值bestFeature, bestFeatureSplitValue</p> 
  <p style="text-indent:50px;">bestFeature, bestFeatureSplitValue = chooseBestFeatureSplit(dataSet, leaf_type, err_type, ops)</p> 
  <p style="text-indent:50px;">&nbsp; #根据最佳划分特征及对应的特征值将数据集分成两部分lset, rset</p> 
  <p style="text-indent:50px;">lset, rset = binSplitDataset(dataSet, bestFeature, bestFeatureSplitValue)</p> 
  <p style="text-indent:50px;">&nbsp; #将前面得到的两部分数据作为递归的输入，开始两次递归，创建整个决策树</p> 
  <p style="text-indent:50px;">ltree = createTree(lset, leaf_type, err_type, ops)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; rtree = createTree(rset, leaf_type, err_type, ops)</p> 
  <p style="text-indent:50px;">#记录左右子树的信息</p> 
  <p style="text-indent:50px;">regTree={}</p> 
  <p style="text-indent:50px;">regTree['splitFIdx'] = bestFeature<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; regTree['splitFVal'] = bestFeatureSplitValue</p> 
  <p style="text-indent:50px;">regTree['left'] = ltree</p> 
  <p style="text-indent:50px;">regTree['right'] =&nbsp; rtree &nbsp; &nbsp;&nbsp;</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; #&nbsp; 返回决策树</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return regTree</p> 
  <p>各个击破：</p> 
  <p>#按照特征及对应的特征值把整个数据集划分为两部分：一部分大于该特征值，一部分小于等于该特征值</p> 
  <pre>
<code class="language-html hljs">def binSplitDataset(dataset, feature, feature_value):
    mat0 = dataset[nonzero(dataset[:, feature] &gt; feature_value)[0], :]
    mat1 = dataset[nonzero(dataset[:, feature] &lt;= feature_value)[0], :]
    return mat0, mat1</code></pre> 
  <pre>
<code class="language-html hljs">def regErr(dataSet): #计算目标的总误差，
    return var(dataSet[:, -1])*shape(dataSet)[0]

def regLeaf(dataSet): #生成叶节点，在回归树中是目标变量特征的均值
    return mean(dataSet[:, -1])</code></pre> 
  <p><span style="color:#f33b45;">#选择最佳划分特征及特征值</span></p> 
  <pre>
<code class="language-html hljs">def chooseBestFeatureSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):
    print('hello, begin to choose best feature split')
    tolS = ops[0] #由用户指定的参数，用于控制函数的停止,实际允许的误差下降值
    tolN = ops[1] #切分的最小样本数
    if len(set(dataSet[:, -1].T.tolist()[0])) == 1:  #当输入数据集所有标签都相同时，说明节点纯度100%，函数返回
        return None, leafType(dataSet)

    minErr = 1e6
    bestFeature = 0        #最佳划分特征对应的Id
    bestFeatureSplitValue = 0.0    #最佳划分特征值
    origin_error = errType(dataSet)   #求未划分之前初始数据的总方差

    num_sample, n = shape(dataSet)    #获取样本数量及维度
    for feature_idx in range(n - 1):  #数据最后一列对应的目标变量不在遍历范围内
        for split_value in dataSet[:, feature_idx]:   #遍历特征Id为feature_idx的所有特征值
            #feature_value作为拆分的阈值
            l_data, r_data = binSplitDataset(dataSet, feature_idx, split_value)
            if shape(l_data)[0] &lt; tolN or shape(r_data)[0] &lt; tolN:  #划分出的两数据集大小不满足要求, 继续下一次划分
                continue
            splitErr = errType(l_data) + errType(r_data)    #计算节点划分过程产生的误差

            if minErr &gt; splitErr:     #寻找最小划分误差及对应的特征id,特征值
                minErr = splitErr
                bestFeature = feature_idx
                bestFeatureSplitValue = split_value

    if origin_error - minErr &lt; tolS:  #切分数据集后效果提升不够大， 则不应该进行切分工作而直接将该节点作为叶节点
        return None, leafType(dataSet)
    l_data, r_data = binSplitDataset(dataSet, bestFeature, bestFeatureSplitValue) #使用最佳特征值进行划分
    # plt.subplot(121)
    # plt.plot(l_data[:, 0], l_data[:, 1], 'ro')
    # plt.plot(r_data[:, 0], r_data[:, 1], 'go')
    # plt.show()

    if shape(l_data)[0] &lt; tolN or shape(r_data)[0] &lt; tolN:  #最佳特征值划分得到的结果不满足用户要求，则返回输入数据
        return None, leafType(dataSet)                        目标变量的均值

    return bestFeature, bestFeatureSplitValue       #返回最佳划分特征Id及特征值</code></pre> 
  <p>至此一棵决策树就建立起来了，可利用下面的方法存储和载入模型</p> 
  <pre>
<code class="language-html hljs">def storeTree(intree, filename):
    import pickle
    fw = open(filename, 'wb+')
    pickle.dump(intree, fw)
    fw.close()

def grabTree(filename):
    import pickle
    fr = open(filename, 'rb+')
    return pickle.load(fr)</code></pre> 
  <p><span style="color:#f33b45;">CART回归树如何避免过拟合：</span></p> 
  <p>预剪枝：实际上createTree函数中设置输入参数ops（停止条件）就是进行预剪枝的一种方法，当按某一特征值划分数据集时，如果得到的两个子集的大小不满足ops的要求，停止此次划分；数据集划分前和划分后的误差未达到用户要求时，此次划分无效。该方法有一定的局限性：ops对误差的数量级非常敏感，而且事实上我们并不知道ops该设置为多少</p> 
  <p>后剪枝：由于预剪枝的局限性，我们考虑后剪枝技术，即从上到下找到叶节点，用测试集来判断叶节点合并能否降低测试误差，</p> 
  <p>如果能降低误差则合并叶节点。一定注意使用的是<span style="color:#f33b45;">测试集</span>来进行后剪枝操作</p> 
  <p>伪代码如下：</p> 
  <p>基于已有的树切分测试数据：</p> 
  <p>&nbsp; &nbsp;&nbsp; 如果任意子集依然是一棵树，则递归该过程</p> 
  <p>&nbsp; &nbsp; 当左右两个子集都不是树时，计算当前两个叶节点合并后误差</p> 
  <p>&nbsp; &nbsp; 计算合并前的误差</p> 
  <p>&nbsp; &nbsp; 如果合并后的误差小于合并前的误差则合并</p> 
  <pre>
<code class="language-html hljs">def prune(tree, testData):
    #没有测试数据则对树进行塌陷处理，即返回树平均值
    if shape(testData)[0] == 0: return getMean(tree)
    # if isTree(tree):
    #如果输入tree的左节点或右节点依然是一个子树，则划分当前数据集，并进入子树递归
    if ((type(tree['left'])).__name__ == 'dict'  or (type(tree['right'])).__name__ == 'dict'):
        l_data, r_data = binSplitDataset(testData, tree['splitFIdx'], tree['splitFVal'])
    if isTree(tree['left']):
        tree['left'] = prune(tree['left'], l_data)
    if isTree(tree['right']):
        tree['right'] = prune(tree['right'], r_data)

    #当左分支和右分支都是叶节点时
    if not isTree(tree['left']) and not isTree(tree['right']):
        ldata, rdata = binSplitDataset(testData, tree['splitFIdx'], tree['splitFVal'])
        treeMean = (tree['left']+tree['right'])/2.0     #计算两个叶节点的均值，即两叶节点合并后的均值，与testData对应
        #当
        if shape(ldata)[0] == 0 and shape(rdata)[0] != 0 or shape(ldata)[0] != 0 and shape(rdata)[0] == 0:
            return treeMean
        elif shape(ldata)[0] != 0 and shape(rdata)[0] != 0:
            no_combine_err = sum(power(ldata[:, -1] - tree['left'], 2)) + sum(power(rdata[:, -1] - tree['right'], 2))
        combineErr = sum(power(testData[:, -1] - treeMean, 2))
        if combineErr &lt; no_combine_err:    //合并后的误差小于合并前的误差则进行合并
            return treeMean  #进行合并
        else:
            return tree      #否则返回输入的tree
    else:
        return tree</code></pre> 
  <p><span style="color:#f33b45;">使用CART回归树进行预测</span></p> 
  <pre>
<code class="language-html hljs">def regTreeEval(tree, input):
    return float(tree)</code></pre> 
  <pre>
<code class="language-html hljs">def treeForeCast(tree, testdata, modelType=regTreeEval):
    # print('tree[splitFIdx], tree[splitFVal]', tree['splitFIdx'], tree['splitFVal'])
    # print(testdata[:, tree['splitFIdx']])

    #递归停止条件：当输入tree不是子树，而是叶节点时，返回叶节点的均值
    if (type(tree)).__name__ != 'dict': return modelType(tree, testdata)

    #测试数据在最佳划分特征下的数值大于最佳划分阈值则进入左子树进行递归，否则进入右子树进行递归
    if(testdata[:, tree['splitFIdx']] &gt; tree['splitFVal']):
        if (type(tree['left'])).__name__ == 'dict':
            return treeForeCast(tree['left'], testdata, modelType)
        else:
            return modelType(tree['left'], testdata)
    else:
        if (type(tree['right'])).__name__ == 'dict':
            return treeForeCast(tree['right'], testdata, modelType)
        else:
            return modelType(tree['right'], testdata

def createForeCast(tree, testdata, modelType=regTreeEval):
    m = len(testdata)
    h_cat = mat(zeros((m, 1)))
    for i in range(m):
        #对每个测试数据分别进行预测
        h_cat[i, 0] = treeForeCast(tree, mat(testdata[i]), modelType)
    return h_cat</code></pre> 
  <p>上面是对CART树的较为全面的解释</p> 
  <h2>二、前向分布算法与Gradient Boosting</h2> 
  <h3>1. 前向分步算法（考虑加法模型）</h3> 
  <p>要理解GBDT算法，得先来了解一下什么是前向分步算法。下面一起来瞧瞧。</p> 
  <p>加法模型是这样的：</p> 
  <p><img alt="f(x)=\sum_{m=1}^{M}{\beta_{m}b(x;\gamma_{m})}" class="has" src="https://www.zhihu.com/equation?tex=f%28x%29%3D%5Csum_%7Bm%3D1%7D%5E%7BM%7D%7B%5Cbeta_%7Bm%7Db%28x%3B%5Cgamma_%7Bm%7D%29%7D">&nbsp;（就是基学习器的一种线性组合）</p> 
  <p>其中，&nbsp;<img alt="b(x;\gamma_{m})" class="has" src="https://www.zhihu.com/equation?tex=b%28x%3B%5Cgamma_%7Bm%7D%29">&nbsp;为基函数，&nbsp;<img alt="\gamma_{m}" class="has" src="https://www.zhihu.com/equation?tex=%5Cgamma_%7Bm%7D">&nbsp;为基函数的参数，&nbsp;<img alt="\beta_{m}" class="has" src="https://www.zhihu.com/equation?tex=%5Cbeta_%7Bm%7D">&nbsp;为基函数的系数。</p> 
  <p>在给定训练数据及损失函数&nbsp;<img alt="L（y,f(x)）" class="has" src="https://www.zhihu.com/equation?tex=L%EF%BC%88y%2Cf%28x%29%EF%BC%89">&nbsp;的条件下，学习加法模型成为损失函数极小化问题：</p> 
  <p><img alt="min_{\beta_{m},\gamma_{m}}\sum_{i=1}^{N}{L(y_{i},\sum_{m=1}^{M}{\beta_{m}b(x_{i};\gamma_{m})})}" class="has" src="https://www.zhihu.com/equation?tex=min_%7B%5Cbeta_%7Bm%7D%2C%5Cgamma_%7Bm%7D%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7BL%28y_%7Bi%7D%2C%5Csum_%7Bm%3D1%7D%5E%7BM%7D%7B%5Cbeta_%7Bm%7Db%28x_%7Bi%7D%3B%5Cgamma_%7Bm%7D%29%7D%29%7D"></p> 
  <p>前向分步算法求解这一优化问题的思路：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步去逼近上述的目标函数式，就可简化优化的复杂度，每一步只需优化如下损失函数：</p> 
  <p><img alt="min_{\beta,\gamma}\sum_{i=1}^{N}{L(y_{i},\beta}b(x;\gamma))" class="has" src="https://www.zhihu.com/equation?tex=min_%7B%5Cbeta%2C%5Cgamma%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7BL%28y_%7Bi%7D%2C%5Cbeta%7Db%28x%3B%5Cgamma%29%29">&nbsp;（每步学习一个基函数和系数）</p> 
  <p>前向分步算法流程：</p> 
  <p>--------------------------------------------------------------------------------------------</p> 
  <p>输入：训练数据集T=({&nbsp;<img alt="(x_{1},y_{1}),(x_{2},y_{2}),...(x_{N},y_{N})" class="has" src="https://www.zhihu.com/equation?tex=%28x_%7B1%7D%2Cy_%7B1%7D%29%2C%28x_%7B2%7D%2Cy_%7B2%7D%29%2C...%28x_%7BN%7D%2Cy_%7BN%7D%29">&nbsp;})；损失函数L(y,f(x))；基函数集{&nbsp;<img alt="b(x;\gamma)" class="has" src="https://www.zhihu.com/equation?tex=b%28x%3B%5Cgamma%29">&nbsp;}；</p> 
  <p>输出：加法模型f(x)</p> 
  <p>(1) 初始化&nbsp;<img alt="f_{0}(x)=0" class="has" src="https://www.zhihu.com/equation?tex=f_%7B0%7D%28x%29%3D0"></p> 
  <p>(2) 对m=1,2,...,M</p> 
  <p>(a) 极小化损失函数</p> 
  <p><img alt="（\beta_{m},\gamma_{m}）=arg min_{\beta,\gamma}\sum_{i=1}^{N}{L(y_{i},f_{m-1}(x_{i})+\beta}b(x_{i};\gamma))" class="has" src="https://www.zhihu.com/equation?tex=%EF%BC%88%5Cbeta_%7Bm%7D%2C%5Cgamma_%7Bm%7D%EF%BC%89%3Darg+min_%7B%5Cbeta%2C%5Cgamma%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7BL%28y_%7Bi%7D%2Cf_%7Bm-1%7D%28x_%7Bi%7D%29%2B%5Cbeta%7Db%28x_%7Bi%7D%3B%5Cgamma%29%29"></p> 
  <p>得到参数&nbsp;<img alt="\beta_{m},\gamma_{m}" class="has" src="https://www.zhihu.com/equation?tex=%5Cbeta_%7Bm%7D%2C%5Cgamma_%7Bm%7D"></p> 
  <p>(b)更新</p> 
  <p><img alt="f_{m}(x)=f_{m-1}(x)+\beta_{m}b(x;\gamma_{m})" class="has" src="https://www.zhihu.com/equation?tex=f_%7Bm%7D%28x%29%3Df_%7Bm-1%7D%28x%29%2B%5Cbeta_%7Bm%7Db%28x%3B%5Cgamma_%7Bm%7D%29"></p> 
  <p>(3)得到加法模型</p> 
  <p><img alt="f(x)=f_{M}(x)=\sum_{m=1}^{M}{\beta_{m}}b(x;\beta_{m})" class="has" src="https://www.zhihu.com/equation?tex=f%28x%29%3Df_%7BM%7D%28x%29%3D%5Csum_%7Bm%3D1%7D%5E%7BM%7D%7B%5Cbeta_%7Bm%7D%7Db%28x%3B%5Cbeta_%7Bm%7D%29"></p> 
  <p>------------------------------------------------------------------------------------------</p> 
  <p>可见，前向分步算法将同时求解从m=1到M所有参数&nbsp;<img alt="\beta_{m},\gamma_{m}" class="has" src="https://www.zhihu.com/equation?tex=%5Cbeta_%7Bm%7D%2C%5Cgamma_%7Bm%7D">&nbsp;的优化问题简化成逐步求解各个&nbsp;<img alt="\beta_{m},\gamma_{m}" class="has" src="https://www.zhihu.com/equation?tex=%5Cbeta_%7Bm%7D%2C%5Cgamma_%7Bm%7D">&nbsp;的优化问题了。</p> 
  <p>&nbsp;</p> 
  <h3>2. Gradient Boosting : 拟合负梯度</h3> 
  <p>梯度提升树（Gradient Boosting）是提升树（Boosting Tree）的改进算法，故先讲一下boosting tree</p> 
  <p>先来个通俗理解：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。&nbsp;</p> 
  <hr>
  <p>提升树算法：</p> 
  <p>（1）初始化<img alt="f_{0}(x)=0" class="mathcode" src="https://private.codecogs.com/gif.latex?f_%7B0%7D%28x%29%3D0"></p> 
  <p>&nbsp; (2)&nbsp; 对m为1，2，3，...， M，其中m代表第几轮迭代，有如下步骤：</p> 
  <ul>
   <li>&nbsp; 计算残差 &nbsp; &nbsp; &nbsp;&nbsp; <img alt="Res_{mi}=yi-f_{m-1}(x_{i})" class="mathcode" src="https://private.codecogs.com/gif.latex?Res_%7Bmi%7D%3Dyi-f_%7Bm-1%7D%28x_%7Bi%7D%29"> &nbsp; &nbsp; &nbsp; i=1,2, 3, ...N</li> 
   <li>&nbsp;拟合残差<img alt="Res_{mi}" class="mathcode" src="https://private.codecogs.com/gif.latex?Res_%7Bmi%7D">学习一个回归树，得到<img alt="h_{m}(x)" class="mathcode" src="https://private.codecogs.com/gif.latex?h_%7Bm%7D%28x%29"></li> 
   <li>&nbsp;更新<img alt="f_{m}(x) = f_{m-1}+h_{m}(x)" class="mathcode" src="https://private.codecogs.com/gif.latex?f_%7Bm%7D%28x%29%20%3D%20f_%7Bm-1%7D&amp;plus;h_%7Bm%7D%28x%29"></li> 
  </ul>
  <p>&nbsp; (3) 得到回归问题提升树</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="f_{M}(x) = \sum _{i=1}^{M}(h_i(x))" class="mathcode" src="https://private.codecogs.com/gif.latex?f_%7BM%7D%28x%29%20%3D%20%5Csum%20_%7Bi%3D1%7D%5E%7BM%7D%28h_i%28x%29%29"></p> 
  <hr>
  <p>上面伪代码中的残差是什么？</p> 
  <p>在提升算法树中，假设上一轮得到强学习器:&nbsp; <img alt="f_{t-1}(x)" class="mathcode" src="https://private.codecogs.com/gif.latex?f_%7Bt-1%7D%28x%29"></p> 
  <p>损失函数为：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="L(y, f_{t-1}(x))" class="mathcode" src="https://private.codecogs.com/gif.latex?L%28y%2C%20f_%7Bt-1%7D%28x%29%29"></p> 
  <p>本轮迭代的目标是找到一个弱学习器：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="h_t(x)" class="mathcode" src="https://private.codecogs.com/gif.latex?h_t%28x%29"></p> 
  <p>最小化本轮的损失函数：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img alt="L(y, f_{t}(x))=L(y, f_{t-1}(x) + h_t(x))" class="mathcode" src="https://private.codecogs.com/gif.latex?L%28y%2C%20f_%7Bt%7D%28x%29%29%3DL%28y%2C%20f_%7Bt-1%7D%28x%29%20&amp;plus;%20h_t%28x%29%29"></p> 
  <p>当采用平方损失函数时：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="L(y, f_{t}(x))=L(y, f_{t-1}(x) + h_t(x)) =(y - (f_{t-1}(x)+h_t(x)))^{2} =(r-h_t(x))^{2}" class="mathcode" src="https://private.codecogs.com/gif.latex?L%28y%2C%20f_%7Bt%7D%28x%29%29%3DL%28y%2C%20f_%7Bt-1%7D%28x%29%20&amp;plus;%20h_t%28x%29%29%20%3D%28y%20-%20%28f_%7Bt-1%7D%28x%29&amp;plus;h_t%28x%29%29%29%5E%7B2%7D%20%3D%28r-h_t%28x%29%29%5E%7B2%7D"></p> 
  <p>这里：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img alt="r=y-f_{t-1}(x)" class="mathcode" src="https://private.codecogs.com/gif.latex?r%3Dy-f_%7Bt-1%7D%28x%29"></p> 
  <p>是当前模型拟合数据的残差（residual）所以，对于提升树来说只需要简单地拟合当前模型的残差。<br>   回到我们上面讲的那个通俗易懂的例子中，第一次迭代的残差是10岁，第二 次残差4岁……</p> 
  <p>当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Freidman提出了梯度提升树算法，这是利用最速下降的近似方法，其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。<br> 那么负梯度长什么样呢？<br> 第t轮的第i个样本的损失函数的负梯度为：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="" class="has" height="82" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407194233864.png" width="254"></p> 
  <p>此时不同的损失函数将会得到不同的负梯度，如果选择平方损失</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="" class="has" height="53" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190407201334633.png" width="261"></p> 
  <p>负梯度为：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="" class="has" height="69" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019040720141554.png" width="365"></p> 
  <p>此时我们发现GBDT的负梯度就是残差，所以说对于回归问题，我们要拟合的就是残差。</p> 
  <p>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。<br><br> &nbsp;</p> 
  <h2>三、损失函数</h2> 
  <p>在GBDT算法中，损失函数的选择十分重要。针对不同的问题，损失函数有不同的选择。</p> 
  <p>1.对于分类算法，其损失函数一般由对数损失函数和指数损失函数两种。</p> 
  <p>(1)指数损失函数表达式：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="L(y,f(x))=e^{(-yf(x))}" class="has" src="https://www.zhihu.com/equation?tex=L%28y%2Cf%28x%29%29%3De%5E%7B%28-yf%28x%29%29%7D"></p> 
  <p>其负梯度计算和叶子节点的最佳残差拟合参见Adaboost原理篇。<br> (2)如果是对数损失函数，分为二元分类和多元分类两种，<br> 对于二元分类</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; L(y,h(x)) = log(1+exp(−yh(x)))</p> 
  <p>&nbsp;</p> 
  <p>2.对于回归算法，常用损失函数有如下4种。</p> 
  <p>(1)平方损失函数：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img alt="L(y,f(x))=(y-f(x))^{2}" class="has" src="https://www.zhihu.com/equation?tex=L%28y%2Cf%28x%29%29%3D%28y-f%28x%29%29%5E%7B2%7D"></p> 
  <p>(2)绝对损失函数：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img alt="L(y,f(x)）=|y-f(x)|" class="has" src="https://www.zhihu.com/equation?tex=L%28y%2Cf%28x%29%EF%BC%89%3D%7Cy-f%28x%29%7C"></p> 
  <p>对应负梯度误差为：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img alt="sign(y_{i}-f(x_{i}))" class="has" src="https://www.zhihu.com/equation?tex=sign%28y_%7Bi%7D-f%28x_%7Bi%7D%29%29"></p> 
  <p>(3)Huber损失，它是均方差和绝对损失的折中产物，对于远离中心的异常点，采用绝对损失误差，而对于靠近中心的点则采用平方损失。这个界限一般用分位数点度量。损失函数如下</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img alt="" class="has" src="https://img2018.cnblogs.com/blog/1246026/201903/1246026-20190304104133615-596284835.png"></p> 
  <p>（4）分位数损失。它对应的是分位数回归的损失函数，表达式为：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="L(y,f(x))=\sum_{y\geq f(x)}^{}{\theta|y-f(x)|}+\sum_{y<f(x)}^{}{(1-\theta)|y-f(x)|}" class="has" src="https://www.zhihu.com/equation?tex=L%28y%2Cf%28x%29%29%3D%5Csum_%7By%5Cgeq+f%28x%29%7D%5E%7B%7D%7B%5Ctheta%7Cy-f%28x%29%7C%7D%2B%5Csum_%7By%3Cf%28x%29%7D%5E%7B%7D%7B%281-%5Ctheta%29%7Cy-f%28x%29%7C%7D"></p> 
  <p>其中&nbsp;<img alt="\theta" class="has" src="https://www.zhihu.com/equation?tex=%5Ctheta">&nbsp;为分位数，需要我们在回归之前指定。对应的负梯度误差为：</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; <img alt="" class="has" src="https://pic1.zhimg.com/80/v2-167f017cefced0ebd76b128cf502dd84_hd.jpg" width="318"></p> 
  <p>对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <h2>四、回归问题</h2> 
  <p>梯度提升算法（回归问题）流程：</p> 
  <p>--------------------------------------------------------------------------------------------</p> 
  <p>输入：训练数据集T={&nbsp;<img alt="(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{N},y_{N})" class="has" src="https://www.zhihu.com/equation?tex=%28x_%7B1%7D%2Cy_%7B1%7D%29%2C%28x_%7B2%7D%2Cy_%7B2%7D%29%2C...%2C%28x_%7BN%7D%2Cy_%7BN%7D%29">&nbsp;}，&nbsp;<img alt="x_{i}\in\chi\subseteq R^{n},y_{i}\in\gamma\subseteq R" class="has" src="https://www.zhihu.com/equation?tex=+++++++x_%7Bi%7D%5Cin%5Cchi%5Csubseteq+R%5E%7Bn%7D%2Cy_%7Bi%7D%5Cin%5Cgamma%5Csubseteq+R">；损失函数L(y,f(x))；</p> 
  <p>输出：回归树&nbsp;<img alt="\tilde{f}(x)" class="has" src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bf%7D%28x%29"></p> 
  <p>(1)初始化</p> 
  <p><img alt="f_{0}=arg min_{c}\sum_{i=1}^{N}{L(y_{i},c})" class="has" src="https://www.zhihu.com/equation?tex=f_%7B0%7D%3Darg+min_%7Bc%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7BL%28y_%7Bi%7D%2Cc%7D%29">&nbsp;注：估计使损失函数极小化的常数值，它是只有一个根结点的树</p> 
  <p>(2)对m=1,2,...,M</p> 
  <p>(a)对i=1,2,...N，计算</p> 
  <p><img alt="r_{mi}=-[\frac{\partial L(y_{i},f(x_{i}))}{\partial f(x_{i})}]_{f(x)=f_{m-1}(x)}" class="has" src="https://www.zhihu.com/equation?tex=r_%7Bmi%7D%3D-%5B%5Cfrac%7B%5Cpartial+L%28y_%7Bi%7D%2Cf%28x_%7Bi%7D%29%29%7D%7B%5Cpartial+f%28x_%7Bi%7D%29%7D%5D_%7Bf%28x%29%3Df_%7Bm-1%7D%28x%29%7D">&nbsp;注：&nbsp;计算损失函数在当前模型的值，作为残差的估计</p> 
  <p>(b)对&nbsp;<img alt="r_{mi}" class="has" src="https://www.zhihu.com/equation?tex=r_%7Bmi%7D">&nbsp;拟合一个回归树，得到第m棵树的叶结点区域&nbsp;<img alt="R_{mj}" class="has" src="https://www.zhihu.com/equation?tex=R_%7Bmj%7D">&nbsp;,j=1,2,...,J</p> 
  <p>(c)对j=1,2,...,J，计算</p> 
  <p><img alt="c_{mj}=arg min_{c}\sum_{x_{j}\in R_{mj}}^{}{L(y_{i},f_{m-1}(x_{i})+c)}" class="has" src="https://www.zhihu.com/equation?tex=c_%7Bmj%7D%3Darg+min_%7Bc%7D%5Csum_%7Bx_%7Bj%7D%5Cin+R_%7Bmj%7D%7D%5E%7B%7D%7BL%28y_%7Bi%7D%2Cf_%7Bm-1%7D%28x_%7Bi%7D%29%2Bc%29%7D">&nbsp;注：在损失函数极小化条件下，估计出相应叶结点区域的值</p> 
  <p>(d)更新</p> 
  <p><img alt="f_{m}(x)=f_{m-1}(x)+\sum_{j=1}^{J}{c_{mj}I(x\in R_{mj})}" class="has" src="https://www.zhihu.com/equation?tex=f_%7Bm%7D%28x%29%3Df_%7Bm-1%7D%28x%29%2B%5Csum_%7Bj%3D1%7D%5E%7BJ%7D%7Bc_%7Bmj%7DI%28x%5Cin+R_%7Bmj%7D%29%7D"></p> 
  <p>(3)得到回归树</p> 
  <p><img alt="\tilde{f}(x)=f_{M}(x)=\sum_{m=1}^{M}{}\sum_{j=1}^{J}{c_{mj}I(x\in R_{mj})}" class="has" src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bf%7D%28x%29%3Df_%7BM%7D%28x%29%3D%5Csum_%7Bm%3D1%7D%5E%7BM%7D%7B%7D%5Csum_%7Bj%3D1%7D%5E%7BJ%7D%7Bc_%7Bmj%7DI%28x%5Cin+R_%7Bmj%7D%29%7D"></p> 
  <p>--------------------------------------------------------------------------------------------</p> 
  <h2>五、分类问题（二分类与多分类）</h2> 
  <p>这里看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合输出类别的误差。</p> 
  <p>为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法用类似逻辑回归的对数似然函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。此处仅讨论用对数似然函数的GBDT分类。对于对数似然损失函数，我们有又有二元分类和的多元分类的区别。</p> 
  <p>1.二分类GBDT算法</p> 
  <p>　　对于二分类GBDT，如果用类似逻辑回归的对数似然损失函数，则损失函数为：</p> 
  <p>　　<img alt="L(y,f(x))=log(1+exp(-yf(x)))" class="has" src="https://www.zhihu.com/equation?tex=L%28y%2Cf%28x%29%29%3Dlog%281%2Bexp%28-yf%28x%29%29%29"></p> 
  <p>　　其中&nbsp;<img alt="y\in" class="has" src="https://www.zhihu.com/equation?tex=y%5Cin">&nbsp;{-1,1}。此时的负梯度误差为：</p> 
  <p>　　<img alt="r_{ti}=-[\frac{\partial L(y,f(x_{i}))}{\partial f(x_{i})}_{f(x)=f_{t-1}(x)}=\frac{y_{i}}{1+exp(y_{i}f(x_{i}))}" class="has" src="https://www.zhihu.com/equation?tex=r_%7Bti%7D%3D-%5B%5Cfrac%7B%5Cpartial+L%28y%2Cf%28x_%7Bi%7D%29%29%7D%7B%5Cpartial+f%28x_%7Bi%7D%29%7D_%7Bf%28x%29%3Df_%7Bt-1%7D%28x%29%7D%3D%5Cfrac%7By_%7Bi%7D%7D%7B1%2Bexp%28y_%7Bi%7Df%28x_%7Bi%7D%29%29%7D"></p> 
  <p>　　对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为</p> 
  <p>　　<img alt="c_{tj}=arg min_{c}\sum_{x_{i}\in R_{tj}}^{}{log(1+exp(-y_{i}(f_{t-1}(x_{i})+c})))" class="has" src="https://www.zhihu.com/equation?tex=c_%7Btj%7D%3Darg+min_%7Bc%7D%5Csum_%7Bx_%7Bi%7D%5Cin+R_%7Btj%7D%7D%5E%7B%7D%7Blog%281%2Bexp%28-y_%7Bi%7D%28f_%7Bt-1%7D%28x_%7Bi%7D%29%2Bc%7D%29%29%29"></p> 
  <p>　　由于上式比较难优化，我们一般使用近似值代替</p> 
  <p>　　<img alt="c_{tj}=\frac{\sum_{x_{i}\in R_{tj}}^{}{r_{tj}}}{\sum_{x_{i}\in R_{tj}}^{}{|r_{tj}|(1-|r_{tj}|)}}" class="has" src="https://www.zhihu.com/equation?tex=c_%7Btj%7D%3D%5Cfrac%7B%5Csum_%7Bx_%7Bi%7D%5Cin+R_%7Btj%7D%7D%5E%7B%7D%7Br_%7Btj%7D%7D%7D%7B%5Csum_%7Bx_%7Bi%7D%5Cin+R_%7Btj%7D%7D%5E%7B%7D%7B%7Cr_%7Btj%7D%7C%281-%7Cr_%7Btj%7D%7C%29%7D%7D"></p> 
  <p>　　除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，二分类GBDT与GBDT回归算法过程相同。</p> 
  <p>2.多分类GBDT算法</p> 
  <p>　　多分类GBDT比二分类GBDT复杂些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：</p> 
  <p>　　<img alt="L(y,f(x))=-\sum_{k=1}^{K}{y_{k}log(p_{k}(x))}" class="has" src="https://www.zhihu.com/equation?tex=L%28y%2Cf%28x%29%29%3D-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%7By_%7Bk%7Dlog%28p_%7Bk%7D%28x%29%29%7D"></p> 
  <p>　　其中如果样本输出类别为k，则&nbsp;<img alt="y_{k}" class="has" src="https://www.zhihu.com/equation?tex=y_%7Bk%7D">&nbsp;=1.第k类的概率&nbsp;<img alt="p_{k}(x)" class="has" src="https://www.zhihu.com/equation?tex=p_%7Bk%7D%28x%29">&nbsp;的表达式为：</p> 
  <p>　　<img alt="p_{k}(x)=\frac{exp(f_{k}(x))}{\sum_{l=1}^{K}{exp(f_{l}(x))}}" class="has" src="https://www.zhihu.com/equation?tex=p_%7Bk%7D%28x%29%3D%5Cfrac%7Bexp%28f_%7Bk%7D%28x%29%29%7D%7B%5Csum_%7Bl%3D1%7D%5E%7BK%7D%7Bexp%28f_%7Bl%7D%28x%29%29%7D%7D"></p> 
  <p>　　集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为：</p> 
  <p>　　<img alt="t_{til}=-[\frac{\partial L(y_{i},f(x_{i}))}{\partial f(x_{i})}]_{f_{k}(x)=f_{l,t-1}(x)}=y_{il}-p_{l,t-1}(x_{i})" class="has" src="https://www.zhihu.com/equation?tex=t_%7Btil%7D%3D-%5B%5Cfrac%7B%5Cpartial+L%28y_%7Bi%7D%2Cf%28x_%7Bi%7D%29%29%7D%7B%5Cpartial+f%28x_%7Bi%7D%29%7D%5D_%7Bf_%7Bk%7D%28x%29%3Df_%7Bl%2Ct-1%7D%28x%29%7D%3Dy_%7Bil%7D-p_%7Bl%2Ct-1%7D%28x_%7Bi%7D%29"></p> 
  <p>　　观察上式可以看出，其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。</p> 
  <p>　　对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为：</p> 
  <p>　　<img alt="c_{tjl}=arg min_{c_{jl}}\sum_{i=0}^{m}{}\sum_{k=1}^{K}{L(y_{k},f_{t-1,l}(x)+\sum_{j=0}^{J}{c_{jl}I(x_{i}\in R_{tj})}})" class="has" src="https://www.zhihu.com/equation?tex=c_%7Btjl%7D%3Darg+min_%7Bc_%7Bjl%7D%7D%5Csum_%7Bi%3D0%7D%5E%7Bm%7D%7B%7D%5Csum_%7Bk%3D1%7D%5E%7BK%7D%7BL%28y_%7Bk%7D%2Cf_%7Bt-1%2Cl%7D%28x%29%2B%5Csum_%7Bj%3D0%7D%5E%7BJ%7D%7Bc_%7Bjl%7DI%28x_%7Bi%7D%5Cin+R_%7Btj%7D%29%7D%7D%29"></p> 
  <p>　　由于上式比较难优化，我们一般使用近似值代替</p> 
  <p>　　<img alt="c_{tjl}=\frac{K-1}{K}\frac{\sum_{x_{i}\in R_{tjl}}^{}{r_{til}}}{\sum_{x_{i}\in R_{til}}^{}{|r_{til}|(1-|r_{til}|)}}" class="has" src="https://www.zhihu.com/equation?tex=c_%7Btjl%7D%3D%5Cfrac%7BK-1%7D%7BK%7D%5Cfrac%7B%5Csum_%7Bx_%7Bi%7D%5Cin+R_%7Btjl%7D%7D%5E%7B%7D%7Br_%7Btil%7D%7D%7D%7B%5Csum_%7Bx_%7Bi%7D%5Cin+R_%7Btil%7D%7D%5E%7B%7D%7B%7Cr_%7Btil%7D%7C%281-%7Cr_%7Btil%7D%7C%29%7D%7D"></p> 
  <p>　　除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多分类GBDT与二分类GBDT以及GBDT回归算法过程相同。</p> 
  <h2>六、正则化</h2> 
  <ul>
   <li>对GBDT进行正则化来防止过拟合，主要有三种形式。</li> 
  </ul>
  <p>　　　　1.给每棵数的输出结果乘上一个步长a（learning rate）。</p> 
  <p>　　　　　　对于前面的弱学习器的迭代：</p> 
  <p>　　　　　　<img alt="f_{m}(x)=f_{m-1}(x)+T(x;\gamma_{m})" class="has" src="https://www.zhihu.com/equation?tex=f_%7Bm%7D%28x%29%3Df_%7Bm-1%7D%28x%29%2BT%28x%3B%5Cgamma_%7Bm%7D%29"></p> 
  <p>　　　　　　加上正则化项，则有</p> 
  <p>　　　　　　<img alt="f_{m}(x)=f_{m-1}(x)+aT(x;\gamma_{m})" class="has" src="https://www.zhihu.com/equation?tex=f_%7Bm%7D%28x%29%3Df_%7Bm-1%7D%28x%29%2BaT%28x%3B%5Cgamma_%7Bm%7D%29"></p> 
  <p>　　　　　　此处，a的取值范围为(0,1]。对于同样的训练集学习效果，较小的a意味着需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起决定算法的拟合效果。</p> 
  <p>　　　　2.第二种正则化的方式就是通过子采样比例(subsample)。取值范围为(0,1]。</p> 
  <p>　　　　　　GBDT这里的做法是在每一轮建树时，样本是从原始训练集中采用无放回随机抽样的方式产生，与随机森立的有放回抽样产生采样集的方式不同。若取值为1，则采用全部样本进行训练，若取值小于1，则不选取全部样本进行训练。选择小于1的比例可以减少方差，防止过拟合，但可能会增加样本拟合的偏差。取值要适中，推荐[0.5,0.8]。</p> 
  <p>　　　　3.第三种是对弱学习器即CART回归树进行正则化剪枝。（如控制树的最大深度、节点的最少样本数、最大叶子节点数、节点分支的最小样本数等）</p> 
  <h2>七、GBDT优缺点</h2> 
  <p>1.GBDT优点</p> 
  <ul>
   <li>可以灵活处理各种类型的数据，包括连续值和离散值。</li> 
   <li>在相对较少的调参时间情况下，预测的准确率也比较高，相对SVM而言。</li> 
   <li>在使用一些健壮的损失函数，对异常值得鲁棒性非常强。比如Huber损失函数和Quantile损失函数。</li> 
  </ul>
  <p>2.GBDT缺点</p> 
  <ul>
   <li>由于弱学习器之间存在较强依赖关系，难以并行训练。可以通过自采样的SGBT来达到部分并行。</li> 
  </ul>
  <h2>八、sklearn参数</h2> 
  <p>在scikit-learning中，GradientBoostingClassifier对应GBDT的分类算法，GradientBoostingRegressor对应GBDT的回归算法。</p> 
  <p>具体算法参数情况如下：</p> 
  <pre class="has">
<code>GradientBoostingRegressor(loss=’ls’, learning_rate=0.1, n_estimators=100, 
                subsample=1.0, criterion=’friedman_mse’, min_samples_split=2,
                min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3,
                min_impurity_decrease=0.0, min_impurity_split=None, init=None, 
                random_state=None, max_features=None, alpha=0.9, verbose=0, 
                max_leaf_nodes=None, warm_start=False, presort=’auto’, 
                validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)
</code></pre> 
  <p>参数说明：</p> 
  <ul>
   <li>n_estimators：弱学习器的最大迭代次数，也就是最大弱学习器的个数。</li> 
   <li>learning_rate：步长，即每个学习器的权重缩减系数a，属于GBDT正则化方化手段之一。</li> 
   <li>subsample：子采样，取值(0,1]。决定是否对原始数据集进行采样以及采样的比例，也是GBDT正则化手段之一。</li> 
   <li>init：我们初始化的时候的弱学习器。若不设置，则使用默认的。</li> 
   <li>loss：损失函数，可选{'ls'-平方损失函数，'lad'绝对损失函数-,'huber'-huber损失函数,'quantile'-分位数损失函数}，默认'ls'。</li> 
   <li>alpha：当我们在使用Huber损失"Huber"和分位数损失"quantile"时，需要指定相应的值。默认是0.9，若噪声点比较多，可适当降低这个分位数值。</li> 
   <li>criterion：决策树节搜索最优分割点的准则，默认是"friedman_mse"，可选"mse"-均方误差与'mae"-绝对误差。</li> 
   <li>max_features：划分时考虑的最大特征数，就是特征抽样的意思，默认考虑全部特征。</li> 
   <li>max_depth：树的最大深度。</li> 
   <li>min_samples_split：内部节点再划分所需最小样本数。</li> 
   <li>min_samples_leaf：叶子节点最少样本数。</li> 
   <li>max_leaf_nodes：最大叶子节点数。</li> 
   <li>min_impurity_split：节点划分最小不纯度。</li> 
   <li>presort：是否预先对数据进行排序以加快最优分割点搜索的速度。默认是预先排序，若是稀疏数据，则不会预先排序，另外，稀疏数据不能设置为True。</li> 
   <li>validation<em>fraction：</em>为提前停止而预留的验证数据比例。当n_iter_no_change设置时才能用。</li> 
   <li>n_iter_no_change：当验证分数没有提高时，用于决定是否使用早期停止来终止训练。</li> 
  </ul>
  <h2>八、GBDT应用场景</h2> 
  <p>GBDT几乎可以用于所有回归问题（线性/非线性），相对loigstic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于分类问题。</p> 
  <p><br> &nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
