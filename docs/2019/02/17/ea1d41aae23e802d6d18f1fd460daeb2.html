<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>我们期待的TensorFlow 2.0还有哪些变化？ | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="我们期待的TensorFlow 2.0还有哪些变化？" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="来源 | Google TensorFlow 团队 为提高 TensorFlow 的工作效率，TensorFlow 2.0 进行了多项更改，包括删除了多余的 API，使API 更加一致统一，例如统一的 RNNs (循环神经网络），统一的优化器，并且Python 运行时更好地集成了 Eager execution 。 许多 RFC 已经对 TensorFlow 2.0 的这些更改给出了解释。本指南基于您对 TensorFlow 1.x 有一定的了解的前提，为您介绍在 TensorFlow 2.0 中的开发有什么不同。 API 整理 在 TensorFlow 2.0 中，有许多 1.X 的 API 被删除或移动&nbsp;了。也有部分 1.X 的 API 被 2.0 版本的等价 API 所替代：tf.summary，tf.keras.metrics 和 tf.keras.optimizers。自动应用这些重命名，最简单的方法是使用 TensorFlow 2.0 升级脚本。 Eager execution TensorFlow 1.X 要求用户通过调用 tf.* API 手动的将抽象语法树（图）拼接在一起。然后，它要求用户将一组输出张量和输入张量传递给 session.run() 调用，来手动编译抽象语法树。相比之下，TensorFlow 2.0 executes eagerly（如正常使用 Python 一样）在 2.0 的版本中，其 graphs（抽象语法树）和 sessions 在实现的细节上应该是一样的。 不再有全局变量 TensorFlow 1.X 非常依赖于隐式全局命名空间。当你调用 tf.Variable 时，它会被放入默认图中，即使你忘记了指向它的 Python 变量它也会留在那里。这时，您可以恢复该 tf.Variable()，但前提是您得知道它已创建的名称。如果您无法控制变量的创建，很难做到这一点。因此，各种机制以及寻找用户创建变量的框架不断涌现，试图帮助用户再次找到他们的变量。 TensorFlow 2.0 取消了所有这些机制（Variables 2.0 RFC），支持默认机制：跟踪变量！ 如果你不再用到某个 tf.Variable，它就会被回收。 Functions, not sessions session.run() 的调用几乎类似于函数调用：指定输入和要调用的函数，然后返回一组输出。在 TensorFlow 2.0 中，您可以使用 tf.function() 来修饰 Python 函数以将其标记为 JIT（ Just-In-Time ）编译，以便 TensorFlow 将其作为单个图运行（Functions 2.0 RFC）。 这种机制使得 TensorFlow 2.0 拥有图模式的许多优点： 性能：该函数可以被优化，例如节点修剪，内核融合等 可移植性：该函数可以导出 / 重新导入（SavedModel 2.0 RFC），允许用户重用和将 TensorFlow 函数作为模块共享 # TensorFlow 1.X outputs = session.run(f(placeholder), feed_dict={placeholder: input})# TensorFlow 2.0 outputs = f(input) 由于能够自由地穿插 Python 和 TensorFlow 代码，您能够充分利用 Python 的表现力。而且，可移植的 TensorFlow 在没有 Python 解释器的情况下也可执行。比如：mobile，C ++ 和 JS。避免用户在添加 @tf.function 时重写代码，AutoGraph 会将 Python 构造的一个子集转换成 TensorFlow 等价物。 TensorFlow 2.0 常用的建议 将代码重构为更小的函数 TensorFlow 1.X 中的常见使用模式是 “kitchen sink” 策略，即预先列出所有可能计算的并集，然后通过 session.run() 计算选定的张量。在 TensorFlow 2.0 中，用户应该根据需求将代码重构为更小的函数。通常情况下，没有必要用 tf.function 来修饰这些较小的函数；仅使用 tf.function 来修饰高级计算 — 例如，使用只有一个步骤的训练或使用模型的正向传递，将代码重构为更小的函数。 使用 Keras 层和模型来管理变量 Keras 模型和层提供了方便的变量和 trainable_variables 属性，以递归方式收集所有因变量。这使得本地化管理变量非常方便。 Keras 层 / 模型继承自 tf.train.Checkpointable 并与 @ tf.function 集成，这使得直接检查点或从 Keras 对象导出 SavedModel 成为可能。您不一定要使用 Keras 的 fit() API 来集成。 结合 tf.data.Datasets 和 @tf.function 在迭代适合内存的训练数据时，可以使用常规的 Python 循环。除此之外，tf.data.Dataset 则是从磁盘传输训练数据的最好方法。数据集是可迭代的（不是迭代器），工作方式与其他 Python 循环类似。如果您想使用 AutoGraph 的等效图操作替换 Python 循环，可以通过将代码包装在 tf.function() 中，充分利用数据集异步预取 / 流功能来实现。 @tf.function def train(model, dataset, optimizer): &nbsp;&nbsp;for x, y in dataset: &nbsp;&nbsp;&nbsp;&nbsp;with tf.GradientTape() as tape: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prediction = model(x) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = loss_fn(prediction, y) &nbsp;&nbsp;&nbsp;&nbsp;gradients = tape.gradients(loss, model.trainable_variables) &nbsp;&nbsp;&nbsp;&nbsp;optimizer.apply_gradients(gradients, model.trainable_variables) 如果您使用 Keras.fit() API，则无需担心数据集迭代。 model.compile(optimizer=optimizer, loss=loss_fn) model.fit(dataset) 利用 AutoGraph 和 Python 控制流程 AutoGraph 提供了一种将依赖于数据的控制流转换为图模式等价的方法，如 tf.cond 和 tf.while_loop。 数据相关控制流常见出现于序列模型中。tf.keras.layers.RNN 包装了 RNN 单元，允许您静态或动态地展开循环神经网络。为了演示，您可以重新实现动态展开，如下所示： class DynamicRNN(tf.keras.Model): &nbsp; &nbsp;&nbsp;def __init__(self, rnn_cell): &nbsp;&nbsp;&nbsp;&nbsp;super(DynamicRNN, self).__init__(self) &nbsp;&nbsp;&nbsp;&nbsp;self.cell = rnn_cell &nbsp; &nbsp;&nbsp;def call(self, input_data): &nbsp;&nbsp;&nbsp;&nbsp;# [batch, time, features] -&gt; [time, batch, features] &nbsp;&nbsp;&nbsp;&nbsp;input_data = tf.transpose(input_data, [1, 0, 2]) &nbsp;&nbsp;&nbsp;&nbsp;outputs = tf.TensorArray(tf.float32, input_data.shape[0]) &nbsp;&nbsp;&nbsp;&nbsp;state = self.cell.zero_state(input_data.shape[1], dtype=tf.float32) &nbsp;&nbsp;&nbsp;&nbsp;for i in tf.range(input_data.shape[0]): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output, state = self.cell(input_data[i], state) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outputs = outputs.write(i, output) &nbsp;&nbsp;&nbsp;&nbsp;return tf.transpose(outputs.stack(), [1, 0, 2]), state 使用 tf.metrics 聚合数据，使用 tf.summary 记录数据 一套完整的 tf.summary 接口即将发布。您可以使用以下命令访问 tf.summary 的 2.0 版本： from&nbsp;tensorflow.python.ops import&nbsp;summary_ops_v2 有关详细信息，请参阅文末链接： https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/effective_tf2.md （本文仅代表作者观点，转载请联系原作者） 精彩推荐 推荐阅读： 对标Bert？刷屏的GPT 2.0意味着什么 一次性掌握机器学习基础知识脉络 | 公开课笔记 Python助你抢红包 3分钟实现9种经典排序算法的可视化｜Python 骗局翻新, 暗网活跃度倍增, 2018加密货币犯罪报告敢看吗？ 云漫圈 | 学Python还是Java, 8张漫画带你全面分析 35 岁程序员，年后第一天被辞退 手机辐射排行榜：小米、一加远超 iPhone；阿里开工彩票最高奖金 1000 万；苹果再遭集体诉讼 2月报告：Python逆袭成功？踢馆Java，碾压C++！ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 点击“阅读原文”，打开CSDN&nbsp;APP&nbsp;阅读更贴心。" />
<meta property="og:description" content="来源 | Google TensorFlow 团队 为提高 TensorFlow 的工作效率，TensorFlow 2.0 进行了多项更改，包括删除了多余的 API，使API 更加一致统一，例如统一的 RNNs (循环神经网络），统一的优化器，并且Python 运行时更好地集成了 Eager execution 。 许多 RFC 已经对 TensorFlow 2.0 的这些更改给出了解释。本指南基于您对 TensorFlow 1.x 有一定的了解的前提，为您介绍在 TensorFlow 2.0 中的开发有什么不同。 API 整理 在 TensorFlow 2.0 中，有许多 1.X 的 API 被删除或移动&nbsp;了。也有部分 1.X 的 API 被 2.0 版本的等价 API 所替代：tf.summary，tf.keras.metrics 和 tf.keras.optimizers。自动应用这些重命名，最简单的方法是使用 TensorFlow 2.0 升级脚本。 Eager execution TensorFlow 1.X 要求用户通过调用 tf.* API 手动的将抽象语法树（图）拼接在一起。然后，它要求用户将一组输出张量和输入张量传递给 session.run() 调用，来手动编译抽象语法树。相比之下，TensorFlow 2.0 executes eagerly（如正常使用 Python 一样）在 2.0 的版本中，其 graphs（抽象语法树）和 sessions 在实现的细节上应该是一样的。 不再有全局变量 TensorFlow 1.X 非常依赖于隐式全局命名空间。当你调用 tf.Variable 时，它会被放入默认图中，即使你忘记了指向它的 Python 变量它也会留在那里。这时，您可以恢复该 tf.Variable()，但前提是您得知道它已创建的名称。如果您无法控制变量的创建，很难做到这一点。因此，各种机制以及寻找用户创建变量的框架不断涌现，试图帮助用户再次找到他们的变量。 TensorFlow 2.0 取消了所有这些机制（Variables 2.0 RFC），支持默认机制：跟踪变量！ 如果你不再用到某个 tf.Variable，它就会被回收。 Functions, not sessions session.run() 的调用几乎类似于函数调用：指定输入和要调用的函数，然后返回一组输出。在 TensorFlow 2.0 中，您可以使用 tf.function() 来修饰 Python 函数以将其标记为 JIT（ Just-In-Time ）编译，以便 TensorFlow 将其作为单个图运行（Functions 2.0 RFC）。 这种机制使得 TensorFlow 2.0 拥有图模式的许多优点： 性能：该函数可以被优化，例如节点修剪，内核融合等 可移植性：该函数可以导出 / 重新导入（SavedModel 2.0 RFC），允许用户重用和将 TensorFlow 函数作为模块共享 # TensorFlow 1.X outputs = session.run(f(placeholder), feed_dict={placeholder: input})# TensorFlow 2.0 outputs = f(input) 由于能够自由地穿插 Python 和 TensorFlow 代码，您能够充分利用 Python 的表现力。而且，可移植的 TensorFlow 在没有 Python 解释器的情况下也可执行。比如：mobile，C ++ 和 JS。避免用户在添加 @tf.function 时重写代码，AutoGraph 会将 Python 构造的一个子集转换成 TensorFlow 等价物。 TensorFlow 2.0 常用的建议 将代码重构为更小的函数 TensorFlow 1.X 中的常见使用模式是 “kitchen sink” 策略，即预先列出所有可能计算的并集，然后通过 session.run() 计算选定的张量。在 TensorFlow 2.0 中，用户应该根据需求将代码重构为更小的函数。通常情况下，没有必要用 tf.function 来修饰这些较小的函数；仅使用 tf.function 来修饰高级计算 — 例如，使用只有一个步骤的训练或使用模型的正向传递，将代码重构为更小的函数。 使用 Keras 层和模型来管理变量 Keras 模型和层提供了方便的变量和 trainable_variables 属性，以递归方式收集所有因变量。这使得本地化管理变量非常方便。 Keras 层 / 模型继承自 tf.train.Checkpointable 并与 @ tf.function 集成，这使得直接检查点或从 Keras 对象导出 SavedModel 成为可能。您不一定要使用 Keras 的 fit() API 来集成。 结合 tf.data.Datasets 和 @tf.function 在迭代适合内存的训练数据时，可以使用常规的 Python 循环。除此之外，tf.data.Dataset 则是从磁盘传输训练数据的最好方法。数据集是可迭代的（不是迭代器），工作方式与其他 Python 循环类似。如果您想使用 AutoGraph 的等效图操作替换 Python 循环，可以通过将代码包装在 tf.function() 中，充分利用数据集异步预取 / 流功能来实现。 @tf.function def train(model, dataset, optimizer): &nbsp;&nbsp;for x, y in dataset: &nbsp;&nbsp;&nbsp;&nbsp;with tf.GradientTape() as tape: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prediction = model(x) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = loss_fn(prediction, y) &nbsp;&nbsp;&nbsp;&nbsp;gradients = tape.gradients(loss, model.trainable_variables) &nbsp;&nbsp;&nbsp;&nbsp;optimizer.apply_gradients(gradients, model.trainable_variables) 如果您使用 Keras.fit() API，则无需担心数据集迭代。 model.compile(optimizer=optimizer, loss=loss_fn) model.fit(dataset) 利用 AutoGraph 和 Python 控制流程 AutoGraph 提供了一种将依赖于数据的控制流转换为图模式等价的方法，如 tf.cond 和 tf.while_loop。 数据相关控制流常见出现于序列模型中。tf.keras.layers.RNN 包装了 RNN 单元，允许您静态或动态地展开循环神经网络。为了演示，您可以重新实现动态展开，如下所示： class DynamicRNN(tf.keras.Model): &nbsp; &nbsp;&nbsp;def __init__(self, rnn_cell): &nbsp;&nbsp;&nbsp;&nbsp;super(DynamicRNN, self).__init__(self) &nbsp;&nbsp;&nbsp;&nbsp;self.cell = rnn_cell &nbsp; &nbsp;&nbsp;def call(self, input_data): &nbsp;&nbsp;&nbsp;&nbsp;# [batch, time, features] -&gt; [time, batch, features] &nbsp;&nbsp;&nbsp;&nbsp;input_data = tf.transpose(input_data, [1, 0, 2]) &nbsp;&nbsp;&nbsp;&nbsp;outputs = tf.TensorArray(tf.float32, input_data.shape[0]) &nbsp;&nbsp;&nbsp;&nbsp;state = self.cell.zero_state(input_data.shape[1], dtype=tf.float32) &nbsp;&nbsp;&nbsp;&nbsp;for i in tf.range(input_data.shape[0]): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output, state = self.cell(input_data[i], state) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outputs = outputs.write(i, output) &nbsp;&nbsp;&nbsp;&nbsp;return tf.transpose(outputs.stack(), [1, 0, 2]), state 使用 tf.metrics 聚合数据，使用 tf.summary 记录数据 一套完整的 tf.summary 接口即将发布。您可以使用以下命令访问 tf.summary 的 2.0 版本： from&nbsp;tensorflow.python.ops import&nbsp;summary_ops_v2 有关详细信息，请参阅文末链接： https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/effective_tf2.md （本文仅代表作者观点，转载请联系原作者） 精彩推荐 推荐阅读： 对标Bert？刷屏的GPT 2.0意味着什么 一次性掌握机器学习基础知识脉络 | 公开课笔记 Python助你抢红包 3分钟实现9种经典排序算法的可视化｜Python 骗局翻新, 暗网活跃度倍增, 2018加密货币犯罪报告敢看吗？ 云漫圈 | 学Python还是Java, 8张漫画带你全面分析 35 岁程序员，年后第一天被辞退 手机辐射排行榜：小米、一加远超 iPhone；阿里开工彩票最高奖金 1000 万；苹果再遭集体诉讼 2月报告：Python逆袭成功？踢馆Java，碾压C++！ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 点击“阅读原文”，打开CSDN&nbsp;APP&nbsp;阅读更贴心。" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-17T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"来源 | Google TensorFlow 团队 为提高 TensorFlow 的工作效率，TensorFlow 2.0 进行了多项更改，包括删除了多余的 API，使API 更加一致统一，例如统一的 RNNs (循环神经网络），统一的优化器，并且Python 运行时更好地集成了 Eager execution 。 许多 RFC 已经对 TensorFlow 2.0 的这些更改给出了解释。本指南基于您对 TensorFlow 1.x 有一定的了解的前提，为您介绍在 TensorFlow 2.0 中的开发有什么不同。 API 整理 在 TensorFlow 2.0 中，有许多 1.X 的 API 被删除或移动&nbsp;了。也有部分 1.X 的 API 被 2.0 版本的等价 API 所替代：tf.summary，tf.keras.metrics 和 tf.keras.optimizers。自动应用这些重命名，最简单的方法是使用 TensorFlow 2.0 升级脚本。 Eager execution TensorFlow 1.X 要求用户通过调用 tf.* API 手动的将抽象语法树（图）拼接在一起。然后，它要求用户将一组输出张量和输入张量传递给 session.run() 调用，来手动编译抽象语法树。相比之下，TensorFlow 2.0 executes eagerly（如正常使用 Python 一样）在 2.0 的版本中，其 graphs（抽象语法树）和 sessions 在实现的细节上应该是一样的。 不再有全局变量 TensorFlow 1.X 非常依赖于隐式全局命名空间。当你调用 tf.Variable 时，它会被放入默认图中，即使你忘记了指向它的 Python 变量它也会留在那里。这时，您可以恢复该 tf.Variable()，但前提是您得知道它已创建的名称。如果您无法控制变量的创建，很难做到这一点。因此，各种机制以及寻找用户创建变量的框架不断涌现，试图帮助用户再次找到他们的变量。 TensorFlow 2.0 取消了所有这些机制（Variables 2.0 RFC），支持默认机制：跟踪变量！ 如果你不再用到某个 tf.Variable，它就会被回收。 Functions, not sessions session.run() 的调用几乎类似于函数调用：指定输入和要调用的函数，然后返回一组输出。在 TensorFlow 2.0 中，您可以使用 tf.function() 来修饰 Python 函数以将其标记为 JIT（ Just-In-Time ）编译，以便 TensorFlow 将其作为单个图运行（Functions 2.0 RFC）。 这种机制使得 TensorFlow 2.0 拥有图模式的许多优点： 性能：该函数可以被优化，例如节点修剪，内核融合等 可移植性：该函数可以导出 / 重新导入（SavedModel 2.0 RFC），允许用户重用和将 TensorFlow 函数作为模块共享 # TensorFlow 1.X outputs = session.run(f(placeholder), feed_dict={placeholder: input})# TensorFlow 2.0 outputs = f(input) 由于能够自由地穿插 Python 和 TensorFlow 代码，您能够充分利用 Python 的表现力。而且，可移植的 TensorFlow 在没有 Python 解释器的情况下也可执行。比如：mobile，C ++ 和 JS。避免用户在添加 @tf.function 时重写代码，AutoGraph 会将 Python 构造的一个子集转换成 TensorFlow 等价物。 TensorFlow 2.0 常用的建议 将代码重构为更小的函数 TensorFlow 1.X 中的常见使用模式是 “kitchen sink” 策略，即预先列出所有可能计算的并集，然后通过 session.run() 计算选定的张量。在 TensorFlow 2.0 中，用户应该根据需求将代码重构为更小的函数。通常情况下，没有必要用 tf.function 来修饰这些较小的函数；仅使用 tf.function 来修饰高级计算 — 例如，使用只有一个步骤的训练或使用模型的正向传递，将代码重构为更小的函数。 使用 Keras 层和模型来管理变量 Keras 模型和层提供了方便的变量和 trainable_variables 属性，以递归方式收集所有因变量。这使得本地化管理变量非常方便。 Keras 层 / 模型继承自 tf.train.Checkpointable 并与 @ tf.function 集成，这使得直接检查点或从 Keras 对象导出 SavedModel 成为可能。您不一定要使用 Keras 的 fit() API 来集成。 结合 tf.data.Datasets 和 @tf.function 在迭代适合内存的训练数据时，可以使用常规的 Python 循环。除此之外，tf.data.Dataset 则是从磁盘传输训练数据的最好方法。数据集是可迭代的（不是迭代器），工作方式与其他 Python 循环类似。如果您想使用 AutoGraph 的等效图操作替换 Python 循环，可以通过将代码包装在 tf.function() 中，充分利用数据集异步预取 / 流功能来实现。 @tf.function def train(model, dataset, optimizer): &nbsp;&nbsp;for x, y in dataset: &nbsp;&nbsp;&nbsp;&nbsp;with tf.GradientTape() as tape: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prediction = model(x) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = loss_fn(prediction, y) &nbsp;&nbsp;&nbsp;&nbsp;gradients = tape.gradients(loss, model.trainable_variables) &nbsp;&nbsp;&nbsp;&nbsp;optimizer.apply_gradients(gradients, model.trainable_variables) 如果您使用 Keras.fit() API，则无需担心数据集迭代。 model.compile(optimizer=optimizer, loss=loss_fn) model.fit(dataset) 利用 AutoGraph 和 Python 控制流程 AutoGraph 提供了一种将依赖于数据的控制流转换为图模式等价的方法，如 tf.cond 和 tf.while_loop。 数据相关控制流常见出现于序列模型中。tf.keras.layers.RNN 包装了 RNN 单元，允许您静态或动态地展开循环神经网络。为了演示，您可以重新实现动态展开，如下所示： class DynamicRNN(tf.keras.Model): &nbsp; &nbsp;&nbsp;def __init__(self, rnn_cell): &nbsp;&nbsp;&nbsp;&nbsp;super(DynamicRNN, self).__init__(self) &nbsp;&nbsp;&nbsp;&nbsp;self.cell = rnn_cell &nbsp; &nbsp;&nbsp;def call(self, input_data): &nbsp;&nbsp;&nbsp;&nbsp;# [batch, time, features] -&gt; [time, batch, features] &nbsp;&nbsp;&nbsp;&nbsp;input_data = tf.transpose(input_data, [1, 0, 2]) &nbsp;&nbsp;&nbsp;&nbsp;outputs = tf.TensorArray(tf.float32, input_data.shape[0]) &nbsp;&nbsp;&nbsp;&nbsp;state = self.cell.zero_state(input_data.shape[1], dtype=tf.float32) &nbsp;&nbsp;&nbsp;&nbsp;for i in tf.range(input_data.shape[0]): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output, state = self.cell(input_data[i], state) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outputs = outputs.write(i, output) &nbsp;&nbsp;&nbsp;&nbsp;return tf.transpose(outputs.stack(), [1, 0, 2]), state 使用 tf.metrics 聚合数据，使用 tf.summary 记录数据 一套完整的 tf.summary 接口即将发布。您可以使用以下命令访问 tf.summary 的 2.0 版本： from&nbsp;tensorflow.python.ops import&nbsp;summary_ops_v2 有关详细信息，请参阅文末链接： https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/effective_tf2.md （本文仅代表作者观点，转载请联系原作者） 精彩推荐 推荐阅读： 对标Bert？刷屏的GPT 2.0意味着什么 一次性掌握机器学习基础知识脉络 | 公开课笔记 Python助你抢红包 3分钟实现9种经典排序算法的可视化｜Python 骗局翻新, 暗网活跃度倍增, 2018加密货币犯罪报告敢看吗？ 云漫圈 | 学Python还是Java, 8张漫画带你全面分析 35 岁程序员，年后第一天被辞退 手机辐射排行榜：小米、一加远超 iPhone；阿里开工彩票最高奖金 1000 万；苹果再遭集体诉讼 2月报告：Python逆袭成功？踢馆Java，碾压C++！ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 点击“阅读原文”，打开CSDN&nbsp;APP&nbsp;阅读更贴心。","@type":"BlogPosting","url":"/2019/02/17/ea1d41aae23e802d6d18f1fd460daeb2.html","headline":"我们期待的TensorFlow 2.0还有哪些变化？","dateModified":"2019-02-17T00:00:00+08:00","datePublished":"2019-02-17T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/02/17/ea1d41aae23e802d6d18f1fd460daeb2.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>我们期待的TensorFlow 2.0还有哪些变化？</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="rich_media_content" id="js_content"> 
   <p style="min-height:1em;letter-spacing:.544px;text-align:center;margin-left:8px;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/BnSNEaficFAazru8N0otp6l5FkdtfKzLosfZas7UN6ryebT4X4Ho3fia6NjEjibPtjawdLs8NjJj8dxCibjUmg1MQQ/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></p>
   <p style="margin-left:8px;min-height:1em;letter-spacing:.544px;line-height:1.75em;"><br></p>
   <p style="min-height:1em;letter-spacing:.544px;line-height:1.75em;margin-left:8px;"><span style="letter-spacing:1px;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;color:rgb(136,136,136);font-size:14px;">来源 | Google TensorFlow 团队</span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">为提高 TensorFlow 的工作效率，TensorFlow 2.0 进行了多项更改，包括删除了多余的 API，使API 更加一致统一，例如统一的 RNNs (循环神经网络），统一的优化器，并且Python 运行时更好地集成了 Eager execution 。<br></span></p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">许多 RFC 已经对 TensorFlow 2.0 的这些更改给出了解释。本指南基于您对 TensorFlow 1.x 有一定的了解的前提，为您介绍在 TensorFlow 2.0 中的开发有什么不同。</span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(70,118,217);font-size:18px;"><strong><span style="color:rgb(70,118,217);letter-spacing:1px;">API 整理</span></strong></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">在 TensorFlow 2.0 中，有许多 1.X 的 API 被删除或移动&nbsp;了。也有部分 1.X 的 API 被 2.0 版本的等价 API 所替代：tf.summary，tf.keras.metrics 和 tf.keras.optimizers。自动应用这些重命名，最简单的方法是使用 TensorFlow 2.0 升级脚本。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(70,118,217);font-size:18px;"><strong><span style="font-size:18px;color:rgb(70,118,217);letter-spacing:1px;">Eager execution</span></strong></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">TensorFlow 1.X 要求用户通过调用 tf.* API 手动的将抽象语法树（图）拼接在一起。然后，它要求用户将一组输出张量和输入张量传递给 session.run() 调用，来手动编译抽象语法树。相比之下，TensorFlow 2.0 executes eagerly（如正常使用 Python 一样）在 2.0 的版本中，其 graphs（抽象语法树）和 sessions 在实现的细节上应该是一样的。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(70,118,217);font-size:18px;"><strong><span style="font-size:18px;color:rgb(70,118,217);letter-spacing:1px;">不再有全局变量</span></strong></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">TensorFlow 1.X 非常依赖于隐式全局命名空间。当你调用 tf.Variable 时，它会被放入默认图中，即使你忘记了指向它的 Python 变量它也会留在那里。这时，您可以恢复该 tf.Variable()，但前提是您得知道它已创建的名称。如果您无法控制变量的创建，很难做到这一点。因此，各种机制以及寻找用户创建变量的框架不断涌现，试图帮助用户再次找到他们的变量。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">TensorFlow 2.0 取消了所有这些机制（Variables 2.0 RFC），支持默认机制：跟踪变量！ 如果你不再用到某个 tf.Variable，它就会被回收。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(70,118,217);font-size:18px;"><strong><span style="font-size:18px;color:rgb(70,118,217);letter-spacing:1px;">Functions, not sessions</span></strong></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">session.run() 的调用几乎类似于函数调用：指定输入和要调用的函数，然后返回一组输出。在 TensorFlow 2.0 中，您可以使用 tf.function() 来修饰 Python 函数以将其标记为 JIT（ Just-In-Time ）编译，以便 TensorFlow 将其作为单个图运行（Functions 2.0 RFC）。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">这种机制使得 TensorFlow 2.0 拥有图模式的许多优点：</span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <ul class="list-paddingleft-2" style="margin-left:8px;">
    <li><p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">性能：该函数可以被优化，例如节点修剪，内核融合等</span></p></li>
    <li><p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">可移植性：该函数可以导出 / 重新导入（SavedModel 2.0 RFC），允许用户重用和将 TensorFlow 函数作为模块共享</span></p></li>
   </ul>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"></span></p>
   <p style="text-align:left;margin-left:8px;"><br></p>
   <p># TensorFlow 1.X</p>
   <p>outputs = session.run(f(placeholder), feed_dict={placeholder: input})# TensorFlow 2.0</p>
   <p>outputs = f(input)</p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">由于能够自由地穿插 Python 和 TensorFlow 代码，您能够充分利用 Python 的表现力。而且，可移植的 TensorFlow 在没有 Python 解释器的情况下也可执行。比如：mobile，C ++ 和 JS。避免用户在添加 @tf.function 时重写代码，AutoGraph 会将 Python 构造的一个子集转换成 TensorFlow 等价物。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;text-align:center;"><span style="color:rgb(70,118,217);font-size:18px;"><strong><span style="color:rgb(70,118,217);letter-spacing:1px;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;">TensorFlow 2.0 常用的建议</span></strong></span><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(70,118,217);font-size:18px;"><strong><span style="font-size:18px;color:rgb(70,118,217);letter-spacing:1px;">将代码重构为更小的函数</span></strong></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">TensorFlow 1.X 中的常见使用模式是 “kitchen sink” 策略，即预先列出所有可能计算的并集，然后通过 session.run() 计算选定的张量。在 TensorFlow 2.0 中，用户应该根据需求将代码重构为更小的函数。通常情况下，没有必要用 tf.function 来修饰这些较小的函数；仅使用 tf.function 来修饰高级计算 — 例如，使用只有一个步骤的训练或使用模型的正向传递，将代码重构为更小的函数。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(70,118,217);font-size:18px;"><strong><span style="font-size:18px;color:rgb(70,118,217);letter-spacing:1px;">使用 Keras 层和模型来管理变量</span></strong></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">Keras 模型和层提供了方便的变量和 trainable_variables 属性，以递归方式收集所有因变量。这使得本地化管理变量非常方便。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">Keras 层 / 模型继承自 tf.train.Checkpointable 并与 @ tf.function 集成，这使得直接检查点或从 Keras 对象导出 SavedModel 成为可能。您不一定要使用 Keras 的 fit() API 来集成。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(70,118,217);font-size:18px;"><strong><span style="font-size:18px;color:rgb(70,118,217);letter-spacing:1px;">结合 tf.data.Datasets 和 @tf.function</span></strong></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">在迭代适合内存的训练数据时，可以使用常规的 Python 循环。除此之外，tf.data.Dataset 则是从磁盘传输训练数据的最好方法。数据集是可迭代的（不是迭代器），工作方式与其他 Python 循环类似。如果您想使用 AutoGraph 的等效图操作替换 Python 循环，可以通过将代码包装在 tf.function() 中，充分利用数据集异步预取 / 流功能来实现。</span></p>
   <p style="text-align:left;margin-left:8px;"><br></p>
   <p><span style="letter-spacing:0px;">@tf.function</span><br></p>
   <p>def train(model, dataset, optimizer):</p>
   <p>&nbsp;&nbsp;for x, y in dataset:</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;with tf.GradientTape() as tape:</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prediction = model(x)</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss = loss_fn(prediction, y)</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;gradients = tape.gradients(loss, model.trainable_variables)</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;optimizer.apply_gradients(gradients, model.trainable_variables)<br></p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">如果您使用 Keras.fit() API，则无需担心数据集迭代。</span></p>
   <p style="text-align:left;margin-left:8px;"><br></p>
   <p><span style="letter-spacing:0px;">model.compile(optimizer=optimizer, loss=loss_fn)</span><br></p>
   <p>model.fit(dataset)</p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(70,118,217);font-size:18px;"><strong><span style="font-size:18px;color:rgb(70,118,217);letter-spacing:1px;">利用 AutoGraph 和 Python 控制流程</span></strong></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">AutoGraph 提供了一种将依赖于数据的控制流转换为图模式等价的方法，如 tf.cond 和 tf.while_loop。</span></p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">数据相关控制流常见出现于序列模型中。tf.keras.layers.RNN 包装了 RNN 单元，允许您静态或动态地展开循环神经网络。为了演示，您可以重新实现动态展开，如下所示：</span></p>
   <p style="line-height:1.75em;"><br></p>
   <p>class DynamicRNN(tf.keras.Model):</p>
   <p>&nbsp;</p>
   <p>&nbsp;&nbsp;def __init__(self, rnn_cell):</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;super(DynamicRNN, self).__init__(self)</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;self.cell = rnn_cell</p>
   <p>&nbsp;</p>
   <p>&nbsp;&nbsp;def call(self, input_data):</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;# [batch, time, features] -&gt; [time, batch, features]</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;input_data = tf.transpose(input_data, [1, 0, 2])</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;outputs = tf.TensorArray(tf.float32, input_data.shape[0])</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;state = self.cell.zero_state(input_data.shape[1], dtype=tf.float32)</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;for i in tf.range(input_data.shape[0]):</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output, state = self.cell(input_data[i], state)</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outputs = outputs.write(i, output)</p>
   <p>&nbsp;&nbsp;&nbsp;&nbsp;return tf.transpose(outputs.stack(), [1, 0, 2]), state</p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">使用 tf.metrics 聚合数据，使用 tf.summary 记录数据</span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">一套完整的 tf.summary 接口即将发布。您可以使用以下命令访问 tf.summary 的 2.0 版本：</span></p>
   <p style="text-align:left;margin-left:8px;"><br></p>
   <p>from&nbsp;tensorflow.python.ops import&nbsp;summary_ops_v2</p>
   <p><br></p>
   <p style="line-height:1.75em;"><span style="font-size:15px;letter-spacing:1px;color:rgb(136,136,136);">有关详细信息，请参阅文末链接：</span></p>
   <p style="line-height:1.75em;text-align:left;"><span style="font-size:15px;letter-spacing:1px;color:rgb(136,136,136);">https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/effective_tf2.md</span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(136,136,136);font-size:14px;letter-spacing:1px;text-align:left;font-style:italic;"><span style="color:rgb(136,136,136);font-size:14px;font-style:italic;letter-spacing:1px;text-align:left;">（本文仅代表作者观点，转载请联系原作者）</span></span></p>
   <strong><span style="color:rgb(255,76,0);">精彩推荐</span></strong>
   <p style="margin-left:8px;text-align:center;"><a href="https://mp.weixin.qq.com/s?__biz=MzU5MjEwMTE2OQ==&amp;mid=2247484743&amp;idx=1&amp;sn=392cd3d465aa7b0868d8aab781c3088f&amp;scene=21#wechat_redirect" rel="nofollow"><span class="js_jump_icon h5_image_link"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAap90h2r8N7rEM5cQZDSbmp17l2cMOvukNZjc8EjHZqVLznRfjLewfn6bs8HQrNkaQKKdxp0SmATA/640?wx_fmt=png" alt="640?wx_fmt=png"></span></a></p>
   <p style="margin-left:8px;min-height:1em;letter-spacing:.544px;line-height:1.75em;"><strong style="color:rgb(63,63,63);font-family:arial, '宋体', sans-serif;font-size:15px;letter-spacing:1px;text-indent:28px;"><span style="font-size:14px;letter-spacing:.5px;">推荐阅读：</span></strong><br></p>
   <ul class="list-paddingleft-2">
    <li><p><a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/87484501" rel="nofollow">对标Bert？刷屏的GPT 2.0意味着什么</a></p></li>
    <li><p><a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/87484494" rel="nofollow">一次性掌握机器学习基础知识脉络 | 公开课笔记</a><br></p></li>
    <li><p><a href="https://mp.weixin.qq.com/s?__biz=MzU5MjEwMTE2OQ==&amp;mid=2247484717&amp;idx=1&amp;sn=d844404fdaef06a4fff5f674ff5f2a8a&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:15px;">Python助你抢红包</a></p></li>
    <li><p><a href="https://mp.weixin.qq.com/s?__biz=MzU5MjEwMTE2OQ==&amp;mid=2247484697&amp;idx=1&amp;sn=b0222f23a16e2b5e781a156916ccc99e&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:15px;">3分钟实现9种经典排序算法的可视化｜Python</a><br></p></li>
    <li><p><a href="https://blog.csdn.net/Blockchain_lemon/article/details/87485673" rel="nofollow">骗局翻新, 暗网活跃度倍增, 2018加密货币犯罪报告敢看吗？</a></p></li>
    <li><p><a href="https://blog.csdn.net/FL63Zv9Zou86950w/article/details/87485737" rel="nofollow">云漫圈 | 学Python还是Java, 8张漫画带你全面分析</a></p></li>
    <li><p><a href="https://blog.csdn.net/csdnsevenn/article/details/87219854" rel="nofollow">35 岁程序员，年后第一天被辞退</a></p></li>
    <li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5MjAwODM4MA==&amp;mid=2650713787&amp;idx=1&amp;sn=aef97ebe827ef4fb5389e0f47acbd96e&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:15px;">手机辐射排行榜：小米、一加远超 iPhone；阿里开工彩票最高奖金 1000 万；苹果再遭集体诉讼</a></p></li>
    <li><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5MjcxNjc2Ng==&amp;mid=2650559415&amp;idx=2&amp;sn=9f537a4cbb9bc34aa3906e1b0ac014dc&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:15px;">2月报告：Python逆袭成功？踢馆Java，碾压C++！</a><br></p></li>
   </ul>
   <p style="min-height:1em;letter-spacing:.544px;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<img style="letter-spacing:.544px;text-align:right;font-size:16px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZoRnvmibVUT5PvVqSZHcpIrLnqqKA1KEUIAalm7nV4h3RW6dTrGwqtm0mIvP1TicpPEWCh5RAtiaHMQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;min-height:1em;letter-spacing:.544px;text-align:left;"><span style="font-size:14px;letter-spacing:1px;color:rgb(0,128,255);">点击“阅读原文”，打开CSDN&nbsp;APP&nbsp;阅读更贴心。</span></p>
  </div> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
