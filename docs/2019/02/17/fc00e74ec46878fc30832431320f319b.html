<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>实际上，CNN图像分类策略简单到出人意料！ | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="实际上，CNN图像分类策略简单到出人意料！" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="全文约3300字6图，读完可能需要10分钟 建议收藏本文，并转发到朋友圈，让更多人受益 ICLR 2019一篇论文指出：DNN解决ImageNet时的策略似乎比我们想象的要简单得多。这个发现使我们能够构建更具解释性和透明度的图像分类管道，同时也解释了现代CNN中观察到的一些现象。 CNN非常擅长对乱序图像进行分类，但人类并非如此。 在这篇文章中，作者展示了为什么最先进的深度神经网络仍能很好地识别乱码图像，探究其中原因有助于揭示DNN使用让人意想不到的简单策略，对自然图像进行分类。&nbsp; 在ICLR 2019一篇论文指出上述发现能够： 解决ImageNet比许多人想象的要简单得多 使我们能够构建更具解释性和透明度的图像分类pipeline 解释了现代CNN中观察到的一些现象，例如对纹理的偏见以及忽略了对象部分的空间排序 复古bag-of-features模型 在深度学习出现之前，自然图像中的对象识别过程相当粗暴简单：定义一组关键视觉特征（“单词”），识别每个视觉特征在图像中的存在频率（“包”），然后根据这些数字对图像进行分类。 这些模型被称为“特征包”模型（BoF模型）。 举个例子，给定一个人眼和一个羽毛，我们想把图像分类为“人”和“鸟”两类。最简单的BoF模型工作流程是这样的：对于图像中的每只眼睛，它将“人类”的证据增加+1。反之亦然；对于图像中的每个羽毛，它将增加“鸟”的证据+1；无论什么类积累，图像中的大多数证据都是预测的。 这个最简单的BoF模型有一个很好的特性，是它的可解释性和透明的决策制定。我们可以准确地检查哪个图像特征携带了给定的类的证据，证据的空间整合是非常简单的（与深度神经网络中的深度非线性特征整合相比），很容易理解模型如何做出决定。 传统的BoF模型在深度学习开始之前一直非常先进、非常流行。但由于其分类性能过低，而很快失宠。可是，我们怎么确定深度神经网络有没有使用与BoF模型截然不同的决策策略呢？ 一个很深却可解释的BoF网络（BagNet） 为了测试这一点，研究人员将BoF模型的可解释性和透明度与DNN的性能结合起来。 将图像分割成小的q x q图像色块 通过DNN传递补丁以获取每个补丁的类证据（logits） 对所有补丁的证据求和，以达到图像级决策 BagNets的分类策略：对于每个补丁，我们使用DNN提取类证据（logits）并总结所有补丁的总类证据 为了以最简单和最有效的方式实现这一策略，我们采用标准的ResNet-50架构，用1x1卷积替换大多数（但不是全部）3x3卷积。&nbsp; 在这种情况下，最后一个卷积层中的隐藏单元每个只“看到”图像的一小部分（即它们的感受野远小于图像的大小）。&nbsp; 这就避免了对图像的显式分区，并且尽可能接近标准CNN，同时仍然实现概述的策略，我们称之为模型结构BagNet-q：其中q代表最顶层的感受域大小（我们测试q=9,17和33）。BagNet-q的运行时间大约是ResNet-50的运行时间的2.5倍。 在ImageNet上具有不同贴片尺寸的BagNets的性能。 即使对于非常小的贴片尺寸，BagNet上的BagNets性能也令人印象深刻：尺寸为17 x 17像素的图像特征足以达到AlexNet级别的性能，而尺寸为33 x 33像素的特征足以达到约87％的前5精度。通过更仔细地放置3 x 3卷积和额外的超参数调整，可以实现更高的性能值。 这是我们得到的第一个重要结果：只需使用一组小图的特性即可解决ImageNet问题。对象形状或对象部分之间的关系等远程空间关系可以完全忽略，并且不需要解决任务。 BagNets的一大特色是他们透明的决策。例如，我们现在可以查看哪个图像特征对于给定的类最具预测性。 图像功能具有最多的类证据。 我们展示了正确预测类（顶行）的功能和预测错误类（底行）的分散注意力的功能 上图中，最上面的手指图像被识别成tench（丁鱥guì，是淡水钓鱼的主要鱼种，也是鲈鱼等猎食性鱼类的饲料），因为这个类别中的大多数图像，都有一个渔民像举奖杯一样举起丁鱥。 同样，我们还得到一个精确定义的热图，显示图像的哪些部分促使神经网络做出某个决定。 来自BagNets的热图显示了确切的图像部分对决策的贡献。 热图不是近似的，而是显示每个图像部分的真实贡献。 ResNet-50与BagNets惊人相似 BagNets表明，基于本地图像特征和对象类别之间的弱统计相关性，可以在ImageNet上达到高精度。&nbsp; 如果这就够了，为什么像ResNet-50这样的标准深网会学到任何根本不同的东西？ 如果丰富的本地图像特征足以解决任务，那为什么ResNet-50还需要了解复杂的大尺度关系（如对象的形状）？ 为了验证现代DNN遵循与简单的特征包网络类似的策略的假设，我们在BagNets的以下“签名”上测试不同的ResNets，DenseNets和VGG： 决策对图像特征的空间改组是不变的（只能在VGG模型上测试） 不同图像部分的修改应该是独立的（就其对总类证据的影响而言） 标准CNN和BagNets产生的错误应该类似 标准CNN和BagNets应对类似功能敏感 在所有四个实验中，我们发现CNN和BagNets之间的行为非常相似。 例如，在上一个实验中，我们展示了BagNets最敏感的那些图像部分（例如，如果你遮挡那些部分）与CNN最敏感的那些基本相同。&nbsp; 实际上，BagNets的热图（灵敏度的空间图）比由DeepLift（直接为DenseNet-169计算热图）等归因方法生成的热图,更好地预测了DenseNet-169的灵敏度。&nbsp; 当然，DNN并不完全类似于特征包模型，但确实显示出一些偏差。特别是，我们发现网络越深入，功能越来越大，远程依赖性也越来越大。&nbsp; 因此，更深层的神经网络确实改进了更简单的特征包模型，但我认为核心分类策略并没有真正改变。 解释CNN几个奇怪的现象 将CNN的决策视为一种BoF策略，可以解释有关CNN的几个奇怪的观察。首先，它将解释为什么CNN具有如此强烈的纹理偏差；其次，它可以解释为什么CNN对图像部分的混乱如此不敏感；甚至可以解释一般的对抗性贴纸和对抗性扰动的存在，比如人们在图像中的任何地方放置误导信号，并且无论这些信号是否适合图像的其余部分，CNN仍然可以可靠地接收信号。 我们的成果显示，CNN利用自然图像中存在的许多弱统计规律进行分类，并且不会像人类一样跳向图像部分的对象级整合。其他任务和感官方式也是如此。 我们必须认真思考如何构建架构、任务和学习方法，以抵消这种弱统计相关性的趋势。一种方式，是将CNN的归纳偏差从小的局部特征改善为更全局的特征；另一种方式，是删除、或替换网络不应该依赖的那些特征。 然而，最大的问题之一当然是图像分类本身的任务：如果局部图像特征足以解决任务，也就不需要去学习自然界的真实“物理学”，这样我们就必须重构任务，推着模型去学习对象的物理本质。 这样就很可能需要跳出纯粹只通过观察学习，获得输入和输出特征之间相关性的方式，以便允许模型提取因果依赖性。 总结 总之，我们的结果表明CNN可能遵循极其简单的分类策略。科学家认为这个发现可能在2019继续成为关注的焦点，凸显了我们对深度神经网络的内部运作了解甚少。&nbsp; 缺乏理解使我们无法从根本上发展出更好的模型和架构，来缩小人与机器之间的差距。深化我们的理解，将使我们能够找到弥合这一差距的方法。&nbsp; 这将带来异常丰厚的回报：当我们试图将CNN偏向物体的更多物理特性时，我们突然达到了接近人类的噪声稳健性。&nbsp; 我们继续期待在2019年，在这一领域上会出现更多令人兴奋的结果，获得真正了解了真实世界中，物理和因果性质的卷积神经网络。 参考链接： https://medium.com/bethgelab/neural-networks-seem-to-follow-a-puzzlingly-simple-strategy-to-classify-images-f4229317261f 来源：Medium、新智元 ∞∞∞∞∞ 公众号回复“IT派”， 邀你加入&nbsp;IT派&nbsp;{ 深广创投圈 }&nbsp;" />
<meta property="og:description" content="全文约3300字6图，读完可能需要10分钟 建议收藏本文，并转发到朋友圈，让更多人受益 ICLR 2019一篇论文指出：DNN解决ImageNet时的策略似乎比我们想象的要简单得多。这个发现使我们能够构建更具解释性和透明度的图像分类管道，同时也解释了现代CNN中观察到的一些现象。 CNN非常擅长对乱序图像进行分类，但人类并非如此。 在这篇文章中，作者展示了为什么最先进的深度神经网络仍能很好地识别乱码图像，探究其中原因有助于揭示DNN使用让人意想不到的简单策略，对自然图像进行分类。&nbsp; 在ICLR 2019一篇论文指出上述发现能够： 解决ImageNet比许多人想象的要简单得多 使我们能够构建更具解释性和透明度的图像分类pipeline 解释了现代CNN中观察到的一些现象，例如对纹理的偏见以及忽略了对象部分的空间排序 复古bag-of-features模型 在深度学习出现之前，自然图像中的对象识别过程相当粗暴简单：定义一组关键视觉特征（“单词”），识别每个视觉特征在图像中的存在频率（“包”），然后根据这些数字对图像进行分类。 这些模型被称为“特征包”模型（BoF模型）。 举个例子，给定一个人眼和一个羽毛，我们想把图像分类为“人”和“鸟”两类。最简单的BoF模型工作流程是这样的：对于图像中的每只眼睛，它将“人类”的证据增加+1。反之亦然；对于图像中的每个羽毛，它将增加“鸟”的证据+1；无论什么类积累，图像中的大多数证据都是预测的。 这个最简单的BoF模型有一个很好的特性，是它的可解释性和透明的决策制定。我们可以准确地检查哪个图像特征携带了给定的类的证据，证据的空间整合是非常简单的（与深度神经网络中的深度非线性特征整合相比），很容易理解模型如何做出决定。 传统的BoF模型在深度学习开始之前一直非常先进、非常流行。但由于其分类性能过低，而很快失宠。可是，我们怎么确定深度神经网络有没有使用与BoF模型截然不同的决策策略呢？ 一个很深却可解释的BoF网络（BagNet） 为了测试这一点，研究人员将BoF模型的可解释性和透明度与DNN的性能结合起来。 将图像分割成小的q x q图像色块 通过DNN传递补丁以获取每个补丁的类证据（logits） 对所有补丁的证据求和，以达到图像级决策 BagNets的分类策略：对于每个补丁，我们使用DNN提取类证据（logits）并总结所有补丁的总类证据 为了以最简单和最有效的方式实现这一策略，我们采用标准的ResNet-50架构，用1x1卷积替换大多数（但不是全部）3x3卷积。&nbsp; 在这种情况下，最后一个卷积层中的隐藏单元每个只“看到”图像的一小部分（即它们的感受野远小于图像的大小）。&nbsp; 这就避免了对图像的显式分区，并且尽可能接近标准CNN，同时仍然实现概述的策略，我们称之为模型结构BagNet-q：其中q代表最顶层的感受域大小（我们测试q=9,17和33）。BagNet-q的运行时间大约是ResNet-50的运行时间的2.5倍。 在ImageNet上具有不同贴片尺寸的BagNets的性能。 即使对于非常小的贴片尺寸，BagNet上的BagNets性能也令人印象深刻：尺寸为17 x 17像素的图像特征足以达到AlexNet级别的性能，而尺寸为33 x 33像素的特征足以达到约87％的前5精度。通过更仔细地放置3 x 3卷积和额外的超参数调整，可以实现更高的性能值。 这是我们得到的第一个重要结果：只需使用一组小图的特性即可解决ImageNet问题。对象形状或对象部分之间的关系等远程空间关系可以完全忽略，并且不需要解决任务。 BagNets的一大特色是他们透明的决策。例如，我们现在可以查看哪个图像特征对于给定的类最具预测性。 图像功能具有最多的类证据。 我们展示了正确预测类（顶行）的功能和预测错误类（底行）的分散注意力的功能 上图中，最上面的手指图像被识别成tench（丁鱥guì，是淡水钓鱼的主要鱼种，也是鲈鱼等猎食性鱼类的饲料），因为这个类别中的大多数图像，都有一个渔民像举奖杯一样举起丁鱥。 同样，我们还得到一个精确定义的热图，显示图像的哪些部分促使神经网络做出某个决定。 来自BagNets的热图显示了确切的图像部分对决策的贡献。 热图不是近似的，而是显示每个图像部分的真实贡献。 ResNet-50与BagNets惊人相似 BagNets表明，基于本地图像特征和对象类别之间的弱统计相关性，可以在ImageNet上达到高精度。&nbsp; 如果这就够了，为什么像ResNet-50这样的标准深网会学到任何根本不同的东西？ 如果丰富的本地图像特征足以解决任务，那为什么ResNet-50还需要了解复杂的大尺度关系（如对象的形状）？ 为了验证现代DNN遵循与简单的特征包网络类似的策略的假设，我们在BagNets的以下“签名”上测试不同的ResNets，DenseNets和VGG： 决策对图像特征的空间改组是不变的（只能在VGG模型上测试） 不同图像部分的修改应该是独立的（就其对总类证据的影响而言） 标准CNN和BagNets产生的错误应该类似 标准CNN和BagNets应对类似功能敏感 在所有四个实验中，我们发现CNN和BagNets之间的行为非常相似。 例如，在上一个实验中，我们展示了BagNets最敏感的那些图像部分（例如，如果你遮挡那些部分）与CNN最敏感的那些基本相同。&nbsp; 实际上，BagNets的热图（灵敏度的空间图）比由DeepLift（直接为DenseNet-169计算热图）等归因方法生成的热图,更好地预测了DenseNet-169的灵敏度。&nbsp; 当然，DNN并不完全类似于特征包模型，但确实显示出一些偏差。特别是，我们发现网络越深入，功能越来越大，远程依赖性也越来越大。&nbsp; 因此，更深层的神经网络确实改进了更简单的特征包模型，但我认为核心分类策略并没有真正改变。 解释CNN几个奇怪的现象 将CNN的决策视为一种BoF策略，可以解释有关CNN的几个奇怪的观察。首先，它将解释为什么CNN具有如此强烈的纹理偏差；其次，它可以解释为什么CNN对图像部分的混乱如此不敏感；甚至可以解释一般的对抗性贴纸和对抗性扰动的存在，比如人们在图像中的任何地方放置误导信号，并且无论这些信号是否适合图像的其余部分，CNN仍然可以可靠地接收信号。 我们的成果显示，CNN利用自然图像中存在的许多弱统计规律进行分类，并且不会像人类一样跳向图像部分的对象级整合。其他任务和感官方式也是如此。 我们必须认真思考如何构建架构、任务和学习方法，以抵消这种弱统计相关性的趋势。一种方式，是将CNN的归纳偏差从小的局部特征改善为更全局的特征；另一种方式，是删除、或替换网络不应该依赖的那些特征。 然而，最大的问题之一当然是图像分类本身的任务：如果局部图像特征足以解决任务，也就不需要去学习自然界的真实“物理学”，这样我们就必须重构任务，推着模型去学习对象的物理本质。 这样就很可能需要跳出纯粹只通过观察学习，获得输入和输出特征之间相关性的方式，以便允许模型提取因果依赖性。 总结 总之，我们的结果表明CNN可能遵循极其简单的分类策略。科学家认为这个发现可能在2019继续成为关注的焦点，凸显了我们对深度神经网络的内部运作了解甚少。&nbsp; 缺乏理解使我们无法从根本上发展出更好的模型和架构，来缩小人与机器之间的差距。深化我们的理解，将使我们能够找到弥合这一差距的方法。&nbsp; 这将带来异常丰厚的回报：当我们试图将CNN偏向物体的更多物理特性时，我们突然达到了接近人类的噪声稳健性。&nbsp; 我们继续期待在2019年，在这一领域上会出现更多令人兴奋的结果，获得真正了解了真实世界中，物理和因果性质的卷积神经网络。 参考链接： https://medium.com/bethgelab/neural-networks-seem-to-follow-a-puzzlingly-simple-strategy-to-classify-images-f4229317261f 来源：Medium、新智元 ∞∞∞∞∞ 公众号回复“IT派”， 邀你加入&nbsp;IT派&nbsp;{ 深广创投圈 }&nbsp;" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-17T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"全文约3300字6图，读完可能需要10分钟 建议收藏本文，并转发到朋友圈，让更多人受益 ICLR 2019一篇论文指出：DNN解决ImageNet时的策略似乎比我们想象的要简单得多。这个发现使我们能够构建更具解释性和透明度的图像分类管道，同时也解释了现代CNN中观察到的一些现象。 CNN非常擅长对乱序图像进行分类，但人类并非如此。 在这篇文章中，作者展示了为什么最先进的深度神经网络仍能很好地识别乱码图像，探究其中原因有助于揭示DNN使用让人意想不到的简单策略，对自然图像进行分类。&nbsp; 在ICLR 2019一篇论文指出上述发现能够： 解决ImageNet比许多人想象的要简单得多 使我们能够构建更具解释性和透明度的图像分类pipeline 解释了现代CNN中观察到的一些现象，例如对纹理的偏见以及忽略了对象部分的空间排序 复古bag-of-features模型 在深度学习出现之前，自然图像中的对象识别过程相当粗暴简单：定义一组关键视觉特征（“单词”），识别每个视觉特征在图像中的存在频率（“包”），然后根据这些数字对图像进行分类。 这些模型被称为“特征包”模型（BoF模型）。 举个例子，给定一个人眼和一个羽毛，我们想把图像分类为“人”和“鸟”两类。最简单的BoF模型工作流程是这样的：对于图像中的每只眼睛，它将“人类”的证据增加+1。反之亦然；对于图像中的每个羽毛，它将增加“鸟”的证据+1；无论什么类积累，图像中的大多数证据都是预测的。 这个最简单的BoF模型有一个很好的特性，是它的可解释性和透明的决策制定。我们可以准确地检查哪个图像特征携带了给定的类的证据，证据的空间整合是非常简单的（与深度神经网络中的深度非线性特征整合相比），很容易理解模型如何做出决定。 传统的BoF模型在深度学习开始之前一直非常先进、非常流行。但由于其分类性能过低，而很快失宠。可是，我们怎么确定深度神经网络有没有使用与BoF模型截然不同的决策策略呢？ 一个很深却可解释的BoF网络（BagNet） 为了测试这一点，研究人员将BoF模型的可解释性和透明度与DNN的性能结合起来。 将图像分割成小的q x q图像色块 通过DNN传递补丁以获取每个补丁的类证据（logits） 对所有补丁的证据求和，以达到图像级决策 BagNets的分类策略：对于每个补丁，我们使用DNN提取类证据（logits）并总结所有补丁的总类证据 为了以最简单和最有效的方式实现这一策略，我们采用标准的ResNet-50架构，用1x1卷积替换大多数（但不是全部）3x3卷积。&nbsp; 在这种情况下，最后一个卷积层中的隐藏单元每个只“看到”图像的一小部分（即它们的感受野远小于图像的大小）。&nbsp; 这就避免了对图像的显式分区，并且尽可能接近标准CNN，同时仍然实现概述的策略，我们称之为模型结构BagNet-q：其中q代表最顶层的感受域大小（我们测试q=9,17和33）。BagNet-q的运行时间大约是ResNet-50的运行时间的2.5倍。 在ImageNet上具有不同贴片尺寸的BagNets的性能。 即使对于非常小的贴片尺寸，BagNet上的BagNets性能也令人印象深刻：尺寸为17 x 17像素的图像特征足以达到AlexNet级别的性能，而尺寸为33 x 33像素的特征足以达到约87％的前5精度。通过更仔细地放置3 x 3卷积和额外的超参数调整，可以实现更高的性能值。 这是我们得到的第一个重要结果：只需使用一组小图的特性即可解决ImageNet问题。对象形状或对象部分之间的关系等远程空间关系可以完全忽略，并且不需要解决任务。 BagNets的一大特色是他们透明的决策。例如，我们现在可以查看哪个图像特征对于给定的类最具预测性。 图像功能具有最多的类证据。 我们展示了正确预测类（顶行）的功能和预测错误类（底行）的分散注意力的功能 上图中，最上面的手指图像被识别成tench（丁鱥guì，是淡水钓鱼的主要鱼种，也是鲈鱼等猎食性鱼类的饲料），因为这个类别中的大多数图像，都有一个渔民像举奖杯一样举起丁鱥。 同样，我们还得到一个精确定义的热图，显示图像的哪些部分促使神经网络做出某个决定。 来自BagNets的热图显示了确切的图像部分对决策的贡献。 热图不是近似的，而是显示每个图像部分的真实贡献。 ResNet-50与BagNets惊人相似 BagNets表明，基于本地图像特征和对象类别之间的弱统计相关性，可以在ImageNet上达到高精度。&nbsp; 如果这就够了，为什么像ResNet-50这样的标准深网会学到任何根本不同的东西？ 如果丰富的本地图像特征足以解决任务，那为什么ResNet-50还需要了解复杂的大尺度关系（如对象的形状）？ 为了验证现代DNN遵循与简单的特征包网络类似的策略的假设，我们在BagNets的以下“签名”上测试不同的ResNets，DenseNets和VGG： 决策对图像特征的空间改组是不变的（只能在VGG模型上测试） 不同图像部分的修改应该是独立的（就其对总类证据的影响而言） 标准CNN和BagNets产生的错误应该类似 标准CNN和BagNets应对类似功能敏感 在所有四个实验中，我们发现CNN和BagNets之间的行为非常相似。 例如，在上一个实验中，我们展示了BagNets最敏感的那些图像部分（例如，如果你遮挡那些部分）与CNN最敏感的那些基本相同。&nbsp; 实际上，BagNets的热图（灵敏度的空间图）比由DeepLift（直接为DenseNet-169计算热图）等归因方法生成的热图,更好地预测了DenseNet-169的灵敏度。&nbsp; 当然，DNN并不完全类似于特征包模型，但确实显示出一些偏差。特别是，我们发现网络越深入，功能越来越大，远程依赖性也越来越大。&nbsp; 因此，更深层的神经网络确实改进了更简单的特征包模型，但我认为核心分类策略并没有真正改变。 解释CNN几个奇怪的现象 将CNN的决策视为一种BoF策略，可以解释有关CNN的几个奇怪的观察。首先，它将解释为什么CNN具有如此强烈的纹理偏差；其次，它可以解释为什么CNN对图像部分的混乱如此不敏感；甚至可以解释一般的对抗性贴纸和对抗性扰动的存在，比如人们在图像中的任何地方放置误导信号，并且无论这些信号是否适合图像的其余部分，CNN仍然可以可靠地接收信号。 我们的成果显示，CNN利用自然图像中存在的许多弱统计规律进行分类，并且不会像人类一样跳向图像部分的对象级整合。其他任务和感官方式也是如此。 我们必须认真思考如何构建架构、任务和学习方法，以抵消这种弱统计相关性的趋势。一种方式，是将CNN的归纳偏差从小的局部特征改善为更全局的特征；另一种方式，是删除、或替换网络不应该依赖的那些特征。 然而，最大的问题之一当然是图像分类本身的任务：如果局部图像特征足以解决任务，也就不需要去学习自然界的真实“物理学”，这样我们就必须重构任务，推着模型去学习对象的物理本质。 这样就很可能需要跳出纯粹只通过观察学习，获得输入和输出特征之间相关性的方式，以便允许模型提取因果依赖性。 总结 总之，我们的结果表明CNN可能遵循极其简单的分类策略。科学家认为这个发现可能在2019继续成为关注的焦点，凸显了我们对深度神经网络的内部运作了解甚少。&nbsp; 缺乏理解使我们无法从根本上发展出更好的模型和架构，来缩小人与机器之间的差距。深化我们的理解，将使我们能够找到弥合这一差距的方法。&nbsp; 这将带来异常丰厚的回报：当我们试图将CNN偏向物体的更多物理特性时，我们突然达到了接近人类的噪声稳健性。&nbsp; 我们继续期待在2019年，在这一领域上会出现更多令人兴奋的结果，获得真正了解了真实世界中，物理和因果性质的卷积神经网络。 参考链接： https://medium.com/bethgelab/neural-networks-seem-to-follow-a-puzzlingly-simple-strategy-to-classify-images-f4229317261f 来源：Medium、新智元 ∞∞∞∞∞ 公众号回复“IT派”， 邀你加入&nbsp;IT派&nbsp;{ 深广创投圈 }&nbsp;","@type":"BlogPosting","url":"/2019/02/17/fc00e74ec46878fc30832431320f319b.html","headline":"实际上，CNN图像分类策略简单到出人意料！","dateModified":"2019-02-17T00:00:00+08:00","datePublished":"2019-02-17T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/02/17/fc00e74ec46878fc30832431320f319b.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>实际上，CNN图像分类策略简单到出人意料！</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="rich_media_content" id="js_content"> 
   <p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/tvEoHIulOU43YE1qOU8Vd6BLcuLMMibeNibqicibO1dbuF9v1zK6mRakK0y6nEU5lFf34iawGw99mGDcfFeLPlEmRLA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><span style="letter-spacing:.5px;font-size:14px;color:rgb(136,136,136);">全文约3300字6图，读完可能需要10分钟</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><span style="letter-spacing:.5px;font-size:14px;color:rgb(136,136,136);">建议收藏本文，并转发到朋友圈，让更多人受益</span></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><span style="letter-spacing:.5px;font-size:14px;color:rgb(136,136,136);"><br></span></p>
   <h5 style="font-family:Arial, Helvetica, sans-serif;line-height:1.75em;word-spacing:1px;"><span><span style="color:rgb(0,0,0);"><span style="font-size:15px;letter-spacing:.5px;">ICLR 2019一篇论文指出：DNN解决ImageNet时的策略似乎比我们想象的要简单得多。这个发现使我们能够构建更具解释性和透明度的图像分类管道，同时也解释了现代CNN中观察到的一些现象。</span></span></span></h5>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;"></span></p>
   <p style="text-align:center;"><br></p>
   <p style="text-align:center;"><img style="width:556px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/UicQ7HgWiaUb1pABDYgVAJ6HT4lnLI8vWzbgcGkv6aPhwzibUPFwz5xNXicG3YEP08ytKxvGFdIibI4PsdIxbGXLKAg/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></p>
   <p style="text-align:center;"><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">CNN非常擅长对乱序图像进行分类，但人类并非如此。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">在这篇文章中，作者展示了为什么最先进的深度神经网络仍能很好地识别乱码图像，探究其中原因有助于揭示DNN使用让人意想不到的简单策略，对自然图像进行分类。&nbsp;</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">在ICLR 2019一篇论文指出上述发现能够：</span></p>
   <p><br></p>
   <ol class="list-paddingleft-2">
    <li><p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">解决ImageNet比许多人想象的要简单得多</span></p></li>
    <li><p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">使我们能够构建更具解释性和透明度的图像分类pipeline</span></p></li>
    <li><p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;">解释了现代CNN中观察到的一些现象，例如对纹理的偏见以及忽略了对象部分的空间排序</span></p></li>
   </ol>
   <p style="margin-left:8px;line-height:1.75em;"><br></p>复古bag-of-features模型
   <p style="margin-left:8px;line-height:1.75em;"><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">在深度学习出现之前，自然图像中的对象识别过程相当粗暴简单：定义一组关键视觉特征（“单词”），识别每个视觉特征在图像中的存在频率（“包”），然后根据这些数字对图像进行分类。 这些模型被称为“特征包”模型（BoF模型）。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">举个例子，给定一个人眼和一个羽毛，我们想把图像分类为“人”和“鸟”两类。最简单的BoF模型工作流程是这样的：对于图像中的每只眼睛，它将“人类”的证据增加+1。反之亦然；对于图像中的每个羽毛，它将增加“鸟”的证据+1；无论什么类积累，图像中的大多数证据都是预测的。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">这个最简单的BoF模型有一个很好的特性，是它的可解释性和透明的决策制定。我们可以准确地检查哪个图像特征携带了给定的类的证据，证据的空间整合是非常简单的（与深度神经网络中的深度非线性特征整合相比），很容易理解模型如何做出决定。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">传统的BoF模型在深度学习开始之前一直非常先进、非常流行。但由于其分类性能过低，而很快失宠。可是，我们怎么确定深度神经网络有没有使用与BoF模型截然不同的决策策略呢？</span></p>
   <p><br></p>一个很深却可解释的BoF网络（BagNet）
   <p style="margin-left:8px;line-height:1.75em;"><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">为了测试这一点，研究人员将BoF模型的可解释性和透明度与DNN的性能结合起来。</span></p>
   <p><br></p>
   <ul class="list-paddingleft-2" style="list-style-type:circle;">
    <li><p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">将图像分割成小的q x q图像色块</span></p></li>
    <li><p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">通过DNN传递补丁以获取每个补丁的类证据（logits）</span></p></li>
    <li><p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">对所有补丁的证据求和，以达到图像级决策</span></p></li>
   </ul>
   <p><br></p>
   <p style="text-align:center;margin-left:8px;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb1pABDYgVAJ6HT4lnLI8vWzQ3hsZhfgWsb7bQmtN7MtPFTcicMnnk8A2twic4qOaxQGLg7jGbjL1XwA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><span style="letter-spacing:.5px;font-size:14px;color:rgb(136,136,136);">BagNets的分类策略：对于每个补丁，我们使用DNN提取类证据（logits）并总结所有补丁的总类证据</span><br></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">为了以最简单和最有效的方式实现这一策略，我们采用标准的ResNet-50架构，用1x1卷积替换大多数（但不是全部）3x3卷积。&nbsp;</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">在这种情况下，最后一个卷积层中的隐藏单元每个只“看到”图像的一小部分（即它们的感受野远小于图像的大小）。&nbsp;</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">这就避免了对图像的显式分区，并且尽可能接近标准CNN，同时仍然实现概述的策略，我们称之为模型结构BagNet-q：其中q代表最顶层的感受域大小（我们测试q=9,17和33）。BagNet-q的运行时间大约是ResNet-50的运行时间的2.5倍。</span></p>
   <p><br></p>
   <p style="text-align:center;margin-left:8px;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/UicQ7HgWiaUb1pABDYgVAJ6HT4lnLI8vWzP0rEDps2LgUEpdQ0Maia0McXvgZic5K2tbsCGclAFsqgd844MSqiaSzWw/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></p>
   <p style="line-height:1.75em;margin-left:8px;text-align:center;"><span style="letter-spacing:.5px;font-size:14px;color:rgb(136,136,136);">在ImageNet上具有不同贴片尺寸的BagNets的性能。</span><br></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">即使对于非常小的贴片尺寸，BagNet上的BagNets性能也令人印象深刻：尺寸为17 x 17像素的图像特征足以达到AlexNet级别的性能，而尺寸为33 x 33像素的特征足以达到约87％的前5精度。通过更仔细地放置3 x 3卷积和额外的超参数调整，可以实现更高的性能值。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">这是我们得到的第一个重要结果：只需使用一组小图的特性即可解决ImageNet问题。对象形状或对象部分之间的关系等远程空间关系可以完全忽略，并且不需要解决任务。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">BagNets的一大特色是他们透明的决策。例如，我们现在可以查看哪个图像特征对于给定的类最具预测性。</span></p>
   <p><br></p>
   <p style="text-align:center;margin-left:8px;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/UicQ7HgWiaUb1pABDYgVAJ6HT4lnLI8vWzhiczzkfjfNMP6HmfUqoCjeyU5LzXIJ8GcbF7dnBD2icEsADGkxQnnwxA/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"><span style="letter-spacing:.5px;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;text-align:justify;font-size:14px;color:rgb(136,136,136);">图像功能具有最多的类证据。 我们展示了正确预测类（顶行）的功能和预测错误类（底行）的分散注意力的功能</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;"><br></span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">上图中，最上面的手指图像被识别成tench（丁鱥guì，是淡水钓鱼的主要鱼种，也是鲈鱼等猎食性鱼类的饲料），因为这个类别中的大多数图像，都有一个渔民像举奖杯一样举起丁鱥。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">同样，我们还得到一个精确定义的热图，显示图像的哪些部分促使神经网络做出某个决定。</span></p>
   <p><br></p>
   <p style="text-align:center;margin-left:8px;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/UicQ7HgWiaUb1pABDYgVAJ6HT4lnLI8vWzezDBdHtwvcAOlGoib0UqoK8QeC3icq5oxiciayKfEp33RfBzIvOCnCtY7g/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></p>
   <p style="margin-left:8px;line-height:1.75em;text-align:center;"><span style="letter-spacing:.5px;font-size:14px;color:rgb(136,136,136);">来自BagNets的热图显示了确切的图像部分对决策的贡献。 热图不是近似的，而是显示每个图像部分的真实贡献。</span><br></p>
   <p><br></p>ResNet-50与BagNets惊人相似
   <p style="margin-left:8px;line-height:1.75em;"><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">BagNets表明，基于本地图像特征和对象类别之间的弱统计相关性，可以在ImageNet上达到高精度。&nbsp;</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">如果这就够了，为什么像ResNet-50这样的标准深网会学到任何根本不同的东西？ 如果丰富的本地图像特征足以解决任务，那为什么ResNet-50还需要了解复杂的大尺度关系（如对象的形状）？</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">为了验证现代DNN遵循与简单的特征包网络类似的策略的假设，我们在BagNets的以下“签名”上测试不同的ResNets，DenseNets和VGG：</span></p>
   <p><br></p>
   <ul class="list-paddingleft-2" style="list-style-type:circle;">
    <li><p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">决策对图像特征的空间改组是不变的（只能在VGG模型上测试）</span></p></li>
    <li><p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">不同图像部分的修改应该是独立的（就其对总类证据的影响而言）</span></p></li>
    <li><p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">标准CNN和BagNets产生的错误应该类似</span></p></li>
    <li><p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">标准CNN和BagNets应对类似功能敏感</span></p></li>
   </ul>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">在所有四个实验中，我们发现CNN和BagNets之间的行为非常相似。 例如，在上一个实验中，我们展示了BagNets最敏感的那些图像部分（例如，如果你遮挡那些部分）与CNN最敏感的那些基本相同。&nbsp;</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">实际上，BagNets的热图（灵敏度的空间图）比由DeepLift（直接为DenseNet-169计算热图）等归因方法生成的热图,更好地预测了DenseNet-169的灵敏度。&nbsp;</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">当然，DNN并不完全类似于特征包模型，但确实显示出一些偏差。特别是，我们发现网络越深入，功能越来越大，远程依赖性也越来越大。&nbsp;</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">因此，更深层的神经网络确实改进了更简单的特征包模型，但我认为核心分类策略并没有真正改变。</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><br></p>解释CNN几个奇怪的现象
   <p style="margin-left:8px;line-height:1.75em;"><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">将CNN的决策视为一种BoF策略，可以解释有关CNN的几个奇怪的观察。首先，它将解释为什么CNN具有如此强烈的纹理偏差；其次，它可以解释为什么CNN对图像部分的混乱如此不敏感；甚至可以解释一般的对抗性贴纸和对抗性扰动的存在，比如人们在图像中的任何地方放置误导信号，并且无论这些信号是否适合图像的其余部分，CNN仍然可以可靠地接收信号。</span><br></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">我们的成果显示，CNN利用自然图像中存在的许多弱统计规律进行分类，并且不会像人类一样跳向图像部分的对象级整合。其他任务和感官方式也是如此。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">我们必须认真思考如何构建架构、任务和学习方法，以抵消这种弱统计相关性的趋势。一种方式，是将CNN的归纳偏差从小的局部特征改善为更全局的特征；另一种方式，是删除、或替换网络不应该依赖的那些特征。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">然而，最大的问题之一当然是图像分类本身的任务：如果局部图像特征足以解决任务，也就不需要去学习自然界的真实“物理学”，这样我们就必须重构任务，推着模型去学习对象的物理本质。</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">这样就很可能需要跳出纯粹只通过观察学习，获得输入和输出特征之间相关性的方式，以便允许模型提取因果依赖性。</span></p>
   <p><br></p>总结
   <p style="margin-left:8px;line-height:1.75em;"><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">总之，我们的结果表明CNN可能遵循极其简单的分类策略。科学家认为这个发现可能在2019继续成为关注的焦点，凸显了我们对深度神经网络的内部运作了解甚少。&nbsp;</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">缺乏理解使我们无法从根本上发展出更好的模型和架构，来缩小人与机器之间的差距。深化我们的理解，将使我们能够找到弥合这一差距的方法。&nbsp;</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">这将带来异常丰厚的回报：当我们试图将CNN偏向物体的更多物理特性时，我们突然达到了接近人类的噪声稳健性。&nbsp;</span></p>
   <p><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:15px;letter-spacing:.5px;">我们继续期待在2019年，在这一领域上会出现更多令人兴奋的结果，获得真正了解了真实世界中，物理和因果性质的卷积神经网络。</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:14px;letter-spacing:.5px;color:rgb(136,136,136);">参考链接：</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:14px;letter-spacing:.5px;color:rgb(136,136,136);">https://medium.com/bethgelab/neural-networks-seem-to-follow-a-puzzlingly-simple-strategy-to-classify-images-f4229317261f</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><span style="font-size:14px;letter-spacing:.5px;color:rgb(136,136,136);"></span><span style="letter-spacing:.544px;color:rgb(136,136,136);text-align:justify;font-size:12px;">来源：Medium、</span><span style="letter-spacing:.544px;color:rgb(136,136,136);font-size:12px;text-align:justify;">新智元</span></p>
   <p style="min-height:1em;font-family:Arial, sans-serif;font-size:17px;letter-spacing:1.5px;line-height:1.75em;text-align:center;"><span style="text-align:center;letter-spacing:.5px;font-size:15px;color:rgb(51,102,255);">∞</span><span style="text-align:center;letter-spacing:.5px;font-size:15px;color:rgb(153,153,153);">∞∞∞</span><span style="text-align:center;letter-spacing:.5px;font-size:15px;color:rgb(255,153,0);">∞</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/HM0Eu5N4Zuiacvcicgib2daRQ8m8nR6VfjiblOWQ0eXDjQMw0libYaW3EkCtq3U9tdgh2iagv3KpFkYpxUlSWdC3icYQg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="margin-left:8px;text-align:center;line-height:1.75em;"><span style="font-size:18px;letter-spacing:.5px;color:rgb(51,51,51);">公众号回复</span><span style="font-size:18px;letter-spacing:.5px;color:rgb(54,65,173);">“IT派”</span><span style="font-size:18px;letter-spacing:.5px;color:rgb(51,51,51);">，</span></p>
   <p style="margin-left:8px;text-align:center;line-height:1.75em;"><span style="font-size:18px;"><span style="letter-spacing:.5px;color:rgb(51,51,51);">邀你加入&nbsp;</span></span><span style="font-size:18px;color:rgb(255,153,0);letter-spacing:.5px;">IT派&nbsp;{</span><span style="font-size:18px;color:rgb(255,153,0);letter-spacing:.5px;"> 深广创投圈 }&nbsp;</span></p> 
  </div> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
