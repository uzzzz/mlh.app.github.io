<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>ML-决策树-ID3、C4.5、CART公式推导实现 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="ML-决策树-ID3、C4.5、CART公式推导实现" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="目录 1.决策树 2.ID3算法 3.C4.5算法 ID3/C4.5的防过拟合 ID3/C4.5的缺失数据 4.CART算法 1）cart在分类方面：基于基尼系数：Gini(D) 2）cart在回归方面：基于方差/标准差 CART的剪枝 sklearn实现： 分类DecisionTreeClassifier 回归DecisionTreeRegressor &nbsp; 1.决策树 1.1定义 Ck表示第k个类，特征T可取n个不同的值；数据集D将T相同的特征取值划分为一个集合，共划分n个不同子集{D1,D2,...Dn} 输入：训练集D,特征集A,最小信息增益阀值e 输出：决策树T 2.ID3算法 ID3基于信息增益 1）定义Ent：信息熵，D内类越乱越大，D内类一致时为0 2）定义Gain(D,a):信息增益，描述的是D依据属性来划分所能够获得的“纯度提升”：尽可能快的让分下去的数据类别越来越一致 3）流程：&nbsp; ThreeGenerate(D,A):&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 每次对输入的数据集D划分依据a*属性做划分， 实际中不可能一直细分，那样太慢或过拟合，增益小于某个阀值时就直接标记为节点中数据集最大类： 遍历属性A的所有取值v，如果在某取值上集合（数据量）为空，则这个点设置为D中最大类类型，否则递归创建基于数据集Dv和没有属性a*的决策树： &nbsp; ID3存在很多不足，对可取值较多的属性有偏好；长度，密度等无考虑等，改进版为C4.5 3.C4.5算法 基于信息增益率，定义Gain_ratio，在原信息增益基础上除了IV： 属性a可取值的数目越多，IV(a)越大，因此对数目小的属性有偏好。最终C4.5基于一个启发式：先选出信息增益高于平均的，再从其中选择增益率最高的。 其余和上面的ID3流程一致，只是属性划分有如上细微区别。 ID3/C4.5的防过拟合 预剪枝：生成分支的过程中，先评估当且泛化性能（测试交叉验证等），然后评估划分后的性能，若，划分后不能提升泛化性能，则不划分，还是保留这个叶子节点。 后剪枝：先完全训练完，得到决策树，然后从最底层节点往上，评估把这个节点（父节点，有子节点的）设置为叶子节点和不设置两种情况，若，设置为叶子节点能提升性能，则把这个节点设置为叶子节点，否则保留。 设定最大深度：划分决策树的时候统计深度，超过这个深度了就停止划分，输出为叶子节点。 设定节点最小数据量：节点内划入的数据集小于某个量了，再划分可能没意义了，就不划分了。 随机性：基于信息增益，增益率等评价指标，选择最佳属性的时候，最佳属性可以不一定就是最佳，比如可以是在高于平均的属性里面，随机选择一个作为最佳属性。 &nbsp; ID3/C4.5的缺失数据 定义： 新的计算： 属性划分：在划分x样本时，a属性若取值已知，则x划入取值对应的子结点，权重wx不变；若未知，x划入所有子结点，且按照wx=rv.wx划分权值。 &nbsp; 4.CART算法 CART:分类和回归树，是个二叉树。&nbsp;其希望划分下去的数据方差越来越接近。 1）cart在分类方面：基于基尼系数：Gini(D) Gini(D)反应的是数据集D中样本不一致的概率，因此Gini越小，越一致性，纯度越高。 属性划分，基尼指数：Gini_index(D,a) &nbsp;累加D中基于属性a(a1,a2,...,av)划分的不同数据集（a1:D1,a2:D2,...,av:DV）的Gini系数和，反映的是对属性a划分的整体Gini系数评价。 对于连续特征：如属性a身高，取值为:150,160,170,180，可以分别计算基于155,165,175为划分点的二分，分成小于等于划分点，大于划分点的两个数据集D1、D2。如基于165，把所有基于属性a&lt;=165的划入左节点，a&gt;165的划入右节点。然后计算（基尼增益）GiniGain=Gini_index(D1,a)+Gini_index(D2,a)，最后选择最小的划分点，如175，把数据划分为两个部分。 对于离散特征：CART分类时，选择最小Gini_index的属性划分，a*=argmin Gini_index(D,a)，（遍历所有属性A，遍历属性A中可能的取值a，得到最小Gini_index的a*，以及其所属的属性A*），然后把数据分成属性A*中取值为a*的部分D1，和属性A*中除取值a*外的所有数据部分D2，划入左右节点。 &nbsp; 2）cart在回归方面：基于方差/标准差 对于任意划分特征a，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点(c1为D1数据集的样本均值，c2为D2数据集的样本均值): 即：不断基于连续属性数据的中点，做二分划分为左右数据集D1,D2,然后计算均值c1,c2,计算方差，计算方差和，最后取方差最小的划分点作为当且决策树分割点。 而叶子节点的输出：通常采用当前归属到此叶子节点中数据集的均值或者中位数。 阀值：可以定义数据量，比如5，当叶子节点的数据集小于这个量的时候停止划分，之间返回数据集的均值或中位数。 CART的剪枝 白话版：从最小面的子树开始，一直往上到根节点下面的左右子树，依次把每个以 t&nbsp;为根节点的子树(两个，如果存在的话//有些不一定存在左右子树，比如只存在左子树就只考虑左子树Ttl)Ttl,Ttr，分别计算正常不剪枝时（整个决策树）的训练损失C(Tt)，和剪枝记为叶子节点时（叶子节点是值分类为最大类值，回归为平均值）（整个决策树）的训练损失C(T)； 然后按照 2）步计算得到这个 t 节点子树节点(Ttl,Ttr)的阀值(阀值为min更新，确保越往上阀值越小)； 这样一层层往上计算，得到每个节点的a值了，从2）步更新公式来看，上层的阀值&lt;=下层节点的阀值； 因此4）步选择最大的ak开始，自上而下(从a值上层的阀值&lt;=下层节点的阀值来看，这样首先会剪掉最小面的一些树，然后慢慢往上剪)把小于阀值的减掉得到一决策树T1，以此类推得到集合M个数那么多个剪枝决策树T1,T2,...,T|M|； 然后交叉验证（k折、留一等等），选择误差最小的，作为输出。 总结来说就是一句话：产生很多按照不同强度（阀值a）剪出来的剪枝决策树，然后测试效果，选出一个最好的输出。 &nbsp; sklearn实现： 分类DecisionTreeClassifier 具体参数参考官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html import numpy as np from sklearn import tree from graphviz import Source from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split np.random.seed(11) iris = load_iris() X = iris.data Y = iris.target # 导入数据：用于分类 X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=2, test_size=0.2) # 决策树分类器,criterion:标准，max_depth：最大深度 clf = tree.DecisionTreeClassifier(criterion=&#39;gini&#39;, max_depth=3) clf.fit(X_train, Y_train) pdy = clf.predict(X_test) score = clf.score(X_test, Y_test) print(score) dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = Source(dot_data) graph.format = &#39;png&#39; graph.render(&#39;cart_tree&#39;, view=True) 回归DecisionTreeRegressor 具体参数参考官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor import numpy as np from sklearn import tree from graphviz import Source from sklearn.datasets import load_boston boston = load_boston() # 太大不好显示，取前20画CART树 X = boston.data[:20] y = boston.target[:20] # 用于测试，随机取5个 index = np.random.randint(20, boston.data.shape[0],5 ) test_X = boston.data[index] test_y = boston.target[index] # 导入数据：用于分类 # 决策树回归器，mse：均方误差;max_depth：最大深度5 clf_reg = tree.DecisionTreeRegressor(criterion=&#39;mse&#39;, random_state=11, max_depth=5) clf_reg.fit(X, y) print(&#39;测试数据原结果：&#39;, test_y) print(&#39;测试数据预测结果：&#39;, clf_reg.predict(test_X)) dot_data = tree.export_graphviz(clf_reg, out_file=None, feature_names=boston.feature_names, filled=True, rounded=True, special_characters=True) graph = Source(dot_data) graph.format = &#39;png&#39; graph.render(&#39;cart_tree_Regressor&#39;, view=True) # 大致还是很相近的，完全一样不太可能 测试数据原结果： [23. 19.6 30.5 24.4 22.9] 测试数据预测结果： [20.3 17.96666667 22.55 27.1 22.55 ] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<meta property="og:description" content="目录 1.决策树 2.ID3算法 3.C4.5算法 ID3/C4.5的防过拟合 ID3/C4.5的缺失数据 4.CART算法 1）cart在分类方面：基于基尼系数：Gini(D) 2）cart在回归方面：基于方差/标准差 CART的剪枝 sklearn实现： 分类DecisionTreeClassifier 回归DecisionTreeRegressor &nbsp; 1.决策树 1.1定义 Ck表示第k个类，特征T可取n个不同的值；数据集D将T相同的特征取值划分为一个集合，共划分n个不同子集{D1,D2,...Dn} 输入：训练集D,特征集A,最小信息增益阀值e 输出：决策树T 2.ID3算法 ID3基于信息增益 1）定义Ent：信息熵，D内类越乱越大，D内类一致时为0 2）定义Gain(D,a):信息增益，描述的是D依据属性来划分所能够获得的“纯度提升”：尽可能快的让分下去的数据类别越来越一致 3）流程：&nbsp; ThreeGenerate(D,A):&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 每次对输入的数据集D划分依据a*属性做划分， 实际中不可能一直细分，那样太慢或过拟合，增益小于某个阀值时就直接标记为节点中数据集最大类： 遍历属性A的所有取值v，如果在某取值上集合（数据量）为空，则这个点设置为D中最大类类型，否则递归创建基于数据集Dv和没有属性a*的决策树： &nbsp; ID3存在很多不足，对可取值较多的属性有偏好；长度，密度等无考虑等，改进版为C4.5 3.C4.5算法 基于信息增益率，定义Gain_ratio，在原信息增益基础上除了IV： 属性a可取值的数目越多，IV(a)越大，因此对数目小的属性有偏好。最终C4.5基于一个启发式：先选出信息增益高于平均的，再从其中选择增益率最高的。 其余和上面的ID3流程一致，只是属性划分有如上细微区别。 ID3/C4.5的防过拟合 预剪枝：生成分支的过程中，先评估当且泛化性能（测试交叉验证等），然后评估划分后的性能，若，划分后不能提升泛化性能，则不划分，还是保留这个叶子节点。 后剪枝：先完全训练完，得到决策树，然后从最底层节点往上，评估把这个节点（父节点，有子节点的）设置为叶子节点和不设置两种情况，若，设置为叶子节点能提升性能，则把这个节点设置为叶子节点，否则保留。 设定最大深度：划分决策树的时候统计深度，超过这个深度了就停止划分，输出为叶子节点。 设定节点最小数据量：节点内划入的数据集小于某个量了，再划分可能没意义了，就不划分了。 随机性：基于信息增益，增益率等评价指标，选择最佳属性的时候，最佳属性可以不一定就是最佳，比如可以是在高于平均的属性里面，随机选择一个作为最佳属性。 &nbsp; ID3/C4.5的缺失数据 定义： 新的计算： 属性划分：在划分x样本时，a属性若取值已知，则x划入取值对应的子结点，权重wx不变；若未知，x划入所有子结点，且按照wx=rv.wx划分权值。 &nbsp; 4.CART算法 CART:分类和回归树，是个二叉树。&nbsp;其希望划分下去的数据方差越来越接近。 1）cart在分类方面：基于基尼系数：Gini(D) Gini(D)反应的是数据集D中样本不一致的概率，因此Gini越小，越一致性，纯度越高。 属性划分，基尼指数：Gini_index(D,a) &nbsp;累加D中基于属性a(a1,a2,...,av)划分的不同数据集（a1:D1,a2:D2,...,av:DV）的Gini系数和，反映的是对属性a划分的整体Gini系数评价。 对于连续特征：如属性a身高，取值为:150,160,170,180，可以分别计算基于155,165,175为划分点的二分，分成小于等于划分点，大于划分点的两个数据集D1、D2。如基于165，把所有基于属性a&lt;=165的划入左节点，a&gt;165的划入右节点。然后计算（基尼增益）GiniGain=Gini_index(D1,a)+Gini_index(D2,a)，最后选择最小的划分点，如175，把数据划分为两个部分。 对于离散特征：CART分类时，选择最小Gini_index的属性划分，a*=argmin Gini_index(D,a)，（遍历所有属性A，遍历属性A中可能的取值a，得到最小Gini_index的a*，以及其所属的属性A*），然后把数据分成属性A*中取值为a*的部分D1，和属性A*中除取值a*外的所有数据部分D2，划入左右节点。 &nbsp; 2）cart在回归方面：基于方差/标准差 对于任意划分特征a，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点(c1为D1数据集的样本均值，c2为D2数据集的样本均值): 即：不断基于连续属性数据的中点，做二分划分为左右数据集D1,D2,然后计算均值c1,c2,计算方差，计算方差和，最后取方差最小的划分点作为当且决策树分割点。 而叶子节点的输出：通常采用当前归属到此叶子节点中数据集的均值或者中位数。 阀值：可以定义数据量，比如5，当叶子节点的数据集小于这个量的时候停止划分，之间返回数据集的均值或中位数。 CART的剪枝 白话版：从最小面的子树开始，一直往上到根节点下面的左右子树，依次把每个以 t&nbsp;为根节点的子树(两个，如果存在的话//有些不一定存在左右子树，比如只存在左子树就只考虑左子树Ttl)Ttl,Ttr，分别计算正常不剪枝时（整个决策树）的训练损失C(Tt)，和剪枝记为叶子节点时（叶子节点是值分类为最大类值，回归为平均值）（整个决策树）的训练损失C(T)； 然后按照 2）步计算得到这个 t 节点子树节点(Ttl,Ttr)的阀值(阀值为min更新，确保越往上阀值越小)； 这样一层层往上计算，得到每个节点的a值了，从2）步更新公式来看，上层的阀值&lt;=下层节点的阀值； 因此4）步选择最大的ak开始，自上而下(从a值上层的阀值&lt;=下层节点的阀值来看，这样首先会剪掉最小面的一些树，然后慢慢往上剪)把小于阀值的减掉得到一决策树T1，以此类推得到集合M个数那么多个剪枝决策树T1,T2,...,T|M|； 然后交叉验证（k折、留一等等），选择误差最小的，作为输出。 总结来说就是一句话：产生很多按照不同强度（阀值a）剪出来的剪枝决策树，然后测试效果，选出一个最好的输出。 &nbsp; sklearn实现： 分类DecisionTreeClassifier 具体参数参考官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html import numpy as np from sklearn import tree from graphviz import Source from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split np.random.seed(11) iris = load_iris() X = iris.data Y = iris.target # 导入数据：用于分类 X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=2, test_size=0.2) # 决策树分类器,criterion:标准，max_depth：最大深度 clf = tree.DecisionTreeClassifier(criterion=&#39;gini&#39;, max_depth=3) clf.fit(X_train, Y_train) pdy = clf.predict(X_test) score = clf.score(X_test, Y_test) print(score) dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = Source(dot_data) graph.format = &#39;png&#39; graph.render(&#39;cart_tree&#39;, view=True) 回归DecisionTreeRegressor 具体参数参考官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor import numpy as np from sklearn import tree from graphviz import Source from sklearn.datasets import load_boston boston = load_boston() # 太大不好显示，取前20画CART树 X = boston.data[:20] y = boston.target[:20] # 用于测试，随机取5个 index = np.random.randint(20, boston.data.shape[0],5 ) test_X = boston.data[index] test_y = boston.target[index] # 导入数据：用于分类 # 决策树回归器，mse：均方误差;max_depth：最大深度5 clf_reg = tree.DecisionTreeRegressor(criterion=&#39;mse&#39;, random_state=11, max_depth=5) clf_reg.fit(X, y) print(&#39;测试数据原结果：&#39;, test_y) print(&#39;测试数据预测结果：&#39;, clf_reg.predict(test_X)) dot_data = tree.export_graphviz(clf_reg, out_file=None, feature_names=boston.feature_names, filled=True, rounded=True, special_characters=True) graph = Source(dot_data) graph.format = &#39;png&#39; graph.render(&#39;cart_tree_Regressor&#39;, view=True) # 大致还是很相近的，完全一样不太可能 测试数据原结果： [23. 19.6 30.5 24.4 22.9] 测试数据预测结果： [20.3 17.96666667 22.55 27.1 22.55 ] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-20T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"目录 1.决策树 2.ID3算法 3.C4.5算法 ID3/C4.5的防过拟合 ID3/C4.5的缺失数据 4.CART算法 1）cart在分类方面：基于基尼系数：Gini(D) 2）cart在回归方面：基于方差/标准差 CART的剪枝 sklearn实现： 分类DecisionTreeClassifier 回归DecisionTreeRegressor &nbsp; 1.决策树 1.1定义 Ck表示第k个类，特征T可取n个不同的值；数据集D将T相同的特征取值划分为一个集合，共划分n个不同子集{D1,D2,...Dn} 输入：训练集D,特征集A,最小信息增益阀值e 输出：决策树T 2.ID3算法 ID3基于信息增益 1）定义Ent：信息熵，D内类越乱越大，D内类一致时为0 2）定义Gain(D,a):信息增益，描述的是D依据属性来划分所能够获得的“纯度提升”：尽可能快的让分下去的数据类别越来越一致 3）流程：&nbsp; ThreeGenerate(D,A):&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 每次对输入的数据集D划分依据a*属性做划分， 实际中不可能一直细分，那样太慢或过拟合，增益小于某个阀值时就直接标记为节点中数据集最大类： 遍历属性A的所有取值v，如果在某取值上集合（数据量）为空，则这个点设置为D中最大类类型，否则递归创建基于数据集Dv和没有属性a*的决策树： &nbsp; ID3存在很多不足，对可取值较多的属性有偏好；长度，密度等无考虑等，改进版为C4.5 3.C4.5算法 基于信息增益率，定义Gain_ratio，在原信息增益基础上除了IV： 属性a可取值的数目越多，IV(a)越大，因此对数目小的属性有偏好。最终C4.5基于一个启发式：先选出信息增益高于平均的，再从其中选择增益率最高的。 其余和上面的ID3流程一致，只是属性划分有如上细微区别。 ID3/C4.5的防过拟合 预剪枝：生成分支的过程中，先评估当且泛化性能（测试交叉验证等），然后评估划分后的性能，若，划分后不能提升泛化性能，则不划分，还是保留这个叶子节点。 后剪枝：先完全训练完，得到决策树，然后从最底层节点往上，评估把这个节点（父节点，有子节点的）设置为叶子节点和不设置两种情况，若，设置为叶子节点能提升性能，则把这个节点设置为叶子节点，否则保留。 设定最大深度：划分决策树的时候统计深度，超过这个深度了就停止划分，输出为叶子节点。 设定节点最小数据量：节点内划入的数据集小于某个量了，再划分可能没意义了，就不划分了。 随机性：基于信息增益，增益率等评价指标，选择最佳属性的时候，最佳属性可以不一定就是最佳，比如可以是在高于平均的属性里面，随机选择一个作为最佳属性。 &nbsp; ID3/C4.5的缺失数据 定义： 新的计算： 属性划分：在划分x样本时，a属性若取值已知，则x划入取值对应的子结点，权重wx不变；若未知，x划入所有子结点，且按照wx=rv.wx划分权值。 &nbsp; 4.CART算法 CART:分类和回归树，是个二叉树。&nbsp;其希望划分下去的数据方差越来越接近。 1）cart在分类方面：基于基尼系数：Gini(D) Gini(D)反应的是数据集D中样本不一致的概率，因此Gini越小，越一致性，纯度越高。 属性划分，基尼指数：Gini_index(D,a) &nbsp;累加D中基于属性a(a1,a2,...,av)划分的不同数据集（a1:D1,a2:D2,...,av:DV）的Gini系数和，反映的是对属性a划分的整体Gini系数评价。 对于连续特征：如属性a身高，取值为:150,160,170,180，可以分别计算基于155,165,175为划分点的二分，分成小于等于划分点，大于划分点的两个数据集D1、D2。如基于165，把所有基于属性a&lt;=165的划入左节点，a&gt;165的划入右节点。然后计算（基尼增益）GiniGain=Gini_index(D1,a)+Gini_index(D2,a)，最后选择最小的划分点，如175，把数据划分为两个部分。 对于离散特征：CART分类时，选择最小Gini_index的属性划分，a*=argmin Gini_index(D,a)，（遍历所有属性A，遍历属性A中可能的取值a，得到最小Gini_index的a*，以及其所属的属性A*），然后把数据分成属性A*中取值为a*的部分D1，和属性A*中除取值a*外的所有数据部分D2，划入左右节点。 &nbsp; 2）cart在回归方面：基于方差/标准差 对于任意划分特征a，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点(c1为D1数据集的样本均值，c2为D2数据集的样本均值): 即：不断基于连续属性数据的中点，做二分划分为左右数据集D1,D2,然后计算均值c1,c2,计算方差，计算方差和，最后取方差最小的划分点作为当且决策树分割点。 而叶子节点的输出：通常采用当前归属到此叶子节点中数据集的均值或者中位数。 阀值：可以定义数据量，比如5，当叶子节点的数据集小于这个量的时候停止划分，之间返回数据集的均值或中位数。 CART的剪枝 白话版：从最小面的子树开始，一直往上到根节点下面的左右子树，依次把每个以 t&nbsp;为根节点的子树(两个，如果存在的话//有些不一定存在左右子树，比如只存在左子树就只考虑左子树Ttl)Ttl,Ttr，分别计算正常不剪枝时（整个决策树）的训练损失C(Tt)，和剪枝记为叶子节点时（叶子节点是值分类为最大类值，回归为平均值）（整个决策树）的训练损失C(T)； 然后按照 2）步计算得到这个 t 节点子树节点(Ttl,Ttr)的阀值(阀值为min更新，确保越往上阀值越小)； 这样一层层往上计算，得到每个节点的a值了，从2）步更新公式来看，上层的阀值&lt;=下层节点的阀值； 因此4）步选择最大的ak开始，自上而下(从a值上层的阀值&lt;=下层节点的阀值来看，这样首先会剪掉最小面的一些树，然后慢慢往上剪)把小于阀值的减掉得到一决策树T1，以此类推得到集合M个数那么多个剪枝决策树T1,T2,...,T|M|； 然后交叉验证（k折、留一等等），选择误差最小的，作为输出。 总结来说就是一句话：产生很多按照不同强度（阀值a）剪出来的剪枝决策树，然后测试效果，选出一个最好的输出。 &nbsp; sklearn实现： 分类DecisionTreeClassifier 具体参数参考官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html import numpy as np from sklearn import tree from graphviz import Source from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split np.random.seed(11) iris = load_iris() X = iris.data Y = iris.target # 导入数据：用于分类 X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=2, test_size=0.2) # 决策树分类器,criterion:标准，max_depth：最大深度 clf = tree.DecisionTreeClassifier(criterion=&#39;gini&#39;, max_depth=3) clf.fit(X_train, Y_train) pdy = clf.predict(X_test) score = clf.score(X_test, Y_test) print(score) dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = Source(dot_data) graph.format = &#39;png&#39; graph.render(&#39;cart_tree&#39;, view=True) 回归DecisionTreeRegressor 具体参数参考官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor import numpy as np from sklearn import tree from graphviz import Source from sklearn.datasets import load_boston boston = load_boston() # 太大不好显示，取前20画CART树 X = boston.data[:20] y = boston.target[:20] # 用于测试，随机取5个 index = np.random.randint(20, boston.data.shape[0],5 ) test_X = boston.data[index] test_y = boston.target[index] # 导入数据：用于分类 # 决策树回归器，mse：均方误差;max_depth：最大深度5 clf_reg = tree.DecisionTreeRegressor(criterion=&#39;mse&#39;, random_state=11, max_depth=5) clf_reg.fit(X, y) print(&#39;测试数据原结果：&#39;, test_y) print(&#39;测试数据预测结果：&#39;, clf_reg.predict(test_X)) dot_data = tree.export_graphviz(clf_reg, out_file=None, feature_names=boston.feature_names, filled=True, rounded=True, special_characters=True) graph = Source(dot_data) graph.format = &#39;png&#39; graph.render(&#39;cart_tree_Regressor&#39;, view=True) # 大致还是很相近的，完全一样不太可能 测试数据原结果： [23. 19.6 30.5 24.4 22.9] 测试数据预测结果： [20.3 17.96666667 22.55 27.1 22.55 ] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;","@type":"BlogPosting","url":"/2019/02/20/890c36a81ae316435b671eb5529f1bfe.html","headline":"ML-决策树-ID3、C4.5、CART公式推导实现","dateModified":"2019-02-20T00:00:00+08:00","datePublished":"2019-02-20T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/02/20/890c36a81ae316435b671eb5529f1bfe.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>ML-决策树-ID3、C4.5、CART公式推导实现</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p id="main-toc"><strong>目录</strong></p> 
  <p id="1.%E5%86%B3%E7%AD%96%E6%A0%91-toc" style="margin-left:0px;"><a href="#1.%E5%86%B3%E7%AD%96%E6%A0%91" rel="nofollow">1.决策树</a></p> 
  <p id="2.ID3%E7%AE%97%E6%B3%95-toc" style="margin-left:0px;"><a href="#2.ID3%E7%AE%97%E6%B3%95" rel="nofollow">2.ID3算法</a></p> 
  <p id="3.C4.5%E7%AE%97%E6%B3%95-toc" style="margin-left:0px;"><a href="#3.C4.5%E7%AE%97%E6%B3%95" rel="nofollow">3.C4.5算法</a></p> 
  <p id="ID3%2FC4.5%E7%9A%84%E9%98%B2%E8%BF%87%E6%8B%9F%E5%90%88-toc" style="margin-left:0px;"><a href="#ID3%2FC4.5%E7%9A%84%E9%98%B2%E8%BF%87%E6%8B%9F%E5%90%88" rel="nofollow">ID3/C4.5的防过拟合</a></p> 
  <p id="ID3%2FC4.5%E7%9A%84%E7%BC%BA%E5%A4%B1%E6%95%B0%E6%8D%AE-toc" style="margin-left:0px;"><a href="#ID3%2FC4.5%E7%9A%84%E7%BC%BA%E5%A4%B1%E6%95%B0%E6%8D%AE" rel="nofollow">ID3/C4.5的缺失数据</a></p> 
  <p id="4.CART%E7%AE%97%E6%B3%95-toc" style="margin-left:0px;"><a href="#4.CART%E7%AE%97%E6%B3%95" rel="nofollow">4.CART算法</a></p> 
  <p id="1%EF%BC%89cart%E5%9C%A8%E5%88%86%E7%B1%BB%E6%96%B9%E9%9D%A2%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0%EF%BC%9AGini(D)-toc" style="margin-left:40px;"><a href="#1%EF%BC%89cart%E5%9C%A8%E5%88%86%E7%B1%BB%E6%96%B9%E9%9D%A2%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0%EF%BC%9AGini(D)" rel="nofollow">1）cart在分类方面：基于基尼系数：Gini(D)</a></p> 
  <p id="2%EF%BC%89cart%E5%9C%A8%E5%9B%9E%E5%BD%92%E6%96%B9%E9%9D%A2%EF%BC%9A%E5%9F%BA%E4%BA%8E%E6%96%B9%E5%B7%AE%2F%E6%A0%87%E5%87%86%E5%B7%AE-toc" style="margin-left:40px;"><a href="#2%EF%BC%89cart%E5%9C%A8%E5%9B%9E%E5%BD%92%E6%96%B9%E9%9D%A2%EF%BC%9A%E5%9F%BA%E4%BA%8E%E6%96%B9%E5%B7%AE%2F%E6%A0%87%E5%87%86%E5%B7%AE" rel="nofollow">2）cart在回归方面：基于方差/标准差</a></p> 
  <p id="CART%E7%9A%84%E5%89%AA%E6%9E%9D-toc" style="margin-left:0px;"><a href="#CART%E7%9A%84%E5%89%AA%E6%9E%9D" rel="nofollow">CART的剪枝</a></p> 
  <p id="sklearn%E5%AE%9E%E7%8E%B0%EF%BC%9A-toc" style="margin-left:0px;"><a href="#sklearn%E5%AE%9E%E7%8E%B0%EF%BC%9A" rel="nofollow">sklearn实现：</a></p> 
  <p id="%E5%88%86%E7%B1%BBDecisionTreeClassifier-toc" style="margin-left:40px;"><a href="#%E5%88%86%E7%B1%BBDecisionTreeClassifier" rel="nofollow">分类DecisionTreeClassifier</a></p> 
  <p id="%E5%9B%9E%E5%BD%92DecisionTreeRegressor-toc" style="margin-left:40px;"><a href="#%E5%9B%9E%E5%BD%92DecisionTreeRegressor" rel="nofollow">回归DecisionTreeRegressor</a></p> 
  <p id="5.GDBT-toc" style="margin-left:0px;">&nbsp;</p> 
  <hr id="hr-toc">
  <h1 id="1.%E5%86%B3%E7%AD%96%E6%A0%91"><strong><strong><strong>1.决策树</strong></strong></strong></h1> 
  <p style="text-indent:50px;"><strong><strong><strong>1.1定义</strong></strong></strong></p> 
  <p style="text-indent:50px;">Ck表示第k个类，特征T可取n个不同的值；数据集D将T相同的特征取值划分为一个集合，共划分n个不同子集{D1,D2,...Dn}</p> 
  <p style="text-indent:50px;">输入：训练集D,特征集A,最小信息增益阀值e</p> 
  <p style="text-indent:50px;">输出：决策树T</p> 
  <h1 id="2.ID3%E7%AE%97%E6%B3%95" style="margin-left:0pt;"><strong>2.ID3算法</strong></h1> 
  <p style="text-indent:50px;">ID3基于信息增益</p> 
  <p style="text-indent:50px;">1）定义Ent：信息熵，D内类越乱越大，D内类一致时为0</p> 
  <p style="text-align:center;"><img alt="" class="has" height="77" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218213206227.png" width="532"></p> 
  <p style="text-indent:50px;">2）定义Gain(D,a):信息增益，描述的是D依据属性来划分所能够获得的“纯度提升”：尽可能快的让分下去的数据类别越来越一致</p> 
  <p style="text-align:center;"><img alt="" class="has" height="78" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218213243636.png" width="653"></p> 
  <p style="text-indent:50px;">3）流程：&nbsp;</p> 
  <p><span style="color:#f33b45;"><strong>ThreeGenerate(D,A):&nbsp;</strong></span></p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img alt="" class="has" height="188" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218214617409.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="413"></p> 
  <p style="text-indent:50px;">每次对输入的数据集D划分依据a*属性做划分，</p> 
  <p style="text-align:center;"><img alt="" class="has" height="97" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218213654443.png" width="255"></p> 
  <p style="text-indent:50px;">实际中不可能一直细分，那样太慢或过拟合，增益小于某个阀值时就直接标记为节点中数据集最大类：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="63" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218213801986.png" width="474"></p> 
  <p style="text-indent:50px;">遍历属性A的所有取值v，如果在某取值上集合（数据量）为空，则这个点设置为D中最大类类型，否则递归创建基于数据集Dv和没有属性a*的决策树：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="199" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218214124407.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="517"></p> 
  <p>&nbsp;</p> 
  <p style="text-indent:50px;">ID3存在很多不足，对可取值较多的属性有偏好；长度，密度等无考虑等，改进版为C4.5</p> 
  <hr>
  <h1 id="3.C4.5%E7%AE%97%E6%B3%95"><strong>3.C4.5算法</strong></h1> 
  <p style="text-indent:50px;">基于信息增益率，定义Gain_ratio，在原信息增益基础上除了IV：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="81" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019021821550768.png" width="499"></p> 
  <p style="text-indent:50px;">属性a可取值的数目越多，IV(a)越大，因此对数目小的属性有偏好。最终C4.5基于一个启发式：先选出信息增益高于平均的，再从其中选择增益率最高的。</p> 
  <p style="text-indent:50px;">其余和上面的ID3流程一致，只是属性划分有如上细微区别。</p> 
  <hr>
  <h1 id="ID3%2FC4.5%E7%9A%84%E9%98%B2%E8%BF%87%E6%8B%9F%E5%90%88"><strong>ID3/C4.5的防过拟合</strong></h1> 
  <p>预剪枝：生成分支的过程中，先评估当且泛化性能（测试交叉验证等），然后评估划分后的性能，若，划分后<span style="color:#f33b45;">不能提升泛化性</span>能，则不划分，还是保留这个叶子节点。</p> 
  <p>后剪枝：先完全训练完，得到决策树，然后从最底层节点往上，评估把这个节点（父节点，有子节点的）设置为叶子节点和不设置两种情况，若，设置为叶子节点能提升性能，则把这个节点设置为叶子节点，否则保留。</p> 
  <p>设定最大深度：划分决策树的时候统计深度，超过这个深度了就停止划分，输出为叶子节点。</p> 
  <p>设定节点最小数据量：节点内划入的数据集小于某个量了，再划分可能没意义了，就不划分了。</p> 
  <p>随机性：基于信息增益，增益率等评价指标，选择最佳属性的时候，最佳属性可以不一定就是最佳，比如可以是在高于平均的属性里面，随机选择一个作为最佳属性。</p> 
  <p>&nbsp;</p> 
  <h1 id="ID3%2FC4.5%E7%9A%84%E7%BC%BA%E5%A4%B1%E6%95%B0%E6%8D%AE"><strong>ID3/C4.5的缺失数据</strong></h1> 
  <p><strong>定义：</strong></p> 
  <p style="text-align:center;"><img alt="" class="has" height="386" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190219145012623.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="554"></p> 
  <p>新的计算：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="122" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190219145127926.png" width="565"></p> 
  <p>属性划分：在划分x样本时，a属性若取值已知，则x划入<span style="color:#0000ff;">取值对应的子结点</span>，权重wx不变；若<span style="color:#0000ff;">未知，x划入所有子结点，且按照wx=rv.wx划分权值。</span></p> 
  <p>&nbsp;</p> 
  <hr>
  <h1 id="4.CART%E7%AE%97%E6%B3%95"><strong>4.CART算法</strong></h1> 
  <p style="text-indent:50px;">CART:<span style="color:#f33b45;">分类和回归</span>树，是个二叉树。&nbsp;其希望划分下去的数据方差越来越接近。</p> 
  <p style="text-align:center;"><img alt="" class="has" height="500" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190225133928130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="717"></p> 
  <h2 id="1%EF%BC%89cart%E5%9C%A8%E5%88%86%E7%B1%BB%E6%96%B9%E9%9D%A2%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0%EF%BC%9AGini(D)"><span style="color:#f33b45;"><strong>1）cart在分类方面</strong>：基于基尼系数：Gini(D)</span></h2> 
  <p style="text-align:center;"><img alt="" class="has" height="69" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019021822015088.png" width="276"></p> 
  <p style="text-indent:50px;">Gini(D)反应的是数据集D中样本不一致的概率，因此Gini越小，越一致性，纯度越高。</p> 
  <p style="text-indent:50px;">属性划分，基尼指数：Gini_index(D,a)</p> 
  <p style="text-align:center;"><img alt="" class="has" height="70" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218220454165.png" width="322"></p> 
  <p style="text-indent:50px;">&nbsp;累加D中基于属性a(a1,a2,...,av)划分的不同数据集（a1:D1,a2:D2,...,av:DV）的Gini系数和，反映的是对属性a划分的整体Gini系数评价。</p> 
  <p style="text-indent:50px;"><span style="color:#f33b45;">对于连续特征：</span>如属性a身高，取值为:150,160,170,180，可以分别计算基于155,165,175为<strong><span style="color:#f33b45;">划分点的</span></strong>二分，分成<span style="color:#f33b45;">小于等于</span>划分点，<span style="color:#f33b45;">大于</span>划分点的两个数据集<span style="color:#f33b45;">D1、D2</span>。如基于165，把所有基于属性a&lt;=165的划入左节点，a&gt;165的划入右节点。然后计算<span style="color:#f33b45;">（基尼增益）GiniGain=Gini_index(D1,a)+Gini_index(D2,a)</span>，最后选择最小的划分点，如175，把数据划分为两个部分。</p> 
  <p style="text-indent:50px;"><span style="color:#f33b45;">对于离散特征</span>：CART分类时，选择最小Gini_index的属性划分，a*=argmin Gini_index(D,a)，（遍历所有属性A，遍历属性A中可能的取值a，得到最小Gini_index的a*，以及其所属的属性A*），然后<span style="color:#f33b45;">把数据分成属性A*中取值为a*的部分D1，和属性A*中除取值a*外的所有数据部分D2</span>，划入左右节点。</p> 
  <p style="text-indent:50px;">&nbsp;</p> 
  <h2 id="2%EF%BC%89cart%E5%9C%A8%E5%9B%9E%E5%BD%92%E6%96%B9%E9%9D%A2%EF%BC%9A%E5%9F%BA%E4%BA%8E%E6%96%B9%E5%B7%AE%2F%E6%A0%87%E5%87%86%E5%B7%AE"><strong><span style="color:#f33b45;">2）cart在回归方面：基于方差/标准差</span></strong></h2> 
  <p style="text-indent:50px;">对于任意划分特征a，对应的任意<strong><span style="color:#f33b45;">划分点</span></strong>s<span style="color:#f33b45;">两边划分成的数据集D1和D2</span>，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点(c1为D1数据集的样本均值，c2为D2数据集的样本均值):</p> 
  <p style="text-indent:50px;"><img alt="\underbrace{min}_{A,s}\Bigg[\underbrace{min}_{c_1}\sum\limits_{x_i \in D_1(A,s)}(y_i - c_1)^2 + \underbrace{min}_{c_2}\sum\limits_{x_i \in D_2(A,s)}(y_i - c_2)^2\Bigg]" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cunderbrace%7Bmin%7D_%7BA%2Cs%7D%5CBigg%5B%5Cunderbrace%7Bmin%7D_%7Bc_1%7D%5Csum%5Climits_%7Bx_i%20%5Cin%20D_1%28A%2Cs%29%7D%28y_i%20-%20c_1%29%5E2%20&amp;plus;%20%5Cunderbrace%7Bmin%7D_%7Bc_2%7D%5Csum%5Climits_%7Bx_i%20%5Cin%20D_2%28A%2Cs%29%7D%28y_i%20-%20c_2%29%5E2%5CBigg%5D"></p> 
  <p style="text-indent:50px;">即：不断基于连续属性数据的中点，做二分划分为左右数据集D1,D2,然后计算均值c1,c2,计算方差，计算方差和，最后取方差最小的划分点作为当且决策树分割点。</p> 
  <p style="text-indent:50px;">而<span style="color:#f33b45;">叶子节点的输出</span>：通常采用当前归属到此叶子节点中数据集的均值或者中位数。</p> 
  <p style="text-indent:50px;">阀值：可以定义数据量，比如5，当叶子节点的数据集小于这个量的时候停止划分，之间返回数据集的均值或中位数。</p> 
  <hr>
  <h1 id="CART%E7%9A%84%E5%89%AA%E6%9E%9D"><strong>CART的剪枝</strong></h1> 
  <p style="text-indent:50px;">白话版：从最小面的子树开始，一直往上到根节点下面的左右子树，依次把每个以 t&nbsp;为根节点的子树(两个，如果存在的话//有些不一定存在左右子树，比如只存在左子树就只考虑左子树Ttl)Ttl,Ttr，分别计算正常不剪枝时（整个决策树）的训练损失C(Tt)，和剪枝记为叶子节点时（叶子节点是值分类为最大类值，回归为平均值）（整个决策树）的训练损失C(T)；</p> 
  <p style="text-indent:50px;">然后按照 2）步计算得到这个 t 节点子树节点(Ttl,Ttr)的阀值(<span style="color:#f33b45;">阀值为min更新，确保越往上阀值越小</span>)；</p> 
  <p style="text-indent:50px;">这样一层层往上计算，得到每个节点的a值了，从2）步更新公式来看，<span style="color:#f33b45;">上层的阀值&lt;=下层节点的阀值</span>；</p> 
  <p style="text-indent:50px;">因此4）步选择最大的ak开始，自上而下(<span style="color:#f33b45;">从a值上层的阀值&lt;=下层节点的阀值来看，这样首先会剪掉最小面的一些树，然后慢慢往上剪</span>)把小于阀值的减掉得到一决策树T1，以此类推<span style="color:#f33b45;">得到集合M个数那么多个剪枝决策树T1,T2,...,T|M|</span>；</p> 
  <p style="text-indent:50px;">然后交叉验证（k折、留一等等），选择误差最小的，作为输出。</p> 
  <p style="text-indent:50px;">总结来说就是一句话：产生很多按照不同强度（阀值a）剪出来的剪枝决策树，然后测试效果，选出一个最好的输出。</p> 
  <p style="text-align:center;"><img alt="" class="has" height="481" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190219111839124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="857"></p> 
  <p>&nbsp;</p> 
  <hr>
  <h1 id="sklearn%E5%AE%9E%E7%8E%B0%EF%BC%9A">sklearn实现：</h1> 
  <h2 id="%E5%88%86%E7%B1%BBDecisionTreeClassifier">分类DecisionTreeClassifier</h2> 
  <p>具体参数参考官方文档：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="nofollow">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a></p> 
  <pre class="has">
<code>import numpy as np
from sklearn import tree
from graphviz import Source
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

np.random.seed(11)

iris = load_iris()
X = iris.data
Y = iris.target
# 导入数据：用于分类
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=2, test_size=0.2)

# 决策树分类器,criterion:标准，max_depth：最大深度
clf = tree.DecisionTreeClassifier(criterion='gini', max_depth=3)

clf.fit(X_train, Y_train)
pdy = clf.predict(X_test)

score = clf.score(X_test, Y_test)
print(score)

dot_data = tree.export_graphviz(clf, out_file=None, feature_names=iris.feature_names,
                                class_names=iris.target_names,
                                filled=True, rounded=True,
                                special_characters=True)
graph = Source(dot_data)
graph.format = 'png'
graph.render('cart_tree', view=True)
</code></pre> 
  <p><img alt="" class="has" height="326" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019021914101879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="427"></p> 
  <h2 id="%E5%9B%9E%E5%BD%92DecisionTreeRegressor">回归DecisionTreeRegressor</h2> 
  <p>具体参数参考官方文档：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" rel="nofollow">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor</a></p> 
  <pre class="has">
<code>import numpy as np
from sklearn import tree
from graphviz import Source
from sklearn.datasets import load_boston

boston = load_boston()
# 太大不好显示，取前20画CART树
X = boston.data[:20]
y = boston.target[:20]

# 用于测试，随机取5个
index = np.random.randint(20, boston.data.shape[0],5 )
test_X = boston.data[index]
test_y = boston.target[index]

# 导入数据：用于分类

# 决策树回归器，mse：均方误差;max_depth：最大深度5
clf_reg = tree.DecisionTreeRegressor(criterion='mse', random_state=11, max_depth=5)
clf_reg.fit(X, y)


print('测试数据原结果：', test_y)
print('测试数据预测结果：', clf_reg.predict(test_X))


dot_data = tree.export_graphviz(clf_reg, out_file=None, feature_names=boston.feature_names,
                                filled=True, rounded=True,
                                special_characters=True)
graph = Source(dot_data)
graph.format = 'png'
graph.render('cart_tree_Regressor', view=True)

# 大致还是很相近的，完全一样不太可能
测试数据原结果： [23.  19.6 30.5 24.4 22.9]
测试数据预测结果： [20.3        17.96666667 22.55       27.1        22.55      ]
</code></pre> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" height="478" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190219142200604.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="557"></p> 
  <p>&nbsp;</p> 
  <h1 id="5.GDBT">&nbsp;</h1> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
