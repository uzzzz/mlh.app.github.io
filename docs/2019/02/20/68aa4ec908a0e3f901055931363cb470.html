<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>深度学总结：Image Style Transfer pytorch方式实现，这个是非基于autoencoder和domain adversrial方式 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="深度学总结：Image Style Transfer pytorch方式实现，这个是非基于autoencoder和domain adversrial方式" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="文章目录 论文链接： 主要思路： pytorch实现： 计算content的Loss: 计算style 的Loss: 计算total的Loss: 训练过程： 论文链接： https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf 作者运用了一个披着深度学习外表的传统方法做这个问题，技术不提倡，思路、想法很天才。 主要思路： 1、想办法分离style features和content features; 2、怎么得到content features，作者偷懒直接取vgg19里面conv4_2的features，当作content features； 3、怎么得到style features，作者发挥天才的偷懒，认为style就是features之间的相关度，作者直接取vgg19里面conv1_1，conv2_1，conv3_1，conv4_1，conv5_1这5层的feature map来算风格相关度，作者又发现不同的layer对风格有影响，前面的风格细腻，后面的风格粗犷，又给这5个层的loss误差加了权重。 4、那么目标是什么了？就是new_image和content_image的内容接近，new_image和stytle_image的风格接近， L t o t a l L_{total} Ltotal​ = L c o n t e n t L_{content} Lcontent​+ L s t y l e L_{style} Lstyle​，首先要像，其次才是风格，所以 L c o n t e n t L_{content} Lcontent​的比重要大。 pytorch实现： 计算content的Loss: 前面提到了作者偷懒直接取vgg19里面conv4_2的features，当作content features，那么只需要比较两个feature map的差别就行了。 # the content loss content_loss = torch.mean((target_features[&#39;conv4_2&#39;] - content_features[&#39;conv4_2&#39;])**2) 计算style 的Loss: def gram_matrix(tensor): &quot;&quot;&quot; Calculate the Gram Matrix of a given tensor Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix &quot;&quot;&quot; # get the batch_size, depth, height, and width of the Tensor _, d, h, w = tensor.size() # reshape so we&#39;re multiplying the features for each channel tensor = tensor.view(d, h * w) # calculate the gram matrix gram = torch.mm(tensor, tensor.t()) return gram # weights for each style layer # weighting earlier layers more will result in *larger* style artifacts # notice we are excluding `conv4_2` our content representation style_weights = {&#39;conv1_1&#39;: 1., &#39;conv2_1&#39;: 0.75, &#39;conv3_1&#39;: 0.2, &#39;conv4_1&#39;: 0.2, &#39;conv5_1&#39;: 0.2} # get style features only once before training style_features = get_features(style, vgg) # calculate the gram matrices for each layer of our style representation style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features} for layer in style_weights: # get the &quot;target&quot; style representation for the layer target_feature = target_features[layer] target_gram = gram_matrix(target_feature) _, d, h, w = target_feature.shape # get the &quot;style&quot; style representation style_gram = style_grams[layer] # the style loss for one layer, weighted appropriately layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2) # add to the style loss style_loss += layer_style_loss / (d * h * w) 计算total的Loss: content_weight = 1 # alpha style_weight = 1e6 # beta # calculate the *total* loss total_loss = content_weight * content_loss + style_weight * style_loss 训练过程： 有人可能问VGG参数固定住了，需要的参数是什么了，参数就是新图啊？通过上述loss调整图像的像素值，这个和我们一般了解到的有点不一样。 新图就是在原图的基础上慢慢变化,f复制原图并设置新图为trainable的： # create a third &quot;target&quot; image and prep it for change # it is a good idea to start of with the target as a copy of our *content* image # then iteratively change its style target = content.clone().requires_grad_(True).to(device) 训练大概代码如下： # for displaying the target image, intermittently show_every = 400 # iteration hyperparameters optimizer = optim.Adam([target], lr=0.003) steps = 2000 # decide how many iterations to update your image (5000) for ii in range(1, steps+1): # get the features from your target image target_features = get_features(target, vgg) # the content loss content_loss = torch.mean((target_features[&#39;conv4_2&#39;] - content_features[&#39;conv4_2&#39;])**2) # the style loss # initialize the style loss to 0 style_loss = 0 # then add to it for each layer&#39;s gram matrix loss for layer in style_weights: # get the &quot;target&quot; style representation for the layer target_feature = target_features[layer] target_gram = gram_matrix(target_feature) _, d, h, w = target_feature.shape # get the &quot;style&quot; style representation style_gram = style_grams[layer] # the style loss for one layer, weighted appropriately layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2) # add to the style loss style_loss += layer_style_loss / (d * h * w) # calculate the *total* loss total_loss = content_weight * content_loss + style_weight * style_loss # update your target image optimizer.zero_grad() total_loss.backward() optimizer.step()" />
<meta property="og:description" content="文章目录 论文链接： 主要思路： pytorch实现： 计算content的Loss: 计算style 的Loss: 计算total的Loss: 训练过程： 论文链接： https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf 作者运用了一个披着深度学习外表的传统方法做这个问题，技术不提倡，思路、想法很天才。 主要思路： 1、想办法分离style features和content features; 2、怎么得到content features，作者偷懒直接取vgg19里面conv4_2的features，当作content features； 3、怎么得到style features，作者发挥天才的偷懒，认为style就是features之间的相关度，作者直接取vgg19里面conv1_1，conv2_1，conv3_1，conv4_1，conv5_1这5层的feature map来算风格相关度，作者又发现不同的layer对风格有影响，前面的风格细腻，后面的风格粗犷，又给这5个层的loss误差加了权重。 4、那么目标是什么了？就是new_image和content_image的内容接近，new_image和stytle_image的风格接近， L t o t a l L_{total} Ltotal​ = L c o n t e n t L_{content} Lcontent​+ L s t y l e L_{style} Lstyle​，首先要像，其次才是风格，所以 L c o n t e n t L_{content} Lcontent​的比重要大。 pytorch实现： 计算content的Loss: 前面提到了作者偷懒直接取vgg19里面conv4_2的features，当作content features，那么只需要比较两个feature map的差别就行了。 # the content loss content_loss = torch.mean((target_features[&#39;conv4_2&#39;] - content_features[&#39;conv4_2&#39;])**2) 计算style 的Loss: def gram_matrix(tensor): &quot;&quot;&quot; Calculate the Gram Matrix of a given tensor Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix &quot;&quot;&quot; # get the batch_size, depth, height, and width of the Tensor _, d, h, w = tensor.size() # reshape so we&#39;re multiplying the features for each channel tensor = tensor.view(d, h * w) # calculate the gram matrix gram = torch.mm(tensor, tensor.t()) return gram # weights for each style layer # weighting earlier layers more will result in *larger* style artifacts # notice we are excluding `conv4_2` our content representation style_weights = {&#39;conv1_1&#39;: 1., &#39;conv2_1&#39;: 0.75, &#39;conv3_1&#39;: 0.2, &#39;conv4_1&#39;: 0.2, &#39;conv5_1&#39;: 0.2} # get style features only once before training style_features = get_features(style, vgg) # calculate the gram matrices for each layer of our style representation style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features} for layer in style_weights: # get the &quot;target&quot; style representation for the layer target_feature = target_features[layer] target_gram = gram_matrix(target_feature) _, d, h, w = target_feature.shape # get the &quot;style&quot; style representation style_gram = style_grams[layer] # the style loss for one layer, weighted appropriately layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2) # add to the style loss style_loss += layer_style_loss / (d * h * w) 计算total的Loss: content_weight = 1 # alpha style_weight = 1e6 # beta # calculate the *total* loss total_loss = content_weight * content_loss + style_weight * style_loss 训练过程： 有人可能问VGG参数固定住了，需要的参数是什么了，参数就是新图啊？通过上述loss调整图像的像素值，这个和我们一般了解到的有点不一样。 新图就是在原图的基础上慢慢变化,f复制原图并设置新图为trainable的： # create a third &quot;target&quot; image and prep it for change # it is a good idea to start of with the target as a copy of our *content* image # then iteratively change its style target = content.clone().requires_grad_(True).to(device) 训练大概代码如下： # for displaying the target image, intermittently show_every = 400 # iteration hyperparameters optimizer = optim.Adam([target], lr=0.003) steps = 2000 # decide how many iterations to update your image (5000) for ii in range(1, steps+1): # get the features from your target image target_features = get_features(target, vgg) # the content loss content_loss = torch.mean((target_features[&#39;conv4_2&#39;] - content_features[&#39;conv4_2&#39;])**2) # the style loss # initialize the style loss to 0 style_loss = 0 # then add to it for each layer&#39;s gram matrix loss for layer in style_weights: # get the &quot;target&quot; style representation for the layer target_feature = target_features[layer] target_gram = gram_matrix(target_feature) _, d, h, w = target_feature.shape # get the &quot;style&quot; style representation style_gram = style_grams[layer] # the style loss for one layer, weighted appropriately layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2) # add to the style loss style_loss += layer_style_loss / (d * h * w) # calculate the *total* loss total_loss = content_weight * content_loss + style_weight * style_loss # update your target image optimizer.zero_grad() total_loss.backward() optimizer.step()" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-20T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"文章目录 论文链接： 主要思路： pytorch实现： 计算content的Loss: 计算style 的Loss: 计算total的Loss: 训练过程： 论文链接： https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf 作者运用了一个披着深度学习外表的传统方法做这个问题，技术不提倡，思路、想法很天才。 主要思路： 1、想办法分离style features和content features; 2、怎么得到content features，作者偷懒直接取vgg19里面conv4_2的features，当作content features； 3、怎么得到style features，作者发挥天才的偷懒，认为style就是features之间的相关度，作者直接取vgg19里面conv1_1，conv2_1，conv3_1，conv4_1，conv5_1这5层的feature map来算风格相关度，作者又发现不同的layer对风格有影响，前面的风格细腻，后面的风格粗犷，又给这5个层的loss误差加了权重。 4、那么目标是什么了？就是new_image和content_image的内容接近，new_image和stytle_image的风格接近， L t o t a l L_{total} Ltotal​ = L c o n t e n t L_{content} Lcontent​+ L s t y l e L_{style} Lstyle​，首先要像，其次才是风格，所以 L c o n t e n t L_{content} Lcontent​的比重要大。 pytorch实现： 计算content的Loss: 前面提到了作者偷懒直接取vgg19里面conv4_2的features，当作content features，那么只需要比较两个feature map的差别就行了。 # the content loss content_loss = torch.mean((target_features[&#39;conv4_2&#39;] - content_features[&#39;conv4_2&#39;])**2) 计算style 的Loss: def gram_matrix(tensor): &quot;&quot;&quot; Calculate the Gram Matrix of a given tensor Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix &quot;&quot;&quot; # get the batch_size, depth, height, and width of the Tensor _, d, h, w = tensor.size() # reshape so we&#39;re multiplying the features for each channel tensor = tensor.view(d, h * w) # calculate the gram matrix gram = torch.mm(tensor, tensor.t()) return gram # weights for each style layer # weighting earlier layers more will result in *larger* style artifacts # notice we are excluding `conv4_2` our content representation style_weights = {&#39;conv1_1&#39;: 1., &#39;conv2_1&#39;: 0.75, &#39;conv3_1&#39;: 0.2, &#39;conv4_1&#39;: 0.2, &#39;conv5_1&#39;: 0.2} # get style features only once before training style_features = get_features(style, vgg) # calculate the gram matrices for each layer of our style representation style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features} for layer in style_weights: # get the &quot;target&quot; style representation for the layer target_feature = target_features[layer] target_gram = gram_matrix(target_feature) _, d, h, w = target_feature.shape # get the &quot;style&quot; style representation style_gram = style_grams[layer] # the style loss for one layer, weighted appropriately layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2) # add to the style loss style_loss += layer_style_loss / (d * h * w) 计算total的Loss: content_weight = 1 # alpha style_weight = 1e6 # beta # calculate the *total* loss total_loss = content_weight * content_loss + style_weight * style_loss 训练过程： 有人可能问VGG参数固定住了，需要的参数是什么了，参数就是新图啊？通过上述loss调整图像的像素值，这个和我们一般了解到的有点不一样。 新图就是在原图的基础上慢慢变化,f复制原图并设置新图为trainable的： # create a third &quot;target&quot; image and prep it for change # it is a good idea to start of with the target as a copy of our *content* image # then iteratively change its style target = content.clone().requires_grad_(True).to(device) 训练大概代码如下： # for displaying the target image, intermittently show_every = 400 # iteration hyperparameters optimizer = optim.Adam([target], lr=0.003) steps = 2000 # decide how many iterations to update your image (5000) for ii in range(1, steps+1): # get the features from your target image target_features = get_features(target, vgg) # the content loss content_loss = torch.mean((target_features[&#39;conv4_2&#39;] - content_features[&#39;conv4_2&#39;])**2) # the style loss # initialize the style loss to 0 style_loss = 0 # then add to it for each layer&#39;s gram matrix loss for layer in style_weights: # get the &quot;target&quot; style representation for the layer target_feature = target_features[layer] target_gram = gram_matrix(target_feature) _, d, h, w = target_feature.shape # get the &quot;style&quot; style representation style_gram = style_grams[layer] # the style loss for one layer, weighted appropriately layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2) # add to the style loss style_loss += layer_style_loss / (d * h * w) # calculate the *total* loss total_loss = content_weight * content_loss + style_weight * style_loss # update your target image optimizer.zero_grad() total_loss.backward() optimizer.step()","@type":"BlogPosting","url":"/2019/02/20/68aa4ec908a0e3f901055931363cb470.html","headline":"深度学总结：Image Style Transfer pytorch方式实现，这个是非基于autoencoder和domain adversrial方式","dateModified":"2019-02-20T00:00:00+08:00","datePublished":"2019-02-20T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/02/20/68aa4ec908a0e3f901055931363cb470.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>深度学总结：Image Style Transfer pytorch方式实现，这个是非基于autoencoder和domain adversrial方式</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <li><a href="#_1" rel="nofollow">论文链接：</a></li>
    <li><a href="#_4" rel="nofollow">主要思路：</a></li>
    <li><a href="#pytorch_10" rel="nofollow">pytorch实现：</a></li>
    <ul>
     <li><a href="#contentLoss_11" rel="nofollow">计算content的Loss:</a></li>
     <li><a href="#style_Loss_19" rel="nofollow">计算style 的Loss:</a></li>
     <li><a href="#totalLoss_71" rel="nofollow">计算total的Loss:</a></li>
     <li><a href="#_83" rel="nofollow">训练过程：</a></li>
    </ul>
   </ul>
  </div>
  <p></p> 
  <h1><a id="_1"></a>论文链接：</h1> 
  <p><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="nofollow">https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf</a><br> 作者运用了一个披着深度学习外表的传统方法做这个问题，技术不提倡，思路、想法很天才。</p> 
  <h1><a id="_4"></a>主要思路：</h1> 
  <p>1、想办法分离style features和content features;<br> 2、怎么得到content features，作者偷懒直接取vgg19里面conv4_2的features，当作content features；<br> 3、怎么得到style features，作者发挥天才的偷懒，认为style就是features之间的相关度，作者直接取vgg19里面conv1_1，conv2_1，conv3_1，conv4_1，conv5_1这5层的feature map来算风格相关度，作者又发现不同的layer对风格有影响，前面的风格细腻，后面的风格粗犷，又给这5个层的loss误差加了权重。<br> 4、那么目标是什么了？就是new_image和content_image的内容接近，new_image和stytle_image的风格接近，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <msub>
          <mi>
           L
          </mi>
          <mrow>
           <mi>
            t
           </mi>
           <mi>
            o
           </mi>
           <mi>
            t
           </mi>
           <mi>
            a
           </mi>
           <mi>
            l
           </mi>
          </mrow>
         </msub>
        </mrow>
        <annotation encoding="application/x-tex">
         L_{total}
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mord mathit mtight">o</span><span class="mord mathit mtight">t</span><span class="mord mathit mtight">a</span><span class="mord mathit mtight" style="margin-right: 0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> = <span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <msub>
          <mi>
           L
          </mi>
          <mrow>
           <mi>
            c
           </mi>
           <mi>
            o
           </mi>
           <mi>
            n
           </mi>
           <mi>
            t
           </mi>
           <mi>
            e
           </mi>
           <mi>
            n
           </mi>
           <mi>
            t
           </mi>
          </mrow>
         </msub>
        </mrow>
        <annotation encoding="application/x-tex">
         L_{content}
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">c</span><span class="mord mathit mtight">o</span><span class="mord mathit mtight">n</span><span class="mord mathit mtight">t</span><span class="mord mathit mtight">e</span><span class="mord mathit mtight">n</span><span class="mord mathit mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>+<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <msub>
          <mi>
           L
          </mi>
          <mrow>
           <mi>
            s
           </mi>
           <mi>
            t
           </mi>
           <mi>
            y
           </mi>
           <mi>
            l
           </mi>
           <mi>
            e
           </mi>
          </mrow>
         </msub>
        </mrow>
        <annotation encoding="application/x-tex">
         L_{style}
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span class="mord mathit">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">s</span><span class="mord mathit mtight">t</span><span class="mord mathit mtight" style="margin-right: 0.03588em;">y</span><span class="mord mathit mtight" style="margin-right: 0.01968em;">l</span><span class="mord mathit mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span>，首先要像，其次才是风格，所以<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <msub>
          <mi>
           L
          </mi>
          <mrow>
           <mi>
            c
           </mi>
           <mi>
            o
           </mi>
           <mi>
            n
           </mi>
           <mi>
            t
           </mi>
           <mi>
            e
           </mi>
           <mi>
            n
           </mi>
           <mi>
            t
           </mi>
          </mrow>
         </msub>
        </mrow>
        <annotation encoding="application/x-tex">
         L_{content}
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.83333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathit">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.280556em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">c</span><span class="mord mathit mtight">o</span><span class="mord mathit mtight">n</span><span class="mord mathit mtight">t</span><span class="mord mathit mtight">e</span><span class="mord mathit mtight">n</span><span class="mord mathit mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>的比重要大。</p> 
  <h1><a id="pytorch_10"></a>pytorch实现：</h1> 
  <h2><a id="contentLoss_11"></a>计算content的Loss:</h2> 
  <p>前面提到了作者偷懒直接取vgg19里面conv4_2的features，当作content features，那么只需要比较两个feature map的差别就行了。<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190220160543599.png" alt="在这里插入图片描述"></p> 
  <pre><code class="prism language-python"><span class="token comment"># the content loss</span>
    content_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>target_features<span class="token punctuation">[</span><span class="token string">'conv4_2'</span><span class="token punctuation">]</span> <span class="token operator">-</span> content_features<span class="token punctuation">[</span><span class="token string">'conv4_2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span>
</code></pre> 
  <h2><a id="style_Loss_19"></a>计算style 的Loss:</h2> 
  <p><img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190220161017156.png" alt="在这里插入图片描述"></p> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">gram_matrix</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Calculate the Gram Matrix of a given tensor Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix """</span>
    
    <span class="token comment"># get the batch_size, depth, height, and width of the Tensor</span>
    _<span class="token punctuation">,</span> d<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># reshape so we're multiplying the features for each channel</span>
    tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>view<span class="token punctuation">(</span>d<span class="token punctuation">,</span> h <span class="token operator">*</span> w<span class="token punctuation">)</span>
    
    <span class="token comment"># calculate the gram matrix</span>
    gram <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> gram

</code></pre> 
  <p><img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190220161241597.png" alt="在这里插入图片描述"><br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190220161259144.png" alt="在这里插入图片描述"></p> 
  <pre><code class="prism language-python"><span class="token comment"># weights for each style layer </span>
<span class="token comment"># weighting earlier layers more will result in *larger* style artifacts</span>
<span class="token comment"># notice we are excluding `conv4_2` our content representation</span>
style_weights <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'conv1_1'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>
                 <span class="token string">'conv2_1'</span><span class="token punctuation">:</span> <span class="token number">0.75</span><span class="token punctuation">,</span>
                 <span class="token string">'conv3_1'</span><span class="token punctuation">:</span> <span class="token number">0.2</span><span class="token punctuation">,</span>
                 <span class="token string">'conv4_1'</span><span class="token punctuation">:</span> <span class="token number">0.2</span><span class="token punctuation">,</span>
                 <span class="token string">'conv5_1'</span><span class="token punctuation">:</span> <span class="token number">0.2</span><span class="token punctuation">}</span>
                 
<span class="token comment"># get style features only once before training</span>
style_features <span class="token operator">=</span> get_features<span class="token punctuation">(</span>style<span class="token punctuation">,</span> vgg<span class="token punctuation">)</span>

<span class="token comment"># calculate the gram matrices for each layer of our style representation</span>
style_grams <span class="token operator">=</span> <span class="token punctuation">{</span>layer<span class="token punctuation">:</span> gram_matrix<span class="token punctuation">(</span>style_features<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> layer <span class="token keyword">in</span> style_features<span class="token punctuation">}</span>

    <span class="token keyword">for</span> layer <span class="token keyword">in</span> style_weights<span class="token punctuation">:</span>
        <span class="token comment"># get the "target" style representation for the layer</span>
        target_feature <span class="token operator">=</span> target_features<span class="token punctuation">[</span>layer<span class="token punctuation">]</span>
        target_gram <span class="token operator">=</span> gram_matrix<span class="token punctuation">(</span>target_feature<span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> d<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> target_feature<span class="token punctuation">.</span>shape
        <span class="token comment"># get the "style" style representation</span>
        style_gram <span class="token operator">=</span> style_grams<span class="token punctuation">[</span>layer<span class="token punctuation">]</span>
        <span class="token comment"># the style loss for one layer, weighted appropriately</span>
        layer_style_loss <span class="token operator">=</span> style_weights<span class="token punctuation">[</span>layer<span class="token punctuation">]</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>target_gram <span class="token operator">-</span> style_gram<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># add to the style loss</span>
        style_loss <span class="token operator">+=</span> layer_style_loss <span class="token operator">/</span> <span class="token punctuation">(</span>d <span class="token operator">*</span> h <span class="token operator">*</span> w<span class="token punctuation">)</span>

</code></pre> 
  <h2><a id="totalLoss_71"></a>计算total的Loss:</h2> 
  <p><img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190220161929248.png" alt="在这里插入图片描述"></p> 
  <pre><code class="prism language-python">
content_weight <span class="token operator">=</span> <span class="token number">1</span>  <span class="token comment"># alpha</span>
style_weight <span class="token operator">=</span> <span class="token number">1e6</span>  <span class="token comment"># beta</span>

<span class="token comment"># calculate the *total* loss</span>
total_loss <span class="token operator">=</span> content_weight <span class="token operator">*</span> content_loss <span class="token operator">+</span> style_weight <span class="token operator">*</span> style_loss
    
</code></pre> 
  <h2><a id="_83"></a>训练过程：</h2> 
  <p>有人可能问VGG参数固定住了，需要的参数是什么了，参数就是新图啊？通过上述loss调整图像的像素值，这个和我们一般了解到的有点不一样。<br> 新图就是在原图的基础上慢慢变化,f复制原图并设置新图为trainable的：</p> 
  <pre><code class="prism language-python"><span class="token comment"># create a third "target" image and prep it for change</span>
<span class="token comment"># it is a good idea to start of with the target as a copy of our *content* image</span>
<span class="token comment"># then iteratively change its style</span>
target <span class="token operator">=</span> content<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
</code></pre> 
  <p>训练大概代码如下：</p> 
  <pre><code class="prism language-python"><span class="token comment"># for displaying the target image, intermittently</span>
show_every <span class="token operator">=</span> <span class="token number">400</span>

<span class="token comment"># iteration hyperparameters</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token punctuation">[</span>target<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.003</span><span class="token punctuation">)</span>
steps <span class="token operator">=</span> <span class="token number">2000</span>  <span class="token comment"># decide how many iterations to update your image (5000)</span>

<span class="token keyword">for</span> ii <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> steps<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token comment"># get the features from your target image</span>
    target_features <span class="token operator">=</span> get_features<span class="token punctuation">(</span>target<span class="token punctuation">,</span> vgg<span class="token punctuation">)</span>
    
    <span class="token comment"># the content loss</span>
    content_loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>target_features<span class="token punctuation">[</span><span class="token string">'conv4_2'</span><span class="token punctuation">]</span> <span class="token operator">-</span> content_features<span class="token punctuation">[</span><span class="token string">'conv4_2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span>
    
    <span class="token comment"># the style loss</span>
    <span class="token comment"># initialize the style loss to 0</span>
    style_loss <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token comment"># then add to it for each layer's gram matrix loss</span>
    <span class="token keyword">for</span> layer <span class="token keyword">in</span> style_weights<span class="token punctuation">:</span>
        <span class="token comment"># get the "target" style representation for the layer</span>
        target_feature <span class="token operator">=</span> target_features<span class="token punctuation">[</span>layer<span class="token punctuation">]</span>
        target_gram <span class="token operator">=</span> gram_matrix<span class="token punctuation">(</span>target_feature<span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> d<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> target_feature<span class="token punctuation">.</span>shape
        <span class="token comment"># get the "style" style representation</span>
        style_gram <span class="token operator">=</span> style_grams<span class="token punctuation">[</span>layer<span class="token punctuation">]</span>
        <span class="token comment"># the style loss for one layer, weighted appropriately</span>
        layer_style_loss <span class="token operator">=</span> style_weights<span class="token punctuation">[</span>layer<span class="token punctuation">]</span> <span class="token operator">*</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">(</span>target_gram <span class="token operator">-</span> style_gram<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token comment"># add to the style loss</span>
        style_loss <span class="token operator">+=</span> layer_style_loss <span class="token operator">/</span> <span class="token punctuation">(</span>d <span class="token operator">*</span> h <span class="token operator">*</span> w<span class="token punctuation">)</span>
        
    <span class="token comment"># calculate the *total* loss</span>
    total_loss <span class="token operator">=</span> content_weight <span class="token operator">*</span> content_loss <span class="token operator">+</span> style_weight <span class="token operator">*</span> style_loss
    
    <span class="token comment"># update your target image</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    total_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
