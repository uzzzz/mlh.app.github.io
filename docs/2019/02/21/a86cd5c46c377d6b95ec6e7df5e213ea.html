<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>史上最强NLP知识集合：知识结构、发展历程、导师名单 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="史上最强NLP知识集合：知识结构、发展历程、导师名单" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="转载自 数据简化DataSimp&nbsp; 作者 秦陇纪&nbsp; 郭一璞 编辑&nbsp; 量子位 报道 | 公众号 QbitAI 本篇推送包含三篇文章， 《自然语言处理技术发展史十大里程碑》 《语言处理NLP知识结构》 《自然语言处理NLP国内研究方向机构导师》 总共超过20000字，量子位建议先码再看。 自然语言处理技术发展史十大里程碑 文|秦陇纪，参考|黄昌宁、张小凤、Sebatian Ruder 自然语言是人类独有的智慧结晶。 自然语言处理(NaturalLanguage Processing，NLP)是计算机科学领域与人工智能领域中的一个重要方向，旨在研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。用自然语言与计算机进行通信，有着十分重要的实际应用意义，也有着革命性的理论意义。 由于理解自然语言，需要关于外在世界的广泛知识以及运用操作这些知识的能力，所以自然语言处理，也被视为解决人工智能完备(AI-complete)的核心问题之一。对自然语言处理的研究也是充满魅力和挑战的。 微软亚洲研究院黄昌宁、张小凤在2013年发表论文，就过去50年以来自然语言处理(NLP)研究领域中的发现和发展要点进行阐述，其中包括两个事实和三大重要成果。 近年来，自然语言处理的语料库调查显示如下两个事实： (1)对于句法分析来说，基于单一标记的短语结构规则是不充分的；单个标记的PSG规则不足以进行自然语言描述； (2)PSG规则在文本语料库中具有偏差分布，即PSG规则的总数似乎不能够涵盖大型语料库中发现的语言现象，这不符合语言学家的期望。短语结构规则在真实文本中的分布呈现严重扭曲。换言之，有限数目的短语结构规则不能覆盖大规模语料中的语法现象。这与原先人们的预期大相径庭。 NLP技术发展历程在很大程度上受到以上两个事实的影响，在该领域中可以称得上里程碑式的成果有如下三个： (1)复杂特征集和合一语法； (2)语言学研究中的词汇主义； (3)语料库方法和统计语言模型。业内人士普遍认为，大规模语言知识的开发和自动获取是NLP技术的瓶颈问题。因此，语料库建设和统计学习理论将成为该领域中的关键课题。 一、NLP研究传统问题 自然语言处理(NLP)是计算机科学、信息工程和人工智能的子领域，涉及计算机和人类(自然)语言之间的交互，尤其是编程实现计算机处理和分析大量自然语言数据。自然语言处理的挑战包括语音识别，自然语言理解和自然语言生成。 信息输入、检索、人机对话等需求增多，使自然语言处理(NLP)成为21世纪初的热门学科。从50年代机器翻译和人工智能研究算起，NLP至今有长达半个世纪的历史了。 近年来这一领域中里程碑式的理论和方法贡献有如下三个： (1)复杂特征集和合一语法； (2)语言学研究中的词汇主义； (3)语料库方法和统计语言模型。 这三个成果将继续对语言学、计算语言学和NLP的研究产生深远影响。为了理解这些成果的意义，先介绍一下两个相关事实。 自然语言处理中识别句子 句法结构的句法分析的全过程： (1)把句子中的词一个一个地切分出来； (2)查词典，给句子中的每个词指派一个合适的词性(part of speech)； (3)用句法规则把句子里包含的句法成分，如名词短语、动词短语、小句等，逐个地识别出来。 (4)判断每个短语的句法功能，如主语、谓语、宾语等，及其语义角色，最终得到句子的意义表示，如逻辑语义表达式。 1.1、事实一：语言的结构歧义问题 第一个事实(黄昌宁，张小凤，2013)是：短语结构语法(PhraseStructure Grammar，简称PSG)不能有效地描写自然语言。 PSG在Chomsky的语言学理论[1]中占有重要地位，并且在自然语言的句法描写中担当着举足轻重的角色。但是它有一些根本性的弱点，主要表现为它使用的是像词类和短语类那样的单一标记，因此不能有效地指明和解释自然语言中的结构歧义问题。 让我们先来看一看汉语中“V+N”组合。假如我们把“打击，委托，调查”等词指派为动词(V)；把“力度，方式，盗版，甲方”等词视为名词(N)，而且同意“打击力度”、“委托方式”是名词短语(NP)，“打击盗版”、“委托甲方”是动词短语(VP)，那么就会产生如下两条有歧义的句法规则： (1) NP → V N (2) VP → V N 换句话讲，当计算机观察到文本中相邻出现的“V+N”词类序列时，仍不能确定它们组成的究竟是NP还是VP。我们把这样的歧义叫做“短语类型歧义”。例如： • 该公司正在招聘[销售V人员N]NP。 • 地球在不断[改变V形状N]VP。 下面再来看“N+V”的组合，也同样会产生带有短语类型歧义的规则对，如： (3) NP → N V 例：市场调查；政治影响。 (4) S → N V 例：价格攀升；局势稳定。 其中标记S代表小句。 不仅如此，有时当机器观察到相邻出现的“N+V”词类序列时，甚至不能判断它们是不是在同一个短语中。也就是说，“N+V”词类序列可能组成名词短语NP或小句S，也有可能根本就不在同一个短语里。后面这种歧义称为“短语边界歧义”。下面是两个相关的例句： • 中国的[铁路N建设V]NP发展很快。 • [中国的铁路N]NP建设V得很快。 前一个例句中，“铁路建设”组成一个NP；而在后一个例句中，这两个相邻的词却分属于两个不同的短语。这足以说明，基于单一标记的PSG不能充分地描述自然语言中的句法歧义现象。下面让我们再来看一些这样的例子。 (5)NP → V N1de N2 (6)VP → V N1de N2 其中de代表结构助词“的”。例如，“[削苹果]VP的刀”是NP; 而“削[苹果的皮]NP”则是VP。这里既有短语类型歧义，又有短语边界歧义。比如，“削V苹果N”这两个相邻的词，可能构成一个VP，也可能分处于两个相邻的短语中。 (7)NP → P N1de N2 (8)PP → P N1de N2 规则中P和PP分别表示介词和介词短语。例如，“[对上海]PP的印象”是NP; 而“对[上海的学生]NP”则是PP。相邻词“对P 上海N”可能组成一个PP，也可能分处于两个短语中。 (9)NP → NumPN1 de N2 其中NumP 表示数量短语。规则(9)虽然表示的是一个NP，但可分别代表两种结构意义： (9a)NumP [N1de N2]NP 如：五个[公司的职员]NP (9b)[NumPN1]NP de N2 如：[五个公司]NP 的职员 (10)NP → N1 N2N3 规则(10)表示的也是一个NP，但“N1+ N2”先结合，还是“N2 +N3”先结合，会出现两种不同的结构方式和意义，即： (10a)[N1 N2]NPN3 如：[现代汉语]NP 词典 (10b)N1 [N2N3]NP 如：新版[汉语词典]NP 以上讨论的第一个事实说明： 由于约束力不够，单一标记的PSG规则不能充分消解短语类型和短语边界的歧义。用数学的语言来讲，PSG规则是必要的，却不是充分的。因此机器仅仅根据规则右边的一个词类序列来判断它是不是一个短语，或者是什么短语，其实都有某种不确定性。 采用复杂特征集和词汇主义方法来重建自然语言的语法系统，是近二十年来全球语言学界就此作出的最重要的努力。 1.2、事实二：词频统计的齐夫律 通过大规模语料的调查，人们发现一种语言的短语规则的分布也符合所谓的齐夫率(Zipf’s Law)。 Zipf是一个统计学家和语言学家。他提出，如果对某个语言单位(不论是英语的字母或词)进行统计，把这个语言单位在一个语料库里出现的频度(frequency)记作F，而且根据频度的降序对每个单元指派一个整数的阶次(rank) R。结果发现R和F的乘积近似为一个常数。即 F*R ≈ const (常数) 被观察的语言单元的阶次R与其频度F成反比关系。词频统计方面齐夫律显示，不管被考察的语料仅仅一本长篇小说，还是一个大规模的语料库，最常出现的100个词的出现次数会占到语料库总词次数(tokens)的近一半。 假如语料库的规模是100万词次，那么其中频度最高的100个词的累计出现次数大概是50万词次。如果整个语料库含有5万词型(types)，那么其中的一半(也就是2.5万条左右)在该语料库中只出现过一次。即使把语料库的规模加大十倍，变成1000万词次，统计规律大体不变。 有趣的是，80年代英国人Sampson对英语语料库中的PSG规则进行统计，发现它们的分布同样是扭曲的，大体表现为齐夫率。也就是说，一方面经常遇到的语法规则只有几十条左右，它们的出现频度非常非常高；另一方面，规则库中大约一半左右的规则在语料库中只出现过一次。 随着语料库规模的扩大，新的规则仍不断呈现。Noam Chomsky曾提出过这样的假设，认为对一种自然语言来说，其语法规则的数目总是有限的，但据此生成的句子数目却是无限的。但语料库调查的结果不是这个样子。这个发现至少说明，单纯依靠语言学家的语感来编写语法规则不可能胜任大规模真实文本处理的需求，必须寻找可以从语料库中直接获取大规模语言知识的新方法。 几十年来，NLP学界曾发表过许多灿烂成果，有词法学、语法学、语义学的，有句法分析算法的，还有众多著名的自然语言应用系统。那么究竟什么是对该领域影响最大的、里程碑式的成果呢？ 二、NLP十大里程碑 2.1、里程碑一：1985复杂特征集 复杂特征集(complex feature set)又叫做多重属性(multiple features)描写。语言学里，这种描写方法最早出现在语音学中。美国计算语言学家Martin Kay于1985年在“功能合一语法”(FunctionalUnification Grammar，简称FUG)新语法理论中，提出“复杂特征集”(complex feature set)概念。后来被Chomsky学派采用来扩展PSG的描写能力。 △&nbsp;美国计算语言学家Martin Kay 现在在语言学界、计算语言学界，语法系统在词汇层的描写中常采用复杂特征集，利用这些属性来强化句法规则的约束力。一个复杂特征集F包含任意多个特征名fi和特征值vi对。其形式如： F = {…, fi=vi, …}, i=1,…,n 特征值vi既可以是一个简单的数字或符号，也可以是另外一个复杂特征集。这种递归式的定义使复杂特征集获得了强大的表现能力。举例来说，北京大学俞士汶开发的《现代汉语语法信息词典》[10]，对一个动词定义了约40项属性描写，对一个名词定义了约27项属性描写。 一条含有词汇和短语属性约束的句法规则具有如下的一般形式： : &lt;属性约束&gt; : &lt;属性传递&gt; 一般来说，PSG规则包括右部(条件：符号序列的匹配模式)和左部(动作：短语归并结果)。词语的“属性约束”直接来自系统的词库，而短语的“属性约束”则是在自底向上的短语归并过程中从其构成成分的中心语(head)那里继承过来的。在Chomsky的理论中这叫做X-bar理论。 X-bar代表某个词类X所构成的、仍具有该词类属性的一个成分。如果X=N，就是一个具有名词特性的N-bar。当一条PSG规则的右部匹配成功，且“属性约束”部分得到满足，这条规则才能被执行。此时，规则左部所命名的的短语被生成，该短语的复杂特征集通过“属性传递”部分动态生成。 80年代末、90年代初学术界提出了一系列新的语法，如广义短语结构语法(GPSG)、中心语驱动的短语结构语法(HPSG)、词汇功能语法(LFG)等等。这些形式语法其实都是在词汇和短语的复杂特征集描写背景下产生的。合一(unification)算法则是针对复杂特征集的运算而提出来的。“合一”是实现属性匹配和赋值的一种算法，所以上述这些新语法又统称为“基于合一的语法”。 2.2、里程碑二：1966词汇主义 NLP领域第二个里程碑式贡献是词汇主义(lexicalism)。 1966年，韩礼德(Halliday)提出词汇不是用来填充语法确定的一套“空位”(slots)，而是一个独立的语言学层面；词汇研究可以作为对语法理论的补充，却不是语法理论的一部分，他主张把词汇从语法研究中独立地分离出来。 语言学家Hudson宣称，词汇主义是当今语言学理论头号发展倾向。出现原因也同上节两事实有关。词汇主义方法不仅提出一种颗粒度更细的语言知识表示形式，而且体现一语言知识递增式开发和积累的新思路。 首先解释一个背景矛盾。 一方面，语言学界一向认为，不划分词类就无法讲语法，如前面介绍的短语结构语法，语法“不可能”根据个别单独的词来写规则。但是另一方面，人们近来又注意到，任何归类其实都会丢失个体的某些重要信息。所以从前文提到的第一个事实出发，要想强化语法约束能力，词汇的描写应当深入到比词类更细微的词语本身上来。 换句话讲，语言学呼唤在词汇层采用颗粒度更小的描写单元。从本质上来说，词汇主义倾向反映了语言描写的主体已经从句法层转移到了词汇层；这也就是所谓的“小语法，大词库”的思想。下面让我们来看与词汇主义有关的一些工作。 2.2.1、词汇语法学(Lexicon-grammar) 法国巴黎大学Gross教授60年代创立研究中心LADL(http://www.ladl.jussieu.fr/)，提出了词汇语法的概念。 • 把12,000个主要动词分成50个子类。 • 每个动词都有一个特定的论元集。 • 每一类动词都有一个特定的矩阵, 其中每个动词都用400个不同句式来逐一描写(“+”代表可进入该句式；“-”表示不能)。 • 已开发英、法、德、西等欧洲语言的大规模描写。 • INTEX是一个适用于大规模语料分析的工具，已先后被世界五十多个研究中心采用。 2.2.2、框架语义学(Frame Semantics) Fillmore是格语法(Case Grammar)创始人，前几年主持美国自然科学基金的一个名为框架语义学的项目(http://www.icsi.berkeley.edu/~framenet)。该项目从WordNet上选取了2000个动词，从中得到75个语义框架。例如，动词”categorize”的框架被定义为: 一个人(Cognizer)把某个对象(Item)视为某个类(Category)。 同原先的格框架相比，原来一般化的动作主体被具体化为认知者Cognizer，动作客体被具体化为事物Item，并根据特定体动词的性质增加了一个作为分类结果的语义角色Category。 项目组还从英国国家语料库中挑出50,000个相关句子，通过人工给每个句子标注了相应的语义角色。例句： Kimcategorized the book as fiction. (Cog) (Itm)(Cat) 2.2.3、WordNet WordNet(http://www.cogsci.princeton.edu:80/~wn/)是一个描写英语词汇层语义关系的词库，1990年由普林斯顿大学Miller开发。至今有很多版本，全部公布在因特网上，供研究人员自由下载。 欧洲有一个Euro-WordNet，以类似的格式来表现各种欧洲语言的词汇层语义关系。WordNet刻意描写的是词语之间的各种语义关系，如同义关系(synonymy)、反义关系(antonymy)、上下位关系(hyponymy)，部分-整体关系(part-of)等等。 这种词汇语义学又叫做关系语义学，这一学派同传统的语义场理论和和语义属性描写理论相比，其最大的优势在于第一次在一种语言的整个词汇表上实现了词汇层的语义描写。这是其他学派从来没有做到的。其他理论迄今仅仅停留在教科书或某些学术论文中，从来就没有得到工程规模的应用。下面是WordNet的概况： • 95,600条实词词型(动词、名词、形容词) • 被划分成70,100个同义词集(synsets) 2.2.4 知网网(How-Net) 知网是董振东和董强设计的一个汉语语义知识网(http://www.keenage.com)，访问只有主页。 • 自下而上地依据概念对汉语实词进行了穷尽的分类。 • 15,000个动词被划分成810类。 • 定义了300个名词类，100个形容词类。 • 全部概念用400个语义元语来定义。 知网特点是既有WordNet所描写的同一类词间语义关系(如：同义、反义、上下位、部分-整体等)，又描写不同类词之间的论旨关系和语义角色。 2.2.5 MindNet MindNet是微软研究院NLP组设计的词汇语义网(http://research.microsoft.com/nlp/)，用三元组(triple)作为全部知识的表示基元。一个三元组由两个节点和一条连接边组成。每个节点代表一个概念，连接两个概念节点的边表示概念之间的语义依存关系。全部三元组通过句法分析器自动获取。 具体通过对两部英语词典(Longman Dictionaryof Contemporary English，AmericanHeritage Dictionary)和一部百科全书(Encarta)中的全部句子进行分析，获得每个句子的逻辑语义表示(logical form，简称LF)。 而LF本来就是由三元组构成的，如(W1, V-Obj,W2)表示：W1是一个动词，W2是其宾语中的中心词，因此W2从属于W1，它们之间的关系是V-Obj。比如(play, V-Obj,basketball)便是一个具体的三元组。又如(W1, H-Mod,W2)，W1代表一个偏正短语中的中心词(head word)，W2是其修饰语(modifier)，因此W2从属于W1，它们之间的关系是H-Mod。 这种资源是完全自动做出来的，所得三元组不可能没有错误。但是那些出现频度很高的三元组一般来说正确。MindNet已经应用到像语法检查、句法结构排歧、词义排歧、机器翻译等许多场合。 2.3 里程碑三：1976统计语言模型 第三大贡献是语料库方法，或叫统计语言模型。 首先成功利用数学方法解决自然语言处理问题的是语音和语言处理大师弗雷德·贾里尼克(Fred Jelinek)。1968年始在IBM研究中心兼职1974年全职加入，他领导一批杰出科学家利用大型计算机处理人类语言问题，学术休假(SabbaticalLeave)时(约1972-1976年间)提出统计语言模型。 1990s李开复用统计语言模型把997个词的语音识别问题简化成了20词识别问题，实现了有史以来第一次大词汇量非特定人连续语言的识别。常用统计语言模型，包括N元文法模型(N-gram Model)、隐马尔科夫模型(Hidden MarkovModel，简称HMM)、最大熵模型(MaximumEntropy Model)等。 △&nbsp;美国工程院院士Frederick Jelinek 如果用变量W代表一个文本中顺序排列的n个词，即W = w1w2…wn，则统计语言模型的任务是给出任意一个词序列W在文本中出现的概率P(W)。 利用概率的乘积公式，P(W)可展开为： P(W) =P(w1)P(w2/w1)P(w3/ w1 w2)…P(wn/w1 w2…wn-1)&nbsp;(1) 式中P(w1)表示第一个词w1的出现概率，P(w2/w1)表示在w1出现的情况下第二个词w2出现的条件概率，依此类推。 不难看出，为了预测词wn的出现概率，必须已知它前面所有词的出现概率。从计算上来看，这太复杂了。如果近似认为任意一个词wi的出现概率只同它紧邻的前一个词有关，那么计算就得以大大简化。这就是所谓的二元模型(bigram)，由(1)式得： P(W) ≈ P(w1)∏i=2,…,nP(wi/ wi-1 )&nbsp;(2) 式中∏i=2,…,nP(wi/ wi-1 )表示多个概率的连乘。 需要着重指出的是：这些概率参数都可以通过大规模语料库来估值。比如二元概率 P(wi/ wi-1) ≈count(wi-1 wi) / count(wi-1)&nbsp;(3) 式中count(…)表示一个特定词序列在整个语料库中出现的累计次数。若语料库的总词次数为N，则任意词wi在该语料库中的出现概率可估计如下： P(wi) ≈count(wi) / N&nbsp;(4) 同理，如果近似认为任意词wi的出现只同它紧邻前两个词有关，就得到一个三元模型(trigram)： P(W) ≈P(w1)P(w2/w1) ∏i=3,…,nP(wi/wi-2 w-1 )&nbsp;(5) 统计语言模型的方法有点像天气预报。用来估计概率参数的大规模语料库好比是一个地区历年积累起来的气象记录，而用三元模型来做天气预报，就像是根据前两天的天气情况来预测当天的天气。天气预报当然不可能百分之百正确。这也算是概率统计方法的一个特点。 2.3.1、语音识别 语音识别作为计算机汉字键盘输入的一种图代方式，越来越受到信息界人士的青睐。所谓听写机就是这样的商品。据报道中国的移动电话用户已超过一亿，随着移动电话和个人数字助理(PDA)的普及，尤其是当这些随身携带的器件都可以无线上网的时候，广大用户更迫切期望通过语音识别或手写板而不是小键盘来输入简短的文字信息。 其实，语音识别任务可视为计算以下条件概率的极大值问题： W*= argmaxWP(W/speech signal) = argmaxWP(speech signal/W) P(W) / P(speech signal) = argmaxWP(speech signal/W) P(W)&nbsp;(6) 式中数学符号argmaxW表示对不同的候选词序列W计算条件概率P(W/speech signal)的值，从而使W*成为其中条件概率值最大的那个词序列，这也就是计算机选定的识别结果。换句话讲，通过式(6)的计算，计算机找到了最适合当前输入语音信号speech signal的词串W。 式(6)第二行是利用贝叶斯定律转写的结果，因为条件概率P(speech signal/W)比较容易估值。公式的分母P(speech signa)对给定的语音信号是一个常数，不影响极大值的计算，故可以从公式中删除。在第三行所示的结果中，P(W)就是前面所讲得统计语言模型，一般采用式(5)所示的三元模型；P(speechsignal/W)叫做声学模型。 讲到这儿，细心的读者可能已经明白，汉语拼音输入法中的拼音－汉字转换任务其实也是用同样方法实现的，而且两者所用的汉语语言模型(即二元或三元模型)是同一个模型。 据笔者所知，目前市场上的听写机产品和微软拼音输入法(3.0版)都是用词的三元模型实现的，几乎完全不用句法-语义分析手段。为什么会出现这样的局面呢？这是优胜劣汰的客观规律所决定的。可比的评测结果表明，用三元模型实现的拼音-汉字转换系统，其出错率比其它产品减少约50%。 2.3.2、词性标注 一个词库中大约14%的词型具有不只一个词性。而在一个语料库中，占总词次数约30%的词具有不止一个词性。所以对一个文本中的每一个词进行词性标注，就是通过上下文的约束，实现词性歧义的消解。历史上曾经先后出现过两个自动词性标注系统。一个采用上下文相关的规则，叫做TAGGIT(1971)，另一个应用词类的二元模型，叫做CLAWS(1987)。 两个系统都分别对100万词次的英语非受限文本实施了词性标注。结果显示，采用统计语言模型的CLAWS系统的标注正确率大大高于基于规则方法的TAGGIT系统。请看下表的对比： 令C和W分别代表词类标记序列和词序列，则词性标注问题可视为计算以下条件概率的极大值: C*= argmaxCP(C/W) = argmaxCP(W/C)P(C) / P(W) ≈ argmaxC∏i=1,…,nP(wi/ci )P(ci /ci-1 )&nbsp;(7) 式中P(C/W)是已知输入词序列W的情况下，出现词类标记序列C的条件概率。数学符号argmaxC表示通过考察不同的候选词类标记序列C，来寻找使条件概率取最大值的那个词类标记序列C*。后者应当就是对W的词性标注结果。 公式第二行是利用贝叶斯定律转写的结果，由于分母P(W)对给定的W是一个常数，不影响极大值的计算，可以从公式中删除。接着对公式进行近似。首先，引入独立性假设，认为任意一个词wi的出现概率近似只同当前词的词类标记ci有关，而与周围(上下文)的词类标记无关。于是词汇概率可计算如下： P(W/C) ≈∏i=1,…,n P(wi/ci )&nbsp;(8) 其次，采用二元假设，即近似认为任意一个词类标记ci的出现概率只同它紧邻的前一个词类标记ci-1有关。有 P(C) ≈ P(c1)∏i=2,…,n P(ci /ci-1 )&nbsp;(9) P(ci /ci-1 )是词类标记的转移概率，也叫做基于词类的二元模型。 上述这两个概率参数都可以通过带词性标记的语料库来分别估计： P(wi/ci ) ≈count(wi,ci) / count(ci)&nbsp;(10) P(ci /ci-1 ) ≈count(ci-1ci) / count(ci-1)&nbsp;(11) 据文献报道，采用统计语言模型方法汉语和英语的次性标注正确率都可以达到96%左右[6]。 2.3.3、介词短语PP的依附歧义 英语中介词短语究竟依附于前面的名词还是前面的动词，是句法分析中常见的结构歧义问题。下例用语料库方法来解决这个问题，以及这种方法究竟能达到多高的正确率。 例句：Pierre Vinken,61 years old, joined the board as a nonexecutive director. 令A=1表示名词依附，A=0为动词依附，则上述例句的PP依附问题可表为： (A=0,V=joined, N1=board, P=as, N2=director) 令V, N1, N2分别代表句中动词短语、宾语短语、介宾短语的中心词，并在一个带有句法标注的语料库(又称树库)中统计如下四元组的概率Pr： Pr = (A=1 /V=v, N1=n1, P=p, N2=n2)&nbsp;(10) 对输入句子进行PP 依附判断的算法如下： 若Pr = (1 / v, n1, p, n2) ≥ 0.5, 则判定PP依附于n1, 否则判定PP依附于v。 Collins和Brooks实验使用的语料库是宾夕法尼亚大学标注的华尔街日报(WSJ)树库，包括：训练集20,801个四元组，测试集3,097个四元组。他们对PP依附自动判定精度的上下限作了如下分析： 一律视为名词依附(即A≡1) 59.0% 只考虑介词p的最常见附加72.2% 三位专家只根据四个中心词判断88.2% 三位专家根据全句判断93.2% 很明显，自动判断精确率的下限是72.2%，因为机器不会比只考虑句中介词p的最常见依附做得更差了；上限是88.2%，因为机器不可能比三位专家根据四个中心词作出的判断更高明。 论文报告，在被测试的3,097个四元组中，系统正确判断的四元组为2,606个，因此平均精确率为84.1%。这与上面提到的上限值88.2%相比，应该说是相当不错的结果。 传统三大技术里程碑小结 语言学家在不论是复杂特征集和合一语法，还是词汇主义方法，都是原先所谓的理性主义框架下做出的重大贡献。词汇主义方法提出了一种颗粒度更细的语言知识表示形式，而且体现了一种语言知识递增式开发和积累的新思路，值得特别推崇。 尤其值得重视的是，在众多词汇资源的开发过程中，语料库和统计学习方法发挥了很大的作用。这是经验主义方法和理性主义方法相互融合的可喜开端，也是国内知名语言学者冯志伟等人认可的研究范式。 语料库方法和统计语言模型，国内同行中实际上存在不同评价。有种观点认为NLP必须建立在语言理解基础上，他们不大相信统计语言模型在语音识别、词性标注、信息检索等应用领域中所取得的进展。这些争论不能澄清，是因为同行间缺少统一评测。有评测才会有鉴别。 评判某方法优劣应公开、公平、相互可比的评测标准，而非研究员设计“自评”。黄昌宁、张小凤2013年论文表示，语料库方法和统计语言模型是当前自然语言处理技术的主流，其实用价值已在很多应用系统中得到充分证实。统计语言模型研究在结构化对象的统计建模方面，仍有广阔发展空间。 自然语言处理领域业界知名博主Sebatian Ruder在2018年文章从神经网络技术角度，总结NLP领域近15年重大进展、8大里程碑事件，提及很多神经网络模型。这些模型建立在同一时期非神经网络技术之上，如上述三大里程碑。下面接着看后续NLP技术的发展。 2.4、里程碑四：2001神经语言模型(Neural language models) 语言模型解决的是在给定已出现词语的文本中，预测下一个单词的任务。这是最简单的语言处理任务，有许多具体实际应用，如智能键盘、电子邮件回复建议等。语言模型历史由来已久，经典方法基于n-grams模型(利用前面n个词语预测下一个单词)，并利用平滑操作处理不可见的n-grams。 第一个神经语言模型，前馈神经网络(feed-forward neural network)，是Bengio等人于2001年提出的。模型以某词语之前出现的n个词语作为输入向量，也就是现在大家说的词嵌入(word embeddings)向量。这些词嵌入在级联后进入一个隐藏层，该层的输出然后通过一个softmax层。如图3所示。 △&nbsp;前馈神经网络语言模型 而现在构建语言模型的前馈神经网络，已被循环神经网络(RNNs)和长短期记忆神经网络(LSTMs)取代。 虽然后来提出许多新模型在经典LSTM上进行了扩展，但它仍然是强有力的基础模型。甚至Bengio等人的经典前馈神经网络在某些设定下也和更复杂的模型效果相当，因为这些任务只需要考虑邻近的词语。理解这些语言模型究竟捕捉了哪些信息，也是当今一个活跃的研究领域。 语言模型的建立是一种无监督学习(unsupervisedlearning)，Yann LeCun称之为预测学习(predictivelearning)，是获得世界如何运作常识的先决条件。 关于语言模型最引人注目的是，尽管它很简单，但却与后文许多核心进展息息相关。反过来，这也意味着NLP领域许多重要进展都可以简化为某种形式的语言模型构建。但要实现对自然语言真正意义上的理解，仅仅从原始文本中进行学习是不够的，我们需要新的方法和模型。 2.5、里程碑五：2008多任务学习(Multi-task learning) 多任务学习是在多个任务下训练的模型之间共享参数的方法，在神经网络中通过捆绑不同层的权重轻松实现。多任务学习思想1993年Rich Caruana首次提出，并应用于道路追踪和肺炎预测。多任务学习鼓励模型学习对多个任务有效的表征描述。这对于学习一般的、低级的描述形式、集中模型的注意力或在训练数据有限的环境中特别有用。 多任务学习2008年被Collobert和Weston等人首次在自然语言处理领域应用于神经网络。在他们的模型中，词嵌入矩阵被两个在不同任务下训练的模型共享，如图4所示。 △&nbsp;词嵌入矩阵共享 共享的词嵌入矩阵使模型可以相互协作，共享矩阵中的低层级信息，而词嵌入矩阵往往构成了模型中需要训练的绝大部分参数。 Collobert和Weston发表于2008年的论文，影响远远超过了它在多任务学习中的应用。它开创的诸如预训练词嵌入和使用卷积神经网络处理文本的方法，在接下来的几年被广泛应用。他们也因此获得2018年机器学习国际会议(ICML)的test-of-time奖。 如今，多任务学习在自然语言处理领域广泛使用，而利用现有或“人工”任务已经成为NLP指令库中的一个有用工具。 虽然参数的共享是预先定义好的，但在优化的过程中却可以学习不同的共享模式。当模型越来越多地在多个任务上进行测评以评估其泛化能力时，多任务学习就变得愈加重要，近年来也涌现出更多针对多任务学习的评估基准。 2.6、里程碑六：2013词嵌入 稀疏向量对文本进行表示的词袋模型，在自然语言处理领域有很长历史。而用稠密的向量对词语进行描述，也就是词嵌入，则在2001年首次出现。2013年Mikolov等人工作主要创新之处在于，通过去除隐藏层和近似计算目标使词嵌入模型的训练更为高效。 尽管这些改变本质上十分简单，但它们与高效的word2vec(wordto vector用来产生词向量的相关模型)组合在一起，使得大规模的词嵌入模型训练成为可能。 Word2vec有两种不同的实现方法：CBOW(continuousbag-of-words)和skip-gram。它们在预测目标上有所不同：一个是根据周围的词语预测中心词语，另一个则恰恰相反。如图5所示。 △&nbsp;CBOW和skip-gram架构 虽然这些嵌入与使用前馈神经网络学习的嵌入在概念上没有区别，但是在一个非常大语料库上的训练使它们能够获取诸如性别、动词时态和国际事务等单词之间的特定关系。如下图 4 所示。 △&nbsp;word2vec捕获的联系 这些关系和它们背后的意义激起了人们对词嵌入的兴趣，许多研究都在关注这些线性关系的来源。然而，使词嵌入成为目前自然语言处理领域中流砥柱的，是将预训练的词嵌入矩阵用于初始化可以提高大量下游任务性能的事实。 虽然word2vec捕捉到的关系具有直观且几乎不可思议的特性，但后来的研究表明，word2vec本身并没有什么特殊之处：词嵌入也可以通过矩阵分解来学习，经过适当的调试，经典的矩阵分解方法SVD和LSA都可以获得相似的结果。从那时起，大量的工作开始探索词嵌入的不同方面。尽管有很多发展，word2vec仍是目前应用最为广泛的选择。 Word2vec应用范围也超出了词语级别：带有负采样的skip-gram——一个基于上下文学习词嵌入的方便目标，已经被用于学习句子的表征。它甚至超越了自然语言处理的范围，被应用于网络和生物序列等领域。 一个激动人心的研究方向是在同一空间中构建不同语言的词嵌入模型，以达到(零样本)跨语言转换的目的。通过无监督学习构建这样的映射变得越来越有希望(至少对于相似的语言来说)，这也为语料资源较少的语言和无监督机器翻译的应用程序创造可能。 2.7、里程碑七：2013RNN/CNN用于NLP的神经网络 2013和2014年是自然语言处理领域神经网络时代的开始。其中三种类型的神经网络应用最为广泛：循环神经网络(recurrentneural networks)、卷积神经网络(convolutionalneural networks)和结构递归神经网络(recursiveneural networks)。 循环神经网络是NLP领域处理动态输入序列最自然的选择。Vanilla循环神经网络很快被经典的长短期记忆网络(long-shorttermmemory networks，LSTM)代替，该模型能更好地解决梯度消失和梯度爆炸问题。 在2013年之前，人们仍认为循环神经网络很难训练，直到Ilya Sutskever博士的论文改变了循环神经网络这一名声。双向的长短期记忆记忆网络通常被用于同时处理出现在左侧和右侧的文本内容。LSTM 结构如图7所示。 △&nbsp;LSTM网络 应用于文本的卷积神经网络只在两个维度上进行操作，卷积层只需要在时序维度上移动即可。图8展示了应用于自然语言处理的卷积神经网络的典型结构。 △&nbsp;卷积神经网络 与循环神经网络相比，卷积神经网络的一个优点是具有更好的并行性。 因为卷积操作中每个时间步的状态只依赖于局部上下文，而不是循环神经网络中那样依赖于所有过去的状态。卷积神经网络可以使用更大的卷积层涵盖更广泛的上下文内容。卷积神经网络也可以和长短期记忆网络进行组合和堆叠，还可以用来加速长短期记忆网络的训练。 循环神经网络和卷积神经网络都将语言视为一个序列。但从语言学的角度来看，语言是具有层级结构的：词语组成高阶的短语和小句，它们本身可以根据一定的产生规则递归地组合。这激发了利用结构递归神经网络，以树形结构取代序列来表示语言的想法，如图9所示。 △&nbsp;结构递归神经网络 结构递归神经网络自下而上构建序列的表示，与从左至右或从右至左对序列进行处理的循环神经网络形成鲜明的对比。树中的每个节点是通过子节点的表征计算得到的。一个树也可以视为在循环神经网络上施加不同的处理顺序，所以长短期记忆网络则可以很容易地被扩展为一棵树。 不只是循环神经网络和长短期记忆网络可以扩展到使用层次结构，词嵌入也可以在语法语境中学习，语言模型可以基于句法堆栈生成词汇，图形卷积神经网络可以树状结构运行。 2.8、里程碑八：2014序列到序列模型(Sequence-to-sequencemodels) 2014年，Sutskever等人提出序列到序列学习，即使用神经网络将一个序列映射到另一个序列的一般化框架。在这个框架中，一个作为编码器的神经网络对句子符号进行处理，并将其压缩成向量表示；然后，一个作为解码器的神经网络根据编码器的状态逐个预测输出符号，并将前一个预测得到的输出符号作为预测下一个输出符号的输入。如图10所示。 △&nbsp;序列到序列模型 机器翻译是这一框架的杀手级应用。2016年，谷歌宣布他们将用神经机器翻译模型取代基于短语的整句机器翻译模型。谷歌大脑负责人Jeff Dean表示，这意味着用500行神经网络模型代码取代50万行基于短语的机器翻译代码。 由于其灵活性，该框架在自然语言生成任务上被广泛应用，其编码器和解码器分别由不同的模型来担任。更重要的是，解码器不仅可以适用于序列，在任意表示上均可以应用。比如基于图片生成描述(如图11)、基于表格生成文本、根据源代码改变生成描述，以及众多其他应用。 △&nbsp;基于图像生成标题(Vinyalset al., 2015) 序列到序列的学习甚至可以应用到自然语言处理领域常见的结构化预测任务中，也就是输出具有特定的结构。为简单起见，输出就像选区解析一样被线性化(如图12)。在给定足够多训练数据用于语法解析的情况下，神经网络已经被证明具有产生线性输出和识别命名实体的能力。 △&nbsp;线性化选区解析树(Vinyalset al., 2015) 序列的编码器和解码器通常都是基于循环神经网络，但也可以使用其他模型。新的结构主要都从机器翻译的工作中诞生，它已经成了序列到序列模型的培养基。近期提出的模型有深度长短期记忆网络、卷积编码器、Transformer(一个基于自注意力机制的全新神经网络架构)以及长短期记忆依赖网络和的 Transformer 结合体等。 2.9、里程碑九：2015注意力机制和基于记忆的神经网络 注意力机制是神经网络机器翻译(NMT)的核心创新之一，也是使神经网络机器翻译优于经典的基于短语的机器翻译的关键。序列到序列学习的主要瓶颈是，需要将源序列的全部内容压缩为固定大小的向量。注意力机制通过让解码器回顾源序列的隐藏状态，以此为解码器提供加权平均值的输入来缓解这一问题，如图13所示。 △&nbsp;注意力机制 之后，各种形式的注意力机制涌现而出。注意力机制被广泛接受，在各种需要根据输入的特定部分做出决策的任务上都有潜在的应用。它已经被应用于句法分析、阅读理解、单样本学习等任务中。它的输入甚至不需要是一个序列，而可以包含其他表示，比如图像的描述(图14)。 注意力机制一个有用的附带作用是它通过注意力权重来检测输入的哪一部分与特定的输出相关，从而提供了一种罕见的虽然还是比较浅层次的，对模型内部运作机制的窥探。 △&nbsp;图像描述模型中的视觉注意力机制指示在生成”飞盘”时所关注的内容 注意力机制不仅仅局限于输入序列。自注意力机制可用来观察句子或文档中周围的单词，获得包含更多上下文信息的词语表示。多层的自注意力机制是神经机器翻译前沿模型Transformer的核心。 注意力机制可以视为模糊记忆的一种形式，其记忆的内容包括模型之前的隐藏状态，由模型选择从记忆中检索哪些内容。与此同时，更多具有明确记忆单元的模型被提出。 他们有很多不同的变化形式，比如神经图灵机(NeuralTuring Machines)、记忆网络(MemoryNetwork)、端到端的记忆网络(End-to-endMemory Newtorks)、动态记忆网络(DynamicMemoryNetworks)、神经可微计算机(NeuralDifferentiable Computer)、循环实体网络(RecurrentEntityNetwork)。 记忆的存取通常与注意力机制相似，基于与当前状态且可以读取和写入。这些模型之间的差异体现在它们如何实现和利用存储模块。 比如说，端到端的记忆网络对输入进行多次处理并更新内存，以实行多次推理。神经图灵机也有一个基于位置的寻址方式，使它们可以学习简单的计算机程序，比如排序。 基于记忆的模型通常用于需要长时间保留信息的任务中，例如语言模型构建和阅读理解。记忆模块的概念非常通用，知识库和表格都可以作为记忆模块，记忆模块也可以基于输入的全部或部分内容进行填充。 2.10、里程碑十：2018预训练语言模型 预训练的词嵌入与上下文无关，仅用于初始化模型中的第一层。近几个月以来，许多有监督的任务被用来预训练神经网络。相比之下，语言模型只需要未标记的文本，因此其训练可以扩展到数十亿单词的语料、新的领域、新的语言。预训练的语言模型于 2015年被首次提出，但直到最近它才被证明在大量不同类型的任务中均十分有效。语言模型嵌入可以作为目标模型中的特征，或者根据具体任务进行调整。如下图所示，语言模型嵌入为许多任务的效果带来了巨大的改进。 △&nbsp;改进的语言模型嵌入 使用预训练的语言模型可以在数据量十分少的情况下有效学习。由于语言模型的训练只需要无标签的数据，因此他们对于数据稀缺的低资源语言特别有利。 2018年10月，谷歌AI语言组发布BERT语言模型预训练，已被证明可有效改进许多自然语言处理任务(Dai and Le, 2015; Peters et al., 2017, 2018; Radford etal., 2018; Howard and Ruder, 2018)。 这些任务包括句子级任务，如自然语言推理inference(Bowman et al., 2015; Williams et al., 2018)和释义paraphrasing(Dolan and Brockett, 2005)，旨在通过整体分析来预测句子之间的关系；以及词块级任务，如命名实体识别(Tjong KimSang and De Meulder, 2003)和SQuAD问题回答(Rajpurkar et al., 2016)，其中模型需要在词块级别生成细粒度输出。 近年七大技术里程碑小结 除了上述七大技术里程碑，一些其他进展虽不如上面提到的那样流行，但仍产生了广泛的影响。 基于字符的描述(Character-based representations)，在字符层级上使用卷积神经网络和长短期记忆网络，以获得一个基于字符的词语描述，目前已经相当常见了，特别是对于那些语言形态丰富的语种或那些形态信息十分重要、包含许多未知单词的任务。据目前所知，基于字符的描述最初用于序列标注，现在，基于字符的描述方法，减轻了必须以增加计算成本为代价建立固定词汇表的问题，并使完全基于字符的机器翻译的应用成为可能。 对抗学习(Adversarial learning)，在机器学习领域已经取得了广泛应用，在自然语言处理领域也被应用于不同的任务中。对抗样例的应用也日益广泛，他们不仅仅是探测模型弱点的工具，更能使模型更具鲁棒性(robust)。(虚拟的)对抗性训练，也就是最坏情况的扰动，和域对抗性损失(domain-adversariallosses)都是可以使模型更具鲁棒性的有效正则化方式。生成对抗网络(GANs)目前在自然语言生成任务上还不太有效，但在匹配分布上十分有用。 强化学习(Reinforcement learning)，在具有时间依赖性任务上证明有效，比如在训练期间选择数据和对话建模。在机器翻译和概括任务中，强化学习可以有效地直接优化“红色”和“蓝色”这样不可微的度量，不必去优化像交叉熵这样的代理损失函数。同样，逆向强化学习(inversereinforcement learning)在类似视频故事描述这样的奖励机制非常复杂且难以具体化的任务中，也非常有用。 自然语言处理NLP知识结构 文|秦陇纪，数据简化DataSimp 自然语言处理(计算机语言学、自然语言理解)涉及：字处理，词处理，语句处理，篇章处理词处理分词、词性标注、实体识别、词义消歧语句处理句法分析(SyntacticAnalysis)、语义分析(SenmanticAnalysis)等。其中，重点有： 1、句法语义分析：分词，词性标记，命名实体识别。 2、信息抽取 3、文本挖掘：文本聚类，情感分析。基于统计。 4、机器翻译：基于规则，基于统计，基于神经网络。 5、信息检索 6、问答系统 7、对话系统建议…本文总结的自然语言处理历史、模型、知识体系结构内容，涉及NLP的语言理论、算法和工程实践各方面，内容繁杂。参考黄志洪老师自然语言处理课程、宗成庆老师《统计自然语言处理》，郑捷2017年电子工业出版社出版的图书《NLP汉语自然语言处理原理与实践》，以及国外著名NLP书籍的英文资料、汉译版资料。 一、NLP知识结构概述 1)自然语言处理：利用计算机为工具，对书面实行或者口头形式进行各种各样的处理和加工的技术，是研究人与人交际中以及人与计算机交际中的演员问题的一门学科，是人工智能的主要内容。 2)自然语言处理是研究语言能力和语言应用的模型，建立计算机(算法)框架来实现这样的语言模型，并完善、评测、最终用于设计各种实用系统。 3)研究问题(主要)： 信息检索 机器翻译 文档分类 问答系统 信息过滤 自动文摘 信息抽取 文本挖掘 舆情分析 机器写作 语音识别 研究模式：自然语言场景问题，数学算法，算法如何应用到解决这些问题，预料训练，相关实际应用 自然语言的困难： 场景的困难：语言的多样性、多变性、歧义性 学习的困难：艰难的数学模型(hmm,crf,EM,深度学习等) 语料的困难：什么的语料？语料的作用？如何获取语料？ 二、NLP知识十大结构 2.1、形式语言与自动机 语言：按照一定规律构成的句子或者字符串的有限或者无限的集合。 描述语言的三种途径： 穷举法 文法(产生式系统)描述 自动机 自然语言不是人为设计而是自然进化的，形式语言比如：运算符号、化学分子式、编程语言 形式语言理论朱啊哟研究的是内部结构模式这类语言的纯粹的语法领域，从语言学而来，作为一种理解自然语言的句法规律，在计算机科学中，形式语言通常作为定义编程和语法结构的基础 形式语言与自动机基础知识： 集合论 图论 自动机的应用： 1，单词自动查错纠正 2，词性消歧(什么是词性？什么的词性标注？为什么需要标注？如何标注？) 形式语言的缺陷： 1、对于像汉语，英语这样的大型自然语言系统，难以构造精确的文法 2、不符合人类学习语言的习惯 3、有些句子语法正确，但在语义上却不可能，形式语言无法排出这些句子 4、解决方向：基于大量语料，采用统计学手段建立模型 2.2、语言模型 1)、语言模型(重要)：通过语料计算某个句子出现的概率(概率表示)，常用的有2-元模型，3-元模型 2)、语言模型应用： 语音识别歧义消除例如，给定拼音串：ta shi yan yan jiu saun fa de 可能的汉字串：踏实烟酒算法的他是研究酸法的他是研究算法的，显然，最后一句才符合。 3)、语言模型的启示： 1、开启自然语言处理的统计方法 2、统计方法的一般步骤： 收集大量语料 对语料进行统计分析，得出知识 针对场景建立算法模型 解释和应用结果 4)、语言模型性能评价，包括评价目标，评价的难点，常用指标(交叉熵，困惑度) 5)、数据平滑： 数据平滑的概念，为什么需要平滑 平滑的方法，加一法，加法平滑法，古德-图灵法，J-M法，Katz平滑法等 6)、语言模型的缺陷： 语料来自不同的领域，而语言模型对文本类型、主题等十分敏感 n与相邻的n-1个词相关，假设不是很成立。 2.3、概率图模型 生成模型与判别模型，贝叶斯网络，马尔科夫链与隐马尔科夫模型(HMM) 1)概率图模型概述(什么的概率图模型，参考清华大学教材《概率图模型》) 2)马尔科夫过程(定义，理解) 3)隐马尔科夫过程(定义，理解) HMM的三个基本问题(定义，解法，应用) 注：第一个问题，涉及最大似然估计法，第二个问题涉及EM算法，第三个问题涉及维特比算法，内容很多，要重点理解，(参考书李航《统计学习方法》，网上博客，笔者github) 2.4、马尔科夫网，最大熵模型，条件随机场(CRF) 1)、HMM的三个基本问题的参数估计与计算 2)、什么是熵 3)、EM算法(应用十分广泛，好好理解) 4)、HMM的应用 5)、层次化马尔科夫模型与马尔科夫网络 提出原因，HMM存在两个问题 6)、最大熵马尔科夫模型 优点：与HMM相比，允许使用特征刻画观察序列，训练高效 缺点：存在标记偏置问题 7)、条件随机场及其应用(概念，模型过程，与HMM关系) 参数估计方法(GIS算法，改进IIS算法) CRF基本问题：特征选取(特征模板)、概率计算、参数训练、解码(维特比) 应用场景： 词性标注类问题(现在一般用RNN+CRF) 中文分词(发展过程，经典算法，了解开源工具jieba分词) 中文人名，地名识别 8)、CRF++ 2.5、命名实体识别，词性标注，内容挖掘、语义分析与篇章分析(大量用到前面的算法) 1)、命名实体识别问题 相关概率，定义 相关任务类型 方法(基于规程-&gt;基于大规模语料库) 2)、未登录词的解决方法(搜索引擎，基于语料) 3)、CRF解决命名实体识别(NER)流程总结： 训练阶段：确定特征模板，不同场景(人名，地名等)所使用的特征模板不同，对现有语料进行分词，在分词结果基础上进行词性标注(可能手工)，NER对应的标注问题是基于词的，然后训练CRF模型，得到对应权值参数值 识别过程：将待识别文档分词，然后送入CRF模型进行识别计算(维特比算法)，得到标注序列，然后根据标注划分出命名实体 4)、词性标注(理解含义，意义)及其一致性检查方法(位置属性向量，词性标注序列向量，聚类或者分类算法) 2.6、句法分析 1)、句法分析理解以及意义 1、句法结构分析 完全句法分析 浅层分析 2、依存关系分析 2)、句法分析方法 1、基于规则的句法结构分析 2、基于统计的语法结构分析 2.7 文本分类，情感分析 1)、文本分类，文本排重 文本分类：在预定义的分类体系下，根据文本的特征，将给定的文本与一个或者多个类别相关联 典型应用：垃圾邮件判定，网页自动分类 2)、文本表示，特征选取与权重计算，词向量 文本特征选择常用方法： 1、基于本文频率的特征提取法 2、信息增量法 3、X2(卡方)统计量 4、互信息法 3)、分类器设计 SVM，贝叶斯，决策树等 4)、分类器性能评测 1、召回率 2、正确率 3、F1值 5)、主题模型(LDA)与PLSA LDA模型十分强大，基于贝叶斯改进了PLSA，可以提取出本章的主题词和关键词，建模过程复杂，难以理解。 6)、情感分析 借助计算机帮助用户快速获取，整理和分析相关评论信息，对带有感情色彩的主观文本进行分析，处理和归纳例如，评论自动分析，水军识别。 某种意义上看，情感分析也是一种特殊的分类问题 7)、应用案例 2.8、信息检索，搜索引擎及其原理 1)、信息检索起源于图书馆资料查询检索，引入计算机技术后，从单纯的文本查询扩展到包含图片，音视频等多媒体信息检索，检索对象由数据库扩展到互联网。 1、点对点检索 2、精确匹配模型与相关匹配模型 3、检索系统关键技术：标引，相关度计算 2)、常见模型：布尔模型，向量空间模型，概率模型 3)、常用技术：倒排索引，隐语义分析(LDA等) 4)、评测指标 2.9、自动文摘与信息抽取，机器翻译，问答系统 1)、统计机器翻译的的思路，过程，难点，以及解决 2)、问答系统 基本组成：问题分析，信息检索，答案抽取 类型：基于问题-答案，基于自由文本 典型的解决思路 3)、自动文摘的意义，常用方法 4)、信息抽取模型(LDA等) 2.10深度学习在自然语言中的应用 1)、单词表示，比如词向量的训练(wordvoc) 2)、自动写文本 写新闻等 3)、机器翻译 4)、基于CNN、RNN的文本分类 5)、深度学习与CRF结合用于词性标注 三、中文NLP知识目录 选自郑捷2017年电子工业出版社出版的图书《NLP汉语自然语言处理原理与实践》。 第1章 中文语言的机器处理 1 1.1 历史回顾 2 1.1.1 从科幻到现实 2 1.1.2 早期的探索 3 1.1.3 规则派还是统计派 3 1.1.4 从机器学习到认知计算 5 1.2 现代自然语言系统简介 6 1.2.1 NLP流程与开源框架 6 1.2.2 哈工大NLP平台及其演示环境 9 1.2.3 StanfordNLP团队及其演示环境 11 1.2.4 NLTK开发环境 13 1.3 整合中文分词模块 16 1.3.1 安装Ltp Python组件 17 1.3.2 使用Ltp 3.3进行中文分词 18 1.3.3 使用结巴分词模块 20 1.4 整合词性标注模块 22 1.4.1 Ltp 3.3词性标注 23 1.4.2 安装StanfordNLP并编写Python接口类 24 1.4.3 执行Stanford词性标注 28 1.5 整合命名实体识别模块 29 1.5.1 Ltp 3.3命名实体识别 29 1.5.2 Stanford命名实体识别 30 1.6 整合句法解析模块 32 1.6.1 Ltp 3.3句法依存树 33 1.6.2 StanfordParser类 35 1.6.3 Stanford短语结构树 36 1.6.4 Stanford依存句法树 37 1.7 整合语义角色标注模块 38 1.8 结语 40 第2章 汉语语言学研究回顾 42 2.1 文字符号的起源 42 2.1.1 从记事谈起 43 2.1.2 古文字的形成 47 2.2 六书及其他 48 2.2.1 象形 48 2.2.2 指事 50 2.2.3 会意 51 2.2.4 形声 53 2.2.5 转注 54 2.2.6 假借 55 2.3 字形的流变 56 2.3.1 笔与墨的形成与变革 56 2.3.2 隶变的方式 58 2.3.3 汉字的符号化与结构 61 2.4 汉语的发展 67 2.4.1 完整语义的基本形式——句子 68 2.4.2 语言的初始形态与文言文 71 2.4.3 白话文与复音词 73 2.4.4 白话文与句法研究 78 2.5 三个平面中的语义研究 80 2.5.1 词汇与本体论 81 2.5.2 格语法及其框架 84 2.6 结语 86 第3章 词汇与分词技术 88 3.1 中文分词 89 3.1.1 什么是词与分词规范 90 3.1.2 两种分词标准 93 3.1.3 歧义、机械分词、语言模型 94 3.1.4 词汇的构成与未登录词 97 3.2 系统总体流程与词典结构 98 3.2.1 概述 98 3.2.2 中文分词流程 99 3.2.3 分词词典结构 103 3.2.4 命名实体的词典结构 105 3.2.5 词典的存储结构 108 3.3 算法部分源码解析 111 3.3.1 系统配置 112 3.3.2 Main方法与例句 113 3.3.3 句子切分 113 3.3.4 分词流程 117 3.3.5 一元词网 118 3.3.6 二元词图 125 3.3.7 NShort算法原理 130 3.3.8 后处理规则集 136 3.3.9 命名实体识别 137 3.3.10 细分阶段与最短路径 140 3.4 结语 142 第4章 NLP中的概率图模型 143 4.1 概率论回顾 143 4.1.1 多元概率论的几个基本概念 144 4.1.2 贝叶斯与朴素贝叶斯算法 146 4.1.3 文本分类 148 4.1.4 文本分类的实现 151 4.2 信息熵 154 4.2.1 信息量与信息熵 154 4.2.2 互信息、联合熵、条件熵 156 4.2.3 交叉熵和KL散度 158 4.2.4 信息熵的NLP的意义 159 4.3 NLP与概率图模型 160 4.3.1 概率图模型的几个基本问题 161 4.3.2 产生式模型和判别式模型 162 4.3.3 统计语言模型与NLP算法设计 164 4.3.4 极大似然估计 167 4.4 隐马尔科夫模型简介 169 4.4.1 马尔科夫链 169 4.4.2 隐马尔科夫模型 170 4.4.3 HMMs的一个实例 171 4.4.4 Viterbi算法的实现 176 4.5 最大熵模型 179 4.5.1 从词性标注谈起 179 4.5.2 特征和约束 181 4.5.3 最大熵原理 183 4.5.4 公式推导 185 4.5.5 对偶问题的极大似然估计 186 4.5.6 GIS实现 188 4.6 条件随机场模型 193 4.6.1 随机场 193 4.6.2 无向图的团(Clique)与因子分解 194 4.6.3 线性链条件随机场 195 4.6.4 CRF的概率计算 198 4.6.5 CRF的参数学习 199 4.6.6 CRF预测标签 200 4.7 结语 201 第5章 词性、语块与命名实体识别 202 5.1 汉语词性标注 203 5.1.1 汉语的词性 203 5.1.2 宾州树库的词性标注规范 205 5.1.3stanfordNLP标注词性 210 5.1.4 训练模型文件 213 5.2 语义组块标注 219 5.2.1 语义组块的种类 220 5.2.2 细说NP 221 5.2.3 细说VP 223 5.2.4 其他语义块 227 5.2.5 语义块的抽取 229 5.2.6 CRF的使用 232 5.3 命名实体识别 240 5.3.1 命名实体 241 5.3.2 分词架构与专名词典 243 5.3.3 算法的策略——词典与统计相结合 245 5.3.4 算法的策略——层叠式架构 252 5.4 结语 259 第6章 句法理论与自动分析 260 6.1 转换生成语法 261 6.1.1 乔姆斯基的语言观 261 6.1.2 短语结构文法 263 6.1.3 汉语句类 269 6.1.4 谓词论元与空范畴 274 6.1.5 轻动词分析理论 279 6.1.6 NLTK操作句法树 280 6.2 依存句法理论 283 6.2.1 配价理论 283 6.2.2 配价词典 285 6.2.3 依存理论概述 287 6.2.4 Ltp依存分析介绍 290 6.2.5 Stanford依存转换、解析 293 6.3 PCFG短语结构句法分析 298 6.3.1 PCFG短语结构 298 6.3.2 内向算法和外向算法 301 6.3.3 Viterbi算法 303 6.3.4 参数估计 304 6.3.5 Stanford的PCFG算法训练 305 6.4 结语 310 第7章 建设语言资源库 311 7.1 语料库概述 311 7.1.1 语料库的简史 312 7.1.2 语言资源库的分类 314 7.1.3 语料库的设计实例：国家语委语料库 315 7.1.4 语料库的层次加工 321 7.2 语法语料库 323 7.2.1 中文分词语料库 323 7.2.2 中文分词的测评 326 7.2.3 宾州大学CTB简介 327 7.3 语义知识库 333 7.3.1 知识库与HowNet简介 333 7.3.2 发掘义原 334 7.3.3 语义角色 336 7.3.4 分类原则与事件分类 344 7.3.5 实体分类 347 7.3.6 属性与分类 352 7.3.7 相似度计算与实例 353 7.4 语义网与百科知识库 360 7.4.1 语义网理论介绍 360 7.4.2 维基百科知识库 364 7.4.3 DBpedia抽取原理 365 7.5 结语 368 第8章 语义与认知 370 8.1 回顾现代语义学 371 8.1.1 语义三角论 371 8.1.2 语义场论 373 8.1.3 基于逻辑的语义学 376 8.2 认知语言学概述 377 8.2.1 象似性原理 379 8.2.2 顺序象似性 380 8.2.3 距离象似性 380 8.2.4 重叠象似性 381 8.3 意象图式的构成 383 8.3.1 主观性与焦点 383 8.3.2 范畴化：概念的认知 385 8.3.3 主体与背景 390 8.3.4 意象图式 392 8.3.5 社交中的图式 396 8.3.6 完形：压缩与省略 398 8.4 隐喻与转喻 401 8.4.1 隐喻的结构 402 8.4.2 隐喻的认知本质 403 8.4.3 隐喻计算的系统架构 405 8.4.4 隐喻计算的实现 408 8.5 构式语法 412 8.5.1 构式的概念 413 8.5.2 句法与构式 415 8.5.3 构式知识库 417 8.6 结语 420 第9章 NLP中的深度学习 422 9.1 神经网络回顾 422 9.1.1 神经网络框架 423 9.1.2 梯度下降法推导 425 9.1.3 梯度下降法的实现 427 9.1.4 BP神经网络介绍和推导 430 9.2 Word2Vec简介 433 9.2.1 词向量及其表达 434 9.2.2 Word2Vec的算法原理 436 9.2.3 训练词向量 439 9.2.4 大规模上下位关系的自动识别 443 9.3 NLP与RNN 448 9.3.1Simple-RNN 449 9.3.2 LSTM原理 454 9.3.3 LSTM的Python实现 460 9.4 深度学习框架与应用 467 9.4.1 Keras框架介绍 467 9.4.2 Keras序列标注 471 9.4.3 依存句法的算法原理 478 9.4.4 Stanford依存解析的训练过程 483 9.5 结语 488 第10章 语义计算的架构 490 10.1 句子的语义和语法预处理 490 10.1.1 长句切分和融合 491 10.1.2 共指消解 496 10.2 语义角色 502 10.2.1 谓词论元与语义角色 502 10.2.2PropBank简介 505 10.2.3 CPB中的特殊句式 506 10.2.4 名词性谓词的语义角色 509 10.2.5PropBank展开 512 10.3 句子的语义解析 517 10.3.1 语义依存 517 10.3.2 完整架构 524 10.3.3 实体关系抽取 527 10.4 结语 531 [29] 自然语言处理NLP国内研究方向机构导师 文|中文信息协会《中文信息处理发展报告2016》，数据简化DataSimp 文字语言VS数字信息 数字、文字和自然语言一样，都是信息的载体，他们之间原本有着天然的联系。语言和数学的产生都是为了交流，从文字、数字和语言的发展历史，可以了解到语言、文字和数字有着内在的联系。自然语言处理NLP主要涉及三种文本，自由文本、结构化文本、半结构化文本。 自然语言理解Natural Language Understanding(NLU)，实现人机间自然语言通信，意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本表达给定的意图、思想等。自然语言生成NLG，是人工或机器生成语言。 斯坦福自然语言处理NLP工具资料收集、斯坦福分词、Stanford中文实体识别，最早做自然语言处理的网址https://nlp.stanford.edu/software/segmenter.shtml。 哈尔滨工业大学智能技术与自然语言处理研究室(IntelligentTechnology &amp; Natural Language Processing Lab, ITNLPLab)是国内较早从事自然语言处理和语言智能技术的研究室。 除了新兴的文本数据简化领域：秦陇纪(数据简化技术中心筹)，自然语言处理NaturalLanguage Processing领域主要包括基础研究和应用研究。 基础研究 词法与句法分析：李正华、陈文亮、张民(苏州大学) 语义分析：周国栋、李军辉(苏州大学) 篇章分析：王厚峰、李素建(北京大学) 语言认知模型：王少楠，宗成庆(中科院自动化研究所) 语言表示与深度学习：黄萱菁、邱锡鹏(复旦大学) 知识图谱与计算：李涓子、候磊(清华大学) 应用研究 文本分类与聚类：涂存超，刘知远(清华大学) 信息抽取：孙乐、韩先培(中国科学院软件研究所) 情感分析：黄民烈(清华大学) 自动文摘：万小军、姚金戈(北京大学) 信息检索：刘奕群、马少平(清华大学) 信息推荐与过滤：王斌(中科院信工所)，鲁骁(国家计算机网络应急中心) 自动问答：赵军、刘康，何世柱(中科院自动化研究所) 机器翻译：张家俊、宗成庆(中科院自动化研究所) 社会媒体处理：刘挺、丁效(哈尔滨工业大学) 语音技术：说话人识别——郑方(清华大学)，王仁宇(江苏师范大学) 语音合成——陶建华(中科院自动化研究所) 语音识别——王东(清华大学) 文字识别：刘成林(中科院自动化研究所) 多模态信息处理：陈晓鸥(北京大学) 医疗健康信息处理：陈清财、汤步洲(哈尔滨工业大学) 少数民族语言信息处理：吾守尔•斯拉木(新疆大学) —&nbsp;完&nbsp;—" />
<meta property="og:description" content="转载自 数据简化DataSimp&nbsp; 作者 秦陇纪&nbsp; 郭一璞 编辑&nbsp; 量子位 报道 | 公众号 QbitAI 本篇推送包含三篇文章， 《自然语言处理技术发展史十大里程碑》 《语言处理NLP知识结构》 《自然语言处理NLP国内研究方向机构导师》 总共超过20000字，量子位建议先码再看。 自然语言处理技术发展史十大里程碑 文|秦陇纪，参考|黄昌宁、张小凤、Sebatian Ruder 自然语言是人类独有的智慧结晶。 自然语言处理(NaturalLanguage Processing，NLP)是计算机科学领域与人工智能领域中的一个重要方向，旨在研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。用自然语言与计算机进行通信，有着十分重要的实际应用意义，也有着革命性的理论意义。 由于理解自然语言，需要关于外在世界的广泛知识以及运用操作这些知识的能力，所以自然语言处理，也被视为解决人工智能完备(AI-complete)的核心问题之一。对自然语言处理的研究也是充满魅力和挑战的。 微软亚洲研究院黄昌宁、张小凤在2013年发表论文，就过去50年以来自然语言处理(NLP)研究领域中的发现和发展要点进行阐述，其中包括两个事实和三大重要成果。 近年来，自然语言处理的语料库调查显示如下两个事实： (1)对于句法分析来说，基于单一标记的短语结构规则是不充分的；单个标记的PSG规则不足以进行自然语言描述； (2)PSG规则在文本语料库中具有偏差分布，即PSG规则的总数似乎不能够涵盖大型语料库中发现的语言现象，这不符合语言学家的期望。短语结构规则在真实文本中的分布呈现严重扭曲。换言之，有限数目的短语结构规则不能覆盖大规模语料中的语法现象。这与原先人们的预期大相径庭。 NLP技术发展历程在很大程度上受到以上两个事实的影响，在该领域中可以称得上里程碑式的成果有如下三个： (1)复杂特征集和合一语法； (2)语言学研究中的词汇主义； (3)语料库方法和统计语言模型。业内人士普遍认为，大规模语言知识的开发和自动获取是NLP技术的瓶颈问题。因此，语料库建设和统计学习理论将成为该领域中的关键课题。 一、NLP研究传统问题 自然语言处理(NLP)是计算机科学、信息工程和人工智能的子领域，涉及计算机和人类(自然)语言之间的交互，尤其是编程实现计算机处理和分析大量自然语言数据。自然语言处理的挑战包括语音识别，自然语言理解和自然语言生成。 信息输入、检索、人机对话等需求增多，使自然语言处理(NLP)成为21世纪初的热门学科。从50年代机器翻译和人工智能研究算起，NLP至今有长达半个世纪的历史了。 近年来这一领域中里程碑式的理论和方法贡献有如下三个： (1)复杂特征集和合一语法； (2)语言学研究中的词汇主义； (3)语料库方法和统计语言模型。 这三个成果将继续对语言学、计算语言学和NLP的研究产生深远影响。为了理解这些成果的意义，先介绍一下两个相关事实。 自然语言处理中识别句子 句法结构的句法分析的全过程： (1)把句子中的词一个一个地切分出来； (2)查词典，给句子中的每个词指派一个合适的词性(part of speech)； (3)用句法规则把句子里包含的句法成分，如名词短语、动词短语、小句等，逐个地识别出来。 (4)判断每个短语的句法功能，如主语、谓语、宾语等，及其语义角色，最终得到句子的意义表示，如逻辑语义表达式。 1.1、事实一：语言的结构歧义问题 第一个事实(黄昌宁，张小凤，2013)是：短语结构语法(PhraseStructure Grammar，简称PSG)不能有效地描写自然语言。 PSG在Chomsky的语言学理论[1]中占有重要地位，并且在自然语言的句法描写中担当着举足轻重的角色。但是它有一些根本性的弱点，主要表现为它使用的是像词类和短语类那样的单一标记，因此不能有效地指明和解释自然语言中的结构歧义问题。 让我们先来看一看汉语中“V+N”组合。假如我们把“打击，委托，调查”等词指派为动词(V)；把“力度，方式，盗版，甲方”等词视为名词(N)，而且同意“打击力度”、“委托方式”是名词短语(NP)，“打击盗版”、“委托甲方”是动词短语(VP)，那么就会产生如下两条有歧义的句法规则： (1) NP → V N (2) VP → V N 换句话讲，当计算机观察到文本中相邻出现的“V+N”词类序列时，仍不能确定它们组成的究竟是NP还是VP。我们把这样的歧义叫做“短语类型歧义”。例如： • 该公司正在招聘[销售V人员N]NP。 • 地球在不断[改变V形状N]VP。 下面再来看“N+V”的组合，也同样会产生带有短语类型歧义的规则对，如： (3) NP → N V 例：市场调查；政治影响。 (4) S → N V 例：价格攀升；局势稳定。 其中标记S代表小句。 不仅如此，有时当机器观察到相邻出现的“N+V”词类序列时，甚至不能判断它们是不是在同一个短语中。也就是说，“N+V”词类序列可能组成名词短语NP或小句S，也有可能根本就不在同一个短语里。后面这种歧义称为“短语边界歧义”。下面是两个相关的例句： • 中国的[铁路N建设V]NP发展很快。 • [中国的铁路N]NP建设V得很快。 前一个例句中，“铁路建设”组成一个NP；而在后一个例句中，这两个相邻的词却分属于两个不同的短语。这足以说明，基于单一标记的PSG不能充分地描述自然语言中的句法歧义现象。下面让我们再来看一些这样的例子。 (5)NP → V N1de N2 (6)VP → V N1de N2 其中de代表结构助词“的”。例如，“[削苹果]VP的刀”是NP; 而“削[苹果的皮]NP”则是VP。这里既有短语类型歧义，又有短语边界歧义。比如，“削V苹果N”这两个相邻的词，可能构成一个VP，也可能分处于两个相邻的短语中。 (7)NP → P N1de N2 (8)PP → P N1de N2 规则中P和PP分别表示介词和介词短语。例如，“[对上海]PP的印象”是NP; 而“对[上海的学生]NP”则是PP。相邻词“对P 上海N”可能组成一个PP，也可能分处于两个短语中。 (9)NP → NumPN1 de N2 其中NumP 表示数量短语。规则(9)虽然表示的是一个NP，但可分别代表两种结构意义： (9a)NumP [N1de N2]NP 如：五个[公司的职员]NP (9b)[NumPN1]NP de N2 如：[五个公司]NP 的职员 (10)NP → N1 N2N3 规则(10)表示的也是一个NP，但“N1+ N2”先结合，还是“N2 +N3”先结合，会出现两种不同的结构方式和意义，即： (10a)[N1 N2]NPN3 如：[现代汉语]NP 词典 (10b)N1 [N2N3]NP 如：新版[汉语词典]NP 以上讨论的第一个事实说明： 由于约束力不够，单一标记的PSG规则不能充分消解短语类型和短语边界的歧义。用数学的语言来讲，PSG规则是必要的，却不是充分的。因此机器仅仅根据规则右边的一个词类序列来判断它是不是一个短语，或者是什么短语，其实都有某种不确定性。 采用复杂特征集和词汇主义方法来重建自然语言的语法系统，是近二十年来全球语言学界就此作出的最重要的努力。 1.2、事实二：词频统计的齐夫律 通过大规模语料的调查，人们发现一种语言的短语规则的分布也符合所谓的齐夫率(Zipf’s Law)。 Zipf是一个统计学家和语言学家。他提出，如果对某个语言单位(不论是英语的字母或词)进行统计，把这个语言单位在一个语料库里出现的频度(frequency)记作F，而且根据频度的降序对每个单元指派一个整数的阶次(rank) R。结果发现R和F的乘积近似为一个常数。即 F*R ≈ const (常数) 被观察的语言单元的阶次R与其频度F成反比关系。词频统计方面齐夫律显示，不管被考察的语料仅仅一本长篇小说，还是一个大规模的语料库，最常出现的100个词的出现次数会占到语料库总词次数(tokens)的近一半。 假如语料库的规模是100万词次，那么其中频度最高的100个词的累计出现次数大概是50万词次。如果整个语料库含有5万词型(types)，那么其中的一半(也就是2.5万条左右)在该语料库中只出现过一次。即使把语料库的规模加大十倍，变成1000万词次，统计规律大体不变。 有趣的是，80年代英国人Sampson对英语语料库中的PSG规则进行统计，发现它们的分布同样是扭曲的，大体表现为齐夫率。也就是说，一方面经常遇到的语法规则只有几十条左右，它们的出现频度非常非常高；另一方面，规则库中大约一半左右的规则在语料库中只出现过一次。 随着语料库规模的扩大，新的规则仍不断呈现。Noam Chomsky曾提出过这样的假设，认为对一种自然语言来说，其语法规则的数目总是有限的，但据此生成的句子数目却是无限的。但语料库调查的结果不是这个样子。这个发现至少说明，单纯依靠语言学家的语感来编写语法规则不可能胜任大规模真实文本处理的需求，必须寻找可以从语料库中直接获取大规模语言知识的新方法。 几十年来，NLP学界曾发表过许多灿烂成果，有词法学、语法学、语义学的，有句法分析算法的，还有众多著名的自然语言应用系统。那么究竟什么是对该领域影响最大的、里程碑式的成果呢？ 二、NLP十大里程碑 2.1、里程碑一：1985复杂特征集 复杂特征集(complex feature set)又叫做多重属性(multiple features)描写。语言学里，这种描写方法最早出现在语音学中。美国计算语言学家Martin Kay于1985年在“功能合一语法”(FunctionalUnification Grammar，简称FUG)新语法理论中，提出“复杂特征集”(complex feature set)概念。后来被Chomsky学派采用来扩展PSG的描写能力。 △&nbsp;美国计算语言学家Martin Kay 现在在语言学界、计算语言学界，语法系统在词汇层的描写中常采用复杂特征集，利用这些属性来强化句法规则的约束力。一个复杂特征集F包含任意多个特征名fi和特征值vi对。其形式如： F = {…, fi=vi, …}, i=1,…,n 特征值vi既可以是一个简单的数字或符号，也可以是另外一个复杂特征集。这种递归式的定义使复杂特征集获得了强大的表现能力。举例来说，北京大学俞士汶开发的《现代汉语语法信息词典》[10]，对一个动词定义了约40项属性描写，对一个名词定义了约27项属性描写。 一条含有词汇和短语属性约束的句法规则具有如下的一般形式： : &lt;属性约束&gt; : &lt;属性传递&gt; 一般来说，PSG规则包括右部(条件：符号序列的匹配模式)和左部(动作：短语归并结果)。词语的“属性约束”直接来自系统的词库，而短语的“属性约束”则是在自底向上的短语归并过程中从其构成成分的中心语(head)那里继承过来的。在Chomsky的理论中这叫做X-bar理论。 X-bar代表某个词类X所构成的、仍具有该词类属性的一个成分。如果X=N，就是一个具有名词特性的N-bar。当一条PSG规则的右部匹配成功，且“属性约束”部分得到满足，这条规则才能被执行。此时，规则左部所命名的的短语被生成，该短语的复杂特征集通过“属性传递”部分动态生成。 80年代末、90年代初学术界提出了一系列新的语法，如广义短语结构语法(GPSG)、中心语驱动的短语结构语法(HPSG)、词汇功能语法(LFG)等等。这些形式语法其实都是在词汇和短语的复杂特征集描写背景下产生的。合一(unification)算法则是针对复杂特征集的运算而提出来的。“合一”是实现属性匹配和赋值的一种算法，所以上述这些新语法又统称为“基于合一的语法”。 2.2、里程碑二：1966词汇主义 NLP领域第二个里程碑式贡献是词汇主义(lexicalism)。 1966年，韩礼德(Halliday)提出词汇不是用来填充语法确定的一套“空位”(slots)，而是一个独立的语言学层面；词汇研究可以作为对语法理论的补充，却不是语法理论的一部分，他主张把词汇从语法研究中独立地分离出来。 语言学家Hudson宣称，词汇主义是当今语言学理论头号发展倾向。出现原因也同上节两事实有关。词汇主义方法不仅提出一种颗粒度更细的语言知识表示形式，而且体现一语言知识递增式开发和积累的新思路。 首先解释一个背景矛盾。 一方面，语言学界一向认为，不划分词类就无法讲语法，如前面介绍的短语结构语法，语法“不可能”根据个别单独的词来写规则。但是另一方面，人们近来又注意到，任何归类其实都会丢失个体的某些重要信息。所以从前文提到的第一个事实出发，要想强化语法约束能力，词汇的描写应当深入到比词类更细微的词语本身上来。 换句话讲，语言学呼唤在词汇层采用颗粒度更小的描写单元。从本质上来说，词汇主义倾向反映了语言描写的主体已经从句法层转移到了词汇层；这也就是所谓的“小语法，大词库”的思想。下面让我们来看与词汇主义有关的一些工作。 2.2.1、词汇语法学(Lexicon-grammar) 法国巴黎大学Gross教授60年代创立研究中心LADL(http://www.ladl.jussieu.fr/)，提出了词汇语法的概念。 • 把12,000个主要动词分成50个子类。 • 每个动词都有一个特定的论元集。 • 每一类动词都有一个特定的矩阵, 其中每个动词都用400个不同句式来逐一描写(“+”代表可进入该句式；“-”表示不能)。 • 已开发英、法、德、西等欧洲语言的大规模描写。 • INTEX是一个适用于大规模语料分析的工具，已先后被世界五十多个研究中心采用。 2.2.2、框架语义学(Frame Semantics) Fillmore是格语法(Case Grammar)创始人，前几年主持美国自然科学基金的一个名为框架语义学的项目(http://www.icsi.berkeley.edu/~framenet)。该项目从WordNet上选取了2000个动词，从中得到75个语义框架。例如，动词”categorize”的框架被定义为: 一个人(Cognizer)把某个对象(Item)视为某个类(Category)。 同原先的格框架相比，原来一般化的动作主体被具体化为认知者Cognizer，动作客体被具体化为事物Item，并根据特定体动词的性质增加了一个作为分类结果的语义角色Category。 项目组还从英国国家语料库中挑出50,000个相关句子，通过人工给每个句子标注了相应的语义角色。例句： Kimcategorized the book as fiction. (Cog) (Itm)(Cat) 2.2.3、WordNet WordNet(http://www.cogsci.princeton.edu:80/~wn/)是一个描写英语词汇层语义关系的词库，1990年由普林斯顿大学Miller开发。至今有很多版本，全部公布在因特网上，供研究人员自由下载。 欧洲有一个Euro-WordNet，以类似的格式来表现各种欧洲语言的词汇层语义关系。WordNet刻意描写的是词语之间的各种语义关系，如同义关系(synonymy)、反义关系(antonymy)、上下位关系(hyponymy)，部分-整体关系(part-of)等等。 这种词汇语义学又叫做关系语义学，这一学派同传统的语义场理论和和语义属性描写理论相比，其最大的优势在于第一次在一种语言的整个词汇表上实现了词汇层的语义描写。这是其他学派从来没有做到的。其他理论迄今仅仅停留在教科书或某些学术论文中，从来就没有得到工程规模的应用。下面是WordNet的概况： • 95,600条实词词型(动词、名词、形容词) • 被划分成70,100个同义词集(synsets) 2.2.4 知网网(How-Net) 知网是董振东和董强设计的一个汉语语义知识网(http://www.keenage.com)，访问只有主页。 • 自下而上地依据概念对汉语实词进行了穷尽的分类。 • 15,000个动词被划分成810类。 • 定义了300个名词类，100个形容词类。 • 全部概念用400个语义元语来定义。 知网特点是既有WordNet所描写的同一类词间语义关系(如：同义、反义、上下位、部分-整体等)，又描写不同类词之间的论旨关系和语义角色。 2.2.5 MindNet MindNet是微软研究院NLP组设计的词汇语义网(http://research.microsoft.com/nlp/)，用三元组(triple)作为全部知识的表示基元。一个三元组由两个节点和一条连接边组成。每个节点代表一个概念，连接两个概念节点的边表示概念之间的语义依存关系。全部三元组通过句法分析器自动获取。 具体通过对两部英语词典(Longman Dictionaryof Contemporary English，AmericanHeritage Dictionary)和一部百科全书(Encarta)中的全部句子进行分析，获得每个句子的逻辑语义表示(logical form，简称LF)。 而LF本来就是由三元组构成的，如(W1, V-Obj,W2)表示：W1是一个动词，W2是其宾语中的中心词，因此W2从属于W1，它们之间的关系是V-Obj。比如(play, V-Obj,basketball)便是一个具体的三元组。又如(W1, H-Mod,W2)，W1代表一个偏正短语中的中心词(head word)，W2是其修饰语(modifier)，因此W2从属于W1，它们之间的关系是H-Mod。 这种资源是完全自动做出来的，所得三元组不可能没有错误。但是那些出现频度很高的三元组一般来说正确。MindNet已经应用到像语法检查、句法结构排歧、词义排歧、机器翻译等许多场合。 2.3 里程碑三：1976统计语言模型 第三大贡献是语料库方法，或叫统计语言模型。 首先成功利用数学方法解决自然语言处理问题的是语音和语言处理大师弗雷德·贾里尼克(Fred Jelinek)。1968年始在IBM研究中心兼职1974年全职加入，他领导一批杰出科学家利用大型计算机处理人类语言问题，学术休假(SabbaticalLeave)时(约1972-1976年间)提出统计语言模型。 1990s李开复用统计语言模型把997个词的语音识别问题简化成了20词识别问题，实现了有史以来第一次大词汇量非特定人连续语言的识别。常用统计语言模型，包括N元文法模型(N-gram Model)、隐马尔科夫模型(Hidden MarkovModel，简称HMM)、最大熵模型(MaximumEntropy Model)等。 △&nbsp;美国工程院院士Frederick Jelinek 如果用变量W代表一个文本中顺序排列的n个词，即W = w1w2…wn，则统计语言模型的任务是给出任意一个词序列W在文本中出现的概率P(W)。 利用概率的乘积公式，P(W)可展开为： P(W) =P(w1)P(w2/w1)P(w3/ w1 w2)…P(wn/w1 w2…wn-1)&nbsp;(1) 式中P(w1)表示第一个词w1的出现概率，P(w2/w1)表示在w1出现的情况下第二个词w2出现的条件概率，依此类推。 不难看出，为了预测词wn的出现概率，必须已知它前面所有词的出现概率。从计算上来看，这太复杂了。如果近似认为任意一个词wi的出现概率只同它紧邻的前一个词有关，那么计算就得以大大简化。这就是所谓的二元模型(bigram)，由(1)式得： P(W) ≈ P(w1)∏i=2,…,nP(wi/ wi-1 )&nbsp;(2) 式中∏i=2,…,nP(wi/ wi-1 )表示多个概率的连乘。 需要着重指出的是：这些概率参数都可以通过大规模语料库来估值。比如二元概率 P(wi/ wi-1) ≈count(wi-1 wi) / count(wi-1)&nbsp;(3) 式中count(…)表示一个特定词序列在整个语料库中出现的累计次数。若语料库的总词次数为N，则任意词wi在该语料库中的出现概率可估计如下： P(wi) ≈count(wi) / N&nbsp;(4) 同理，如果近似认为任意词wi的出现只同它紧邻前两个词有关，就得到一个三元模型(trigram)： P(W) ≈P(w1)P(w2/w1) ∏i=3,…,nP(wi/wi-2 w-1 )&nbsp;(5) 统计语言模型的方法有点像天气预报。用来估计概率参数的大规模语料库好比是一个地区历年积累起来的气象记录，而用三元模型来做天气预报，就像是根据前两天的天气情况来预测当天的天气。天气预报当然不可能百分之百正确。这也算是概率统计方法的一个特点。 2.3.1、语音识别 语音识别作为计算机汉字键盘输入的一种图代方式，越来越受到信息界人士的青睐。所谓听写机就是这样的商品。据报道中国的移动电话用户已超过一亿，随着移动电话和个人数字助理(PDA)的普及，尤其是当这些随身携带的器件都可以无线上网的时候，广大用户更迫切期望通过语音识别或手写板而不是小键盘来输入简短的文字信息。 其实，语音识别任务可视为计算以下条件概率的极大值问题： W*= argmaxWP(W/speech signal) = argmaxWP(speech signal/W) P(W) / P(speech signal) = argmaxWP(speech signal/W) P(W)&nbsp;(6) 式中数学符号argmaxW表示对不同的候选词序列W计算条件概率P(W/speech signal)的值，从而使W*成为其中条件概率值最大的那个词序列，这也就是计算机选定的识别结果。换句话讲，通过式(6)的计算，计算机找到了最适合当前输入语音信号speech signal的词串W。 式(6)第二行是利用贝叶斯定律转写的结果，因为条件概率P(speech signal/W)比较容易估值。公式的分母P(speech signa)对给定的语音信号是一个常数，不影响极大值的计算，故可以从公式中删除。在第三行所示的结果中，P(W)就是前面所讲得统计语言模型，一般采用式(5)所示的三元模型；P(speechsignal/W)叫做声学模型。 讲到这儿，细心的读者可能已经明白，汉语拼音输入法中的拼音－汉字转换任务其实也是用同样方法实现的，而且两者所用的汉语语言模型(即二元或三元模型)是同一个模型。 据笔者所知，目前市场上的听写机产品和微软拼音输入法(3.0版)都是用词的三元模型实现的，几乎完全不用句法-语义分析手段。为什么会出现这样的局面呢？这是优胜劣汰的客观规律所决定的。可比的评测结果表明，用三元模型实现的拼音-汉字转换系统，其出错率比其它产品减少约50%。 2.3.2、词性标注 一个词库中大约14%的词型具有不只一个词性。而在一个语料库中，占总词次数约30%的词具有不止一个词性。所以对一个文本中的每一个词进行词性标注，就是通过上下文的约束，实现词性歧义的消解。历史上曾经先后出现过两个自动词性标注系统。一个采用上下文相关的规则，叫做TAGGIT(1971)，另一个应用词类的二元模型，叫做CLAWS(1987)。 两个系统都分别对100万词次的英语非受限文本实施了词性标注。结果显示，采用统计语言模型的CLAWS系统的标注正确率大大高于基于规则方法的TAGGIT系统。请看下表的对比： 令C和W分别代表词类标记序列和词序列，则词性标注问题可视为计算以下条件概率的极大值: C*= argmaxCP(C/W) = argmaxCP(W/C)P(C) / P(W) ≈ argmaxC∏i=1,…,nP(wi/ci )P(ci /ci-1 )&nbsp;(7) 式中P(C/W)是已知输入词序列W的情况下，出现词类标记序列C的条件概率。数学符号argmaxC表示通过考察不同的候选词类标记序列C，来寻找使条件概率取最大值的那个词类标记序列C*。后者应当就是对W的词性标注结果。 公式第二行是利用贝叶斯定律转写的结果，由于分母P(W)对给定的W是一个常数，不影响极大值的计算，可以从公式中删除。接着对公式进行近似。首先，引入独立性假设，认为任意一个词wi的出现概率近似只同当前词的词类标记ci有关，而与周围(上下文)的词类标记无关。于是词汇概率可计算如下： P(W/C) ≈∏i=1,…,n P(wi/ci )&nbsp;(8) 其次，采用二元假设，即近似认为任意一个词类标记ci的出现概率只同它紧邻的前一个词类标记ci-1有关。有 P(C) ≈ P(c1)∏i=2,…,n P(ci /ci-1 )&nbsp;(9) P(ci /ci-1 )是词类标记的转移概率，也叫做基于词类的二元模型。 上述这两个概率参数都可以通过带词性标记的语料库来分别估计： P(wi/ci ) ≈count(wi,ci) / count(ci)&nbsp;(10) P(ci /ci-1 ) ≈count(ci-1ci) / count(ci-1)&nbsp;(11) 据文献报道，采用统计语言模型方法汉语和英语的次性标注正确率都可以达到96%左右[6]。 2.3.3、介词短语PP的依附歧义 英语中介词短语究竟依附于前面的名词还是前面的动词，是句法分析中常见的结构歧义问题。下例用语料库方法来解决这个问题，以及这种方法究竟能达到多高的正确率。 例句：Pierre Vinken,61 years old, joined the board as a nonexecutive director. 令A=1表示名词依附，A=0为动词依附，则上述例句的PP依附问题可表为： (A=0,V=joined, N1=board, P=as, N2=director) 令V, N1, N2分别代表句中动词短语、宾语短语、介宾短语的中心词，并在一个带有句法标注的语料库(又称树库)中统计如下四元组的概率Pr： Pr = (A=1 /V=v, N1=n1, P=p, N2=n2)&nbsp;(10) 对输入句子进行PP 依附判断的算法如下： 若Pr = (1 / v, n1, p, n2) ≥ 0.5, 则判定PP依附于n1, 否则判定PP依附于v。 Collins和Brooks实验使用的语料库是宾夕法尼亚大学标注的华尔街日报(WSJ)树库，包括：训练集20,801个四元组，测试集3,097个四元组。他们对PP依附自动判定精度的上下限作了如下分析： 一律视为名词依附(即A≡1) 59.0% 只考虑介词p的最常见附加72.2% 三位专家只根据四个中心词判断88.2% 三位专家根据全句判断93.2% 很明显，自动判断精确率的下限是72.2%，因为机器不会比只考虑句中介词p的最常见依附做得更差了；上限是88.2%，因为机器不可能比三位专家根据四个中心词作出的判断更高明。 论文报告，在被测试的3,097个四元组中，系统正确判断的四元组为2,606个，因此平均精确率为84.1%。这与上面提到的上限值88.2%相比，应该说是相当不错的结果。 传统三大技术里程碑小结 语言学家在不论是复杂特征集和合一语法，还是词汇主义方法，都是原先所谓的理性主义框架下做出的重大贡献。词汇主义方法提出了一种颗粒度更细的语言知识表示形式，而且体现了一种语言知识递增式开发和积累的新思路，值得特别推崇。 尤其值得重视的是，在众多词汇资源的开发过程中，语料库和统计学习方法发挥了很大的作用。这是经验主义方法和理性主义方法相互融合的可喜开端，也是国内知名语言学者冯志伟等人认可的研究范式。 语料库方法和统计语言模型，国内同行中实际上存在不同评价。有种观点认为NLP必须建立在语言理解基础上，他们不大相信统计语言模型在语音识别、词性标注、信息检索等应用领域中所取得的进展。这些争论不能澄清，是因为同行间缺少统一评测。有评测才会有鉴别。 评判某方法优劣应公开、公平、相互可比的评测标准，而非研究员设计“自评”。黄昌宁、张小凤2013年论文表示，语料库方法和统计语言模型是当前自然语言处理技术的主流，其实用价值已在很多应用系统中得到充分证实。统计语言模型研究在结构化对象的统计建模方面，仍有广阔发展空间。 自然语言处理领域业界知名博主Sebatian Ruder在2018年文章从神经网络技术角度，总结NLP领域近15年重大进展、8大里程碑事件，提及很多神经网络模型。这些模型建立在同一时期非神经网络技术之上，如上述三大里程碑。下面接着看后续NLP技术的发展。 2.4、里程碑四：2001神经语言模型(Neural language models) 语言模型解决的是在给定已出现词语的文本中，预测下一个单词的任务。这是最简单的语言处理任务，有许多具体实际应用，如智能键盘、电子邮件回复建议等。语言模型历史由来已久，经典方法基于n-grams模型(利用前面n个词语预测下一个单词)，并利用平滑操作处理不可见的n-grams。 第一个神经语言模型，前馈神经网络(feed-forward neural network)，是Bengio等人于2001年提出的。模型以某词语之前出现的n个词语作为输入向量，也就是现在大家说的词嵌入(word embeddings)向量。这些词嵌入在级联后进入一个隐藏层，该层的输出然后通过一个softmax层。如图3所示。 △&nbsp;前馈神经网络语言模型 而现在构建语言模型的前馈神经网络，已被循环神经网络(RNNs)和长短期记忆神经网络(LSTMs)取代。 虽然后来提出许多新模型在经典LSTM上进行了扩展，但它仍然是强有力的基础模型。甚至Bengio等人的经典前馈神经网络在某些设定下也和更复杂的模型效果相当，因为这些任务只需要考虑邻近的词语。理解这些语言模型究竟捕捉了哪些信息，也是当今一个活跃的研究领域。 语言模型的建立是一种无监督学习(unsupervisedlearning)，Yann LeCun称之为预测学习(predictivelearning)，是获得世界如何运作常识的先决条件。 关于语言模型最引人注目的是，尽管它很简单，但却与后文许多核心进展息息相关。反过来，这也意味着NLP领域许多重要进展都可以简化为某种形式的语言模型构建。但要实现对自然语言真正意义上的理解，仅仅从原始文本中进行学习是不够的，我们需要新的方法和模型。 2.5、里程碑五：2008多任务学习(Multi-task learning) 多任务学习是在多个任务下训练的模型之间共享参数的方法，在神经网络中通过捆绑不同层的权重轻松实现。多任务学习思想1993年Rich Caruana首次提出，并应用于道路追踪和肺炎预测。多任务学习鼓励模型学习对多个任务有效的表征描述。这对于学习一般的、低级的描述形式、集中模型的注意力或在训练数据有限的环境中特别有用。 多任务学习2008年被Collobert和Weston等人首次在自然语言处理领域应用于神经网络。在他们的模型中，词嵌入矩阵被两个在不同任务下训练的模型共享，如图4所示。 △&nbsp;词嵌入矩阵共享 共享的词嵌入矩阵使模型可以相互协作，共享矩阵中的低层级信息，而词嵌入矩阵往往构成了模型中需要训练的绝大部分参数。 Collobert和Weston发表于2008年的论文，影响远远超过了它在多任务学习中的应用。它开创的诸如预训练词嵌入和使用卷积神经网络处理文本的方法，在接下来的几年被广泛应用。他们也因此获得2018年机器学习国际会议(ICML)的test-of-time奖。 如今，多任务学习在自然语言处理领域广泛使用，而利用现有或“人工”任务已经成为NLP指令库中的一个有用工具。 虽然参数的共享是预先定义好的，但在优化的过程中却可以学习不同的共享模式。当模型越来越多地在多个任务上进行测评以评估其泛化能力时，多任务学习就变得愈加重要，近年来也涌现出更多针对多任务学习的评估基准。 2.6、里程碑六：2013词嵌入 稀疏向量对文本进行表示的词袋模型，在自然语言处理领域有很长历史。而用稠密的向量对词语进行描述，也就是词嵌入，则在2001年首次出现。2013年Mikolov等人工作主要创新之处在于，通过去除隐藏层和近似计算目标使词嵌入模型的训练更为高效。 尽管这些改变本质上十分简单，但它们与高效的word2vec(wordto vector用来产生词向量的相关模型)组合在一起，使得大规模的词嵌入模型训练成为可能。 Word2vec有两种不同的实现方法：CBOW(continuousbag-of-words)和skip-gram。它们在预测目标上有所不同：一个是根据周围的词语预测中心词语，另一个则恰恰相反。如图5所示。 △&nbsp;CBOW和skip-gram架构 虽然这些嵌入与使用前馈神经网络学习的嵌入在概念上没有区别，但是在一个非常大语料库上的训练使它们能够获取诸如性别、动词时态和国际事务等单词之间的特定关系。如下图 4 所示。 △&nbsp;word2vec捕获的联系 这些关系和它们背后的意义激起了人们对词嵌入的兴趣，许多研究都在关注这些线性关系的来源。然而，使词嵌入成为目前自然语言处理领域中流砥柱的，是将预训练的词嵌入矩阵用于初始化可以提高大量下游任务性能的事实。 虽然word2vec捕捉到的关系具有直观且几乎不可思议的特性，但后来的研究表明，word2vec本身并没有什么特殊之处：词嵌入也可以通过矩阵分解来学习，经过适当的调试，经典的矩阵分解方法SVD和LSA都可以获得相似的结果。从那时起，大量的工作开始探索词嵌入的不同方面。尽管有很多发展，word2vec仍是目前应用最为广泛的选择。 Word2vec应用范围也超出了词语级别：带有负采样的skip-gram——一个基于上下文学习词嵌入的方便目标，已经被用于学习句子的表征。它甚至超越了自然语言处理的范围，被应用于网络和生物序列等领域。 一个激动人心的研究方向是在同一空间中构建不同语言的词嵌入模型，以达到(零样本)跨语言转换的目的。通过无监督学习构建这样的映射变得越来越有希望(至少对于相似的语言来说)，这也为语料资源较少的语言和无监督机器翻译的应用程序创造可能。 2.7、里程碑七：2013RNN/CNN用于NLP的神经网络 2013和2014年是自然语言处理领域神经网络时代的开始。其中三种类型的神经网络应用最为广泛：循环神经网络(recurrentneural networks)、卷积神经网络(convolutionalneural networks)和结构递归神经网络(recursiveneural networks)。 循环神经网络是NLP领域处理动态输入序列最自然的选择。Vanilla循环神经网络很快被经典的长短期记忆网络(long-shorttermmemory networks，LSTM)代替，该模型能更好地解决梯度消失和梯度爆炸问题。 在2013年之前，人们仍认为循环神经网络很难训练，直到Ilya Sutskever博士的论文改变了循环神经网络这一名声。双向的长短期记忆记忆网络通常被用于同时处理出现在左侧和右侧的文本内容。LSTM 结构如图7所示。 △&nbsp;LSTM网络 应用于文本的卷积神经网络只在两个维度上进行操作，卷积层只需要在时序维度上移动即可。图8展示了应用于自然语言处理的卷积神经网络的典型结构。 △&nbsp;卷积神经网络 与循环神经网络相比，卷积神经网络的一个优点是具有更好的并行性。 因为卷积操作中每个时间步的状态只依赖于局部上下文，而不是循环神经网络中那样依赖于所有过去的状态。卷积神经网络可以使用更大的卷积层涵盖更广泛的上下文内容。卷积神经网络也可以和长短期记忆网络进行组合和堆叠，还可以用来加速长短期记忆网络的训练。 循环神经网络和卷积神经网络都将语言视为一个序列。但从语言学的角度来看，语言是具有层级结构的：词语组成高阶的短语和小句，它们本身可以根据一定的产生规则递归地组合。这激发了利用结构递归神经网络，以树形结构取代序列来表示语言的想法，如图9所示。 △&nbsp;结构递归神经网络 结构递归神经网络自下而上构建序列的表示，与从左至右或从右至左对序列进行处理的循环神经网络形成鲜明的对比。树中的每个节点是通过子节点的表征计算得到的。一个树也可以视为在循环神经网络上施加不同的处理顺序，所以长短期记忆网络则可以很容易地被扩展为一棵树。 不只是循环神经网络和长短期记忆网络可以扩展到使用层次结构，词嵌入也可以在语法语境中学习，语言模型可以基于句法堆栈生成词汇，图形卷积神经网络可以树状结构运行。 2.8、里程碑八：2014序列到序列模型(Sequence-to-sequencemodels) 2014年，Sutskever等人提出序列到序列学习，即使用神经网络将一个序列映射到另一个序列的一般化框架。在这个框架中，一个作为编码器的神经网络对句子符号进行处理，并将其压缩成向量表示；然后，一个作为解码器的神经网络根据编码器的状态逐个预测输出符号，并将前一个预测得到的输出符号作为预测下一个输出符号的输入。如图10所示。 △&nbsp;序列到序列模型 机器翻译是这一框架的杀手级应用。2016年，谷歌宣布他们将用神经机器翻译模型取代基于短语的整句机器翻译模型。谷歌大脑负责人Jeff Dean表示，这意味着用500行神经网络模型代码取代50万行基于短语的机器翻译代码。 由于其灵活性，该框架在自然语言生成任务上被广泛应用，其编码器和解码器分别由不同的模型来担任。更重要的是，解码器不仅可以适用于序列，在任意表示上均可以应用。比如基于图片生成描述(如图11)、基于表格生成文本、根据源代码改变生成描述，以及众多其他应用。 △&nbsp;基于图像生成标题(Vinyalset al., 2015) 序列到序列的学习甚至可以应用到自然语言处理领域常见的结构化预测任务中，也就是输出具有特定的结构。为简单起见，输出就像选区解析一样被线性化(如图12)。在给定足够多训练数据用于语法解析的情况下，神经网络已经被证明具有产生线性输出和识别命名实体的能力。 △&nbsp;线性化选区解析树(Vinyalset al., 2015) 序列的编码器和解码器通常都是基于循环神经网络，但也可以使用其他模型。新的结构主要都从机器翻译的工作中诞生，它已经成了序列到序列模型的培养基。近期提出的模型有深度长短期记忆网络、卷积编码器、Transformer(一个基于自注意力机制的全新神经网络架构)以及长短期记忆依赖网络和的 Transformer 结合体等。 2.9、里程碑九：2015注意力机制和基于记忆的神经网络 注意力机制是神经网络机器翻译(NMT)的核心创新之一，也是使神经网络机器翻译优于经典的基于短语的机器翻译的关键。序列到序列学习的主要瓶颈是，需要将源序列的全部内容压缩为固定大小的向量。注意力机制通过让解码器回顾源序列的隐藏状态，以此为解码器提供加权平均值的输入来缓解这一问题，如图13所示。 △&nbsp;注意力机制 之后，各种形式的注意力机制涌现而出。注意力机制被广泛接受，在各种需要根据输入的特定部分做出决策的任务上都有潜在的应用。它已经被应用于句法分析、阅读理解、单样本学习等任务中。它的输入甚至不需要是一个序列，而可以包含其他表示，比如图像的描述(图14)。 注意力机制一个有用的附带作用是它通过注意力权重来检测输入的哪一部分与特定的输出相关，从而提供了一种罕见的虽然还是比较浅层次的，对模型内部运作机制的窥探。 △&nbsp;图像描述模型中的视觉注意力机制指示在生成”飞盘”时所关注的内容 注意力机制不仅仅局限于输入序列。自注意力机制可用来观察句子或文档中周围的单词，获得包含更多上下文信息的词语表示。多层的自注意力机制是神经机器翻译前沿模型Transformer的核心。 注意力机制可以视为模糊记忆的一种形式，其记忆的内容包括模型之前的隐藏状态，由模型选择从记忆中检索哪些内容。与此同时，更多具有明确记忆单元的模型被提出。 他们有很多不同的变化形式，比如神经图灵机(NeuralTuring Machines)、记忆网络(MemoryNetwork)、端到端的记忆网络(End-to-endMemory Newtorks)、动态记忆网络(DynamicMemoryNetworks)、神经可微计算机(NeuralDifferentiable Computer)、循环实体网络(RecurrentEntityNetwork)。 记忆的存取通常与注意力机制相似，基于与当前状态且可以读取和写入。这些模型之间的差异体现在它们如何实现和利用存储模块。 比如说，端到端的记忆网络对输入进行多次处理并更新内存，以实行多次推理。神经图灵机也有一个基于位置的寻址方式，使它们可以学习简单的计算机程序，比如排序。 基于记忆的模型通常用于需要长时间保留信息的任务中，例如语言模型构建和阅读理解。记忆模块的概念非常通用，知识库和表格都可以作为记忆模块，记忆模块也可以基于输入的全部或部分内容进行填充。 2.10、里程碑十：2018预训练语言模型 预训练的词嵌入与上下文无关，仅用于初始化模型中的第一层。近几个月以来，许多有监督的任务被用来预训练神经网络。相比之下，语言模型只需要未标记的文本，因此其训练可以扩展到数十亿单词的语料、新的领域、新的语言。预训练的语言模型于 2015年被首次提出，但直到最近它才被证明在大量不同类型的任务中均十分有效。语言模型嵌入可以作为目标模型中的特征，或者根据具体任务进行调整。如下图所示，语言模型嵌入为许多任务的效果带来了巨大的改进。 △&nbsp;改进的语言模型嵌入 使用预训练的语言模型可以在数据量十分少的情况下有效学习。由于语言模型的训练只需要无标签的数据，因此他们对于数据稀缺的低资源语言特别有利。 2018年10月，谷歌AI语言组发布BERT语言模型预训练，已被证明可有效改进许多自然语言处理任务(Dai and Le, 2015; Peters et al., 2017, 2018; Radford etal., 2018; Howard and Ruder, 2018)。 这些任务包括句子级任务，如自然语言推理inference(Bowman et al., 2015; Williams et al., 2018)和释义paraphrasing(Dolan and Brockett, 2005)，旨在通过整体分析来预测句子之间的关系；以及词块级任务，如命名实体识别(Tjong KimSang and De Meulder, 2003)和SQuAD问题回答(Rajpurkar et al., 2016)，其中模型需要在词块级别生成细粒度输出。 近年七大技术里程碑小结 除了上述七大技术里程碑，一些其他进展虽不如上面提到的那样流行，但仍产生了广泛的影响。 基于字符的描述(Character-based representations)，在字符层级上使用卷积神经网络和长短期记忆网络，以获得一个基于字符的词语描述，目前已经相当常见了，特别是对于那些语言形态丰富的语种或那些形态信息十分重要、包含许多未知单词的任务。据目前所知，基于字符的描述最初用于序列标注，现在，基于字符的描述方法，减轻了必须以增加计算成本为代价建立固定词汇表的问题，并使完全基于字符的机器翻译的应用成为可能。 对抗学习(Adversarial learning)，在机器学习领域已经取得了广泛应用，在自然语言处理领域也被应用于不同的任务中。对抗样例的应用也日益广泛，他们不仅仅是探测模型弱点的工具，更能使模型更具鲁棒性(robust)。(虚拟的)对抗性训练，也就是最坏情况的扰动，和域对抗性损失(domain-adversariallosses)都是可以使模型更具鲁棒性的有效正则化方式。生成对抗网络(GANs)目前在自然语言生成任务上还不太有效，但在匹配分布上十分有用。 强化学习(Reinforcement learning)，在具有时间依赖性任务上证明有效，比如在训练期间选择数据和对话建模。在机器翻译和概括任务中，强化学习可以有效地直接优化“红色”和“蓝色”这样不可微的度量，不必去优化像交叉熵这样的代理损失函数。同样，逆向强化学习(inversereinforcement learning)在类似视频故事描述这样的奖励机制非常复杂且难以具体化的任务中，也非常有用。 自然语言处理NLP知识结构 文|秦陇纪，数据简化DataSimp 自然语言处理(计算机语言学、自然语言理解)涉及：字处理，词处理，语句处理，篇章处理词处理分词、词性标注、实体识别、词义消歧语句处理句法分析(SyntacticAnalysis)、语义分析(SenmanticAnalysis)等。其中，重点有： 1、句法语义分析：分词，词性标记，命名实体识别。 2、信息抽取 3、文本挖掘：文本聚类，情感分析。基于统计。 4、机器翻译：基于规则，基于统计，基于神经网络。 5、信息检索 6、问答系统 7、对话系统建议…本文总结的自然语言处理历史、模型、知识体系结构内容，涉及NLP的语言理论、算法和工程实践各方面，内容繁杂。参考黄志洪老师自然语言处理课程、宗成庆老师《统计自然语言处理》，郑捷2017年电子工业出版社出版的图书《NLP汉语自然语言处理原理与实践》，以及国外著名NLP书籍的英文资料、汉译版资料。 一、NLP知识结构概述 1)自然语言处理：利用计算机为工具，对书面实行或者口头形式进行各种各样的处理和加工的技术，是研究人与人交际中以及人与计算机交际中的演员问题的一门学科，是人工智能的主要内容。 2)自然语言处理是研究语言能力和语言应用的模型，建立计算机(算法)框架来实现这样的语言模型，并完善、评测、最终用于设计各种实用系统。 3)研究问题(主要)： 信息检索 机器翻译 文档分类 问答系统 信息过滤 自动文摘 信息抽取 文本挖掘 舆情分析 机器写作 语音识别 研究模式：自然语言场景问题，数学算法，算法如何应用到解决这些问题，预料训练，相关实际应用 自然语言的困难： 场景的困难：语言的多样性、多变性、歧义性 学习的困难：艰难的数学模型(hmm,crf,EM,深度学习等) 语料的困难：什么的语料？语料的作用？如何获取语料？ 二、NLP知识十大结构 2.1、形式语言与自动机 语言：按照一定规律构成的句子或者字符串的有限或者无限的集合。 描述语言的三种途径： 穷举法 文法(产生式系统)描述 自动机 自然语言不是人为设计而是自然进化的，形式语言比如：运算符号、化学分子式、编程语言 形式语言理论朱啊哟研究的是内部结构模式这类语言的纯粹的语法领域，从语言学而来，作为一种理解自然语言的句法规律，在计算机科学中，形式语言通常作为定义编程和语法结构的基础 形式语言与自动机基础知识： 集合论 图论 自动机的应用： 1，单词自动查错纠正 2，词性消歧(什么是词性？什么的词性标注？为什么需要标注？如何标注？) 形式语言的缺陷： 1、对于像汉语，英语这样的大型自然语言系统，难以构造精确的文法 2、不符合人类学习语言的习惯 3、有些句子语法正确，但在语义上却不可能，形式语言无法排出这些句子 4、解决方向：基于大量语料，采用统计学手段建立模型 2.2、语言模型 1)、语言模型(重要)：通过语料计算某个句子出现的概率(概率表示)，常用的有2-元模型，3-元模型 2)、语言模型应用： 语音识别歧义消除例如，给定拼音串：ta shi yan yan jiu saun fa de 可能的汉字串：踏实烟酒算法的他是研究酸法的他是研究算法的，显然，最后一句才符合。 3)、语言模型的启示： 1、开启自然语言处理的统计方法 2、统计方法的一般步骤： 收集大量语料 对语料进行统计分析，得出知识 针对场景建立算法模型 解释和应用结果 4)、语言模型性能评价，包括评价目标，评价的难点，常用指标(交叉熵，困惑度) 5)、数据平滑： 数据平滑的概念，为什么需要平滑 平滑的方法，加一法，加法平滑法，古德-图灵法，J-M法，Katz平滑法等 6)、语言模型的缺陷： 语料来自不同的领域，而语言模型对文本类型、主题等十分敏感 n与相邻的n-1个词相关，假设不是很成立。 2.3、概率图模型 生成模型与判别模型，贝叶斯网络，马尔科夫链与隐马尔科夫模型(HMM) 1)概率图模型概述(什么的概率图模型，参考清华大学教材《概率图模型》) 2)马尔科夫过程(定义，理解) 3)隐马尔科夫过程(定义，理解) HMM的三个基本问题(定义，解法，应用) 注：第一个问题，涉及最大似然估计法，第二个问题涉及EM算法，第三个问题涉及维特比算法，内容很多，要重点理解，(参考书李航《统计学习方法》，网上博客，笔者github) 2.4、马尔科夫网，最大熵模型，条件随机场(CRF) 1)、HMM的三个基本问题的参数估计与计算 2)、什么是熵 3)、EM算法(应用十分广泛，好好理解) 4)、HMM的应用 5)、层次化马尔科夫模型与马尔科夫网络 提出原因，HMM存在两个问题 6)、最大熵马尔科夫模型 优点：与HMM相比，允许使用特征刻画观察序列，训练高效 缺点：存在标记偏置问题 7)、条件随机场及其应用(概念，模型过程，与HMM关系) 参数估计方法(GIS算法，改进IIS算法) CRF基本问题：特征选取(特征模板)、概率计算、参数训练、解码(维特比) 应用场景： 词性标注类问题(现在一般用RNN+CRF) 中文分词(发展过程，经典算法，了解开源工具jieba分词) 中文人名，地名识别 8)、CRF++ 2.5、命名实体识别，词性标注，内容挖掘、语义分析与篇章分析(大量用到前面的算法) 1)、命名实体识别问题 相关概率，定义 相关任务类型 方法(基于规程-&gt;基于大规模语料库) 2)、未登录词的解决方法(搜索引擎，基于语料) 3)、CRF解决命名实体识别(NER)流程总结： 训练阶段：确定特征模板，不同场景(人名，地名等)所使用的特征模板不同，对现有语料进行分词，在分词结果基础上进行词性标注(可能手工)，NER对应的标注问题是基于词的，然后训练CRF模型，得到对应权值参数值 识别过程：将待识别文档分词，然后送入CRF模型进行识别计算(维特比算法)，得到标注序列，然后根据标注划分出命名实体 4)、词性标注(理解含义，意义)及其一致性检查方法(位置属性向量，词性标注序列向量，聚类或者分类算法) 2.6、句法分析 1)、句法分析理解以及意义 1、句法结构分析 完全句法分析 浅层分析 2、依存关系分析 2)、句法分析方法 1、基于规则的句法结构分析 2、基于统计的语法结构分析 2.7 文本分类，情感分析 1)、文本分类，文本排重 文本分类：在预定义的分类体系下，根据文本的特征，将给定的文本与一个或者多个类别相关联 典型应用：垃圾邮件判定，网页自动分类 2)、文本表示，特征选取与权重计算，词向量 文本特征选择常用方法： 1、基于本文频率的特征提取法 2、信息增量法 3、X2(卡方)统计量 4、互信息法 3)、分类器设计 SVM，贝叶斯，决策树等 4)、分类器性能评测 1、召回率 2、正确率 3、F1值 5)、主题模型(LDA)与PLSA LDA模型十分强大，基于贝叶斯改进了PLSA，可以提取出本章的主题词和关键词，建模过程复杂，难以理解。 6)、情感分析 借助计算机帮助用户快速获取，整理和分析相关评论信息，对带有感情色彩的主观文本进行分析，处理和归纳例如，评论自动分析，水军识别。 某种意义上看，情感分析也是一种特殊的分类问题 7)、应用案例 2.8、信息检索，搜索引擎及其原理 1)、信息检索起源于图书馆资料查询检索，引入计算机技术后，从单纯的文本查询扩展到包含图片，音视频等多媒体信息检索，检索对象由数据库扩展到互联网。 1、点对点检索 2、精确匹配模型与相关匹配模型 3、检索系统关键技术：标引，相关度计算 2)、常见模型：布尔模型，向量空间模型，概率模型 3)、常用技术：倒排索引，隐语义分析(LDA等) 4)、评测指标 2.9、自动文摘与信息抽取，机器翻译，问答系统 1)、统计机器翻译的的思路，过程，难点，以及解决 2)、问答系统 基本组成：问题分析，信息检索，答案抽取 类型：基于问题-答案，基于自由文本 典型的解决思路 3)、自动文摘的意义，常用方法 4)、信息抽取模型(LDA等) 2.10深度学习在自然语言中的应用 1)、单词表示，比如词向量的训练(wordvoc) 2)、自动写文本 写新闻等 3)、机器翻译 4)、基于CNN、RNN的文本分类 5)、深度学习与CRF结合用于词性标注 三、中文NLP知识目录 选自郑捷2017年电子工业出版社出版的图书《NLP汉语自然语言处理原理与实践》。 第1章 中文语言的机器处理 1 1.1 历史回顾 2 1.1.1 从科幻到现实 2 1.1.2 早期的探索 3 1.1.3 规则派还是统计派 3 1.1.4 从机器学习到认知计算 5 1.2 现代自然语言系统简介 6 1.2.1 NLP流程与开源框架 6 1.2.2 哈工大NLP平台及其演示环境 9 1.2.3 StanfordNLP团队及其演示环境 11 1.2.4 NLTK开发环境 13 1.3 整合中文分词模块 16 1.3.1 安装Ltp Python组件 17 1.3.2 使用Ltp 3.3进行中文分词 18 1.3.3 使用结巴分词模块 20 1.4 整合词性标注模块 22 1.4.1 Ltp 3.3词性标注 23 1.4.2 安装StanfordNLP并编写Python接口类 24 1.4.3 执行Stanford词性标注 28 1.5 整合命名实体识别模块 29 1.5.1 Ltp 3.3命名实体识别 29 1.5.2 Stanford命名实体识别 30 1.6 整合句法解析模块 32 1.6.1 Ltp 3.3句法依存树 33 1.6.2 StanfordParser类 35 1.6.3 Stanford短语结构树 36 1.6.4 Stanford依存句法树 37 1.7 整合语义角色标注模块 38 1.8 结语 40 第2章 汉语语言学研究回顾 42 2.1 文字符号的起源 42 2.1.1 从记事谈起 43 2.1.2 古文字的形成 47 2.2 六书及其他 48 2.2.1 象形 48 2.2.2 指事 50 2.2.3 会意 51 2.2.4 形声 53 2.2.5 转注 54 2.2.6 假借 55 2.3 字形的流变 56 2.3.1 笔与墨的形成与变革 56 2.3.2 隶变的方式 58 2.3.3 汉字的符号化与结构 61 2.4 汉语的发展 67 2.4.1 完整语义的基本形式——句子 68 2.4.2 语言的初始形态与文言文 71 2.4.3 白话文与复音词 73 2.4.4 白话文与句法研究 78 2.5 三个平面中的语义研究 80 2.5.1 词汇与本体论 81 2.5.2 格语法及其框架 84 2.6 结语 86 第3章 词汇与分词技术 88 3.1 中文分词 89 3.1.1 什么是词与分词规范 90 3.1.2 两种分词标准 93 3.1.3 歧义、机械分词、语言模型 94 3.1.4 词汇的构成与未登录词 97 3.2 系统总体流程与词典结构 98 3.2.1 概述 98 3.2.2 中文分词流程 99 3.2.3 分词词典结构 103 3.2.4 命名实体的词典结构 105 3.2.5 词典的存储结构 108 3.3 算法部分源码解析 111 3.3.1 系统配置 112 3.3.2 Main方法与例句 113 3.3.3 句子切分 113 3.3.4 分词流程 117 3.3.5 一元词网 118 3.3.6 二元词图 125 3.3.7 NShort算法原理 130 3.3.8 后处理规则集 136 3.3.9 命名实体识别 137 3.3.10 细分阶段与最短路径 140 3.4 结语 142 第4章 NLP中的概率图模型 143 4.1 概率论回顾 143 4.1.1 多元概率论的几个基本概念 144 4.1.2 贝叶斯与朴素贝叶斯算法 146 4.1.3 文本分类 148 4.1.4 文本分类的实现 151 4.2 信息熵 154 4.2.1 信息量与信息熵 154 4.2.2 互信息、联合熵、条件熵 156 4.2.3 交叉熵和KL散度 158 4.2.4 信息熵的NLP的意义 159 4.3 NLP与概率图模型 160 4.3.1 概率图模型的几个基本问题 161 4.3.2 产生式模型和判别式模型 162 4.3.3 统计语言模型与NLP算法设计 164 4.3.4 极大似然估计 167 4.4 隐马尔科夫模型简介 169 4.4.1 马尔科夫链 169 4.4.2 隐马尔科夫模型 170 4.4.3 HMMs的一个实例 171 4.4.4 Viterbi算法的实现 176 4.5 最大熵模型 179 4.5.1 从词性标注谈起 179 4.5.2 特征和约束 181 4.5.3 最大熵原理 183 4.5.4 公式推导 185 4.5.5 对偶问题的极大似然估计 186 4.5.6 GIS实现 188 4.6 条件随机场模型 193 4.6.1 随机场 193 4.6.2 无向图的团(Clique)与因子分解 194 4.6.3 线性链条件随机场 195 4.6.4 CRF的概率计算 198 4.6.5 CRF的参数学习 199 4.6.6 CRF预测标签 200 4.7 结语 201 第5章 词性、语块与命名实体识别 202 5.1 汉语词性标注 203 5.1.1 汉语的词性 203 5.1.2 宾州树库的词性标注规范 205 5.1.3stanfordNLP标注词性 210 5.1.4 训练模型文件 213 5.2 语义组块标注 219 5.2.1 语义组块的种类 220 5.2.2 细说NP 221 5.2.3 细说VP 223 5.2.4 其他语义块 227 5.2.5 语义块的抽取 229 5.2.6 CRF的使用 232 5.3 命名实体识别 240 5.3.1 命名实体 241 5.3.2 分词架构与专名词典 243 5.3.3 算法的策略——词典与统计相结合 245 5.3.4 算法的策略——层叠式架构 252 5.4 结语 259 第6章 句法理论与自动分析 260 6.1 转换生成语法 261 6.1.1 乔姆斯基的语言观 261 6.1.2 短语结构文法 263 6.1.3 汉语句类 269 6.1.4 谓词论元与空范畴 274 6.1.5 轻动词分析理论 279 6.1.6 NLTK操作句法树 280 6.2 依存句法理论 283 6.2.1 配价理论 283 6.2.2 配价词典 285 6.2.3 依存理论概述 287 6.2.4 Ltp依存分析介绍 290 6.2.5 Stanford依存转换、解析 293 6.3 PCFG短语结构句法分析 298 6.3.1 PCFG短语结构 298 6.3.2 内向算法和外向算法 301 6.3.3 Viterbi算法 303 6.3.4 参数估计 304 6.3.5 Stanford的PCFG算法训练 305 6.4 结语 310 第7章 建设语言资源库 311 7.1 语料库概述 311 7.1.1 语料库的简史 312 7.1.2 语言资源库的分类 314 7.1.3 语料库的设计实例：国家语委语料库 315 7.1.4 语料库的层次加工 321 7.2 语法语料库 323 7.2.1 中文分词语料库 323 7.2.2 中文分词的测评 326 7.2.3 宾州大学CTB简介 327 7.3 语义知识库 333 7.3.1 知识库与HowNet简介 333 7.3.2 发掘义原 334 7.3.3 语义角色 336 7.3.4 分类原则与事件分类 344 7.3.5 实体分类 347 7.3.6 属性与分类 352 7.3.7 相似度计算与实例 353 7.4 语义网与百科知识库 360 7.4.1 语义网理论介绍 360 7.4.2 维基百科知识库 364 7.4.3 DBpedia抽取原理 365 7.5 结语 368 第8章 语义与认知 370 8.1 回顾现代语义学 371 8.1.1 语义三角论 371 8.1.2 语义场论 373 8.1.3 基于逻辑的语义学 376 8.2 认知语言学概述 377 8.2.1 象似性原理 379 8.2.2 顺序象似性 380 8.2.3 距离象似性 380 8.2.4 重叠象似性 381 8.3 意象图式的构成 383 8.3.1 主观性与焦点 383 8.3.2 范畴化：概念的认知 385 8.3.3 主体与背景 390 8.3.4 意象图式 392 8.3.5 社交中的图式 396 8.3.6 完形：压缩与省略 398 8.4 隐喻与转喻 401 8.4.1 隐喻的结构 402 8.4.2 隐喻的认知本质 403 8.4.3 隐喻计算的系统架构 405 8.4.4 隐喻计算的实现 408 8.5 构式语法 412 8.5.1 构式的概念 413 8.5.2 句法与构式 415 8.5.3 构式知识库 417 8.6 结语 420 第9章 NLP中的深度学习 422 9.1 神经网络回顾 422 9.1.1 神经网络框架 423 9.1.2 梯度下降法推导 425 9.1.3 梯度下降法的实现 427 9.1.4 BP神经网络介绍和推导 430 9.2 Word2Vec简介 433 9.2.1 词向量及其表达 434 9.2.2 Word2Vec的算法原理 436 9.2.3 训练词向量 439 9.2.4 大规模上下位关系的自动识别 443 9.3 NLP与RNN 448 9.3.1Simple-RNN 449 9.3.2 LSTM原理 454 9.3.3 LSTM的Python实现 460 9.4 深度学习框架与应用 467 9.4.1 Keras框架介绍 467 9.4.2 Keras序列标注 471 9.4.3 依存句法的算法原理 478 9.4.4 Stanford依存解析的训练过程 483 9.5 结语 488 第10章 语义计算的架构 490 10.1 句子的语义和语法预处理 490 10.1.1 长句切分和融合 491 10.1.2 共指消解 496 10.2 语义角色 502 10.2.1 谓词论元与语义角色 502 10.2.2PropBank简介 505 10.2.3 CPB中的特殊句式 506 10.2.4 名词性谓词的语义角色 509 10.2.5PropBank展开 512 10.3 句子的语义解析 517 10.3.1 语义依存 517 10.3.2 完整架构 524 10.3.3 实体关系抽取 527 10.4 结语 531 [29] 自然语言处理NLP国内研究方向机构导师 文|中文信息协会《中文信息处理发展报告2016》，数据简化DataSimp 文字语言VS数字信息 数字、文字和自然语言一样，都是信息的载体，他们之间原本有着天然的联系。语言和数学的产生都是为了交流，从文字、数字和语言的发展历史，可以了解到语言、文字和数字有着内在的联系。自然语言处理NLP主要涉及三种文本，自由文本、结构化文本、半结构化文本。 自然语言理解Natural Language Understanding(NLU)，实现人机间自然语言通信，意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本表达给定的意图、思想等。自然语言生成NLG，是人工或机器生成语言。 斯坦福自然语言处理NLP工具资料收集、斯坦福分词、Stanford中文实体识别，最早做自然语言处理的网址https://nlp.stanford.edu/software/segmenter.shtml。 哈尔滨工业大学智能技术与自然语言处理研究室(IntelligentTechnology &amp; Natural Language Processing Lab, ITNLPLab)是国内较早从事自然语言处理和语言智能技术的研究室。 除了新兴的文本数据简化领域：秦陇纪(数据简化技术中心筹)，自然语言处理NaturalLanguage Processing领域主要包括基础研究和应用研究。 基础研究 词法与句法分析：李正华、陈文亮、张民(苏州大学) 语义分析：周国栋、李军辉(苏州大学) 篇章分析：王厚峰、李素建(北京大学) 语言认知模型：王少楠，宗成庆(中科院自动化研究所) 语言表示与深度学习：黄萱菁、邱锡鹏(复旦大学) 知识图谱与计算：李涓子、候磊(清华大学) 应用研究 文本分类与聚类：涂存超，刘知远(清华大学) 信息抽取：孙乐、韩先培(中国科学院软件研究所) 情感分析：黄民烈(清华大学) 自动文摘：万小军、姚金戈(北京大学) 信息检索：刘奕群、马少平(清华大学) 信息推荐与过滤：王斌(中科院信工所)，鲁骁(国家计算机网络应急中心) 自动问答：赵军、刘康，何世柱(中科院自动化研究所) 机器翻译：张家俊、宗成庆(中科院自动化研究所) 社会媒体处理：刘挺、丁效(哈尔滨工业大学) 语音技术：说话人识别——郑方(清华大学)，王仁宇(江苏师范大学) 语音合成——陶建华(中科院自动化研究所) 语音识别——王东(清华大学) 文字识别：刘成林(中科院自动化研究所) 多模态信息处理：陈晓鸥(北京大学) 医疗健康信息处理：陈清财、汤步洲(哈尔滨工业大学) 少数民族语言信息处理：吾守尔•斯拉木(新疆大学) —&nbsp;完&nbsp;—" />
<link rel="canonical" href="https://mlh.app/2019/02/21/a86cd5c46c377d6b95ec6e7df5e213ea.html" />
<meta property="og:url" content="https://mlh.app/2019/02/21/a86cd5c46c377d6b95ec6e7df5e213ea.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-21T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"转载自 数据简化DataSimp&nbsp; 作者 秦陇纪&nbsp; 郭一璞 编辑&nbsp; 量子位 报道 | 公众号 QbitAI 本篇推送包含三篇文章， 《自然语言处理技术发展史十大里程碑》 《语言处理NLP知识结构》 《自然语言处理NLP国内研究方向机构导师》 总共超过20000字，量子位建议先码再看。 自然语言处理技术发展史十大里程碑 文|秦陇纪，参考|黄昌宁、张小凤、Sebatian Ruder 自然语言是人类独有的智慧结晶。 自然语言处理(NaturalLanguage Processing，NLP)是计算机科学领域与人工智能领域中的一个重要方向，旨在研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。用自然语言与计算机进行通信，有着十分重要的实际应用意义，也有着革命性的理论意义。 由于理解自然语言，需要关于外在世界的广泛知识以及运用操作这些知识的能力，所以自然语言处理，也被视为解决人工智能完备(AI-complete)的核心问题之一。对自然语言处理的研究也是充满魅力和挑战的。 微软亚洲研究院黄昌宁、张小凤在2013年发表论文，就过去50年以来自然语言处理(NLP)研究领域中的发现和发展要点进行阐述，其中包括两个事实和三大重要成果。 近年来，自然语言处理的语料库调查显示如下两个事实： (1)对于句法分析来说，基于单一标记的短语结构规则是不充分的；单个标记的PSG规则不足以进行自然语言描述； (2)PSG规则在文本语料库中具有偏差分布，即PSG规则的总数似乎不能够涵盖大型语料库中发现的语言现象，这不符合语言学家的期望。短语结构规则在真实文本中的分布呈现严重扭曲。换言之，有限数目的短语结构规则不能覆盖大规模语料中的语法现象。这与原先人们的预期大相径庭。 NLP技术发展历程在很大程度上受到以上两个事实的影响，在该领域中可以称得上里程碑式的成果有如下三个： (1)复杂特征集和合一语法； (2)语言学研究中的词汇主义； (3)语料库方法和统计语言模型。业内人士普遍认为，大规模语言知识的开发和自动获取是NLP技术的瓶颈问题。因此，语料库建设和统计学习理论将成为该领域中的关键课题。 一、NLP研究传统问题 自然语言处理(NLP)是计算机科学、信息工程和人工智能的子领域，涉及计算机和人类(自然)语言之间的交互，尤其是编程实现计算机处理和分析大量自然语言数据。自然语言处理的挑战包括语音识别，自然语言理解和自然语言生成。 信息输入、检索、人机对话等需求增多，使自然语言处理(NLP)成为21世纪初的热门学科。从50年代机器翻译和人工智能研究算起，NLP至今有长达半个世纪的历史了。 近年来这一领域中里程碑式的理论和方法贡献有如下三个： (1)复杂特征集和合一语法； (2)语言学研究中的词汇主义； (3)语料库方法和统计语言模型。 这三个成果将继续对语言学、计算语言学和NLP的研究产生深远影响。为了理解这些成果的意义，先介绍一下两个相关事实。 自然语言处理中识别句子 句法结构的句法分析的全过程： (1)把句子中的词一个一个地切分出来； (2)查词典，给句子中的每个词指派一个合适的词性(part of speech)； (3)用句法规则把句子里包含的句法成分，如名词短语、动词短语、小句等，逐个地识别出来。 (4)判断每个短语的句法功能，如主语、谓语、宾语等，及其语义角色，最终得到句子的意义表示，如逻辑语义表达式。 1.1、事实一：语言的结构歧义问题 第一个事实(黄昌宁，张小凤，2013)是：短语结构语法(PhraseStructure Grammar，简称PSG)不能有效地描写自然语言。 PSG在Chomsky的语言学理论[1]中占有重要地位，并且在自然语言的句法描写中担当着举足轻重的角色。但是它有一些根本性的弱点，主要表现为它使用的是像词类和短语类那样的单一标记，因此不能有效地指明和解释自然语言中的结构歧义问题。 让我们先来看一看汉语中“V+N”组合。假如我们把“打击，委托，调查”等词指派为动词(V)；把“力度，方式，盗版，甲方”等词视为名词(N)，而且同意“打击力度”、“委托方式”是名词短语(NP)，“打击盗版”、“委托甲方”是动词短语(VP)，那么就会产生如下两条有歧义的句法规则： (1) NP → V N (2) VP → V N 换句话讲，当计算机观察到文本中相邻出现的“V+N”词类序列时，仍不能确定它们组成的究竟是NP还是VP。我们把这样的歧义叫做“短语类型歧义”。例如： • 该公司正在招聘[销售V人员N]NP。 • 地球在不断[改变V形状N]VP。 下面再来看“N+V”的组合，也同样会产生带有短语类型歧义的规则对，如： (3) NP → N V 例：市场调查；政治影响。 (4) S → N V 例：价格攀升；局势稳定。 其中标记S代表小句。 不仅如此，有时当机器观察到相邻出现的“N+V”词类序列时，甚至不能判断它们是不是在同一个短语中。也就是说，“N+V”词类序列可能组成名词短语NP或小句S，也有可能根本就不在同一个短语里。后面这种歧义称为“短语边界歧义”。下面是两个相关的例句： • 中国的[铁路N建设V]NP发展很快。 • [中国的铁路N]NP建设V得很快。 前一个例句中，“铁路建设”组成一个NP；而在后一个例句中，这两个相邻的词却分属于两个不同的短语。这足以说明，基于单一标记的PSG不能充分地描述自然语言中的句法歧义现象。下面让我们再来看一些这样的例子。 (5)NP → V N1de N2 (6)VP → V N1de N2 其中de代表结构助词“的”。例如，“[削苹果]VP的刀”是NP; 而“削[苹果的皮]NP”则是VP。这里既有短语类型歧义，又有短语边界歧义。比如，“削V苹果N”这两个相邻的词，可能构成一个VP，也可能分处于两个相邻的短语中。 (7)NP → P N1de N2 (8)PP → P N1de N2 规则中P和PP分别表示介词和介词短语。例如，“[对上海]PP的印象”是NP; 而“对[上海的学生]NP”则是PP。相邻词“对P 上海N”可能组成一个PP，也可能分处于两个短语中。 (9)NP → NumPN1 de N2 其中NumP 表示数量短语。规则(9)虽然表示的是一个NP，但可分别代表两种结构意义： (9a)NumP [N1de N2]NP 如：五个[公司的职员]NP (9b)[NumPN1]NP de N2 如：[五个公司]NP 的职员 (10)NP → N1 N2N3 规则(10)表示的也是一个NP，但“N1+ N2”先结合，还是“N2 +N3”先结合，会出现两种不同的结构方式和意义，即： (10a)[N1 N2]NPN3 如：[现代汉语]NP 词典 (10b)N1 [N2N3]NP 如：新版[汉语词典]NP 以上讨论的第一个事实说明： 由于约束力不够，单一标记的PSG规则不能充分消解短语类型和短语边界的歧义。用数学的语言来讲，PSG规则是必要的，却不是充分的。因此机器仅仅根据规则右边的一个词类序列来判断它是不是一个短语，或者是什么短语，其实都有某种不确定性。 采用复杂特征集和词汇主义方法来重建自然语言的语法系统，是近二十年来全球语言学界就此作出的最重要的努力。 1.2、事实二：词频统计的齐夫律 通过大规模语料的调查，人们发现一种语言的短语规则的分布也符合所谓的齐夫率(Zipf’s Law)。 Zipf是一个统计学家和语言学家。他提出，如果对某个语言单位(不论是英语的字母或词)进行统计，把这个语言单位在一个语料库里出现的频度(frequency)记作F，而且根据频度的降序对每个单元指派一个整数的阶次(rank) R。结果发现R和F的乘积近似为一个常数。即 F*R ≈ const (常数) 被观察的语言单元的阶次R与其频度F成反比关系。词频统计方面齐夫律显示，不管被考察的语料仅仅一本长篇小说，还是一个大规模的语料库，最常出现的100个词的出现次数会占到语料库总词次数(tokens)的近一半。 假如语料库的规模是100万词次，那么其中频度最高的100个词的累计出现次数大概是50万词次。如果整个语料库含有5万词型(types)，那么其中的一半(也就是2.5万条左右)在该语料库中只出现过一次。即使把语料库的规模加大十倍，变成1000万词次，统计规律大体不变。 有趣的是，80年代英国人Sampson对英语语料库中的PSG规则进行统计，发现它们的分布同样是扭曲的，大体表现为齐夫率。也就是说，一方面经常遇到的语法规则只有几十条左右，它们的出现频度非常非常高；另一方面，规则库中大约一半左右的规则在语料库中只出现过一次。 随着语料库规模的扩大，新的规则仍不断呈现。Noam Chomsky曾提出过这样的假设，认为对一种自然语言来说，其语法规则的数目总是有限的，但据此生成的句子数目却是无限的。但语料库调查的结果不是这个样子。这个发现至少说明，单纯依靠语言学家的语感来编写语法规则不可能胜任大规模真实文本处理的需求，必须寻找可以从语料库中直接获取大规模语言知识的新方法。 几十年来，NLP学界曾发表过许多灿烂成果，有词法学、语法学、语义学的，有句法分析算法的，还有众多著名的自然语言应用系统。那么究竟什么是对该领域影响最大的、里程碑式的成果呢？ 二、NLP十大里程碑 2.1、里程碑一：1985复杂特征集 复杂特征集(complex feature set)又叫做多重属性(multiple features)描写。语言学里，这种描写方法最早出现在语音学中。美国计算语言学家Martin Kay于1985年在“功能合一语法”(FunctionalUnification Grammar，简称FUG)新语法理论中，提出“复杂特征集”(complex feature set)概念。后来被Chomsky学派采用来扩展PSG的描写能力。 △&nbsp;美国计算语言学家Martin Kay 现在在语言学界、计算语言学界，语法系统在词汇层的描写中常采用复杂特征集，利用这些属性来强化句法规则的约束力。一个复杂特征集F包含任意多个特征名fi和特征值vi对。其形式如： F = {…, fi=vi, …}, i=1,…,n 特征值vi既可以是一个简单的数字或符号，也可以是另外一个复杂特征集。这种递归式的定义使复杂特征集获得了强大的表现能力。举例来说，北京大学俞士汶开发的《现代汉语语法信息词典》[10]，对一个动词定义了约40项属性描写，对一个名词定义了约27项属性描写。 一条含有词汇和短语属性约束的句法规则具有如下的一般形式： : &lt;属性约束&gt; : &lt;属性传递&gt; 一般来说，PSG规则包括右部(条件：符号序列的匹配模式)和左部(动作：短语归并结果)。词语的“属性约束”直接来自系统的词库，而短语的“属性约束”则是在自底向上的短语归并过程中从其构成成分的中心语(head)那里继承过来的。在Chomsky的理论中这叫做X-bar理论。 X-bar代表某个词类X所构成的、仍具有该词类属性的一个成分。如果X=N，就是一个具有名词特性的N-bar。当一条PSG规则的右部匹配成功，且“属性约束”部分得到满足，这条规则才能被执行。此时，规则左部所命名的的短语被生成，该短语的复杂特征集通过“属性传递”部分动态生成。 80年代末、90年代初学术界提出了一系列新的语法，如广义短语结构语法(GPSG)、中心语驱动的短语结构语法(HPSG)、词汇功能语法(LFG)等等。这些形式语法其实都是在词汇和短语的复杂特征集描写背景下产生的。合一(unification)算法则是针对复杂特征集的运算而提出来的。“合一”是实现属性匹配和赋值的一种算法，所以上述这些新语法又统称为“基于合一的语法”。 2.2、里程碑二：1966词汇主义 NLP领域第二个里程碑式贡献是词汇主义(lexicalism)。 1966年，韩礼德(Halliday)提出词汇不是用来填充语法确定的一套“空位”(slots)，而是一个独立的语言学层面；词汇研究可以作为对语法理论的补充，却不是语法理论的一部分，他主张把词汇从语法研究中独立地分离出来。 语言学家Hudson宣称，词汇主义是当今语言学理论头号发展倾向。出现原因也同上节两事实有关。词汇主义方法不仅提出一种颗粒度更细的语言知识表示形式，而且体现一语言知识递增式开发和积累的新思路。 首先解释一个背景矛盾。 一方面，语言学界一向认为，不划分词类就无法讲语法，如前面介绍的短语结构语法，语法“不可能”根据个别单独的词来写规则。但是另一方面，人们近来又注意到，任何归类其实都会丢失个体的某些重要信息。所以从前文提到的第一个事实出发，要想强化语法约束能力，词汇的描写应当深入到比词类更细微的词语本身上来。 换句话讲，语言学呼唤在词汇层采用颗粒度更小的描写单元。从本质上来说，词汇主义倾向反映了语言描写的主体已经从句法层转移到了词汇层；这也就是所谓的“小语法，大词库”的思想。下面让我们来看与词汇主义有关的一些工作。 2.2.1、词汇语法学(Lexicon-grammar) 法国巴黎大学Gross教授60年代创立研究中心LADL(http://www.ladl.jussieu.fr/)，提出了词汇语法的概念。 • 把12,000个主要动词分成50个子类。 • 每个动词都有一个特定的论元集。 • 每一类动词都有一个特定的矩阵, 其中每个动词都用400个不同句式来逐一描写(“+”代表可进入该句式；“-”表示不能)。 • 已开发英、法、德、西等欧洲语言的大规模描写。 • INTEX是一个适用于大规模语料分析的工具，已先后被世界五十多个研究中心采用。 2.2.2、框架语义学(Frame Semantics) Fillmore是格语法(Case Grammar)创始人，前几年主持美国自然科学基金的一个名为框架语义学的项目(http://www.icsi.berkeley.edu/~framenet)。该项目从WordNet上选取了2000个动词，从中得到75个语义框架。例如，动词”categorize”的框架被定义为: 一个人(Cognizer)把某个对象(Item)视为某个类(Category)。 同原先的格框架相比，原来一般化的动作主体被具体化为认知者Cognizer，动作客体被具体化为事物Item，并根据特定体动词的性质增加了一个作为分类结果的语义角色Category。 项目组还从英国国家语料库中挑出50,000个相关句子，通过人工给每个句子标注了相应的语义角色。例句： Kimcategorized the book as fiction. (Cog) (Itm)(Cat) 2.2.3、WordNet WordNet(http://www.cogsci.princeton.edu:80/~wn/)是一个描写英语词汇层语义关系的词库，1990年由普林斯顿大学Miller开发。至今有很多版本，全部公布在因特网上，供研究人员自由下载。 欧洲有一个Euro-WordNet，以类似的格式来表现各种欧洲语言的词汇层语义关系。WordNet刻意描写的是词语之间的各种语义关系，如同义关系(synonymy)、反义关系(antonymy)、上下位关系(hyponymy)，部分-整体关系(part-of)等等。 这种词汇语义学又叫做关系语义学，这一学派同传统的语义场理论和和语义属性描写理论相比，其最大的优势在于第一次在一种语言的整个词汇表上实现了词汇层的语义描写。这是其他学派从来没有做到的。其他理论迄今仅仅停留在教科书或某些学术论文中，从来就没有得到工程规模的应用。下面是WordNet的概况： • 95,600条实词词型(动词、名词、形容词) • 被划分成70,100个同义词集(synsets) 2.2.4 知网网(How-Net) 知网是董振东和董强设计的一个汉语语义知识网(http://www.keenage.com)，访问只有主页。 • 自下而上地依据概念对汉语实词进行了穷尽的分类。 • 15,000个动词被划分成810类。 • 定义了300个名词类，100个形容词类。 • 全部概念用400个语义元语来定义。 知网特点是既有WordNet所描写的同一类词间语义关系(如：同义、反义、上下位、部分-整体等)，又描写不同类词之间的论旨关系和语义角色。 2.2.5 MindNet MindNet是微软研究院NLP组设计的词汇语义网(http://research.microsoft.com/nlp/)，用三元组(triple)作为全部知识的表示基元。一个三元组由两个节点和一条连接边组成。每个节点代表一个概念，连接两个概念节点的边表示概念之间的语义依存关系。全部三元组通过句法分析器自动获取。 具体通过对两部英语词典(Longman Dictionaryof Contemporary English，AmericanHeritage Dictionary)和一部百科全书(Encarta)中的全部句子进行分析，获得每个句子的逻辑语义表示(logical form，简称LF)。 而LF本来就是由三元组构成的，如(W1, V-Obj,W2)表示：W1是一个动词，W2是其宾语中的中心词，因此W2从属于W1，它们之间的关系是V-Obj。比如(play, V-Obj,basketball)便是一个具体的三元组。又如(W1, H-Mod,W2)，W1代表一个偏正短语中的中心词(head word)，W2是其修饰语(modifier)，因此W2从属于W1，它们之间的关系是H-Mod。 这种资源是完全自动做出来的，所得三元组不可能没有错误。但是那些出现频度很高的三元组一般来说正确。MindNet已经应用到像语法检查、句法结构排歧、词义排歧、机器翻译等许多场合。 2.3 里程碑三：1976统计语言模型 第三大贡献是语料库方法，或叫统计语言模型。 首先成功利用数学方法解决自然语言处理问题的是语音和语言处理大师弗雷德·贾里尼克(Fred Jelinek)。1968年始在IBM研究中心兼职1974年全职加入，他领导一批杰出科学家利用大型计算机处理人类语言问题，学术休假(SabbaticalLeave)时(约1972-1976年间)提出统计语言模型。 1990s李开复用统计语言模型把997个词的语音识别问题简化成了20词识别问题，实现了有史以来第一次大词汇量非特定人连续语言的识别。常用统计语言模型，包括N元文法模型(N-gram Model)、隐马尔科夫模型(Hidden MarkovModel，简称HMM)、最大熵模型(MaximumEntropy Model)等。 △&nbsp;美国工程院院士Frederick Jelinek 如果用变量W代表一个文本中顺序排列的n个词，即W = w1w2…wn，则统计语言模型的任务是给出任意一个词序列W在文本中出现的概率P(W)。 利用概率的乘积公式，P(W)可展开为： P(W) =P(w1)P(w2/w1)P(w3/ w1 w2)…P(wn/w1 w2…wn-1)&nbsp;(1) 式中P(w1)表示第一个词w1的出现概率，P(w2/w1)表示在w1出现的情况下第二个词w2出现的条件概率，依此类推。 不难看出，为了预测词wn的出现概率，必须已知它前面所有词的出现概率。从计算上来看，这太复杂了。如果近似认为任意一个词wi的出现概率只同它紧邻的前一个词有关，那么计算就得以大大简化。这就是所谓的二元模型(bigram)，由(1)式得： P(W) ≈ P(w1)∏i=2,…,nP(wi/ wi-1 )&nbsp;(2) 式中∏i=2,…,nP(wi/ wi-1 )表示多个概率的连乘。 需要着重指出的是：这些概率参数都可以通过大规模语料库来估值。比如二元概率 P(wi/ wi-1) ≈count(wi-1 wi) / count(wi-1)&nbsp;(3) 式中count(…)表示一个特定词序列在整个语料库中出现的累计次数。若语料库的总词次数为N，则任意词wi在该语料库中的出现概率可估计如下： P(wi) ≈count(wi) / N&nbsp;(4) 同理，如果近似认为任意词wi的出现只同它紧邻前两个词有关，就得到一个三元模型(trigram)： P(W) ≈P(w1)P(w2/w1) ∏i=3,…,nP(wi/wi-2 w-1 )&nbsp;(5) 统计语言模型的方法有点像天气预报。用来估计概率参数的大规模语料库好比是一个地区历年积累起来的气象记录，而用三元模型来做天气预报，就像是根据前两天的天气情况来预测当天的天气。天气预报当然不可能百分之百正确。这也算是概率统计方法的一个特点。 2.3.1、语音识别 语音识别作为计算机汉字键盘输入的一种图代方式，越来越受到信息界人士的青睐。所谓听写机就是这样的商品。据报道中国的移动电话用户已超过一亿，随着移动电话和个人数字助理(PDA)的普及，尤其是当这些随身携带的器件都可以无线上网的时候，广大用户更迫切期望通过语音识别或手写板而不是小键盘来输入简短的文字信息。 其实，语音识别任务可视为计算以下条件概率的极大值问题： W*= argmaxWP(W/speech signal) = argmaxWP(speech signal/W) P(W) / P(speech signal) = argmaxWP(speech signal/W) P(W)&nbsp;(6) 式中数学符号argmaxW表示对不同的候选词序列W计算条件概率P(W/speech signal)的值，从而使W*成为其中条件概率值最大的那个词序列，这也就是计算机选定的识别结果。换句话讲，通过式(6)的计算，计算机找到了最适合当前输入语音信号speech signal的词串W。 式(6)第二行是利用贝叶斯定律转写的结果，因为条件概率P(speech signal/W)比较容易估值。公式的分母P(speech signa)对给定的语音信号是一个常数，不影响极大值的计算，故可以从公式中删除。在第三行所示的结果中，P(W)就是前面所讲得统计语言模型，一般采用式(5)所示的三元模型；P(speechsignal/W)叫做声学模型。 讲到这儿，细心的读者可能已经明白，汉语拼音输入法中的拼音－汉字转换任务其实也是用同样方法实现的，而且两者所用的汉语语言模型(即二元或三元模型)是同一个模型。 据笔者所知，目前市场上的听写机产品和微软拼音输入法(3.0版)都是用词的三元模型实现的，几乎完全不用句法-语义分析手段。为什么会出现这样的局面呢？这是优胜劣汰的客观规律所决定的。可比的评测结果表明，用三元模型实现的拼音-汉字转换系统，其出错率比其它产品减少约50%。 2.3.2、词性标注 一个词库中大约14%的词型具有不只一个词性。而在一个语料库中，占总词次数约30%的词具有不止一个词性。所以对一个文本中的每一个词进行词性标注，就是通过上下文的约束，实现词性歧义的消解。历史上曾经先后出现过两个自动词性标注系统。一个采用上下文相关的规则，叫做TAGGIT(1971)，另一个应用词类的二元模型，叫做CLAWS(1987)。 两个系统都分别对100万词次的英语非受限文本实施了词性标注。结果显示，采用统计语言模型的CLAWS系统的标注正确率大大高于基于规则方法的TAGGIT系统。请看下表的对比： 令C和W分别代表词类标记序列和词序列，则词性标注问题可视为计算以下条件概率的极大值: C*= argmaxCP(C/W) = argmaxCP(W/C)P(C) / P(W) ≈ argmaxC∏i=1,…,nP(wi/ci )P(ci /ci-1 )&nbsp;(7) 式中P(C/W)是已知输入词序列W的情况下，出现词类标记序列C的条件概率。数学符号argmaxC表示通过考察不同的候选词类标记序列C，来寻找使条件概率取最大值的那个词类标记序列C*。后者应当就是对W的词性标注结果。 公式第二行是利用贝叶斯定律转写的结果，由于分母P(W)对给定的W是一个常数，不影响极大值的计算，可以从公式中删除。接着对公式进行近似。首先，引入独立性假设，认为任意一个词wi的出现概率近似只同当前词的词类标记ci有关，而与周围(上下文)的词类标记无关。于是词汇概率可计算如下： P(W/C) ≈∏i=1,…,n P(wi/ci )&nbsp;(8) 其次，采用二元假设，即近似认为任意一个词类标记ci的出现概率只同它紧邻的前一个词类标记ci-1有关。有 P(C) ≈ P(c1)∏i=2,…,n P(ci /ci-1 )&nbsp;(9) P(ci /ci-1 )是词类标记的转移概率，也叫做基于词类的二元模型。 上述这两个概率参数都可以通过带词性标记的语料库来分别估计： P(wi/ci ) ≈count(wi,ci) / count(ci)&nbsp;(10) P(ci /ci-1 ) ≈count(ci-1ci) / count(ci-1)&nbsp;(11) 据文献报道，采用统计语言模型方法汉语和英语的次性标注正确率都可以达到96%左右[6]。 2.3.3、介词短语PP的依附歧义 英语中介词短语究竟依附于前面的名词还是前面的动词，是句法分析中常见的结构歧义问题。下例用语料库方法来解决这个问题，以及这种方法究竟能达到多高的正确率。 例句：Pierre Vinken,61 years old, joined the board as a nonexecutive director. 令A=1表示名词依附，A=0为动词依附，则上述例句的PP依附问题可表为： (A=0,V=joined, N1=board, P=as, N2=director) 令V, N1, N2分别代表句中动词短语、宾语短语、介宾短语的中心词，并在一个带有句法标注的语料库(又称树库)中统计如下四元组的概率Pr： Pr = (A=1 /V=v, N1=n1, P=p, N2=n2)&nbsp;(10) 对输入句子进行PP 依附判断的算法如下： 若Pr = (1 / v, n1, p, n2) ≥ 0.5, 则判定PP依附于n1, 否则判定PP依附于v。 Collins和Brooks实验使用的语料库是宾夕法尼亚大学标注的华尔街日报(WSJ)树库，包括：训练集20,801个四元组，测试集3,097个四元组。他们对PP依附自动判定精度的上下限作了如下分析： 一律视为名词依附(即A≡1) 59.0% 只考虑介词p的最常见附加72.2% 三位专家只根据四个中心词判断88.2% 三位专家根据全句判断93.2% 很明显，自动判断精确率的下限是72.2%，因为机器不会比只考虑句中介词p的最常见依附做得更差了；上限是88.2%，因为机器不可能比三位专家根据四个中心词作出的判断更高明。 论文报告，在被测试的3,097个四元组中，系统正确判断的四元组为2,606个，因此平均精确率为84.1%。这与上面提到的上限值88.2%相比，应该说是相当不错的结果。 传统三大技术里程碑小结 语言学家在不论是复杂特征集和合一语法，还是词汇主义方法，都是原先所谓的理性主义框架下做出的重大贡献。词汇主义方法提出了一种颗粒度更细的语言知识表示形式，而且体现了一种语言知识递增式开发和积累的新思路，值得特别推崇。 尤其值得重视的是，在众多词汇资源的开发过程中，语料库和统计学习方法发挥了很大的作用。这是经验主义方法和理性主义方法相互融合的可喜开端，也是国内知名语言学者冯志伟等人认可的研究范式。 语料库方法和统计语言模型，国内同行中实际上存在不同评价。有种观点认为NLP必须建立在语言理解基础上，他们不大相信统计语言模型在语音识别、词性标注、信息检索等应用领域中所取得的进展。这些争论不能澄清，是因为同行间缺少统一评测。有评测才会有鉴别。 评判某方法优劣应公开、公平、相互可比的评测标准，而非研究员设计“自评”。黄昌宁、张小凤2013年论文表示，语料库方法和统计语言模型是当前自然语言处理技术的主流，其实用价值已在很多应用系统中得到充分证实。统计语言模型研究在结构化对象的统计建模方面，仍有广阔发展空间。 自然语言处理领域业界知名博主Sebatian Ruder在2018年文章从神经网络技术角度，总结NLP领域近15年重大进展、8大里程碑事件，提及很多神经网络模型。这些模型建立在同一时期非神经网络技术之上，如上述三大里程碑。下面接着看后续NLP技术的发展。 2.4、里程碑四：2001神经语言模型(Neural language models) 语言模型解决的是在给定已出现词语的文本中，预测下一个单词的任务。这是最简单的语言处理任务，有许多具体实际应用，如智能键盘、电子邮件回复建议等。语言模型历史由来已久，经典方法基于n-grams模型(利用前面n个词语预测下一个单词)，并利用平滑操作处理不可见的n-grams。 第一个神经语言模型，前馈神经网络(feed-forward neural network)，是Bengio等人于2001年提出的。模型以某词语之前出现的n个词语作为输入向量，也就是现在大家说的词嵌入(word embeddings)向量。这些词嵌入在级联后进入一个隐藏层，该层的输出然后通过一个softmax层。如图3所示。 △&nbsp;前馈神经网络语言模型 而现在构建语言模型的前馈神经网络，已被循环神经网络(RNNs)和长短期记忆神经网络(LSTMs)取代。 虽然后来提出许多新模型在经典LSTM上进行了扩展，但它仍然是强有力的基础模型。甚至Bengio等人的经典前馈神经网络在某些设定下也和更复杂的模型效果相当，因为这些任务只需要考虑邻近的词语。理解这些语言模型究竟捕捉了哪些信息，也是当今一个活跃的研究领域。 语言模型的建立是一种无监督学习(unsupervisedlearning)，Yann LeCun称之为预测学习(predictivelearning)，是获得世界如何运作常识的先决条件。 关于语言模型最引人注目的是，尽管它很简单，但却与后文许多核心进展息息相关。反过来，这也意味着NLP领域许多重要进展都可以简化为某种形式的语言模型构建。但要实现对自然语言真正意义上的理解，仅仅从原始文本中进行学习是不够的，我们需要新的方法和模型。 2.5、里程碑五：2008多任务学习(Multi-task learning) 多任务学习是在多个任务下训练的模型之间共享参数的方法，在神经网络中通过捆绑不同层的权重轻松实现。多任务学习思想1993年Rich Caruana首次提出，并应用于道路追踪和肺炎预测。多任务学习鼓励模型学习对多个任务有效的表征描述。这对于学习一般的、低级的描述形式、集中模型的注意力或在训练数据有限的环境中特别有用。 多任务学习2008年被Collobert和Weston等人首次在自然语言处理领域应用于神经网络。在他们的模型中，词嵌入矩阵被两个在不同任务下训练的模型共享，如图4所示。 △&nbsp;词嵌入矩阵共享 共享的词嵌入矩阵使模型可以相互协作，共享矩阵中的低层级信息，而词嵌入矩阵往往构成了模型中需要训练的绝大部分参数。 Collobert和Weston发表于2008年的论文，影响远远超过了它在多任务学习中的应用。它开创的诸如预训练词嵌入和使用卷积神经网络处理文本的方法，在接下来的几年被广泛应用。他们也因此获得2018年机器学习国际会议(ICML)的test-of-time奖。 如今，多任务学习在自然语言处理领域广泛使用，而利用现有或“人工”任务已经成为NLP指令库中的一个有用工具。 虽然参数的共享是预先定义好的，但在优化的过程中却可以学习不同的共享模式。当模型越来越多地在多个任务上进行测评以评估其泛化能力时，多任务学习就变得愈加重要，近年来也涌现出更多针对多任务学习的评估基准。 2.6、里程碑六：2013词嵌入 稀疏向量对文本进行表示的词袋模型，在自然语言处理领域有很长历史。而用稠密的向量对词语进行描述，也就是词嵌入，则在2001年首次出现。2013年Mikolov等人工作主要创新之处在于，通过去除隐藏层和近似计算目标使词嵌入模型的训练更为高效。 尽管这些改变本质上十分简单，但它们与高效的word2vec(wordto vector用来产生词向量的相关模型)组合在一起，使得大规模的词嵌入模型训练成为可能。 Word2vec有两种不同的实现方法：CBOW(continuousbag-of-words)和skip-gram。它们在预测目标上有所不同：一个是根据周围的词语预测中心词语，另一个则恰恰相反。如图5所示。 △&nbsp;CBOW和skip-gram架构 虽然这些嵌入与使用前馈神经网络学习的嵌入在概念上没有区别，但是在一个非常大语料库上的训练使它们能够获取诸如性别、动词时态和国际事务等单词之间的特定关系。如下图 4 所示。 △&nbsp;word2vec捕获的联系 这些关系和它们背后的意义激起了人们对词嵌入的兴趣，许多研究都在关注这些线性关系的来源。然而，使词嵌入成为目前自然语言处理领域中流砥柱的，是将预训练的词嵌入矩阵用于初始化可以提高大量下游任务性能的事实。 虽然word2vec捕捉到的关系具有直观且几乎不可思议的特性，但后来的研究表明，word2vec本身并没有什么特殊之处：词嵌入也可以通过矩阵分解来学习，经过适当的调试，经典的矩阵分解方法SVD和LSA都可以获得相似的结果。从那时起，大量的工作开始探索词嵌入的不同方面。尽管有很多发展，word2vec仍是目前应用最为广泛的选择。 Word2vec应用范围也超出了词语级别：带有负采样的skip-gram——一个基于上下文学习词嵌入的方便目标，已经被用于学习句子的表征。它甚至超越了自然语言处理的范围，被应用于网络和生物序列等领域。 一个激动人心的研究方向是在同一空间中构建不同语言的词嵌入模型，以达到(零样本)跨语言转换的目的。通过无监督学习构建这样的映射变得越来越有希望(至少对于相似的语言来说)，这也为语料资源较少的语言和无监督机器翻译的应用程序创造可能。 2.7、里程碑七：2013RNN/CNN用于NLP的神经网络 2013和2014年是自然语言处理领域神经网络时代的开始。其中三种类型的神经网络应用最为广泛：循环神经网络(recurrentneural networks)、卷积神经网络(convolutionalneural networks)和结构递归神经网络(recursiveneural networks)。 循环神经网络是NLP领域处理动态输入序列最自然的选择。Vanilla循环神经网络很快被经典的长短期记忆网络(long-shorttermmemory networks，LSTM)代替，该模型能更好地解决梯度消失和梯度爆炸问题。 在2013年之前，人们仍认为循环神经网络很难训练，直到Ilya Sutskever博士的论文改变了循环神经网络这一名声。双向的长短期记忆记忆网络通常被用于同时处理出现在左侧和右侧的文本内容。LSTM 结构如图7所示。 △&nbsp;LSTM网络 应用于文本的卷积神经网络只在两个维度上进行操作，卷积层只需要在时序维度上移动即可。图8展示了应用于自然语言处理的卷积神经网络的典型结构。 △&nbsp;卷积神经网络 与循环神经网络相比，卷积神经网络的一个优点是具有更好的并行性。 因为卷积操作中每个时间步的状态只依赖于局部上下文，而不是循环神经网络中那样依赖于所有过去的状态。卷积神经网络可以使用更大的卷积层涵盖更广泛的上下文内容。卷积神经网络也可以和长短期记忆网络进行组合和堆叠，还可以用来加速长短期记忆网络的训练。 循环神经网络和卷积神经网络都将语言视为一个序列。但从语言学的角度来看，语言是具有层级结构的：词语组成高阶的短语和小句，它们本身可以根据一定的产生规则递归地组合。这激发了利用结构递归神经网络，以树形结构取代序列来表示语言的想法，如图9所示。 △&nbsp;结构递归神经网络 结构递归神经网络自下而上构建序列的表示，与从左至右或从右至左对序列进行处理的循环神经网络形成鲜明的对比。树中的每个节点是通过子节点的表征计算得到的。一个树也可以视为在循环神经网络上施加不同的处理顺序，所以长短期记忆网络则可以很容易地被扩展为一棵树。 不只是循环神经网络和长短期记忆网络可以扩展到使用层次结构，词嵌入也可以在语法语境中学习，语言模型可以基于句法堆栈生成词汇，图形卷积神经网络可以树状结构运行。 2.8、里程碑八：2014序列到序列模型(Sequence-to-sequencemodels) 2014年，Sutskever等人提出序列到序列学习，即使用神经网络将一个序列映射到另一个序列的一般化框架。在这个框架中，一个作为编码器的神经网络对句子符号进行处理，并将其压缩成向量表示；然后，一个作为解码器的神经网络根据编码器的状态逐个预测输出符号，并将前一个预测得到的输出符号作为预测下一个输出符号的输入。如图10所示。 △&nbsp;序列到序列模型 机器翻译是这一框架的杀手级应用。2016年，谷歌宣布他们将用神经机器翻译模型取代基于短语的整句机器翻译模型。谷歌大脑负责人Jeff Dean表示，这意味着用500行神经网络模型代码取代50万行基于短语的机器翻译代码。 由于其灵活性，该框架在自然语言生成任务上被广泛应用，其编码器和解码器分别由不同的模型来担任。更重要的是，解码器不仅可以适用于序列，在任意表示上均可以应用。比如基于图片生成描述(如图11)、基于表格生成文本、根据源代码改变生成描述，以及众多其他应用。 △&nbsp;基于图像生成标题(Vinyalset al., 2015) 序列到序列的学习甚至可以应用到自然语言处理领域常见的结构化预测任务中，也就是输出具有特定的结构。为简单起见，输出就像选区解析一样被线性化(如图12)。在给定足够多训练数据用于语法解析的情况下，神经网络已经被证明具有产生线性输出和识别命名实体的能力。 △&nbsp;线性化选区解析树(Vinyalset al., 2015) 序列的编码器和解码器通常都是基于循环神经网络，但也可以使用其他模型。新的结构主要都从机器翻译的工作中诞生，它已经成了序列到序列模型的培养基。近期提出的模型有深度长短期记忆网络、卷积编码器、Transformer(一个基于自注意力机制的全新神经网络架构)以及长短期记忆依赖网络和的 Transformer 结合体等。 2.9、里程碑九：2015注意力机制和基于记忆的神经网络 注意力机制是神经网络机器翻译(NMT)的核心创新之一，也是使神经网络机器翻译优于经典的基于短语的机器翻译的关键。序列到序列学习的主要瓶颈是，需要将源序列的全部内容压缩为固定大小的向量。注意力机制通过让解码器回顾源序列的隐藏状态，以此为解码器提供加权平均值的输入来缓解这一问题，如图13所示。 △&nbsp;注意力机制 之后，各种形式的注意力机制涌现而出。注意力机制被广泛接受，在各种需要根据输入的特定部分做出决策的任务上都有潜在的应用。它已经被应用于句法分析、阅读理解、单样本学习等任务中。它的输入甚至不需要是一个序列，而可以包含其他表示，比如图像的描述(图14)。 注意力机制一个有用的附带作用是它通过注意力权重来检测输入的哪一部分与特定的输出相关，从而提供了一种罕见的虽然还是比较浅层次的，对模型内部运作机制的窥探。 △&nbsp;图像描述模型中的视觉注意力机制指示在生成”飞盘”时所关注的内容 注意力机制不仅仅局限于输入序列。自注意力机制可用来观察句子或文档中周围的单词，获得包含更多上下文信息的词语表示。多层的自注意力机制是神经机器翻译前沿模型Transformer的核心。 注意力机制可以视为模糊记忆的一种形式，其记忆的内容包括模型之前的隐藏状态，由模型选择从记忆中检索哪些内容。与此同时，更多具有明确记忆单元的模型被提出。 他们有很多不同的变化形式，比如神经图灵机(NeuralTuring Machines)、记忆网络(MemoryNetwork)、端到端的记忆网络(End-to-endMemory Newtorks)、动态记忆网络(DynamicMemoryNetworks)、神经可微计算机(NeuralDifferentiable Computer)、循环实体网络(RecurrentEntityNetwork)。 记忆的存取通常与注意力机制相似，基于与当前状态且可以读取和写入。这些模型之间的差异体现在它们如何实现和利用存储模块。 比如说，端到端的记忆网络对输入进行多次处理并更新内存，以实行多次推理。神经图灵机也有一个基于位置的寻址方式，使它们可以学习简单的计算机程序，比如排序。 基于记忆的模型通常用于需要长时间保留信息的任务中，例如语言模型构建和阅读理解。记忆模块的概念非常通用，知识库和表格都可以作为记忆模块，记忆模块也可以基于输入的全部或部分内容进行填充。 2.10、里程碑十：2018预训练语言模型 预训练的词嵌入与上下文无关，仅用于初始化模型中的第一层。近几个月以来，许多有监督的任务被用来预训练神经网络。相比之下，语言模型只需要未标记的文本，因此其训练可以扩展到数十亿单词的语料、新的领域、新的语言。预训练的语言模型于 2015年被首次提出，但直到最近它才被证明在大量不同类型的任务中均十分有效。语言模型嵌入可以作为目标模型中的特征，或者根据具体任务进行调整。如下图所示，语言模型嵌入为许多任务的效果带来了巨大的改进。 △&nbsp;改进的语言模型嵌入 使用预训练的语言模型可以在数据量十分少的情况下有效学习。由于语言模型的训练只需要无标签的数据，因此他们对于数据稀缺的低资源语言特别有利。 2018年10月，谷歌AI语言组发布BERT语言模型预训练，已被证明可有效改进许多自然语言处理任务(Dai and Le, 2015; Peters et al., 2017, 2018; Radford etal., 2018; Howard and Ruder, 2018)。 这些任务包括句子级任务，如自然语言推理inference(Bowman et al., 2015; Williams et al., 2018)和释义paraphrasing(Dolan and Brockett, 2005)，旨在通过整体分析来预测句子之间的关系；以及词块级任务，如命名实体识别(Tjong KimSang and De Meulder, 2003)和SQuAD问题回答(Rajpurkar et al., 2016)，其中模型需要在词块级别生成细粒度输出。 近年七大技术里程碑小结 除了上述七大技术里程碑，一些其他进展虽不如上面提到的那样流行，但仍产生了广泛的影响。 基于字符的描述(Character-based representations)，在字符层级上使用卷积神经网络和长短期记忆网络，以获得一个基于字符的词语描述，目前已经相当常见了，特别是对于那些语言形态丰富的语种或那些形态信息十分重要、包含许多未知单词的任务。据目前所知，基于字符的描述最初用于序列标注，现在，基于字符的描述方法，减轻了必须以增加计算成本为代价建立固定词汇表的问题，并使完全基于字符的机器翻译的应用成为可能。 对抗学习(Adversarial learning)，在机器学习领域已经取得了广泛应用，在自然语言处理领域也被应用于不同的任务中。对抗样例的应用也日益广泛，他们不仅仅是探测模型弱点的工具，更能使模型更具鲁棒性(robust)。(虚拟的)对抗性训练，也就是最坏情况的扰动，和域对抗性损失(domain-adversariallosses)都是可以使模型更具鲁棒性的有效正则化方式。生成对抗网络(GANs)目前在自然语言生成任务上还不太有效，但在匹配分布上十分有用。 强化学习(Reinforcement learning)，在具有时间依赖性任务上证明有效，比如在训练期间选择数据和对话建模。在机器翻译和概括任务中，强化学习可以有效地直接优化“红色”和“蓝色”这样不可微的度量，不必去优化像交叉熵这样的代理损失函数。同样，逆向强化学习(inversereinforcement learning)在类似视频故事描述这样的奖励机制非常复杂且难以具体化的任务中，也非常有用。 自然语言处理NLP知识结构 文|秦陇纪，数据简化DataSimp 自然语言处理(计算机语言学、自然语言理解)涉及：字处理，词处理，语句处理，篇章处理词处理分词、词性标注、实体识别、词义消歧语句处理句法分析(SyntacticAnalysis)、语义分析(SenmanticAnalysis)等。其中，重点有： 1、句法语义分析：分词，词性标记，命名实体识别。 2、信息抽取 3、文本挖掘：文本聚类，情感分析。基于统计。 4、机器翻译：基于规则，基于统计，基于神经网络。 5、信息检索 6、问答系统 7、对话系统建议…本文总结的自然语言处理历史、模型、知识体系结构内容，涉及NLP的语言理论、算法和工程实践各方面，内容繁杂。参考黄志洪老师自然语言处理课程、宗成庆老师《统计自然语言处理》，郑捷2017年电子工业出版社出版的图书《NLP汉语自然语言处理原理与实践》，以及国外著名NLP书籍的英文资料、汉译版资料。 一、NLP知识结构概述 1)自然语言处理：利用计算机为工具，对书面实行或者口头形式进行各种各样的处理和加工的技术，是研究人与人交际中以及人与计算机交际中的演员问题的一门学科，是人工智能的主要内容。 2)自然语言处理是研究语言能力和语言应用的模型，建立计算机(算法)框架来实现这样的语言模型，并完善、评测、最终用于设计各种实用系统。 3)研究问题(主要)： 信息检索 机器翻译 文档分类 问答系统 信息过滤 自动文摘 信息抽取 文本挖掘 舆情分析 机器写作 语音识别 研究模式：自然语言场景问题，数学算法，算法如何应用到解决这些问题，预料训练，相关实际应用 自然语言的困难： 场景的困难：语言的多样性、多变性、歧义性 学习的困难：艰难的数学模型(hmm,crf,EM,深度学习等) 语料的困难：什么的语料？语料的作用？如何获取语料？ 二、NLP知识十大结构 2.1、形式语言与自动机 语言：按照一定规律构成的句子或者字符串的有限或者无限的集合。 描述语言的三种途径： 穷举法 文法(产生式系统)描述 自动机 自然语言不是人为设计而是自然进化的，形式语言比如：运算符号、化学分子式、编程语言 形式语言理论朱啊哟研究的是内部结构模式这类语言的纯粹的语法领域，从语言学而来，作为一种理解自然语言的句法规律，在计算机科学中，形式语言通常作为定义编程和语法结构的基础 形式语言与自动机基础知识： 集合论 图论 自动机的应用： 1，单词自动查错纠正 2，词性消歧(什么是词性？什么的词性标注？为什么需要标注？如何标注？) 形式语言的缺陷： 1、对于像汉语，英语这样的大型自然语言系统，难以构造精确的文法 2、不符合人类学习语言的习惯 3、有些句子语法正确，但在语义上却不可能，形式语言无法排出这些句子 4、解决方向：基于大量语料，采用统计学手段建立模型 2.2、语言模型 1)、语言模型(重要)：通过语料计算某个句子出现的概率(概率表示)，常用的有2-元模型，3-元模型 2)、语言模型应用： 语音识别歧义消除例如，给定拼音串：ta shi yan yan jiu saun fa de 可能的汉字串：踏实烟酒算法的他是研究酸法的他是研究算法的，显然，最后一句才符合。 3)、语言模型的启示： 1、开启自然语言处理的统计方法 2、统计方法的一般步骤： 收集大量语料 对语料进行统计分析，得出知识 针对场景建立算法模型 解释和应用结果 4)、语言模型性能评价，包括评价目标，评价的难点，常用指标(交叉熵，困惑度) 5)、数据平滑： 数据平滑的概念，为什么需要平滑 平滑的方法，加一法，加法平滑法，古德-图灵法，J-M法，Katz平滑法等 6)、语言模型的缺陷： 语料来自不同的领域，而语言模型对文本类型、主题等十分敏感 n与相邻的n-1个词相关，假设不是很成立。 2.3、概率图模型 生成模型与判别模型，贝叶斯网络，马尔科夫链与隐马尔科夫模型(HMM) 1)概率图模型概述(什么的概率图模型，参考清华大学教材《概率图模型》) 2)马尔科夫过程(定义，理解) 3)隐马尔科夫过程(定义，理解) HMM的三个基本问题(定义，解法，应用) 注：第一个问题，涉及最大似然估计法，第二个问题涉及EM算法，第三个问题涉及维特比算法，内容很多，要重点理解，(参考书李航《统计学习方法》，网上博客，笔者github) 2.4、马尔科夫网，最大熵模型，条件随机场(CRF) 1)、HMM的三个基本问题的参数估计与计算 2)、什么是熵 3)、EM算法(应用十分广泛，好好理解) 4)、HMM的应用 5)、层次化马尔科夫模型与马尔科夫网络 提出原因，HMM存在两个问题 6)、最大熵马尔科夫模型 优点：与HMM相比，允许使用特征刻画观察序列，训练高效 缺点：存在标记偏置问题 7)、条件随机场及其应用(概念，模型过程，与HMM关系) 参数估计方法(GIS算法，改进IIS算法) CRF基本问题：特征选取(特征模板)、概率计算、参数训练、解码(维特比) 应用场景： 词性标注类问题(现在一般用RNN+CRF) 中文分词(发展过程，经典算法，了解开源工具jieba分词) 中文人名，地名识别 8)、CRF++ 2.5、命名实体识别，词性标注，内容挖掘、语义分析与篇章分析(大量用到前面的算法) 1)、命名实体识别问题 相关概率，定义 相关任务类型 方法(基于规程-&gt;基于大规模语料库) 2)、未登录词的解决方法(搜索引擎，基于语料) 3)、CRF解决命名实体识别(NER)流程总结： 训练阶段：确定特征模板，不同场景(人名，地名等)所使用的特征模板不同，对现有语料进行分词，在分词结果基础上进行词性标注(可能手工)，NER对应的标注问题是基于词的，然后训练CRF模型，得到对应权值参数值 识别过程：将待识别文档分词，然后送入CRF模型进行识别计算(维特比算法)，得到标注序列，然后根据标注划分出命名实体 4)、词性标注(理解含义，意义)及其一致性检查方法(位置属性向量，词性标注序列向量，聚类或者分类算法) 2.6、句法分析 1)、句法分析理解以及意义 1、句法结构分析 完全句法分析 浅层分析 2、依存关系分析 2)、句法分析方法 1、基于规则的句法结构分析 2、基于统计的语法结构分析 2.7 文本分类，情感分析 1)、文本分类，文本排重 文本分类：在预定义的分类体系下，根据文本的特征，将给定的文本与一个或者多个类别相关联 典型应用：垃圾邮件判定，网页自动分类 2)、文本表示，特征选取与权重计算，词向量 文本特征选择常用方法： 1、基于本文频率的特征提取法 2、信息增量法 3、X2(卡方)统计量 4、互信息法 3)、分类器设计 SVM，贝叶斯，决策树等 4)、分类器性能评测 1、召回率 2、正确率 3、F1值 5)、主题模型(LDA)与PLSA LDA模型十分强大，基于贝叶斯改进了PLSA，可以提取出本章的主题词和关键词，建模过程复杂，难以理解。 6)、情感分析 借助计算机帮助用户快速获取，整理和分析相关评论信息，对带有感情色彩的主观文本进行分析，处理和归纳例如，评论自动分析，水军识别。 某种意义上看，情感分析也是一种特殊的分类问题 7)、应用案例 2.8、信息检索，搜索引擎及其原理 1)、信息检索起源于图书馆资料查询检索，引入计算机技术后，从单纯的文本查询扩展到包含图片，音视频等多媒体信息检索，检索对象由数据库扩展到互联网。 1、点对点检索 2、精确匹配模型与相关匹配模型 3、检索系统关键技术：标引，相关度计算 2)、常见模型：布尔模型，向量空间模型，概率模型 3)、常用技术：倒排索引，隐语义分析(LDA等) 4)、评测指标 2.9、自动文摘与信息抽取，机器翻译，问答系统 1)、统计机器翻译的的思路，过程，难点，以及解决 2)、问答系统 基本组成：问题分析，信息检索，答案抽取 类型：基于问题-答案，基于自由文本 典型的解决思路 3)、自动文摘的意义，常用方法 4)、信息抽取模型(LDA等) 2.10深度学习在自然语言中的应用 1)、单词表示，比如词向量的训练(wordvoc) 2)、自动写文本 写新闻等 3)、机器翻译 4)、基于CNN、RNN的文本分类 5)、深度学习与CRF结合用于词性标注 三、中文NLP知识目录 选自郑捷2017年电子工业出版社出版的图书《NLP汉语自然语言处理原理与实践》。 第1章 中文语言的机器处理 1 1.1 历史回顾 2 1.1.1 从科幻到现实 2 1.1.2 早期的探索 3 1.1.3 规则派还是统计派 3 1.1.4 从机器学习到认知计算 5 1.2 现代自然语言系统简介 6 1.2.1 NLP流程与开源框架 6 1.2.2 哈工大NLP平台及其演示环境 9 1.2.3 StanfordNLP团队及其演示环境 11 1.2.4 NLTK开发环境 13 1.3 整合中文分词模块 16 1.3.1 安装Ltp Python组件 17 1.3.2 使用Ltp 3.3进行中文分词 18 1.3.3 使用结巴分词模块 20 1.4 整合词性标注模块 22 1.4.1 Ltp 3.3词性标注 23 1.4.2 安装StanfordNLP并编写Python接口类 24 1.4.3 执行Stanford词性标注 28 1.5 整合命名实体识别模块 29 1.5.1 Ltp 3.3命名实体识别 29 1.5.2 Stanford命名实体识别 30 1.6 整合句法解析模块 32 1.6.1 Ltp 3.3句法依存树 33 1.6.2 StanfordParser类 35 1.6.3 Stanford短语结构树 36 1.6.4 Stanford依存句法树 37 1.7 整合语义角色标注模块 38 1.8 结语 40 第2章 汉语语言学研究回顾 42 2.1 文字符号的起源 42 2.1.1 从记事谈起 43 2.1.2 古文字的形成 47 2.2 六书及其他 48 2.2.1 象形 48 2.2.2 指事 50 2.2.3 会意 51 2.2.4 形声 53 2.2.5 转注 54 2.2.6 假借 55 2.3 字形的流变 56 2.3.1 笔与墨的形成与变革 56 2.3.2 隶变的方式 58 2.3.3 汉字的符号化与结构 61 2.4 汉语的发展 67 2.4.1 完整语义的基本形式——句子 68 2.4.2 语言的初始形态与文言文 71 2.4.3 白话文与复音词 73 2.4.4 白话文与句法研究 78 2.5 三个平面中的语义研究 80 2.5.1 词汇与本体论 81 2.5.2 格语法及其框架 84 2.6 结语 86 第3章 词汇与分词技术 88 3.1 中文分词 89 3.1.1 什么是词与分词规范 90 3.1.2 两种分词标准 93 3.1.3 歧义、机械分词、语言模型 94 3.1.4 词汇的构成与未登录词 97 3.2 系统总体流程与词典结构 98 3.2.1 概述 98 3.2.2 中文分词流程 99 3.2.3 分词词典结构 103 3.2.4 命名实体的词典结构 105 3.2.5 词典的存储结构 108 3.3 算法部分源码解析 111 3.3.1 系统配置 112 3.3.2 Main方法与例句 113 3.3.3 句子切分 113 3.3.4 分词流程 117 3.3.5 一元词网 118 3.3.6 二元词图 125 3.3.7 NShort算法原理 130 3.3.8 后处理规则集 136 3.3.9 命名实体识别 137 3.3.10 细分阶段与最短路径 140 3.4 结语 142 第4章 NLP中的概率图模型 143 4.1 概率论回顾 143 4.1.1 多元概率论的几个基本概念 144 4.1.2 贝叶斯与朴素贝叶斯算法 146 4.1.3 文本分类 148 4.1.4 文本分类的实现 151 4.2 信息熵 154 4.2.1 信息量与信息熵 154 4.2.2 互信息、联合熵、条件熵 156 4.2.3 交叉熵和KL散度 158 4.2.4 信息熵的NLP的意义 159 4.3 NLP与概率图模型 160 4.3.1 概率图模型的几个基本问题 161 4.3.2 产生式模型和判别式模型 162 4.3.3 统计语言模型与NLP算法设计 164 4.3.4 极大似然估计 167 4.4 隐马尔科夫模型简介 169 4.4.1 马尔科夫链 169 4.4.2 隐马尔科夫模型 170 4.4.3 HMMs的一个实例 171 4.4.4 Viterbi算法的实现 176 4.5 最大熵模型 179 4.5.1 从词性标注谈起 179 4.5.2 特征和约束 181 4.5.3 最大熵原理 183 4.5.4 公式推导 185 4.5.5 对偶问题的极大似然估计 186 4.5.6 GIS实现 188 4.6 条件随机场模型 193 4.6.1 随机场 193 4.6.2 无向图的团(Clique)与因子分解 194 4.6.3 线性链条件随机场 195 4.6.4 CRF的概率计算 198 4.6.5 CRF的参数学习 199 4.6.6 CRF预测标签 200 4.7 结语 201 第5章 词性、语块与命名实体识别 202 5.1 汉语词性标注 203 5.1.1 汉语的词性 203 5.1.2 宾州树库的词性标注规范 205 5.1.3stanfordNLP标注词性 210 5.1.4 训练模型文件 213 5.2 语义组块标注 219 5.2.1 语义组块的种类 220 5.2.2 细说NP 221 5.2.3 细说VP 223 5.2.4 其他语义块 227 5.2.5 语义块的抽取 229 5.2.6 CRF的使用 232 5.3 命名实体识别 240 5.3.1 命名实体 241 5.3.2 分词架构与专名词典 243 5.3.3 算法的策略——词典与统计相结合 245 5.3.4 算法的策略——层叠式架构 252 5.4 结语 259 第6章 句法理论与自动分析 260 6.1 转换生成语法 261 6.1.1 乔姆斯基的语言观 261 6.1.2 短语结构文法 263 6.1.3 汉语句类 269 6.1.4 谓词论元与空范畴 274 6.1.5 轻动词分析理论 279 6.1.6 NLTK操作句法树 280 6.2 依存句法理论 283 6.2.1 配价理论 283 6.2.2 配价词典 285 6.2.3 依存理论概述 287 6.2.4 Ltp依存分析介绍 290 6.2.5 Stanford依存转换、解析 293 6.3 PCFG短语结构句法分析 298 6.3.1 PCFG短语结构 298 6.3.2 内向算法和外向算法 301 6.3.3 Viterbi算法 303 6.3.4 参数估计 304 6.3.5 Stanford的PCFG算法训练 305 6.4 结语 310 第7章 建设语言资源库 311 7.1 语料库概述 311 7.1.1 语料库的简史 312 7.1.2 语言资源库的分类 314 7.1.3 语料库的设计实例：国家语委语料库 315 7.1.4 语料库的层次加工 321 7.2 语法语料库 323 7.2.1 中文分词语料库 323 7.2.2 中文分词的测评 326 7.2.3 宾州大学CTB简介 327 7.3 语义知识库 333 7.3.1 知识库与HowNet简介 333 7.3.2 发掘义原 334 7.3.3 语义角色 336 7.3.4 分类原则与事件分类 344 7.3.5 实体分类 347 7.3.6 属性与分类 352 7.3.7 相似度计算与实例 353 7.4 语义网与百科知识库 360 7.4.1 语义网理论介绍 360 7.4.2 维基百科知识库 364 7.4.3 DBpedia抽取原理 365 7.5 结语 368 第8章 语义与认知 370 8.1 回顾现代语义学 371 8.1.1 语义三角论 371 8.1.2 语义场论 373 8.1.3 基于逻辑的语义学 376 8.2 认知语言学概述 377 8.2.1 象似性原理 379 8.2.2 顺序象似性 380 8.2.3 距离象似性 380 8.2.4 重叠象似性 381 8.3 意象图式的构成 383 8.3.1 主观性与焦点 383 8.3.2 范畴化：概念的认知 385 8.3.3 主体与背景 390 8.3.4 意象图式 392 8.3.5 社交中的图式 396 8.3.6 完形：压缩与省略 398 8.4 隐喻与转喻 401 8.4.1 隐喻的结构 402 8.4.2 隐喻的认知本质 403 8.4.3 隐喻计算的系统架构 405 8.4.4 隐喻计算的实现 408 8.5 构式语法 412 8.5.1 构式的概念 413 8.5.2 句法与构式 415 8.5.3 构式知识库 417 8.6 结语 420 第9章 NLP中的深度学习 422 9.1 神经网络回顾 422 9.1.1 神经网络框架 423 9.1.2 梯度下降法推导 425 9.1.3 梯度下降法的实现 427 9.1.4 BP神经网络介绍和推导 430 9.2 Word2Vec简介 433 9.2.1 词向量及其表达 434 9.2.2 Word2Vec的算法原理 436 9.2.3 训练词向量 439 9.2.4 大规模上下位关系的自动识别 443 9.3 NLP与RNN 448 9.3.1Simple-RNN 449 9.3.2 LSTM原理 454 9.3.3 LSTM的Python实现 460 9.4 深度学习框架与应用 467 9.4.1 Keras框架介绍 467 9.4.2 Keras序列标注 471 9.4.3 依存句法的算法原理 478 9.4.4 Stanford依存解析的训练过程 483 9.5 结语 488 第10章 语义计算的架构 490 10.1 句子的语义和语法预处理 490 10.1.1 长句切分和融合 491 10.1.2 共指消解 496 10.2 语义角色 502 10.2.1 谓词论元与语义角色 502 10.2.2PropBank简介 505 10.2.3 CPB中的特殊句式 506 10.2.4 名词性谓词的语义角色 509 10.2.5PropBank展开 512 10.3 句子的语义解析 517 10.3.1 语义依存 517 10.3.2 完整架构 524 10.3.3 实体关系抽取 527 10.4 结语 531 [29] 自然语言处理NLP国内研究方向机构导师 文|中文信息协会《中文信息处理发展报告2016》，数据简化DataSimp 文字语言VS数字信息 数字、文字和自然语言一样，都是信息的载体，他们之间原本有着天然的联系。语言和数学的产生都是为了交流，从文字、数字和语言的发展历史，可以了解到语言、文字和数字有着内在的联系。自然语言处理NLP主要涉及三种文本，自由文本、结构化文本、半结构化文本。 自然语言理解Natural Language Understanding(NLU)，实现人机间自然语言通信，意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本表达给定的意图、思想等。自然语言生成NLG，是人工或机器生成语言。 斯坦福自然语言处理NLP工具资料收集、斯坦福分词、Stanford中文实体识别，最早做自然语言处理的网址https://nlp.stanford.edu/software/segmenter.shtml。 哈尔滨工业大学智能技术与自然语言处理研究室(IntelligentTechnology &amp; Natural Language Processing Lab, ITNLPLab)是国内较早从事自然语言处理和语言智能技术的研究室。 除了新兴的文本数据简化领域：秦陇纪(数据简化技术中心筹)，自然语言处理NaturalLanguage Processing领域主要包括基础研究和应用研究。 基础研究 词法与句法分析：李正华、陈文亮、张民(苏州大学) 语义分析：周国栋、李军辉(苏州大学) 篇章分析：王厚峰、李素建(北京大学) 语言认知模型：王少楠，宗成庆(中科院自动化研究所) 语言表示与深度学习：黄萱菁、邱锡鹏(复旦大学) 知识图谱与计算：李涓子、候磊(清华大学) 应用研究 文本分类与聚类：涂存超，刘知远(清华大学) 信息抽取：孙乐、韩先培(中国科学院软件研究所) 情感分析：黄民烈(清华大学) 自动文摘：万小军、姚金戈(北京大学) 信息检索：刘奕群、马少平(清华大学) 信息推荐与过滤：王斌(中科院信工所)，鲁骁(国家计算机网络应急中心) 自动问答：赵军、刘康，何世柱(中科院自动化研究所) 机器翻译：张家俊、宗成庆(中科院自动化研究所) 社会媒体处理：刘挺、丁效(哈尔滨工业大学) 语音技术：说话人识别——郑方(清华大学)，王仁宇(江苏师范大学) 语音合成——陶建华(中科院自动化研究所) 语音识别——王东(清华大学) 文字识别：刘成林(中科院自动化研究所) 多模态信息处理：陈晓鸥(北京大学) 医疗健康信息处理：陈清财、汤步洲(哈尔滨工业大学) 少数民族语言信息处理：吾守尔•斯拉木(新疆大学) —&nbsp;完&nbsp;—","@type":"BlogPosting","url":"https://mlh.app/2019/02/21/a86cd5c46c377d6b95ec6e7df5e213ea.html","headline":"史上最强NLP知识集合：知识结构、发展历程、导师名单","dateModified":"2019-02-21T00:00:00+08:00","datePublished":"2019-02-21T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/02/21/a86cd5c46c377d6b95ec6e7df5e213ea.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>史上最强NLP知识集合：知识结构、发展历程、导师名单</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>转载自 数据简化DataSimp&nbsp;<br> 作者 秦陇纪&nbsp;<br> 郭一璞 编辑&nbsp;<br> 量子位 报道 | 公众号 QbitAI</p> 
  <p>本篇推送包含三篇文章，</p> 
  <blockquote> 
   <p><a href="https://www.baidu.com/s?wd=%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2%E5%8D%81%E5%A4%A7%E9%87%8C%E7%A8%8B%E7%A2%91%E3%80%8B&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" rel="nofollow">《自然语言处理技术发展史十大里程碑》</a></p> 
   <p>《语言处理NLP知识结构》</p> 
   <p><a href="https://www.baidu.com/s?wd=%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP%E5%9B%BD%E5%86%85%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E6%9C%BA%E6%9E%84%E5%AF%BC%E5%B8%88%E3%80%8B&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" rel="nofollow">《自然语言处理NLP国内研究方向机构导师》</a></p> 
  </blockquote> 
  <p><span style="color:#ffbb66;">总共超过20000字，量子位建议先码再看。</span></p> 
  <h3><a name="t0"></a>自然语言处理技术发展史十大里程碑</h3> 
  <p>文|秦陇纪，参考|黄昌宁、张小凤、Sebatian Ruder</p> 
  <p>自然语言是人类独有的智慧结晶。</p> 
  <p>自然语言处理(NaturalLanguage Processing，NLP)是计算机科学领域与人工智能领域中的一个重要方向，旨在研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。用自然语言与计算机进行通信，有着十分重要的实际应用意义，也有着革命性的理论意义。</p> 
  <p>由于理解自然语言，需要关于外在世界的广泛知识以及运用操作这些知识的能力，所以自然语言处理，也被视为解决人工智能完备(AI-complete)的核心问题之一。对自然语言处理的研究也是充满魅力和挑战的。</p> 
  <p>微软亚洲研究院<strong>黄昌宁</strong>、<strong>张小凤</strong>在2013年发表论文，就过去50年以来自然语言处理(NLP)研究领域中的发现和发展要点进行阐述，其中包括两个事实和三大重要成果。</p> 
  <p>近年来，自然语言处理的语料库调查显示如下两个事实：</p> 
  <p>(1)对于句法分析来说，基于单一标记的短语结构规则是不充分的；单个标记的PSG规则不足以进行自然语言描述；</p> 
  <p>(2)PSG规则在文本语料库中具有偏差分布，即PSG规则的总数似乎不能够涵盖大型语料库中发现的语言现象，这不符合语言学家的期望。短语结构规则在真实文本中的分布呈现严重扭曲。换言之，有限数目的短语结构规则不能覆盖大规模语料中的语法现象。这与原先人们的预期大相径庭。</p> 
  <p>NLP技术发展历程在很大程度上受到以上两个事实的影响，在该领域中可以称得上里程碑式的成果有如下三个：</p> 
  <p>(1)复杂特征集和合一语法；</p> 
  <p>(2)语言学研究中的词汇主义；</p> 
  <p>(3)语料库方法和统计语言模型。业内人士普遍认为，大规模语言知识的开发和自动获取是NLP技术的瓶颈问题。因此，语料库建设和统计学习理论将成为该领域中的关键课题。</p> 
  <h1><a name="t1"></a>一、NLP研究传统问题</h1> 
  <p>自然语言处理(NLP)是计算机科学、信息工程和人工智能的子领域，涉及计算机和人类(自然)语言之间的交互，尤其是编程实现计算机处理和分析大量自然语言数据。自然语言处理的挑战包括语音识别，自然语言理解和自然语言生成。</p> 
  <p>信息输入、检索、人机对话等需求增多，使自然语言处理(NLP)成为21世纪初的热门学科。从50年代机器翻译和人工智能研究算起，NLP至今有长达半个世纪的历史了。</p> 
  <p>近年来这一领域中里程碑式的理论和方法贡献有如下三个：</p> 
  <p>(1)复杂特征集和合一语法；</p> 
  <p>(2)语言学研究中的词汇主义；</p> 
  <p>(3)语料库方法和统计语言模型。</p> 
  <p>这三个成果将继续对语言学、计算语言学和NLP的研究产生深远影响。为了理解这些成果的意义，先介绍一下两个相关事实。</p> 
  <p>自然语言处理中<span style="color:#f33b45;"><strong>识别句子 句法结构的</strong></span><span style="color:#3399ea;"><strong>句法分析</strong></span><span style="color:#f33b45;"><strong>的全过程</strong></span>：</p> 
  <p><span style="color:#f33b45;">(1)把句子中的词一个一个地切分出来；</span></p> 
  <p><span style="color:#f33b45;">(2)查词典，给句子中的每个词指派一个合适的词性(part of speech)；</span></p> 
  <p><span style="color:#f33b45;">(3)用句法规则把句子里包含的句法成分，如名词短语、动词短语、小句等，逐个地识别出来。</span></p> 
  <p><span style="color:#f33b45;">(4)判断每个短语的句法功能，如主语、谓语、宾语等，及其语义角色，最终得到句子的意义表示，如逻辑语义表达式</span>。</p> 
  <h2><a name="t2"></a>1.1、事实一：语言的结构歧义问题</h2> 
  <p>第一个事实(黄昌宁，张小凤，2013)是：<span style="color:#f33b45;">短语结构语法(PhraseStructure Grammar，简称PSG)不能有效地描写自然语言</span>。</p> 
  <p>PSG在Chomsky的语言学理论[1]中占有重要地位，并且在自然语言的句法描写中担当着举足轻重的角色。但是它有一些根本性的弱点，主要表现为它使用的是像词类和短语类那样的单一标记，因此<span style="color:#f33b45;">不能有效地指明和解释自然语言中的结构歧义问题</span>。</p> 
  <p>让我们先来看一看<strong>汉语中“V+N”组合</strong>。假如我们把“打击，委托，调查”等词指派为动词(V)；把“力度，方式，盗版，甲方”等词视为名词(N)，而且同意“打击力度”、“委托方式”是名词短语(NP)，“打击盗版”、“委托甲方”是动词短语(VP)，那么就会产生如下两条有歧义的句法规则：</p> 
  <p>(1) NP → V N</p> 
  <p>(2) VP → V N</p> 
  <p>换句话讲，当计算机观察到文本中相邻出现的“V+N”词类序列时，仍不能确定它们组成的究竟是NP还是VP。我们把这样的歧义叫做“<span style="color:#f33b45;">短语类型歧义</span>”。例如：</p> 
  <p>• 该公司正在招聘[销售V人员N]NP。</p> 
  <p>• 地球在不断[改变V形状N]VP。</p> 
  <p>下面再来看“N+V”的组合，也同样会产生带有短语类型歧义的规则对，如：</p> 
  <p>(3) NP → N V 例：市场调查；政治影响。</p> 
  <p>(4) S → N V 例：价格攀升；局势稳定。</p> 
  <p>其中标记S代表小句。</p> 
  <p>不仅如此，有时当机器观察到相邻出现的“N+V”词类序列时，甚至不能判断它们是不是在同一个短语中。也就是说，“N+V”词类序列可能组成名词短语NP或小句S，也有可能根本就不在同一个短语里。后面这种歧义称为“<span style="color:#f33b45;">短语边界歧义</span>”。下面是两个相关的例句：</p> 
  <p>• 中国的[铁路N建设V]NP发展很快。</p> 
  <p>• [中国的铁路N]NP建设V得很快。</p> 
  <p>前一个例句中，“铁路建设”组成一个NP；而在后一个例句中，这两个相邻的词却分属于两个不同的短语。这足以说明，基于单一标记的PSG不能充分地描述自然语言中的句法歧义现象。下面让我们再来看一些这样的例子。</p> 
  <p>(5)NP → V N1de N2</p> 
  <p>(6)VP → V N1de N2</p> 
  <p>其中de代表结构助词“的”。例如，“[削苹果]VP的刀”是NP; 而“削[苹果的皮]NP”则是VP。这里既有短语类型歧义，又有短语边界歧义。比如，“削V苹果N”这两个相邻的词，可能构成一个VP，也可能分处于两个相邻的短语中。</p> 
  <p>(7)NP → P N1de N2</p> 
  <p>(8)PP → P N1de N2</p> 
  <p>规则中P和PP分别表示介词和介词短语。例如，“[对上海]PP的印象”是NP; 而“对[上海的学生]NP”则是PP。相邻词“对P 上海N”可能组成一个PP，也可能分处于两个短语中。</p> 
  <p>(9)NP → NumPN1 de N2</p> 
  <p>其中NumP 表示数量短语。规则(9)虽然表示的是一个NP，但可分别代表两种结构意义：</p> 
  <p>(9a)NumP [N1de N2]NP 如：五个[公司的职员]NP</p> 
  <p>(9b)[NumPN1]NP de N2 如：[五个公司]NP 的职员</p> 
  <p>(10)NP → N1 N2N3</p> 
  <p>规则(10)表示的也是一个NP，但“N1+ N2”先结合，还是“N2 +N3”先结合，会出现两种不同的结构方式和意义，即：</p> 
  <p>(10a)[N1 N2]NPN3 如：[现代汉语]NP 词典</p> 
  <p>(10b)N1 [N2N3]NP 如：新版[汉语词典]NP</p> 
  <p><span style="color:#f33b45;">以上讨论的第一个事实说明：</span></p> 
  <p><span style="color:#f33b45;">由于约束力不够，单一标记的PSG规则不能充分消解短语类型和短语边界的歧义。用数学的语言来讲，PSG规则是必要的，却不是充分的。因此机器仅仅根据规则右边的一个词类序列来判断它是不是一个短语，或者是什么短语，其实都有某种不确定性。</span></p> 
  <p><strong><span style="color:#f33b45;">采用</span><span style="color:#3399ea;">复杂特征集</span><span style="color:#f33b45;">和</span><span style="color:#3399ea;">词汇主义方法</span><span style="color:#f33b45;">来重建自然语言的语法系统</span></strong>，是近二十年来全球语言学界就此作出的最重要的努力。</p> 
  <h2><a name="t3"></a>1.2、事实二：词频统计的齐夫律</h2> 
  <p>通过大规模语料的调查，人们发现一种语言的短语规则的分布也符合所谓的<strong>齐夫率(Zipf’s Law)</strong>。</p> 
  <p>Zipf是一个统计学家和语言学家。他提出，如果对某个语言单位(不论是英语的字母或词)进行统计，把这个语言单位在一个语料库里出现的频度(frequency)记作F，而且根据频度的降序对每个单元指派一个整数的阶次(rank) R。结果发现R和F的乘积近似为一个常数。即</p> 
  <p><strong>F*R ≈ const (常数)</strong></p> 
  <p>被观察的语言单元的阶次R与其频度F成反比关系。词频统计方面齐夫律显示，不管被考察的语料仅仅一本长篇小说，还是一个大规模的语料库，最常出现的100个词的出现次数会占到语料库总词次数(tokens)的近一半。</p> 
  <p>假如语料库的规模是100万词次，那么其中频度最高的100个词的累计出现次数大概是50万词次。如果整个语料库含有5万词型(types)，那么其中的一半(也就是2.5万条左右)在该语料库中只出现过一次。即使把语料库的规模加大十倍，变成1000万词次，统计规律大体不变。</p> 
  <p>有趣的是，80年代英国人Sampson对英语语料库中的PSG规则进行统计，发现它们的分布同样是扭曲的，大体表现为齐夫率。也就是说，一方面经常遇到的语法规则只有几十条左右，它们的出现频度非常非常高；另一方面，规则库中大约一半左右的规则在语料库中只出现过一次。</p> 
  <p>随着语料库规模的扩大，新的规则仍不断呈现。Noam Chomsky曾提出过这样的假设，认为对一种自然语言来说，其语法规则的数目总是有限的，但据此生成的句子数目却是无限的。但语料库调查的结果不是这个样子。这个发现至少说明，单纯依靠语言学家的语感来编写语法规则不可能胜任大规模真实文本处理的需求，必须寻找可以从语料库中直接获取大规模语言知识的新方法。</p> 
  <p>几十年来，NLP学界曾发表过许多灿烂成果，有词法学、语法学、语义学的，有句法分析算法的，还有众多著名的自然语言应用系统。那么究竟什么是对该领域影响最大的、里程碑式的成果呢？</p> 
  <h1><a name="t4"></a>二、NLP十大里程碑</h1> 
  <h2><a name="t5"></a>2.1、里程碑一：1985复杂特征集</h2> 
  <p>复杂特征集(complex feature set)又叫做多重属性(multiple features)描写。语言学里，这种描写方法最早出现在语音学中。美国计算语言学家Martin Kay于1985年在“功能合一语法”(FunctionalUnification Grammar，简称FUG)新语法理论中，提出<strong>“<span style="color:#f33b45;">复杂特征集</span>”(complex feature set)</strong>概念。后来被Chomsky学派采用来扩展PSG的描写能力。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunt8HRnfus2gDic44tMdrlQDJSgZhpl8aR5BI28k0hg2D74e4UGnWcvwQ/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;美国计算语言学家Martin Kay</p> 
  <p><span style="color:#f33b45;">现在在语言学界、计算语言学界，语法系统在词汇层的描写中常采用复杂特征集，利用这些属性来强化句法规则的约束力。</span><span style="color:#3399ea;"><strong>一个复杂特征集F包含任意多个特征名fi和特征值vi对</strong></span>。其形式如：</p> 
  <p><strong><span style="color:#3399ea;">F = {…, fi=vi, …}, i=1,…,n</span></strong></p> 
  <p>特征值vi既可以是一个简单的数字或符号，也可以是另外一个复杂特征集。这种递归式的定义使复杂特征集获得了强大的表现能力。举例来说，北京大学俞士汶开发的《现代汉语语法信息词典》[10]，对一个动词定义了约40项属性描写，对一个名词定义了约27项属性描写。</p> 
  <p>一条含有词汇和短语属性约束的句法规则具有如下的一般形式：</p> 
  <p>: &lt;属性约束&gt;</p> 
  <p>: &lt;属性传递&gt;</p> 
  <p>一般来说，PSG规则包括右部(条件：符号序列的匹配模式)和左部(动作：短语归并结果)。词语的“属性约束”直接来自系统的词库，而短语的“属性约束”则是在自底向上的短语归并过程中从其构成成分的中心语(head)那里继承过来的。在Chomsky的理论中这叫做<strong>X-bar理论</strong>。</p> 
  <p>X-bar代表某个词类X所构成的、仍具有该词类属性的一个成分。如果X=N，就是一个具有名词特性的N-bar。当一条PSG规则的右部匹配成功，且“属性约束”部分得到满足，这条规则才能被执行。此时，规则左部所命名的的短语被生成，该短语的复杂特征集通过“属性传递”部分动态生成。</p> 
  <p>80年代末、90年代初学术界提出了一系列新的语法，如广义短语结构语法(GPSG)、中心语驱动的短语结构语法(HPSG)、词汇功能语法(LFG)等等。这些形式语法其实都是在词汇和短语的复杂特征集描写背景下产生的。合一(unification)算法则是针对复杂特征集的运算而提出来的。“合一”是实现属性匹配和赋值的一种算法，所以上述这些新语法又统称为“<span style="color:#f33b45;"><strong>基于合一的语法</strong></span>”。</p> 
  <h2><a name="t6"></a>2.2、里程碑二：1966词汇主义</h2> 
  <p><span style="color:#f33b45;"><strong>NLP领域第二个里程碑式贡献是词汇主义</strong></span>(lexicalism)。</p> 
  <p>1966年，韩礼德(Halliday)提出词汇不是用来填充语法确定的一套“空位”(slots)，而是一个独立的语言学层面；词汇研究可以作为对语法理论的补充，却不是语法理论的一部分，他主张把词汇从语法研究中独立地分离出来。</p> 
  <p>语言学家Hudson宣称，词汇主义是当今语言学理论头号发展倾向。出现原因也同上节两事实有关。词汇主义方法不仅提出一种颗粒度更细的语言知识表示形式，而且体现一语言知识递增式开发和积累的新思路。</p> 
  <p>首先解释一个背景矛盾。</p> 
  <p>一方面，语言学界一向认为，不划分词类就无法讲语法，如前面介绍的短语结构语法，语法“不可能”根据个别单独的词来写规则。但是另一方面，人们近来又注意到，任何归类其实都会丢失个体的某些重要信息。所以从前文提到的第一个事实出发，要想强化语法约束能力，词汇的描写应当深入到比词类更细微的词语本身上来。</p> 
  <p>换句话讲，语言学呼唤在词汇层采用颗粒度更小的描写单元。从本质上来说，词汇主义倾向反映了语言描写的主体已经从句法层转移到了词汇层；这也就是所谓的“小语法，大词库”的思想。下面让我们来看与词汇主义有关的一些工作。</p> 
  <h2><a name="t7"></a>2.2.1、词汇语法学(Lexicon-grammar)</h2> 
  <p>法国巴黎大学Gross教授60年代创立研究中心LADL(http://www.ladl.jussieu.fr/)，提出了词汇语法的概念。</p> 
  <p>• 把12,000个主要动词分成50个子类。</p> 
  <p>• 每个动词都有一个特定的论元集。</p> 
  <p>• 每一类动词都有一个特定的矩阵, 其中每个动词都用400个不同句式来逐一描写(“+”代表可进入该句式；“-”表示不能)。</p> 
  <p>• 已开发英、法、德、西等欧洲语言的大规模描写。</p> 
  <p>• INTEX是一个适用于大规模语料分析的工具，已先后被世界五十多个研究中心采用。</p> 
  <h2><a name="t8"></a>2.2.2、框架语义学(Frame Semantics)</h2> 
  <p>Fillmore是<strong>格语法(Case Grammar)</strong>创始人，前几年主持美国自然科学基金的一个名为框架语义学的项目(http://www.icsi.berkeley.edu/~framenet)。该项目从WordNet上选取了2000个动词，从中得到75个语义框架。例如，动词”categorize”的框架被定义为:</p> 
  <p>一个人(Cognizer)把某个对象(Item)视为某个类(Category)。</p> 
  <p>同原先的格框架相比，原来一般化的动作主体被具体化为认知者Cognizer，动作客体被具体化为事物Item，并根据特定体动词的性质增加了一个作为分类结果的语义角色Category。</p> 
  <p>项目组还从英国国家语料库中挑出50,000个相关句子，通过人工给每个句子标注了相应的语义角色。例句：</p> 
  <p>Kimcategorized the book as fiction.</p> 
  <p>(Cog) (Itm)(Cat)</p> 
  <h2><a name="t9"></a>2.2.3、WordNet</h2> 
  <p>WordNet(http://www.cogsci.princeton.edu:80/~wn/)是一个描写英语词汇层语义关系的词库，1990年由普林斯顿大学Miller开发。至今有很多版本，全部公布在因特网上，供研究人员自由下载。</p> 
  <p>欧洲有一个Euro-WordNet，以类似的格式来表现各种欧洲语言的词汇层语义关系。WordNet刻意描写的是词语之间的各种语义关系，如同义关系(synonymy)、反义关系(antonymy)、上下位关系(hyponymy)，部分-整体关系(part-of)等等。</p> 
  <p>这种词汇语义学又叫做关系语义学，这一学派同传统的语义场理论和和语义属性描写理论相比，其最大的优势在于第一次在一种语言的整个词汇表上实现了词汇层的语义描写。这是其他学派从来没有做到的。其他理论迄今仅仅停留在教科书或某些学术论文中，从来就没有得到工程规模的应用。下面是WordNet的概况：</p> 
  <p>• 95,600条实词词型(动词、名词、形容词)</p> 
  <p>• 被划分成70,100个同义词集(synsets)</p> 
  <h2><a name="t10"></a>2.2.4 知网网(How-Net)</h2> 
  <p>知网是董振东和董强设计的一个汉语语义知识网(http://www.keenage.com)，访问只有主页。</p> 
  <p>• 自下而上地依据概念对汉语实词进行了穷尽的分类。</p> 
  <p>• 15,000个动词被划分成810类。</p> 
  <p>• 定义了300个名词类，100个形容词类。</p> 
  <p>• 全部概念用400个语义元语来定义。</p> 
  <p>知网特点是既有WordNet所描写的同一类词间语义关系(如：同义、反义、上下位、部分-整体等)，又描写不同类词之间的论旨关系和语义角色。</p> 
  <h2><a name="t11"></a>2.2.5 MindNet</h2> 
  <p>MindNet是微软研究院NLP组设计的词汇语义网(http://research.microsoft.com/nlp/)，用三元组(triple)作为全部知识的表示基元。一个三元组由两个节点和一条连接边组成。每个节点代表一个概念，连接两个概念节点的边表示概念之间的语义依存关系。全部三元组通过句法分析器自动获取。</p> 
  <p>具体通过对两部英语词典(Longman Dictionaryof Contemporary English，AmericanHeritage Dictionary)和一部百科全书(Encarta)中的全部句子进行分析，获得每个句子的逻辑语义表示(logical form，简称LF)。</p> 
  <p>而LF本来就是由三元组构成的，如(W1, V-Obj,W2)表示：W1是一个动词，W2是其宾语中的中心词，因此W2从属于W1，它们之间的关系是V-Obj。比如(play, V-Obj,basketball)便是一个具体的三元组。又如(W1, H-Mod,W2)，W1代表一个偏正短语中的中心词(head word)，W2是其修饰语(modifier)，因此W2从属于W1，它们之间的关系是H-Mod。</p> 
  <p>这种资源是完全自动做出来的，所得三元组不可能没有错误。但是那些出现频度很高的三元组一般来说正确。MindNet已经应用到像语法检查、句法结构排歧、词义排歧、机器翻译等许多场合。</p> 
  <h2><a name="t12"></a>2.3 里程碑三：1976统计语言模型</h2> 
  <p>第三大贡献是<span style="color:#3399ea;"><strong>语料库方法，或叫统计语言模型</strong></span>。</p> 
  <p>首先成功利用数学方法解决自然语言处理问题的是语音和语言处理大师弗雷德·贾里尼克(Fred Jelinek)。1968年始在IBM研究中心兼职1974年全职加入，他领导一批杰出科学家利用大型计算机处理人类语言问题，学术休假(SabbaticalLeave)时(约1972-1976年间)提出统计语言模型。</p> 
  <p>1990s李开复用统计语言模型把997个词的语音识别问题简化成了20词识别问题，实现了有史以来第一次大词汇量非特定人连续语言的识别。常用统计语言模型，包括<span style="color:#f33b45;"><strong>N元文法模型(N-gram Model)、隐马尔科夫模型(Hidden MarkovModel，简称HMM)、最大熵模型(MaximumEntropy Model)等。</strong></span></p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUungXt3YNAkSuGHnXEPXpTiadMxb5GgicogjGpPXicXs4xYFhQFkeI9icIWwA/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;美国工程院院士Frederick Jelinek</p> 
  <p>如果用变量W代表一个文本中顺序排列的n个词，即W = w1w2…wn，则统计语言模型的任务是给出任意一个词序列W在文本中出现的<strong>概率P(W)</strong>。</p> 
  <p>利用概率的乘积公式，P(W)可展开为：</p> 
  <p><strong>P(W) =P(w1)P(w2/w1)P(w3/ w1 w2)…P(wn/w1 w2…wn-1)&nbsp;</strong>(1)</p> 
  <p>式中P(w1)表示第一个词w1的出现概率，P(w2/w1)表示在w1出现的情况下第二个词w2出现的条件概率，依此类推。</p> 
  <p>不难看出，为了预测词wn的出现概率，必须已知它前面所有词的出现概率。从计算上来看，这太复杂了。如果<span style="color:#f33b45;">近似认为任意一个词wi的出现概率只同它紧邻的前一个词有关，那么计算就得以大大简化。这就是所谓的二元模型(bigram)</span>，由(1)式得：</p> 
  <p><span style="color:#f33b45;"><strong>P(W) ≈ P(w1)∏i=2,…,nP(wi/ wi-1 )</strong></span>&nbsp;(2)</p> 
  <p>式中∏i=2,…,nP(wi/ wi-1 )表示多个概率的连乘。</p> 
  <p>需要着重指出的是：这些概率参数都可以通过大规模语料库来估值。比如二元概率</p> 
  <p><strong>P(wi/ wi-1) ≈count(wi-1 wi) / count(wi-1)</strong>&nbsp;(3)</p> 
  <p>式中count(…)表示一个特定词序列在整个语料库中出现的累计次数。若语料库的总词次数为N，则任意词wi在该语料库中的出现概率可估计如下：</p> 
  <p><strong>P(wi) ≈count(wi) / N&nbsp;</strong>(4)</p> 
  <p>同理，如果近似认为任意词wi的出现只同它紧邻前两个词有关，就得到一个三元模型(trigram)：</p> 
  <p><strong>P(W) ≈P(w1)P(w2/w1) ∏i=3,…,nP(wi/wi-2 w-1 )&nbsp;</strong>(5)</p> 
  <p>统计语言模型的方法有点像天气预报。用来估计概率参数的大规模语料库好比是一个地区历年积累起来的气象记录，而用三元模型来做天气预报，就像是根据前两天的天气情况来预测当天的天气。天气预报当然不可能百分之百正确。这也算是概率统计方法的一个特点。</p> 
  <h2><a name="t13"></a>2.3.1、语音识别</h2> 
  <p>语音识别作为计算机汉字键盘输入的一种图代方式，越来越受到信息界人士的青睐。所谓听写机就是这样的商品。据报道中国的移动电话用户已超过一亿，随着移动电话和个人数字助理(PDA)的普及，尤其是当这些随身携带的器件都可以无线上网的时候，广大用户更迫切期望通过语音识别或手写板而不是小键盘来输入简短的文字信息。</p> 
  <p>其实，<span style="color:#f33b45;">语音识别任务可视为计算以下条件概率的极大值问题</span>：</p> 
  <p><strong>W*= argmaxWP(W/speech signal)</strong></p> 
  <p><strong>= argmaxWP(speech signal/W) P(W) / P(speech signal)</strong></p> 
  <p><strong>= argmaxWP(speech signal/W) P(W)</strong>&nbsp;(6)</p> 
  <p>式中数学符号argmaxW表示对不同的候选词序列W计算条件概率P(W/speech signal)的值，从而使W*成为其中条件概率值最大的那个词序列，这也就是计算机选定的识别结果。换句话讲，通过式(6)的计算，计算机找到了最适合当前输入语音信号speech signal的词串W。</p> 
  <p>式(6)第二行是利用贝叶斯定律转写的结果，因为条件概率P(speech signal/W)比较容易估值。公式的分母P(speech signa)对给定的语音信号是一个常数，不影响极大值的计算，故可以从公式中删除。在第三行所示的结果中，P(W)就是前面所讲得统计语言模型，一般采用式(5)所示的三元模型；P(speechsignal/W)叫做<strong>声学模型</strong>。</p> 
  <p>讲到这儿，细心的读者可能已经明白，汉语拼音输入法中的拼音－汉字转换任务其实也是用同样方法实现的，而且两者所用的汉语语言模型(即二元或三元模型)是同一个模型。</p> 
  <p>据笔者所知，目前市场上的听写机产品和微软拼音输入法(3.0版)都是用词的三元模型实现的，几乎完全不用句法-语义分析手段。为什么会出现这样的局面呢？这是优胜劣汰的客观规律所决定的。可比的评测结果表明，用三元模型实现的拼音-汉字转换系统，其出错率比其它产品减少约50%。</p> 
  <h2><a name="t14"></a>2.3.2、词性标注</h2> 
  <p>一个词库中大约14%的词型具有不只一个词性。而在一个语料库中，占总词次数约30%的词具有不止一个词性。所以对一个文本中的每一个词进行词性标注，就是通过上下文的约束，实现词性歧义的消解。历史上曾经先后出现过两个自动词性标注系统。一个采用上下文相关的规则，叫做TAGGIT(1971)，另一个应用词类的二元模型，叫做CLAWS(1987)。</p> 
  <p>两个系统都分别对100万词次的英语非受限文本实施了词性标注。结果显示，采用统计语言模型的CLAWS系统的标注正确率大大高于基于规则方法的TAGGIT系统。请看下表的对比：</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunXEVDUriclrSxaicHvFWTZaicV5nCdiafN2vOwPmApj1yMiaaH6qvjIndic7g/640?wx_fmt=png"></p> 
  <p>令C和W分别代表词类标记序列和词序列，则词性标注问题可视为计算以下条件概率的极大值:</p> 
  <p><strong>C*= argmaxCP(C/W)</strong></p> 
  <p><strong>= argmaxCP(W/C)P(C) / P(W)</strong></p> 
  <p><strong>≈ argmaxC∏i=1,…,nP(wi/ci )P(ci /ci-1 )</strong>&nbsp;(7)</p> 
  <p>式中P(C/W)是已知输入词序列W的情况下，出现词类标记序列C的条件概率。数学符号argmaxC表示通过考察不同的候选词类标记序列C，来寻找使条件概率取最大值的那个词类标记序列C*。后者应当就是对W的词性标注结果。</p> 
  <p>公式第二行是利用贝叶斯定律转写的结果，由于分母P(W)对给定的W是一个常数，不影响极大值的计算，可以从公式中删除。接着对公式进行近似。首先，引入独立性假设，认为任意一个词wi的出现概率近似只同当前词的词类标记ci有关，而与周围(上下文)的词类标记无关。于是词汇概率可计算如下：</p> 
  <p><strong>P(W/C) ≈∏i=1,…,n P(wi/ci )</strong>&nbsp;(8)</p> 
  <p>其次，采用二元假设，即近似认为任意一个词类标记ci的出现概率只同它紧邻的前一个词类标记ci-1有关。有</p> 
  <p><strong>P(C) ≈ P(c1)∏i=2,…,n P(ci /ci-1 )</strong>&nbsp;(9)</p> 
  <p>P(ci /ci-1 )是词类标记的转移概率，也叫做基于词类的二元模型。</p> 
  <p>上述这两个概率参数都可以通过带词性标记的语料库来分别估计：</p> 
  <p><strong>P(wi/ci ) ≈count(wi,ci) / count(ci)</strong>&nbsp;(10)</p> 
  <p><strong>P(ci /ci-1 ) ≈count(ci-1ci) / count(ci-1)</strong>&nbsp;(11)</p> 
  <p>据文献报道，采用统计语言模型方法汉语和英语的次性标注正确率都可以达到96%左右[6]。</p> 
  <h2><a name="t15"></a>2.3.3、介词短语PP的依附歧义</h2> 
  <p>英语中介词短语究竟依附于前面的名词还是前面的动词，是句法分析中常见的结构歧义问题。下例用语料库方法来解决这个问题，以及这种方法究竟能达到多高的正确率。</p> 
  <p>例句：Pierre Vinken,61 years old, joined the board as a nonexecutive director.</p> 
  <p>令A=1表示名词依附，A=0为动词依附，则上述例句的PP依附问题可表为：</p> 
  <p>(A=0,V=joined, N1=board, P=as, N2=director)</p> 
  <p>令V, N1, N2分别代表句中动词短语、宾语短语、介宾短语的中心词，并在一个带有句法标注的语料库(又称树库)中统计如下四元组的概率Pr：</p> 
  <p><strong>Pr = (A=1 /V=v, N1=n1, P=p, N2=n2)</strong>&nbsp;(10)</p> 
  <p>对输入句子进行PP 依附判断的算法如下：</p> 
  <p>若Pr = (1 / v, n1, p, n2) ≥ 0.5,</p> 
  <p>则判定PP依附于n1,</p> 
  <p>否则判定PP依附于v。</p> 
  <p>Collins和Brooks实验使用的语料库是宾夕法尼亚大学标注的<strong>华尔街日报(WSJ)树库</strong>，包括：训练集20,801个四元组，测试集3,097个四元组。他们对PP依附自动判定精度的上下限作了如下分析：</p> 
  <p>一律视为名词依附(即A≡1) 59.0%</p> 
  <p>只考虑介词p的最常见附加72.2%</p> 
  <p>三位专家只根据四个中心词判断88.2%</p> 
  <p>三位专家根据全句判断93.2%</p> 
  <p>很明显，自动判断精确率的下限是72.2%，因为机器不会比只考虑句中介词p的最常见依附做得更差了；上限是88.2%，因为机器不可能比三位专家根据四个中心词作出的判断更高明。</p> 
  <p>论文报告，在被测试的3,097个四元组中，系统正确判断的四元组为2,606个，因此平均精确率为84.1%。这与上面提到的上限值88.2%相比，应该说是相当不错的结果。</p> 
  <h2><a name="t16"></a>传统三大技术里程碑小结</h2> 
  <p>语言学家在不论是复杂特征集和合一语法，还是词汇主义方法，都是原先所谓的理性主义框架下做出的重大贡献。词汇主义方法提出了一种颗粒度更细的语言知识表示形式，而且体现了一种语言知识递增式开发和积累的新思路，值得特别推崇。</p> 
  <p>尤其值得重视的是，在众多词汇资源的开发过程中，语料库和统计学习方法发挥了很大的作用。这是经验主义方法和理性主义方法相互融合的可喜开端，也是国内知名语言学者冯志伟等人认可的研究范式。</p> 
  <p>语料库方法和统计语言模型，国内同行中实际上存在不同评价。有种观点认为NLP必须建立在语言理解基础上，他们不大相信统计语言模型在语音识别、词性标注、信息检索等应用领域中所取得的进展。这些争论不能澄清，是因为同行间缺少统一评测。有评测才会有鉴别。</p> 
  <p>评判某方法优劣应公开、公平、相互可比的评测标准，而非研究员设计“自评”。黄昌宁、张小凤2013年论文表示，语料库方法和统计语言模型是当前自然语言处理技术的主流，其实用价值已在很多应用系统中得到充分证实。统计语言模型研究在结构化对象的统计建模方面，仍有广阔发展空间。</p> 
  <p>自然语言处理领域业界知名博主Sebatian Ruder在2018年文章从神经网络技术角度，总结NLP领域近15年重大进展、8大里程碑事件，提及很多神经网络模型。这些模型建立在同一时期非神经网络技术之上，如上述三大里程碑。下面接着看后续NLP技术的发展。</p> 
  <h2><a name="t17"></a>2.4、里程碑四：2001神经语言模型(Neural language models)</h2> 
  <p>语言模型解决的是在给定已出现词语的文本中，预测下一个单词的任务。这是最简单的语言处理任务，有许多具体实际应用，如智能键盘、电子邮件回复建议等。语言模型历史由来已久，经典方法基于n-grams模型(利用前面n个词语预测下一个单词)，并利用平滑操作处理不可见的n-grams。</p> 
  <p>第一个神经语言模型，前馈神经网络(feed-forward neural network)，是Bengio等人于2001年提出的。<span style="color:#3399ea;"><strong>模型以某词语之前出现的n个词语作为输入向量，也就是现在大家说的词嵌入(word embeddings)向量</strong></span>。这些词嵌入在级联后进入一个隐藏层，该层的输出然后通过一个softmax层。如图3所示。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUuniaMicTNNkVUcBzOXQO1cCxibH2HqGicgX3qPLIIg3qJr23SmHDUIFmnHKQ/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;前馈神经网络语言模型</p> 
  <p>而现在构建语言模型的前馈神经网络，已被循环神经网络(RNNs)和长短期记忆神经网络(LSTMs)取代。</p> 
  <p>虽然后来提出许多新模型在经典LSTM上进行了扩展，但它仍然是强有力的基础模型。甚至Bengio等人的经典前馈神经网络在某些设定下也和更复杂的模型效果相当，因为这些任务只需要考虑邻近的词语。理解这些语言模型究竟捕捉了哪些信息，也是当今一个活跃的研究领域。</p> 
  <p>语言模型的建立是一种无监督学习(unsupervisedlearning)，Yann LeCun称之为预测学习(predictivelearning)，是获得世界如何运作常识的先决条件。</p> 
  <p>关于语言模型最引人注目的是，尽管它很简单，但却与后文许多核心进展息息相关。反过来，这也意味着NLP领域许多重要进展都可以简化为某种形式的语言模型构建。但要实现对自然语言真正意义上的理解，仅仅从原始文本中进行学习是不够的，我们需要新的方法和模型。</p> 
  <h2><a name="t18"></a>2.5、里程碑五：2008多任务学习(Multi-task learning)</h2> 
  <p>多任务学习是在多个任务下训练的模型之间共享参数的方法，在神经网络中通过捆绑不同层的权重轻松实现。多任务学习思想1993年Rich Caruana首次提出，并应用于道路追踪和肺炎预测。多任务学习鼓励模型学习对多个任务有效的表征描述。这对于学习一般的、低级的描述形式、集中模型的注意力或在训练数据有限的环境中特别有用。</p> 
  <p>多任务学习2008年被Collobert和Weston等人首次在自然语言处理领域应用于神经网络。在他们的模型中，词嵌入矩阵被两个在不同任务下训练的模型共享，如图4所示。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunZVw5eLiaPpc4iaRkq4aicZVo9MS6fKov8pGlZ7OySiavYITMxJ6WiaVLgiaQ/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;词嵌入矩阵共享</p> 
  <p>共享的词嵌入矩阵使模型可以相互协作，共享矩阵中的低层级信息，而词嵌入矩阵往往构成了模型中需要训练的绝大部分参数。</p> 
  <p>Collobert和Weston发表于2008年的论文，影响远远超过了它在多任务学习中的应用。它开创的诸如预训练词嵌入和使用卷积神经网络处理文本的方法，在接下来的几年被广泛应用。他们也因此获得2018年机器学习国际会议(ICML)的test-of-time奖。</p> 
  <p>如今，多任务学习在自然语言处理领域广泛使用，而利用现有或“人工”任务已经成为NLP指令库中的一个有用工具。</p> 
  <p>虽然参数的共享是预先定义好的，但在优化的过程中却可以学习不同的共享模式。当模型越来越多地在多个任务上进行测评以评估其泛化能力时，多任务学习就变得愈加重要，近年来也涌现出更多针对多任务学习的评估基准。</p> 
  <h2><a name="t19"></a>2.6、里程碑六：2013词嵌入</h2> 
  <p>稀疏向量对文本进行表示的词袋模型，在自然语言处理领域有很长历史。而用稠密的向量对词语进行描述，也就是词嵌入，则在2001年首次出现。2013年Mikolov等人工作主要创新之处在于，<span style="color:#f33b45;"><strong>通过去除隐藏层和近似计算目标使词嵌入模型的训练更为高效</strong></span>。</p> 
  <p>尽管这些改变本质上十分简单，但它们与高效的<strong>word2vec</strong>(wordto vector用来产生词向量的相关模型)组合在一起，使得大规模的词嵌入模型训练成为可能。</p> 
  <p><span style="color:#f33b45;"><strong>Word2vec</strong></span><span style="color:#3399ea;"><strong>有两种不同的实现方法：CBOW(continuousbag-of-words)和skip-gram。它们在预测目标上有所不同：一个是根据周围的词语预测中心词语，另一个则恰恰相反</strong></span>。如图5所示。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunkyYPqQWVtwicyp1Iy5jZmXxqmc8RyicVIA8m3U9rCzpvnVP4vWtaibafA/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;CBOW和skip-gram架构</p> 
  <p>虽然这些嵌入与使用前馈神经网络学习的嵌入在概念上没有区别，但是在一个非常大语料库上的训练使它们能够获取诸如性别、动词时态和国际事务等单词之间的特定关系。如下图 4 所示。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUune7muNsicksS1OM98ptGUFgsiaUbxphUrSWlbceFrxicMfkyFmCTUeU4vw/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;word2vec捕获的联系</p> 
  <p>这些关系和它们背后的意义激起了人们对词嵌入的兴趣，许多研究都在关注这些线性关系的来源。然而，使词嵌入成为目前自然语言处理领域中流砥柱的，是将预训练的词嵌入矩阵用于初始化可以提高大量下游任务性能的事实。</p> 
  <p>虽然word2vec捕捉到的关系具有直观且几乎不可思议的特性，但后来的研究表明，word2vec本身并没有什么特殊之处：<strong>词嵌入也可以通过矩阵分解来学习，经过适当的调试，经典的矩阵分解方法SVD和LSA都可以获得相似的结果</strong>。从那时起，大量的工作开始探索词嵌入的不同方面。尽管有很多发展，word2vec仍是目前应用最为广泛的选择。</p> 
  <p>Word2vec应用范围也超出了词语级别：带有负采样的skip-gram——一个基于上下文学习词嵌入的方便目标，已经被用于学习句子的表征。它甚至超越了自然语言处理的范围，被应用于网络和生物序列等领域。</p> 
  <p>一个激动人心的研究方向是在同一空间中构建不同语言的词嵌入模型，以达到(零样本)跨语言转换的目的。通过无监督学习构建这样的映射变得越来越有希望(至少对于相似的语言来说)，这也为语料资源较少的语言和无监督机器翻译的应用程序创造可能。</p> 
  <h2><a name="t20"></a>2.7、里程碑七：2013<span style="color:#f33b45;">RNN/CNN用于NLP的神经网络</span></h2> 
  <p>2013和2014年是自然语言处理领域神经网络时代的开始。其中<strong>三种类型的神经网络应用最为广泛：循环神经网络(recurrentneural networks)、卷积神经网络(convolutionalneural networks)和结构递归神经网络(recursiveneural networks)</strong>。</p> 
  <p>循环神经网络是NLP领域处理动态输入序列最自然的选择。Vanilla循环神经网络很快被经典的<span style="color:#f33b45;"><strong>长短期记忆网络(long-shorttermmemory networks，LSTM)</strong></span>代替，<strong>该模型能更好地解决梯度消失和梯度爆炸问题</strong>。</p> 
  <p>在2013年之前，人们仍认为循环神经网络很难训练，直到Ilya Sutskever博士的论文改变了循环神经网络这一名声。双向的长短期记忆记忆网络通常被用于同时处理出现在左侧和右侧的文本内容。LSTM 结构如图7所示。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunx3SacgN9rHBLroNUxmzibaBZsxBEIrASPNPWXClFx2mSuD2nBgxdZCA/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;LSTM网络</p> 
  <p>应用于文本的卷积神经网络只在两个维度上进行操作，卷积层只需要在时序维度上移动即可。图8展示了应用于自然语言处理的卷积神经网络的典型结构。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunIIlz60Y5jF345hlicvYcDwEomlQyCiaucXN2M3Z42ZePtz7gB1xP0Sow/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;卷积神经网络</p> 
  <p>与循环神经网络相比，卷积神经网络的一个优点是具有更好的并行性。</p> 
  <p>因为卷积操作中每个时间步的状态只依赖于局部上下文，而不是循环神经网络中那样依赖于所有过去的状态。卷积神经网络可以使用更大的卷积层涵盖更广泛的上下文内容。卷积神经网络也可以和长短期记忆网络进行组合和堆叠，还可以用来加速长短期记忆网络的训练。</p> 
  <p>循环神经网络和卷积神经网络都将语言视为一个序列。但从语言学的角度来看，语言是具有层级结构的：词语组成高阶的短语和小句，它们本身可以根据一定的产生规则递归地组合。这激发了利用结构递归神经网络，以树形结构取代序列来表示语言的想法，如图9所示。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunmE2iaibZIvAt9ptDTe2HYIf9XvrUkcILO5RoSibt9PJJRJlL55pOdovsg/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;结构递归神经网络</p> 
  <p>结构递归神经网络自下而上构建序列的表示，与从左至右或从右至左对序列进行处理的循环神经网络形成鲜明的对比。树中的每个节点是通过子节点的表征计算得到的。一个树也可以视为在循环神经网络上施加不同的处理顺序，所以长短期记忆网络则可以很容易地被扩展为一棵树。</p> 
  <p>不只是循环神经网络和长短期记忆网络可以扩展到使用层次结构，词嵌入也可以在语法语境中学习，语言模型可以基于句法堆栈生成词汇，图形卷积神经网络可以树状结构运行。</p> 
  <h2><a name="t21"></a>2.8、里程碑八：2014序列到序列模型(Sequence-to-sequencemodels)</h2> 
  <p>2014年，Sutskever等人提出序列到序列学习，即使用神经网络将一个序列映射到另一个序列的一般化框架。在这个框架中，一个作为编码器的神经网络对句子符号进行处理，并将其压缩成向量表示；然后，一个作为解码器的神经网络根据编码器的状态逐个预测输出符号，并将前一个预测得到的输出符号作为预测下一个输出符号的输入。如图10所示。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunEXKqYBicYYz3IvEzc32TgiaST7ZVXq7yVuuEzcIFSP4gWbkjKHBBqKpA/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;序列到序列模型</p> 
  <p>机器翻译是这一框架的杀手级应用。2016年，谷歌宣布他们将用神经机器翻译模型取代基于短语的整句机器翻译模型。谷歌大脑负责人Jeff Dean表示，这意味着用500行神经网络模型代码取代50万行基于短语的机器翻译代码。</p> 
  <p>由于其灵活性，该框架在自然语言生成任务上被广泛应用，其编码器和解码器分别由不同的模型来担任。更重要的是，解码器不仅可以适用于序列，在任意表示上均可以应用。比如基于图片生成描述(如图11)、基于表格生成文本、根据源代码改变生成描述，以及众多其他应用。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunJGlxkibwFhMSeK5D25hwHOqMchYYOQMmH6FGcLHUe6QV8KHibqFK0OEQ/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;基于图像生成标题(Vinyalset al., 2015)</p> 
  <p>序列到序列的学习甚至可以应用到自然语言处理领域常见的结构化预测任务中，也就是输出具有特定的结构。为简单起见，输出就像选区解析一样被线性化(如图12)。在给定足够多训练数据用于语法解析的情况下，神经网络已经被证明具有产生线性输出和识别命名实体的能力。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunJy36hcSqf8nnrfnpL04kPiaJff569qQicVibibzdRvXICia3AjHwt02biaEw/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;线性化选区解析树(Vinyalset al., 2015)</p> 
  <p>序列的编码器和解码器通常都是基于循环神经网络，但也可以使用其他模型。新的结构主要都从机器翻译的工作中诞生，它已经成了序列到序列模型的培养基。近期提出的模型有深度长短期记忆网络、卷积编码器、Transformer(一个基于自注意力机制的全新神经网络架构)以及长短期记忆依赖网络和的 Transformer 结合体等。</p> 
  <h2><a name="t22"></a>2.9、里程碑九：2015注意力机制和基于记忆的神经网络</h2> 
  <p>注意力机制是神经网络机器翻译(NMT)的核心创新之一，也是使神经网络机器翻译优于经典的基于短语的机器翻译的关键。序列到序列学习的主要瓶颈是，需要将源序列的全部内容压缩为固定大小的向量。注意力机制通过让解码器回顾源序列的隐藏状态，以此为解码器提供加权平均值的输入来缓解这一问题，如图13所示。</p> 
  <p><img alt="640?wx_fmt=jpeg" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunCY3IMEReq10ib3LuQTpRInw5Q8CjHFnpLqWSn1GD35edfnibrBCWYztg/640?wx_fmt=jpeg"></p> 
  <p><strong>△</strong>&nbsp;注意力机制</p> 
  <p>之后，各种形式的注意力机制涌现而出。注意力机制被广泛接受，在各种需要根据输入的特定部分做出决策的任务上都有潜在的应用。它已经被应用于句法分析、阅读理解、单样本学习等任务中。它的输入甚至不需要是一个序列，而可以包含其他表示，比如图像的描述(图14)。</p> 
  <p>注意力机制一个有用的附带作用是它通过注意力权重来检测输入的哪一部分与特定的输出相关，从而提供了一种罕见的虽然还是比较浅层次的，对模型内部运作机制的窥探。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUun7maHMSepyGCOpwIfEqXyO3ib5uCrMHhicQmhvF8Jwxekd9WHeV8ODOBA/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;图像描述模型中的视觉注意力机制指示在生成”飞盘”时所关注的内容</p> 
  <p>注意力机制不仅仅局限于输入序列。自注意力机制可用来观察句子或文档中周围的单词，获得包含更多上下文信息的词语表示。多层的自注意力机制是神经机器翻译前沿模型Transformer的核心。</p> 
  <p>注意力机制可以视为模糊记忆的一种形式，其记忆的内容包括模型之前的隐藏状态，由模型选择从记忆中检索哪些内容。与此同时，更多具有明确记忆单元的模型被提出。</p> 
  <p>他们有很多不同的变化形式，比如神经图灵机(NeuralTuring Machines)、记忆网络(MemoryNetwork)、端到端的记忆网络(End-to-endMemory Newtorks)、动态记忆网络(DynamicMemoryNetworks)、神经可微计算机(NeuralDifferentiable Computer)、循环实体网络(RecurrentEntityNetwork)。</p> 
  <p>记忆的存取通常与注意力机制相似，基于与当前状态且可以读取和写入。这些模型之间的差异体现在它们如何实现和利用存储模块。</p> 
  <p>比如说，端到端的记忆网络对输入进行多次处理并更新内存，以实行多次推理。神经图灵机也有一个基于位置的寻址方式，使它们可以学习简单的计算机程序，比如排序。</p> 
  <p>基于记忆的模型通常用于需要长时间保留信息的任务中，例如语言模型构建和阅读理解。记忆模块的概念非常通用，知识库和表格都可以作为记忆模块，记忆模块也可以基于输入的全部或部分内容进行填充。</p> 
  <h2><a name="t23"></a>2.10、里程碑十：2018预训练语言模型</h2> 
  <p>预训练的词嵌入与上下文无关，仅用于初始化模型中的第一层。近几个月以来，许多有监督的任务被用来预训练神经网络。相比之下，语言模型只需要未标记的文本，因此其训练可以扩展到数十亿单词的语料、新的领域、新的语言。预训练的语言模型于 2015年被首次提出，但直到最近它才被证明在大量不同类型的任务中均十分有效。语言模型嵌入可以作为目标模型中的特征，或者根据具体任务进行调整。如下图所示，语言模型嵌入为许多任务的效果带来了巨大的改进。</p> 
  <p><img alt="640?wx_fmt=png" class="has" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBkXCgiavMy58KbbARHDuUunpbAXGuHbjHPDib6mxTUGOp87B37YOQ5EzHoCaI9nGicZ05zZTYDwrJZg/640?wx_fmt=png"></p> 
  <p><strong>△</strong>&nbsp;改进的语言模型嵌入</p> 
  <p>使用预训练的语言模型可以在数据量十分少的情况下有效学习。由于语言模型的训练只需要无标签的数据，因此他们对于数据稀缺的低资源语言特别有利。</p> 
  <p>2018年10月，谷歌AI语言组发布BERT语言模型预训练，已被证明可有效改进许多自然语言处理任务(Dai and Le, 2015; Peters et al., 2017, 2018; Radford etal., 2018; Howard and Ruder, 2018)。</p> 
  <p>这些任务包括句子级任务，如自然语言推理inference(Bowman et al., 2015; Williams et al., 2018)和释义paraphrasing(Dolan and Brockett, 2005)，旨在通过整体分析来预测句子之间的关系；以及词块级任务，如命名实体识别(Tjong KimSang and De Meulder, 2003)和SQuAD问题回答(Rajpurkar et al., 2016)，其中模型需要在词块级别生成细粒度输出。</p> 
  <h2><a name="t24"></a>近年七大技术里程碑小结</h2> 
  <p>除了上述七大技术里程碑，一些其他进展虽不如上面提到的那样流行，但仍产生了广泛的影响。</p> 
  <p><strong>基于字符的描述(Character-based representations)</strong>，在字符层级上使用卷积神经网络和长短期记忆网络，以获得一个基于字符的词语描述，目前已经相当常见了，特别是对于那些语言形态丰富的语种或那些形态信息十分重要、包含许多未知单词的任务。据目前所知，基于字符的描述最初用于序列标注，现在，基于字符的描述方法，减轻了必须以增加计算成本为代价建立固定词汇表的问题，并使完全基于字符的机器翻译的应用成为可能。</p> 
  <p><strong>对抗学习(Adversarial learning)</strong>，在机器学习领域已经取得了广泛应用，在自然语言处理领域也被应用于不同的任务中。对抗样例的应用也日益广泛，他们不仅仅是探测模型弱点的工具，更能使模型更具鲁棒性(robust)。(虚拟的)对抗性训练，也就是最坏情况的扰动，和域对抗性损失(domain-adversariallosses)都是可以使模型更具鲁棒性的有效正则化方式。生成对抗网络(GANs)目前在自然语言生成任务上还不太有效，但在匹配分布上十分有用。</p> 
  <p><strong>强化学习(Reinforcement learning)</strong>，在具有时间依赖性任务上证明有效，比如在训练期间选择数据和对话建模。在机器翻译和概括任务中，强化学习可以有效地直接优化“红色”和“蓝色”这样不可微的度量，不必去优化像交叉熵这样的代理损失函数。同样，逆向强化学习(inversereinforcement learning)在类似视频故事描述这样的奖励机制非常复杂且难以具体化的任务中，也非常有用。</p> 
  <h3><a name="t25"></a>自然语言处理NLP知识结构</h3> 
  <p>文|秦陇纪，数据简化DataSimp</p> 
  <p><span style="color:#f33b45;"><strong>自然语言处理(计算机语言学、自然语言理解)涉及：字处理，词处理，语句处理，篇章处理词处理分词、词性标注、实体识别、词义消歧语句处理句法分析(SyntacticAnalysis)、语义分析(SenmanticAnalysis)等</strong></span>。其中，重点有：</p> 
  <p>1、句法语义分析：分词，词性标记，命名实体识别。</p> 
  <p>2、信息抽取</p> 
  <p>3、文本挖掘：文本聚类，情感分析。基于统计。</p> 
  <p>4、机器翻译：基于规则，基于统计，基于神经网络。</p> 
  <p>5、信息检索</p> 
  <p>6、问答系统</p> 
  <p>7、对话系统建议…本文总结的自然语言处理历史、模型、知识体系结构内容，涉及NLP的语言理论、算法和工程实践各方面，内容繁杂。参考黄志洪老师自然语言处理课程、宗成庆老师《统计自然语言处理》，郑捷2017年电子工业出版社出版的图书<a href="https://www.baidu.com/s?wd=%E3%80%8ANLP%E6%B1%89%E8%AF%AD%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5%E3%80%8B&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" rel="nofollow">《NLP汉语自然语言处理原理与实践》</a>，以及国外著名NLP书籍的英文资料、汉译版资料。</p> 
  <h1><a name="t26"></a>一、NLP知识结构概述</h1> 
  <p>1)自然语言处理：利用计算机为工具，对书面实行或者口头形式进行各种各样的处理和加工的技术，是研究人与人交际中以及人与计算机交际中的演员问题的一门学科，是人工智能的主要内容。</p> 
  <p>2)自然语言处理是研究语言能力和语言应用的模型，建立计算机(算法)框架来实现这样的语言模型，并完善、评测、最终用于设计各种实用系统。</p> 
  <p>3)研究问题(主要)：</p> 
  <blockquote> 
   <p>信息检索<br> 机器翻译<br> 文档分类<br> 问答系统<br> 信息过滤<br> 自动文摘<br> 信息抽取<br> 文本挖掘<br> 舆情分析<br> 机器写作<br> 语音识别</p> 
  </blockquote> 
  <p>研究模式：自然语言场景问题，数学算法，算法如何应用到解决这些问题，预料训练，相关实际应用</p> 
  <p><strong>自然语言的困难</strong>：</p> 
  <p>场景的困难：语言的多样性、多变性、歧义性</p> 
  <p>学习的困难：艰难的数学模型(hmm,crf,EM,深度学习等)</p> 
  <p>语料的困难：什么的语料？语料的作用？如何获取语料？</p> 
  <h1><a name="t27"></a>二、NLP知识十大结构</h1> 
  <h2><a name="t28"></a>2.1、形式语言与自动机</h2> 
  <p>语言：按照一定规律构成的句子或者字符串的有限或者无限的集合。</p> 
  <p>描述语言的三种途径：</p> 
  <blockquote> 
   <p>穷举法<br> 文法(产生式系统)描述<br> 自动机</p> 
  </blockquote> 
  <p>自然语言不是人为设计而是自然进化的，形式语言比如：运算符号、化学分子式、编程语言</p> 
  <p>形式语言理论朱啊哟研究的是内部结构模式这类语言的纯粹的语法领域，从语言学而来，作为一种理解自然语言的句法规律，在计算机科学中，形式语言通常作为定义编程和语法结构的基础</p> 
  <p>形式语言与自动机基础知识：</p> 
  <blockquote> 
   <p>集合论<br> 图论</p> 
  </blockquote> 
  <p>自动机的应用：</p> 
  <blockquote> 
   <p>1，单词自动查错纠正<br> 2，词性消歧(什么是词性？什么的词性标注？为什么需要标注？如何标注？)</p> 
  </blockquote> 
  <p>形式语言的缺陷：</p> 
  <blockquote> 
   <p>1、对于像汉语，英语这样的大型自然语言系统，难以构造精确的文法<br> 2、不符合人类学习语言的习惯<br> 3、有些句子语法正确，但在语义上却不可能，形式语言无法排出这些句子<br> 4、解决方向：基于大量语料，采用统计学手段建立模型</p> 
  </blockquote> 
  <h2><a name="t29"></a>2.2、语言模型</h2> 
  <p>1)、语言模型(重要)：通过语料计算某个句子出现的概率(概率表示)，常用的有2-元模型，3-元模型</p> 
  <p>2)、语言模型应用：</p> 
  <p>语音识别歧义消除例如，给定拼音串：ta shi yan yan jiu saun fa de</p> 
  <p>可能的汉字串：踏实烟酒算法的他是研究酸法的他是研究算法的，显然，最后一句才符合。</p> 
  <p>3)、语言模型的启示：</p> 
  <p>1、开启自然语言处理的统计方法</p> 
  <p>2、统计方法的一般步骤：</p> 
  <blockquote> 
   <p>收集大量语料</p> 
   <p>对语料进行统计分析，得出知识</p> 
   <p>针对场景建立算法模型</p> 
   <p>解释和应用结果</p> 
  </blockquote> 
  <p>4)、语言模型性能评价，包括评价目标，评价的难点，常用指标(交叉熵，困惑度)</p> 
  <p>5)、数据平滑：</p> 
  <p>数据平滑的概念，为什么需要平滑</p> 
  <p>平滑的方法，加一法，加法平滑法，古德-图灵法，J-M法，Katz平滑法等</p> 
  <p>6)、语言模型的缺陷：</p> 
  <p>语料来自不同的领域，而语言模型对文本类型、主题等十分敏感</p> 
  <p>n与相邻的n-1个词相关，假设不是很成立。</p> 
  <h2><a name="t30"></a>2.3、概率图模型</h2> 
  <p>生成模型与判别模型，贝叶斯网络，马尔科夫链与隐马尔科夫模型(HMM)</p> 
  <p>1)概率图模型概述(什么的概率图模型，参考清华大学教材《概率图模型》)</p> 
  <p>2)马尔科夫过程(定义，理解)</p> 
  <p>3)隐马尔科夫过程(定义，理解)</p> 
  <p>HMM的三个基本问题(定义，解法，应用)</p> 
  <p>注：第一个问题，涉及最大似然估计法，第二个问题涉及EM算法，第三个问题涉及维特比算法，内容很多，要重点理解，(参考书李航《统计学习方法》，网上博客，笔者github)</p> 
  <h2><a name="t31"></a>2.4、马尔科夫网，最大熵模型，条件随机场(CRF)</h2> 
  <p>1)、HMM的三个基本问题的参数估计与计算</p> 
  <p>2)、什么是熵</p> 
  <p>3)、EM算法(应用十分广泛，好好理解)</p> 
  <p>4)、HMM的应用</p> 
  <p>5)、层次化马尔科夫模型与马尔科夫网络</p> 
  <p>提出原因，HMM存在两个问题</p> 
  <p>6)、最大熵马尔科夫模型</p> 
  <p>优点：与HMM相比，允许使用特征刻画观察序列，训练高效</p> 
  <p>缺点：存在标记偏置问题</p> 
  <p>7)、条件随机场及其应用(概念，模型过程，与HMM关系)</p> 
  <p>参数估计方法(GIS算法，改进IIS算法)</p> 
  <p>CRF基本问题：特征选取(特征模板)、概率计算、参数训练、解码(维特比)</p> 
  <p>应用场景：</p> 
  <p>词性标注类问题(现在一般用RNN+CRF)</p> 
  <p>中文分词(发展过程，经典算法，了解开源工具jieba分词)</p> 
  <p>中文人名，地名识别</p> 
  <p>8)、CRF++</p> 
  <h2><a name="t32"></a>2.5、命名实体识别，词性标注，内容挖掘、语义分析与篇章分析(大量用到前面的算法)</h2> 
  <p>1)、命名实体识别问题</p> 
  <p>相关概率，定义</p> 
  <p>相关任务类型</p> 
  <p>方法(基于规程-&gt;基于大规模语料库)</p> 
  <p>2)、未登录词的解决方法(搜索引擎，基于语料)</p> 
  <p>3)、<span style="color:#f33b45;">CRF解决命名实体识别(NER)流程总结：</span></p> 
  <p><span style="color:#f33b45;">训练阶段：确定特征模板，不同场景(人名，地名等)所使用的特征模板不同，对现有语料进行分词，在分词结果基础上进行词性标注(可能手工)，NER对应的标注问题是基于词的，然后训练CRF模型，得到对应权值参数值</span></p> 
  <p><span style="color:#f33b45;">识别过程：将待识别文档分词，然后送入CRF模型进行识别计算(维特比算法)，得到标注序列，然后根据标注划分出命名实体</span></p> 
  <p>4)、词性标注(理解含义，意义)及其一致性检查方法(位置属性向量，词性标注序列向量，聚类或者分类算法)</p> 
  <h2><a name="t33"></a>2.6、句法分析</h2> 
  <p>1)、句法分析理解以及意义</p> 
  <p>1、句法结构分析</p> 
  <p>完全句法分析<br> 浅层分析</p> 
  <p>2、依存关系分析</p> 
  <p>2)、句法分析方法</p> 
  <p>1、基于规则的句法结构分析<br> 2、基于统计的语法结构分析</p> 
  <h2><a name="t34"></a>2.7 文本分类，情感分析</h2> 
  <p>1)、文本分类，文本排重</p> 
  <p>文本分类：在预定义的分类体系下，根据文本的特征，将给定的文本与一个或者多个类别相关联</p> 
  <p>典型应用：垃圾邮件判定，网页自动分类</p> 
  <p>2)、文本表示，特征选取与权重计算，词向量</p> 
  <p>文本特征选择常用方法：</p> 
  <p>1、基于本文频率的特征提取法</p> 
  <p>2、信息增量法</p> 
  <p>3、X2(卡方)统计量</p> 
  <p>4、互信息法</p> 
  <p>3)、分类器设计</p> 
  <p>SVM，贝叶斯，决策树等</p> 
  <p>4)、分类器性能评测</p> 
  <p>1、召回率</p> 
  <p>2、正确率</p> 
  <p>3、F1值</p> 
  <p>5)、主题模型(LDA)与PLSA</p> 
  <p>LDA模型十分强大，基于贝叶斯改进了PLSA，可以提取出本章的主题词和关键词，建模过程复杂，难以理解。</p> 
  <p>6)、情感分析</p> 
  <p>借助计算机帮助用户快速获取，整理和分析相关评论信息，对带有感情色彩的主观文本进行分析，处理和归纳例如，评论自动分析，水军识别。</p> 
  <p>某种意义上看，情感分析也是一种特殊的分类问题</p> 
  <p>7)、应用案例</p> 
  <h2><a name="t35"></a>2.8、信息检索，搜索引擎及其原理</h2> 
  <p>1)、信息检索起源于图书馆资料查询检索，引入计算机技术后，从单纯的文本查询扩展到包含图片，音视频等多媒体信息检索，检索对象由数据库扩展到互联网。</p> 
  <p>1、点对点检索</p> 
  <p>2、精确匹配模型与相关匹配模型</p> 
  <p>3、检索系统关键技术：标引，相关度计算</p> 
  <p>2)、常见模型：布尔模型，向量空间模型，概率模型</p> 
  <p>3)、常用技术：倒排索引，隐语义分析(LDA等)</p> 
  <p>4)、评测指标</p> 
  <h2><a name="t36"></a>2.9、自动文摘与信息抽取，机器翻译，问答系统</h2> 
  <p>1)、统计机器翻译的的思路，过程，难点，以及解决</p> 
  <p>2)、问答系统</p> 
  <blockquote> 
   <p>基本组成：问题分析，信息检索，答案抽取<br> 类型：基于问题-答案，基于自由文本<br> 典型的解决思路</p> 
  </blockquote> 
  <p>3)、自动文摘的意义，常用方法</p> 
  <p>4)、信息抽取模型(LDA等)</p> 
  <h2><a name="t37"></a>2.10深度学习在自然语言中的应用</h2> 
  <p>1)、单词表示，比如词向量的训练(wordvoc)</p> 
  <p>2)、自动写文本</p> 
  <p>写新闻等</p> 
  <p>3)、机器翻译</p> 
  <p>4)、基于CNN、RNN的文本分类</p> 
  <p>5)、深度学习与CRF结合用于词性标注</p> 
  <h1><a name="t38"></a>三、中文NLP知识目录</h1> 
  <p>选自郑捷2017年电子工业出版社出版的图书《NLP汉语自然语言处理原理与实践》。</p> 
  <h2><a name="t39"></a>第1章 中文语言的机器处理 1</h2> 
  <p>1.1 历史回顾 2<br> 1.1.1 从科幻到现实 2<br> 1.1.2 早期的探索 3<br> 1.1.3 规则派还是统计派 3<br> 1.1.4 从机器学习到认知计算 5</p> 
  <p>1.2 现代自然语言系统简介 6<br> 1.2.1 NLP流程与开源框架 6<br> 1.2.2 哈工大NLP平台及其演示环境 9<br> 1.2.3 StanfordNLP团队及其演示环境 11<br> 1.2.4 NLTK开发环境 13</p> 
  <p>1.3 整合中文分词模块 16<br> 1.3.1 安装Ltp Python组件 17<br> 1.3.2 使用Ltp 3.3进行中文分词 18<br> 1.3.3 使用结巴分词模块 20</p> 
  <p>1.4 整合词性标注模块 22<br> 1.4.1 Ltp 3.3词性标注 23<br> 1.4.2 安装StanfordNLP并编写Python接口类 24<br> 1.4.3 执行Stanford词性标注 28</p> 
  <p>1.5 整合命名实体识别模块 29<br> 1.5.1 Ltp 3.3命名实体识别 29<br> 1.5.2 Stanford命名实体识别 30</p> 
  <p>1.6 整合句法解析模块 32<br> 1.6.1 Ltp 3.3句法依存树 33<br> 1.6.2 StanfordParser类 35<br> 1.6.3 Stanford短语结构树 36<br> 1.6.4 Stanford依存句法树 37</p> 
  <p>1.7 整合语义角色标注模块 38</p> 
  <p>1.8 结语 40</p> 
  <h2><a name="t40"></a>第2章 汉语语言学研究回顾 42</h2> 
  <p>2.1 文字符号的起源 42<br> 2.1.1 从记事谈起 43<br> 2.1.2 古文字的形成 47</p> 
  <p>2.2 六书及其他 48<br> 2.2.1 象形 48<br> 2.2.2 指事 50<br> 2.2.3 会意 51<br> 2.2.4 形声 53<br> 2.2.5 转注 54<br> 2.2.6 假借 55</p> 
  <p>2.3 字形的流变 56<br> 2.3.1 笔与墨的形成与变革 56<br> 2.3.2 隶变的方式 58<br> 2.3.3 汉字的符号化与结构 61</p> 
  <p>2.4 汉语的发展 67<br> 2.4.1 完整语义的基本形式——句子 68<br> 2.4.2 语言的初始形态与文言文 71<br> 2.4.3 白话文与复音词 73<br> 2.4.4 白话文与句法研究 78</p> 
  <p>2.5 三个平面中的语义研究 80<br> 2.5.1 词汇与本体论 81<br> 2.5.2 格语法及其框架 84</p> 
  <p>2.6 结语 86</p> 
  <h2><a name="t41"></a>第3章 词汇与分词技术 88</h2> 
  <p>3.1 中文分词 89<br> 3.1.1 什么是词与分词规范 90<br> 3.1.2 两种分词标准 93<br> 3.1.3 歧义、机械分词、语言模型 94<br> 3.1.4 词汇的构成与未登录词 97</p> 
  <p>3.2 系统总体流程与词典结构 98<br> 3.2.1 概述 98<br> 3.2.2 中文分词流程 99<br> 3.2.3 分词词典结构 103<br> 3.2.4 命名实体的词典结构 105<br> 3.2.5 词典的存储结构 108</p> 
  <p>3.3 算法部分源码解析 111<br> 3.3.1 系统配置 112<br> 3.3.2 Main方法与例句 113<br> 3.3.3 句子切分 113<br> 3.3.4 分词流程 117<br> 3.3.5 一元词网 118<br> 3.3.6 二元词图 125<br> 3.3.7 NShort算法原理 130<br> 3.3.8 后处理规则集 136<br> 3.3.9 命名实体识别 137<br> 3.3.10 细分阶段与最短路径 140</p> 
  <p>3.4 结语 142</p> 
  <h2><a name="t42"></a>第4章 NLP中的概率图模型 143</h2> 
  <p>4.1 概率论回顾 143<br> 4.1.1 多元概率论的几个基本概念 144<br> 4.1.2 贝叶斯与朴素贝叶斯算法 146<br> 4.1.3 文本分类 148<br> 4.1.4 文本分类的实现 151</p> 
  <p>4.2 信息熵 154<br> 4.2.1 信息量与信息熵 154<br> 4.2.2 互信息、联合熵、条件熵 156<br> 4.2.3 交叉熵和KL散度 158<br> 4.2.4 信息熵的NLP的意义 159</p> 
  <p>4.3 NLP与概率图模型 160<br> 4.3.1 概率图模型的几个基本问题 161<br> 4.3.2 产生式模型和判别式模型 162<br> 4.3.3 统计语言模型与NLP算法设计 164<br> 4.3.4 极大似然估计 167</p> 
  <p>4.4 隐马尔科夫模型简介 169<br> 4.4.1 马尔科夫链 169<br> 4.4.2 隐马尔科夫模型 170<br> 4.4.3 HMMs的一个实例 171<br> 4.4.4 Viterbi算法的实现 176</p> 
  <p>4.5 最大熵模型 179<br> 4.5.1 从词性标注谈起 179<br> 4.5.2 特征和约束 181<br> 4.5.3 最大熵原理 183<br> 4.5.4 公式推导 185<br> 4.5.5 对偶问题的极大似然估计 186<br> 4.5.6 GIS实现 188</p> 
  <p>4.6 条件随机场模型 193<br> 4.6.1 随机场 193<br> 4.6.2 无向图的团(Clique)与因子分解 194<br> 4.6.3 线性链条件随机场 195<br> 4.6.4 CRF的概率计算 198<br> 4.6.5 CRF的参数学习 199<br> 4.6.6 CRF预测标签 200</p> 
  <p>4.7 结语 201</p> 
  <h2><a name="t43"></a>第5章 词性、语块与命名实体识别 202</h2> 
  <p>5.1 汉语词性标注 203<br> 5.1.1 汉语的词性 203<br> 5.1.2 宾州树库的词性标注规范 205<br> 5.1.3stanfordNLP标注词性 210<br> 5.1.4 训练模型文件 213</p> 
  <p>5.2 语义组块标注 219<br> 5.2.1 语义组块的种类 220<br> 5.2.2 细说NP 221<br> 5.2.3 细说VP 223<br> 5.2.4 其他语义块 227<br> 5.2.5 语义块的抽取 229<br> 5.2.6 CRF的使用 232</p> 
  <p>5.3 命名实体识别 240<br> 5.3.1 命名实体 241<br> 5.3.2 分词架构与专名词典 243<br> 5.3.3 算法的策略——词典与统计相结合 245<br> 5.3.4 算法的策略——层叠式架构 252</p> 
  <p>5.4 结语 259</p> 
  <h2><a name="t44"></a>第6章 句法理论与自动分析 260</h2> 
  <p>6.1 转换生成语法 261<br> 6.1.1 乔姆斯基的语言观 261<br> 6.1.2 短语结构文法 263<br> 6.1.3 汉语句类 269<br> 6.1.4 谓词论元与空范畴 274<br> 6.1.5 轻动词分析理论 279<br> 6.1.6 NLTK操作句法树 280</p> 
  <p>6.2 依存句法理论 283<br> 6.2.1 配价理论 283<br> 6.2.2 配价词典 285<br> 6.2.3 依存理论概述 287<br> 6.2.4 Ltp依存分析介绍 290<br> 6.2.5 Stanford依存转换、解析 293</p> 
  <p>6.3 PCFG短语结构句法分析 298<br> 6.3.1 PCFG短语结构 298<br> 6.3.2 内向算法和外向算法 301<br> 6.3.3 Viterbi算法 303<br> 6.3.4 参数估计 304<br> 6.3.5 Stanford的PCFG算法训练 305</p> 
  <p>6.4 结语 310</p> 
  <h2><a name="t45"></a>第7章 建设语言资源库 311</h2> 
  <p>7.1 语料库概述 311<br> 7.1.1 语料库的简史 312<br> 7.1.2 语言资源库的分类 314<br> 7.1.3 语料库的设计实例：国家语委语料库 315<br> 7.1.4 语料库的层次加工 321</p> 
  <p>7.2 语法语料库 323<br> 7.2.1 中文分词语料库 323<br> 7.2.2 中文分词的测评 326<br> 7.2.3 宾州大学CTB简介 327</p> 
  <p>7.3 语义知识库 333<br> 7.3.1 知识库与HowNet简介 333<br> 7.3.2 发掘义原 334<br> 7.3.3 语义角色 336<br> 7.3.4 分类原则与事件分类 344<br> 7.3.5 实体分类 347<br> 7.3.6 属性与分类 352<br> 7.3.7 相似度计算与实例 353</p> 
  <p>7.4 语义网与百科知识库 360<br> 7.4.1 语义网理论介绍 360<br> 7.4.2 维基百科知识库 364<br> 7.4.3 DBpedia抽取原理 365</p> 
  <p>7.5 结语 368</p> 
  <h2><a name="t46"></a>第8章 语义与认知 370</h2> 
  <p>8.1 回顾现代语义学 371<br> 8.1.1 语义三角论 371<br> 8.1.2 语义场论 373<br> 8.1.3 基于逻辑的语义学 376</p> 
  <p>8.2 认知语言学概述 377<br> 8.2.1 象似性原理 379<br> 8.2.2 顺序象似性 380<br> 8.2.3 距离象似性 380<br> 8.2.4 重叠象似性 381</p> 
  <p>8.3 意象图式的构成 383<br> 8.3.1 主观性与焦点 383<br> 8.3.2 范畴化：概念的认知 385<br> 8.3.3 主体与背景 390<br> 8.3.4 意象图式 392<br> 8.3.5 社交中的图式 396<br> 8.3.6 完形：压缩与省略 398</p> 
  <p>8.4 隐喻与转喻 401<br> 8.4.1 隐喻的结构 402<br> 8.4.2 隐喻的认知本质 403<br> 8.4.3 隐喻计算的系统架构 405<br> 8.4.4 隐喻计算的实现 408</p> 
  <p>8.5 构式语法 412<br> 8.5.1 构式的概念 413<br> 8.5.2 句法与构式 415<br> 8.5.3 构式知识库 417</p> 
  <p>8.6 结语 420</p> 
  <h2><a name="t47"></a>第9章 NLP中的深度学习 422</h2> 
  <p>9.1 神经网络回顾 422<br> 9.1.1 神经网络框架 423<br> 9.1.2 梯度下降法推导 425<br> 9.1.3 梯度下降法的实现 427<br> 9.1.4 BP神经网络介绍和推导 430</p> 
  <p>9.2 Word2Vec简介 433<br> 9.2.1 词向量及其表达 434<br> 9.2.2 Word2Vec的算法原理 436<br> 9.2.3 训练词向量 439<br> 9.2.4 大规模上下位关系的自动识别 443</p> 
  <p>9.3 NLP与RNN 448<br> 9.3.1Simple-RNN 449<br> 9.3.2 LSTM原理 454<br> 9.3.3 LSTM的Python实现 460</p> 
  <p>9.4 深度学习框架与应用 467<br> 9.4.1 Keras框架介绍 467<br> 9.4.2 Keras序列标注 471<br> 9.4.3 依存句法的算法原理 478<br> 9.4.4 Stanford依存解析的训练过程 483</p> 
  <p>9.5 结语 488</p> 
  <h2><a name="t48"></a>第10章 语义计算的架构 490</h2> 
  <p>10.1 句子的语义和语法预处理 490<br> 10.1.1 长句切分和融合 491<br> 10.1.2 共指消解 496</p> 
  <p>10.2 语义角色 502<br> 10.2.1 谓词论元与语义角色 502<br> 10.2.2PropBank简介 505<br> 10.2.3 CPB中的特殊句式 506<br> 10.2.4 名词性谓词的语义角色 509<br> 10.2.5PropBank展开 512</p> 
  <p>10.3 句子的语义解析 517<br> 10.3.1 语义依存 517<br> 10.3.2 完整架构 524<br> 10.3.3 实体关系抽取 527</p> 
  <p>10.4 结语 531 [29]</p> 
  <h3><a name="t49"></a>自然语言处理NLP国内研究方向机构导师</h3> 
  <p>文|中文信息协会《中文信息处理发展报告2016》，数据简化DataSimp</p> 
  <h1><a name="t50"></a>文字语言VS数字信息</h1> 
  <p>数字、文字和自然语言一样，都是信息的载体，他们之间原本有着天然的联系。语言和数学的产生都是为了交流，从文字、数字和语言的发展历史，可以了解到语言、文字和数字有着内在的联系。自然语言处理NLP主要涉及三种文本，<strong>自由文本</strong>、<strong>结构化文本</strong>、<strong>半结构化文本</strong>。</p> 
  <p>自然语言理解Natural Language Understanding(NLU)，实现人机间自然语言通信，意味着要使计算机既能理解自然语言文本的意义，也能以自然语言文本表达给定的意图、思想等。自然语言生成NLG，是人工或机器生成语言。</p> 
  <p>斯坦福自然语言处理NLP工具资料收集、斯坦福分词、Stanford中文实体识别，最早做自然语言处理的网址https://nlp.stanford.edu/software/segmenter.shtml。</p> 
  <p>哈尔滨工业大学智能技术与自然语言处理研究室(IntelligentTechnology &amp; Natural Language Processing Lab, ITNLPLab)是国内较早从事自然语言处理和语言智能技术的研究室。</p> 
  <p>除了新兴的文本数据简化领域：秦陇纪(数据简化技术中心筹)，自然语言处理NaturalLanguage Processing领域主要包括基础研究和应用研究。</p> 
  <h1><a name="t51"></a>基础研究</h1> 
  <p>词法与句法分析：李正华、陈文亮、张民(苏州大学)<br> 语义分析：周国栋、李军辉(苏州大学)<br> 篇章分析：王厚峰、李素建(北京大学)<br> 语言认知模型：王少楠，宗成庆(中科院自动化研究所)<br> 语言表示与深度学习：黄萱菁、邱锡鹏(复旦大学)<br> 知识图谱与计算：李涓子、候磊(清华大学)</p> 
  <h1><a name="t52"></a>应用研究</h1> 
  <p>文本分类与聚类：涂存超，刘知远(清华大学)<br> 信息抽取：孙乐、韩先培(中国科学院软件研究所)<br> 情感分析：黄民烈(清华大学)<br> 自动文摘：万小军、姚金戈(北京大学)<br> 信息检索：刘奕群、马少平(清华大学)<br> 信息推荐与过滤：王斌(中科院信工所)，鲁骁(国家计算机网络应急中心)<br> 自动问答：赵军、刘康，何世柱(中科院自动化研究所)<br> 机器翻译：张家俊、宗成庆(中科院自动化研究所)<br> 社会媒体处理：刘挺、丁效(哈尔滨工业大学)<br> 语音技术：说话人识别——郑方(清华大学)，王仁宇(江苏师范大学)<br> 语音合成——陶建华(中科院自动化研究所)<br> 语音识别——王东(清华大学)<br> 文字识别：刘成林(中科院自动化研究所)<br> 多模态信息处理：陈晓鸥(北京大学)<br> 医疗健康信息处理：陈清财、汤步洲(哈尔滨工业大学)<br> 少数民族语言信息处理：吾守尔•斯拉木(新疆大学)</p> 
  <p>—&nbsp;<strong>完</strong>&nbsp;—</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
