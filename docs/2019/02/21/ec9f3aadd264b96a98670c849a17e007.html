<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Adaboost原理及实现 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Adaboost原理及实现" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="一、原理推导 对于m=1,2,…,M 二、Adaboost代码实现 AdaBoost算法实现的是将弱分类器提升成为强分类器，代码中使用的弱分类器是单层决策树，这也是使用的最多的弱分类器，然后我们就可以根据弱分类器构造出强分类器 from numpy import * # 构建一层决策树，返回分类结果 def stumpclassify(dataMatrix,dimen,threshval,threshIneq): #dataMatrix：数据； #dimen：以当前数据第几个维度进行分割； #threshval：分割阈值 #threshIneq：划分方向 retArray = ones((dataMatrix.shape[0],1)) if threshIneq == &#39;lt&#39;: retArray[dataMatrix[:,dimen] &lt;= threshval] = -1.0 else: retArray[dataMatrix[:,dimen] &gt; threshval] = -1.0 return retArray def buildStump(dataMatrix,labels,weight): m,n = dataMatrix.shape numSteps = 20.0 #对特征划分数量 bestStump = {} bestClassEst = mat(zeros((m,1))) minError = inf for i in range(n): #遍历所有特征 rangeMin = min(dataMatrix[:,i]) rangeMax = max(dataMatrix[:,i]) stepSize = (rangeMax-rangeMin)/numSteps for j in range(-1,int(numSteps)+2): #遍历所有分割点 threshVal = rangeMin + j*float(stepSize) for threshIneq in [&#39;lt&#39;,&#39;gt&#39;]: predictedVals = stumpclassify(dataMatrix,i,threshVal,threshIneq) errorAarray = mat(ones((m,1))) errorAarray[predictedVals==labels] = 0 weightedError = weight.T * errorAarray if weightedError &lt; minError: minError = weightedError bestClassEst = predictedVals.copy() bestStump[&#39;dim&#39;] = i bestStump[&#39;threshVal&#39;] = threshVal bestStump[&#39;threshIneq&#39;] = threshIneq return bestStump,minError,bestClassEst def train(dataMatrix,labels,numIter=100): weekClassArr = [] m = dataMatrix.shape[0] weight = mat(ones((m,1))/m) #初始化样本权值 aggClassEst = mat(zeros((m,1))) for i in range(numIter): bestStump,error,classEst = buildStump(dataMatrix,labels,weight) alpha = float(0.5*log((1-error)/max(error,1e-16))) #计算当前基本分类器系数 bestStump[&#39;alpha&#39;] = alpha weekClassArr.append(bestStump) expon = multiply(-1*alpha*labels,classEst) weight = multiply(weight,exp(expon)) #更新样本权值 weight = weight/sum(weight) aggClassEst += alpha*classEst aggErrors = multiply(sign(aggClassEst) != labels,ones((m,1))) errorRate = sum(aggErrors)/m print(&quot;当前共有%d颗树，total error为:%f &quot;%(i+1,errorRate)) if errorRate == 0.0: break return weekClassArr def predict(dataMatrix,classifierArr): dataMatrix = mat(dataMatrix) m = dataMatrix.shape[0] aggClassEst = mat(zeros((m,1))) for i in range(len(classifierArr)): classEst = stumpclassify(dataMatrix,classifierArr[i][&#39;dim&#39;], classifierArr[i][&#39;threshVal&#39;],classifierArr[i][&#39;threshIneq&#39;]) aggClassEst += classifierArr[i][&#39;alpha&#39;]*classEst print(aggClassEst) return sign(aggClassEst) dataMat = matrix([[1. , 2.1], [2. , 1.1], [1.3 , 1.], [1. , 1.], [2. , 1.]]) classLabels = mat([1.0,1.0,-1.0,-1.0,1.0]).T classifierArr = train(dataMat,classLabels,30) t = predict([0,0],classifierArr) print(t) 输出结果：" />
<meta property="og:description" content="一、原理推导 对于m=1,2,…,M 二、Adaboost代码实现 AdaBoost算法实现的是将弱分类器提升成为强分类器，代码中使用的弱分类器是单层决策树，这也是使用的最多的弱分类器，然后我们就可以根据弱分类器构造出强分类器 from numpy import * # 构建一层决策树，返回分类结果 def stumpclassify(dataMatrix,dimen,threshval,threshIneq): #dataMatrix：数据； #dimen：以当前数据第几个维度进行分割； #threshval：分割阈值 #threshIneq：划分方向 retArray = ones((dataMatrix.shape[0],1)) if threshIneq == &#39;lt&#39;: retArray[dataMatrix[:,dimen] &lt;= threshval] = -1.0 else: retArray[dataMatrix[:,dimen] &gt; threshval] = -1.0 return retArray def buildStump(dataMatrix,labels,weight): m,n = dataMatrix.shape numSteps = 20.0 #对特征划分数量 bestStump = {} bestClassEst = mat(zeros((m,1))) minError = inf for i in range(n): #遍历所有特征 rangeMin = min(dataMatrix[:,i]) rangeMax = max(dataMatrix[:,i]) stepSize = (rangeMax-rangeMin)/numSteps for j in range(-1,int(numSteps)+2): #遍历所有分割点 threshVal = rangeMin + j*float(stepSize) for threshIneq in [&#39;lt&#39;,&#39;gt&#39;]: predictedVals = stumpclassify(dataMatrix,i,threshVal,threshIneq) errorAarray = mat(ones((m,1))) errorAarray[predictedVals==labels] = 0 weightedError = weight.T * errorAarray if weightedError &lt; minError: minError = weightedError bestClassEst = predictedVals.copy() bestStump[&#39;dim&#39;] = i bestStump[&#39;threshVal&#39;] = threshVal bestStump[&#39;threshIneq&#39;] = threshIneq return bestStump,minError,bestClassEst def train(dataMatrix,labels,numIter=100): weekClassArr = [] m = dataMatrix.shape[0] weight = mat(ones((m,1))/m) #初始化样本权值 aggClassEst = mat(zeros((m,1))) for i in range(numIter): bestStump,error,classEst = buildStump(dataMatrix,labels,weight) alpha = float(0.5*log((1-error)/max(error,1e-16))) #计算当前基本分类器系数 bestStump[&#39;alpha&#39;] = alpha weekClassArr.append(bestStump) expon = multiply(-1*alpha*labels,classEst) weight = multiply(weight,exp(expon)) #更新样本权值 weight = weight/sum(weight) aggClassEst += alpha*classEst aggErrors = multiply(sign(aggClassEst) != labels,ones((m,1))) errorRate = sum(aggErrors)/m print(&quot;当前共有%d颗树，total error为:%f &quot;%(i+1,errorRate)) if errorRate == 0.0: break return weekClassArr def predict(dataMatrix,classifierArr): dataMatrix = mat(dataMatrix) m = dataMatrix.shape[0] aggClassEst = mat(zeros((m,1))) for i in range(len(classifierArr)): classEst = stumpclassify(dataMatrix,classifierArr[i][&#39;dim&#39;], classifierArr[i][&#39;threshVal&#39;],classifierArr[i][&#39;threshIneq&#39;]) aggClassEst += classifierArr[i][&#39;alpha&#39;]*classEst print(aggClassEst) return sign(aggClassEst) dataMat = matrix([[1. , 2.1], [2. , 1.1], [1.3 , 1.], [1. , 1.], [2. , 1.]]) classLabels = mat([1.0,1.0,-1.0,-1.0,1.0]).T classifierArr = train(dataMat,classLabels,30) t = predict([0,0],classifierArr) print(t) 输出结果：" />
<link rel="canonical" href="https://mlh.app/2019/02/21/ec9f3aadd264b96a98670c849a17e007.html" />
<meta property="og:url" content="https://mlh.app/2019/02/21/ec9f3aadd264b96a98670c849a17e007.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-21T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"一、原理推导 对于m=1,2,…,M 二、Adaboost代码实现 AdaBoost算法实现的是将弱分类器提升成为强分类器，代码中使用的弱分类器是单层决策树，这也是使用的最多的弱分类器，然后我们就可以根据弱分类器构造出强分类器 from numpy import * # 构建一层决策树，返回分类结果 def stumpclassify(dataMatrix,dimen,threshval,threshIneq): #dataMatrix：数据； #dimen：以当前数据第几个维度进行分割； #threshval：分割阈值 #threshIneq：划分方向 retArray = ones((dataMatrix.shape[0],1)) if threshIneq == &#39;lt&#39;: retArray[dataMatrix[:,dimen] &lt;= threshval] = -1.0 else: retArray[dataMatrix[:,dimen] &gt; threshval] = -1.0 return retArray def buildStump(dataMatrix,labels,weight): m,n = dataMatrix.shape numSteps = 20.0 #对特征划分数量 bestStump = {} bestClassEst = mat(zeros((m,1))) minError = inf for i in range(n): #遍历所有特征 rangeMin = min(dataMatrix[:,i]) rangeMax = max(dataMatrix[:,i]) stepSize = (rangeMax-rangeMin)/numSteps for j in range(-1,int(numSteps)+2): #遍历所有分割点 threshVal = rangeMin + j*float(stepSize) for threshIneq in [&#39;lt&#39;,&#39;gt&#39;]: predictedVals = stumpclassify(dataMatrix,i,threshVal,threshIneq) errorAarray = mat(ones((m,1))) errorAarray[predictedVals==labels] = 0 weightedError = weight.T * errorAarray if weightedError &lt; minError: minError = weightedError bestClassEst = predictedVals.copy() bestStump[&#39;dim&#39;] = i bestStump[&#39;threshVal&#39;] = threshVal bestStump[&#39;threshIneq&#39;] = threshIneq return bestStump,minError,bestClassEst def train(dataMatrix,labels,numIter=100): weekClassArr = [] m = dataMatrix.shape[0] weight = mat(ones((m,1))/m) #初始化样本权值 aggClassEst = mat(zeros((m,1))) for i in range(numIter): bestStump,error,classEst = buildStump(dataMatrix,labels,weight) alpha = float(0.5*log((1-error)/max(error,1e-16))) #计算当前基本分类器系数 bestStump[&#39;alpha&#39;] = alpha weekClassArr.append(bestStump) expon = multiply(-1*alpha*labels,classEst) weight = multiply(weight,exp(expon)) #更新样本权值 weight = weight/sum(weight) aggClassEst += alpha*classEst aggErrors = multiply(sign(aggClassEst) != labels,ones((m,1))) errorRate = sum(aggErrors)/m print(&quot;当前共有%d颗树，total error为:%f &quot;%(i+1,errorRate)) if errorRate == 0.0: break return weekClassArr def predict(dataMatrix,classifierArr): dataMatrix = mat(dataMatrix) m = dataMatrix.shape[0] aggClassEst = mat(zeros((m,1))) for i in range(len(classifierArr)): classEst = stumpclassify(dataMatrix,classifierArr[i][&#39;dim&#39;], classifierArr[i][&#39;threshVal&#39;],classifierArr[i][&#39;threshIneq&#39;]) aggClassEst += classifierArr[i][&#39;alpha&#39;]*classEst print(aggClassEst) return sign(aggClassEst) dataMat = matrix([[1. , 2.1], [2. , 1.1], [1.3 , 1.], [1. , 1.], [2. , 1.]]) classLabels = mat([1.0,1.0,-1.0,-1.0,1.0]).T classifierArr = train(dataMat,classLabels,30) t = predict([0,0],classifierArr) print(t) 输出结果：","@type":"BlogPosting","url":"https://mlh.app/2019/02/21/ec9f3aadd264b96a98670c849a17e007.html","headline":"Adaboost原理及实现","dateModified":"2019-02-21T00:00:00+08:00","datePublished":"2019-02-21T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/02/21/ec9f3aadd264b96a98670c849a17e007.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Adaboost原理及实现</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <h1><a id="_0"></a>一、原理推导</h1> 
  <p><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190221201242413.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE0MDQzMA==,size_16,color_FFFFFF,t_70"></p> 
  <h1><a id="m12M_5"></a>对于m=1,2,…,M</h1> 
  <p><img alt="在这里插入图片描述" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190221201606385.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE0MDQzMA==,size_16,color_FFFFFF,t_70"></p> 
  <p><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190221201925443.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE0MDQzMA==,size_16,color_FFFFFF,t_70"><br> <img alt="在这里插入图片描述" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190221202019536.PNG"><br> <img alt="在这里插入图片描述" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190221202115508.PNG"></p> 
  <p><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190221202250754.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzE0MDQzMA==,size_16,color_FFFFFF,t_70"></p> 
  <h1><a id="Adaboost_18"></a>二、Adaboost代码实现</h1> 
  <p>AdaBoost算法实现的是将弱分类器提升成为强分类器，代码中使用的弱分类器是单层决策树，这也是使用的最多的弱分类器，然后我们就可以根据弱分类器构造出强分类器</p> 
  <pre><code>from numpy import *

#   构建一层决策树，返回分类结果
def stumpclassify(dataMatrix,dimen,threshval,threshIneq):
    #dataMatrix：数据；
    #dimen：以当前数据第几个维度进行分割；
    #threshval：分割阈值
    #threshIneq：划分方向
    retArray = ones((dataMatrix.shape[0],1))
    if threshIneq == 'lt':
        retArray[dataMatrix[:,dimen] &lt;= threshval] = -1.0
    else:
        retArray[dataMatrix[:,dimen] &gt; threshval] = -1.0
    return retArray

def buildStump(dataMatrix,labels,weight):
    m,n = dataMatrix.shape
    numSteps = 20.0 #对特征划分数量
    bestStump = {}
    bestClassEst = mat(zeros((m,1)))
    minError = inf
    for i in range(n):  #遍历所有特征
        rangeMin = min(dataMatrix[:,i])
        rangeMax = max(dataMatrix[:,i])
        stepSize = (rangeMax-rangeMin)/numSteps
        for j in range(-1,int(numSteps)+2): #遍历所有分割点
            threshVal = rangeMin + j*float(stepSize)
            for threshIneq in ['lt','gt']:
                predictedVals = stumpclassify(dataMatrix,i,threshVal,threshIneq)
                errorAarray = mat(ones((m,1)))
                errorAarray[predictedVals==labels] = 0
                weightedError = weight.T * errorAarray
                if weightedError &lt; minError:
                    minError = weightedError
                    bestClassEst = predictedVals.copy()
                    bestStump['dim'] = i
                    bestStump['threshVal'] = threshVal
                    bestStump['threshIneq'] = threshIneq
    return bestStump,minError,bestClassEst

def train(dataMatrix,labels,numIter=100):
    weekClassArr = []
    m = dataMatrix.shape[0]
    weight = mat(ones((m,1))/m) #初始化样本权值
    aggClassEst = mat(zeros((m,1)))
    for i in range(numIter):
        bestStump,error,classEst = buildStump(dataMatrix,labels,weight)
        alpha = float(0.5*log((1-error)/max(error,1e-16))) #计算当前基本分类器系数
        bestStump['alpha'] = alpha
        weekClassArr.append(bestStump)
        expon = multiply(-1*alpha*labels,classEst)
        weight = multiply(weight,exp(expon)) #更新样本权值
        weight = weight/sum(weight)
        aggClassEst += alpha*classEst
        aggErrors = multiply(sign(aggClassEst) != labels,ones((m,1)))
        errorRate = sum(aggErrors)/m
        print("当前共有%d颗树，total error为:%f "%(i+1,errorRate))
        if errorRate == 0.0:
            break
    return weekClassArr


def predict(dataMatrix,classifierArr):
    dataMatrix = mat(dataMatrix)
    m = dataMatrix.shape[0]
    aggClassEst = mat(zeros((m,1)))
    for i in range(len(classifierArr)):
        classEst = stumpclassify(dataMatrix,classifierArr[i]['dim'],
                                 classifierArr[i]['threshVal'],classifierArr[i]['threshIneq'])
        aggClassEst += classifierArr[i]['alpha']*classEst
        print(aggClassEst)
    return sign(aggClassEst)

dataMat = matrix([[1. , 2.1],
        [2. , 1.1],
        [1.3 , 1.],
        [1. , 1.],
        [2. , 1.]])
classLabels = mat([1.0,1.0,-1.0,-1.0,1.0]).T
classifierArr = train(dataMat,classLabels,30)
t = predict([0,0],classifierArr)
print(t)
</code></pre> 
  <p>输出结果：<br> <img alt="在这里插入图片描述" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190221202750319.PNG"></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
