<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>如何训练AI玩飞机大战游戏（创号版） | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="如何训练AI玩飞机大战游戏（创号版）" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="虽然没有谷歌强大的集群和DeepMind变态的算法的团队，但基于深度强化学习（Deep Q Network DQN ）的自制小游戏AI效果同样很赞。先上效果图： 下面分四个部分，具体给大家介绍。 /1/背景介绍 2013年DeepMind团队发表论文“Playing Atari with Deep Reinforcement Learning”，用Q-Network模型成功让AI玩起了Atari系列游戏。并于2015年在《Nature》上发表了一篇升级版，“Human-level control through deep reinforcement learning”，自此，在这类游戏领域，人已经无法超过机器了。AI玩游戏的姿势是这样的： 后来的故事大家都很熟悉了，AlphaGo击败世界冠军，星际争霸2职业选手也被打败，连大家接触较多的王者荣耀也不能幸免。 /2/深度强化学习模型 看完了轻松的部分，下面简单介绍一下模型。DQN是DRL的一种算法，它将卷积神经网络（CNN）和Q-Learning结合起来。 Q-learning是强化学习的一种，原理图如下： 也就是Agent在观察得到当前的状态state和回报reward的基础上，选取输出一个动作action，进而影响环境，使环境状态和回报都产生变化。通过不断循环让Agent学习如何在环境中获得更高的回报。 卷积神经网络CNN是图像处理领域非常经典的神经网络模型，在本模型中，输入是原始图像数据，输出为每个动作action对应的评估值。 因此DQN总体结构是这样的： 图比较简单，但原理很清晰，是将Agent中的模型用CNN来代替，环境的State为游戏界面截图，输出为AI的动作，在飞机大战中就是飞机向左、向右还是不动。回报reward具体为，在一次循环中没有被击中为0.1，被击中为-1，击中敌机为1。图中回放记忆单元、当前网络和目标网络都是为了将CNN这种需要大量样本的监督学习融合在强化学习模型中的手段。篇幅限制这里只是概述性的介绍，后期会专门讲。 /3/模型实现 3.1程序的总体结构 程序主函数在PlaneDQN.py中，与DQN模型相关的函数在BrainDQN_Nature.py中，游戏模型在game文件夹中，训练过程保存的训练值在saved_networks文件夹中。 3.2主函数搭建 大家注意看while循环里的结构，其实非常明确： getaction()为在当前的Q值下选取动作 framestep()为运行环境，并输出观测值 process()为对图像数据进行处理的函数 setPerception()根据图像和回报，对网络进行训练 def playPlane(): # Step 1: init BrainDQN actions = 3 brain = BrainDQN(actions) # Step 2: init Plane Game plane = game.GameState() # Step 3: play game # Step 3.1: obtain init state action0 = np.array([1,0,0]) # [1,0,0]do nothing,[0,1,0]left,[0,0,1]right observation0, reward0, terminal = plane.frame_step(action0) observation0 = cv2.cvtColor(cv2.resize(observation0, (80, 80)), cv2.COLOR_BGR2GRAY) ret, observation0 = cv2.threshold(observation0,1,255,cv2.THRESH_BINARY) brain.setInitState(observation0) # Step 3.2: run the game while 1!= 0: action = brain.getAction() nextObservation,reward,terminal = plane.frame_step(action) nextObservation = preprocess(nextObservation) brain.setPerception(nextObservation,action,reward,terminal) 3.3 游戏类GameState和framestep 通过pygame实现游戏界面的搭建，分别建立子弹类、玩家类、敌机类和游戏类，结构代码所示。 class Bullet(pygame.sprite.Sprite): def __init__(self, bullet_img, init_pos): def move(self): # 我方飞机类 class Player(pygame.sprite.Sprite): def __init__(self, plane_img, player_rect, init_pos): def shoot(self, bullet_img): def moveLeft(self): def moveRight(self): # 敌方飞机类 class Enemy(pygame.sprite.Sprite): def __init__(self, enemy_img, enemy_down_imgs, init_pos): def move(self): class GameState: def __init__(self): def frame_step(self, input_actions): if input_actions[0] == 1 or input_actions[1]== 1 or input_actions[2]== 1: # 检查输入正常 if input_actions[0] == 0 and input_actions[1] == 1 and input_actions[2] == 0: self.player.moveLeft() elif input_actions[0] == 0 and input_actions[1] == 0 and input_actions[2] == 1: self.player.moveRight() else: pass else: raise ValueError(&#39;Multiple input actions!&#39;) image_data = pygame.surfarray.array3d(pygame.display.get_surface()) pygame.display.update() clock = pygame.time.Clock() clock.tick(30) return image_data, reward, terminal 其中GameState中的framestep()函数，是整个DQN运行一次使环境发生变化的基础函数，该函数运行一次，会根据inputaction进行动作实施，接着会在该时段对界面上的元素进行移动，并判断是否撞击。最后通过get_surface获取界面图像，最后返回环境的image_data，reward和游戏是否停止的terminal。本文游戏效果图为： 为提高模型收敛速度，在实际运行时将背景图片去掉。 3.4 DQN模型类 该部分为DQN模型的核心，主要有根据参数建立CNN网络的createQNetwork()，进行模型训练的trainQNetwork()，进行动作选择的getAction()。 class BrainDQN: def __init__(self,actions): def createQNetwork(self): return stateInput,QValue,W_conv1,b_conv1,W_conv2,b_conv2,W_conv3,b_conv3,W_fc1,b_fc1,W_fc2,b_fc2 def copyTargetQNetwork(self): self.session.run(self.copyTargetQNetworkOperation) def createTrainingMethod(self): def trainQNetwork(self): def getAction(self): return action def setInitState(self,observation): self.currentState = np.stack((observation, observation, observation, observation), axis = 2) def weight_variable(self,shape): return tf.Variable(initial) def bias_variable(self,shape): return tf.Variable(initial) def conv2d(self,x, W, stride): return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = &quot;SAME&quot;) def max_pool_2x2(self,x): return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], paddin 3.5图像处理 图像预处理调用cv2库函数，对图像进行大小和灰度处理。 def preprocess(observation): observation = cv2.cvtColor(cv2.resize(observation, (80, 80)), cv2.COLOR_BGR2GRAY)#灰度转化 ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY) return np.reshape(observation,(80,80,1)) /4/环境搭建 系统：Ubuntu16.04、win10 Python3.5 pygame 1.9.4 TensorFlow1.11（GPU版） OpenCV-Python 关注公众号“1024程序开发者社区”回复“AI飞机”，获取代码，包含训练500000次的结果。本程序对硬件要求不高，显存2GB以上就可运行。 1024程序开发者社区的交流群已经建立，许多小伙伴已经加入其中，感谢大家的支持。大家可以在群里就技术问题进行交流，还没有加入的小伙伴可以扫描下方“社区物业”二维码，让管理员帮忙拉进群，期待大家的加入。" />
<meta property="og:description" content="虽然没有谷歌强大的集群和DeepMind变态的算法的团队，但基于深度强化学习（Deep Q Network DQN ）的自制小游戏AI效果同样很赞。先上效果图： 下面分四个部分，具体给大家介绍。 /1/背景介绍 2013年DeepMind团队发表论文“Playing Atari with Deep Reinforcement Learning”，用Q-Network模型成功让AI玩起了Atari系列游戏。并于2015年在《Nature》上发表了一篇升级版，“Human-level control through deep reinforcement learning”，自此，在这类游戏领域，人已经无法超过机器了。AI玩游戏的姿势是这样的： 后来的故事大家都很熟悉了，AlphaGo击败世界冠军，星际争霸2职业选手也被打败，连大家接触较多的王者荣耀也不能幸免。 /2/深度强化学习模型 看完了轻松的部分，下面简单介绍一下模型。DQN是DRL的一种算法，它将卷积神经网络（CNN）和Q-Learning结合起来。 Q-learning是强化学习的一种，原理图如下： 也就是Agent在观察得到当前的状态state和回报reward的基础上，选取输出一个动作action，进而影响环境，使环境状态和回报都产生变化。通过不断循环让Agent学习如何在环境中获得更高的回报。 卷积神经网络CNN是图像处理领域非常经典的神经网络模型，在本模型中，输入是原始图像数据，输出为每个动作action对应的评估值。 因此DQN总体结构是这样的： 图比较简单，但原理很清晰，是将Agent中的模型用CNN来代替，环境的State为游戏界面截图，输出为AI的动作，在飞机大战中就是飞机向左、向右还是不动。回报reward具体为，在一次循环中没有被击中为0.1，被击中为-1，击中敌机为1。图中回放记忆单元、当前网络和目标网络都是为了将CNN这种需要大量样本的监督学习融合在强化学习模型中的手段。篇幅限制这里只是概述性的介绍，后期会专门讲。 /3/模型实现 3.1程序的总体结构 程序主函数在PlaneDQN.py中，与DQN模型相关的函数在BrainDQN_Nature.py中，游戏模型在game文件夹中，训练过程保存的训练值在saved_networks文件夹中。 3.2主函数搭建 大家注意看while循环里的结构，其实非常明确： getaction()为在当前的Q值下选取动作 framestep()为运行环境，并输出观测值 process()为对图像数据进行处理的函数 setPerception()根据图像和回报，对网络进行训练 def playPlane(): # Step 1: init BrainDQN actions = 3 brain = BrainDQN(actions) # Step 2: init Plane Game plane = game.GameState() # Step 3: play game # Step 3.1: obtain init state action0 = np.array([1,0,0]) # [1,0,0]do nothing,[0,1,0]left,[0,0,1]right observation0, reward0, terminal = plane.frame_step(action0) observation0 = cv2.cvtColor(cv2.resize(observation0, (80, 80)), cv2.COLOR_BGR2GRAY) ret, observation0 = cv2.threshold(observation0,1,255,cv2.THRESH_BINARY) brain.setInitState(observation0) # Step 3.2: run the game while 1!= 0: action = brain.getAction() nextObservation,reward,terminal = plane.frame_step(action) nextObservation = preprocess(nextObservation) brain.setPerception(nextObservation,action,reward,terminal) 3.3 游戏类GameState和framestep 通过pygame实现游戏界面的搭建，分别建立子弹类、玩家类、敌机类和游戏类，结构代码所示。 class Bullet(pygame.sprite.Sprite): def __init__(self, bullet_img, init_pos): def move(self): # 我方飞机类 class Player(pygame.sprite.Sprite): def __init__(self, plane_img, player_rect, init_pos): def shoot(self, bullet_img): def moveLeft(self): def moveRight(self): # 敌方飞机类 class Enemy(pygame.sprite.Sprite): def __init__(self, enemy_img, enemy_down_imgs, init_pos): def move(self): class GameState: def __init__(self): def frame_step(self, input_actions): if input_actions[0] == 1 or input_actions[1]== 1 or input_actions[2]== 1: # 检查输入正常 if input_actions[0] == 0 and input_actions[1] == 1 and input_actions[2] == 0: self.player.moveLeft() elif input_actions[0] == 0 and input_actions[1] == 0 and input_actions[2] == 1: self.player.moveRight() else: pass else: raise ValueError(&#39;Multiple input actions!&#39;) image_data = pygame.surfarray.array3d(pygame.display.get_surface()) pygame.display.update() clock = pygame.time.Clock() clock.tick(30) return image_data, reward, terminal 其中GameState中的framestep()函数，是整个DQN运行一次使环境发生变化的基础函数，该函数运行一次，会根据inputaction进行动作实施，接着会在该时段对界面上的元素进行移动，并判断是否撞击。最后通过get_surface获取界面图像，最后返回环境的image_data，reward和游戏是否停止的terminal。本文游戏效果图为： 为提高模型收敛速度，在实际运行时将背景图片去掉。 3.4 DQN模型类 该部分为DQN模型的核心，主要有根据参数建立CNN网络的createQNetwork()，进行模型训练的trainQNetwork()，进行动作选择的getAction()。 class BrainDQN: def __init__(self,actions): def createQNetwork(self): return stateInput,QValue,W_conv1,b_conv1,W_conv2,b_conv2,W_conv3,b_conv3,W_fc1,b_fc1,W_fc2,b_fc2 def copyTargetQNetwork(self): self.session.run(self.copyTargetQNetworkOperation) def createTrainingMethod(self): def trainQNetwork(self): def getAction(self): return action def setInitState(self,observation): self.currentState = np.stack((observation, observation, observation, observation), axis = 2) def weight_variable(self,shape): return tf.Variable(initial) def bias_variable(self,shape): return tf.Variable(initial) def conv2d(self,x, W, stride): return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = &quot;SAME&quot;) def max_pool_2x2(self,x): return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], paddin 3.5图像处理 图像预处理调用cv2库函数，对图像进行大小和灰度处理。 def preprocess(observation): observation = cv2.cvtColor(cv2.resize(observation, (80, 80)), cv2.COLOR_BGR2GRAY)#灰度转化 ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY) return np.reshape(observation,(80,80,1)) /4/环境搭建 系统：Ubuntu16.04、win10 Python3.5 pygame 1.9.4 TensorFlow1.11（GPU版） OpenCV-Python 关注公众号“1024程序开发者社区”回复“AI飞机”，获取代码，包含训练500000次的结果。本程序对硬件要求不高，显存2GB以上就可运行。 1024程序开发者社区的交流群已经建立，许多小伙伴已经加入其中，感谢大家的支持。大家可以在群里就技术问题进行交流，还没有加入的小伙伴可以扫描下方“社区物业”二维码，让管理员帮忙拉进群，期待大家的加入。" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-18T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"虽然没有谷歌强大的集群和DeepMind变态的算法的团队，但基于深度强化学习（Deep Q Network DQN ）的自制小游戏AI效果同样很赞。先上效果图： 下面分四个部分，具体给大家介绍。 /1/背景介绍 2013年DeepMind团队发表论文“Playing Atari with Deep Reinforcement Learning”，用Q-Network模型成功让AI玩起了Atari系列游戏。并于2015年在《Nature》上发表了一篇升级版，“Human-level control through deep reinforcement learning”，自此，在这类游戏领域，人已经无法超过机器了。AI玩游戏的姿势是这样的： 后来的故事大家都很熟悉了，AlphaGo击败世界冠军，星际争霸2职业选手也被打败，连大家接触较多的王者荣耀也不能幸免。 /2/深度强化学习模型 看完了轻松的部分，下面简单介绍一下模型。DQN是DRL的一种算法，它将卷积神经网络（CNN）和Q-Learning结合起来。 Q-learning是强化学习的一种，原理图如下： 也就是Agent在观察得到当前的状态state和回报reward的基础上，选取输出一个动作action，进而影响环境，使环境状态和回报都产生变化。通过不断循环让Agent学习如何在环境中获得更高的回报。 卷积神经网络CNN是图像处理领域非常经典的神经网络模型，在本模型中，输入是原始图像数据，输出为每个动作action对应的评估值。 因此DQN总体结构是这样的： 图比较简单，但原理很清晰，是将Agent中的模型用CNN来代替，环境的State为游戏界面截图，输出为AI的动作，在飞机大战中就是飞机向左、向右还是不动。回报reward具体为，在一次循环中没有被击中为0.1，被击中为-1，击中敌机为1。图中回放记忆单元、当前网络和目标网络都是为了将CNN这种需要大量样本的监督学习融合在强化学习模型中的手段。篇幅限制这里只是概述性的介绍，后期会专门讲。 /3/模型实现 3.1程序的总体结构 程序主函数在PlaneDQN.py中，与DQN模型相关的函数在BrainDQN_Nature.py中，游戏模型在game文件夹中，训练过程保存的训练值在saved_networks文件夹中。 3.2主函数搭建 大家注意看while循环里的结构，其实非常明确： getaction()为在当前的Q值下选取动作 framestep()为运行环境，并输出观测值 process()为对图像数据进行处理的函数 setPerception()根据图像和回报，对网络进行训练 def playPlane(): # Step 1: init BrainDQN actions = 3 brain = BrainDQN(actions) # Step 2: init Plane Game plane = game.GameState() # Step 3: play game # Step 3.1: obtain init state action0 = np.array([1,0,0]) # [1,0,0]do nothing,[0,1,0]left,[0,0,1]right observation0, reward0, terminal = plane.frame_step(action0) observation0 = cv2.cvtColor(cv2.resize(observation0, (80, 80)), cv2.COLOR_BGR2GRAY) ret, observation0 = cv2.threshold(observation0,1,255,cv2.THRESH_BINARY) brain.setInitState(observation0) # Step 3.2: run the game while 1!= 0: action = brain.getAction() nextObservation,reward,terminal = plane.frame_step(action) nextObservation = preprocess(nextObservation) brain.setPerception(nextObservation,action,reward,terminal) 3.3 游戏类GameState和framestep 通过pygame实现游戏界面的搭建，分别建立子弹类、玩家类、敌机类和游戏类，结构代码所示。 class Bullet(pygame.sprite.Sprite): def __init__(self, bullet_img, init_pos): def move(self): # 我方飞机类 class Player(pygame.sprite.Sprite): def __init__(self, plane_img, player_rect, init_pos): def shoot(self, bullet_img): def moveLeft(self): def moveRight(self): # 敌方飞机类 class Enemy(pygame.sprite.Sprite): def __init__(self, enemy_img, enemy_down_imgs, init_pos): def move(self): class GameState: def __init__(self): def frame_step(self, input_actions): if input_actions[0] == 1 or input_actions[1]== 1 or input_actions[2]== 1: # 检查输入正常 if input_actions[0] == 0 and input_actions[1] == 1 and input_actions[2] == 0: self.player.moveLeft() elif input_actions[0] == 0 and input_actions[1] == 0 and input_actions[2] == 1: self.player.moveRight() else: pass else: raise ValueError(&#39;Multiple input actions!&#39;) image_data = pygame.surfarray.array3d(pygame.display.get_surface()) pygame.display.update() clock = pygame.time.Clock() clock.tick(30) return image_data, reward, terminal 其中GameState中的framestep()函数，是整个DQN运行一次使环境发生变化的基础函数，该函数运行一次，会根据inputaction进行动作实施，接着会在该时段对界面上的元素进行移动，并判断是否撞击。最后通过get_surface获取界面图像，最后返回环境的image_data，reward和游戏是否停止的terminal。本文游戏效果图为： 为提高模型收敛速度，在实际运行时将背景图片去掉。 3.4 DQN模型类 该部分为DQN模型的核心，主要有根据参数建立CNN网络的createQNetwork()，进行模型训练的trainQNetwork()，进行动作选择的getAction()。 class BrainDQN: def __init__(self,actions): def createQNetwork(self): return stateInput,QValue,W_conv1,b_conv1,W_conv2,b_conv2,W_conv3,b_conv3,W_fc1,b_fc1,W_fc2,b_fc2 def copyTargetQNetwork(self): self.session.run(self.copyTargetQNetworkOperation) def createTrainingMethod(self): def trainQNetwork(self): def getAction(self): return action def setInitState(self,observation): self.currentState = np.stack((observation, observation, observation, observation), axis = 2) def weight_variable(self,shape): return tf.Variable(initial) def bias_variable(self,shape): return tf.Variable(initial) def conv2d(self,x, W, stride): return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = &quot;SAME&quot;) def max_pool_2x2(self,x): return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], paddin 3.5图像处理 图像预处理调用cv2库函数，对图像进行大小和灰度处理。 def preprocess(observation): observation = cv2.cvtColor(cv2.resize(observation, (80, 80)), cv2.COLOR_BGR2GRAY)#灰度转化 ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY) return np.reshape(observation,(80,80,1)) /4/环境搭建 系统：Ubuntu16.04、win10 Python3.5 pygame 1.9.4 TensorFlow1.11（GPU版） OpenCV-Python 关注公众号“1024程序开发者社区”回复“AI飞机”，获取代码，包含训练500000次的结果。本程序对硬件要求不高，显存2GB以上就可运行。 1024程序开发者社区的交流群已经建立，许多小伙伴已经加入其中，感谢大家的支持。大家可以在群里就技术问题进行交流，还没有加入的小伙伴可以扫描下方“社区物业”二维码，让管理员帮忙拉进群，期待大家的加入。","@type":"BlogPosting","url":"/2019/02/18/cbad7e9bbde38b54330aa5cde720b609.html","headline":"如何训练AI玩飞机大战游戏（创号版）","dateModified":"2019-02-18T00:00:00+08:00","datePublished":"2019-02-18T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/02/18/cbad7e9bbde38b54330aa5cde720b609.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>如何训练AI玩飞机大战游戏（创号版）</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p>虽然没有谷歌强大的集群和DeepMind变态的算法的团队，但基于深度强化学习（Deep Q Network DQN ）的自制小游戏AI效果同样很赞。先上效果图：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218191809577.gif" alt="在这里插入图片描述"><br> 下面分四个部分，具体给大家介绍。<br> <strong>/1/背景介绍</strong><br> 2013年DeepMind团队发表论文“Playing Atari with Deep Reinforcement Learning”，用Q-Network模型成功让AI玩起了Atari系列游戏。并于2015年在《Nature》上发表了一篇升级版，“Human-level control through deep reinforcement learning”，自此，在这类游戏领域，人已经无法超过机器了。AI玩游戏的姿势是这样的：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218191904577.gif" alt="在这里插入图片描述"><br> 后来的故事大家都很熟悉了，AlphaGo击败世界冠军，星际争霸2职业选手也被打败，连大家接触较多的王者荣耀也不能幸免。<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218192011895.gif" alt="在这里插入图片描述"><br> <strong>/2/深度强化学习模型</strong><br> 看完了轻松的部分，下面简单介绍一下模型。DQN是DRL的一种算法，它将卷积神经网络（CNN）和Q-Learning结合起来。<br> Q-learning是强化学习的一种，原理图如下：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218192407613.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2lkX2lvdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 也就是Agent在观察得到当前的状态state和回报reward的基础上，选取输出一个动作action，进而影响环境，使环境状态和回报都产生变化。通过不断循环让Agent学习如何在环境中获得更高的回报。<br> 卷积神经网络CNN是图像处理领域非常经典的神经网络模型，在本模型中，输入是原始图像数据，输出为每个动作action对应的评估值。<br> 因此DQN总体结构是这样的：<img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218192319822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2lkX2lvdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 图比较简单，但原理很清晰，是将Agent中的模型用CNN来代替，环境的State为游戏界面截图，输出为AI的动作，在飞机大战中就是飞机向左、向右还是不动。回报reward具体为，在一次循环中没有被击中为0.1，被击中为-1，击中敌机为1。图中回放记忆单元、当前网络和目标网络都是为了将CNN这种需要大量样本的监督学习融合在强化学习模型中的手段。篇幅限制这里只是概述性的介绍，后期会专门讲。</p> 
  <p><strong>/3/模型实现</strong><br> 3.1程序的总体结构<br> 程序主函数在PlaneDQN.py中，与DQN模型相关的函数在BrainDQN_Nature.py中，游戏模型在game文件夹中，训练过程保存的训练值在saved_networks文件夹中。<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218192733709.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2lkX2lvdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 3.2主函数搭建<br> 大家注意看while循环里的结构，其实非常明确：</p> 
  <ul> 
   <li>getaction()为在当前的Q值下选取动作</li> 
   <li>framestep()为运行环境，并输出观测值</li> 
   <li>process()为对图像数据进行处理的函数</li> 
   <li>setPerception()根据图像和回报，对网络进行训练</li> 
  </ul> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">playPlane</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token comment"># Step 1: init BrainDQN</span>
	actions <span class="token operator">=</span> <span class="token number">3</span>
	brain <span class="token operator">=</span> BrainDQN<span class="token punctuation">(</span>actions<span class="token punctuation">)</span>
	<span class="token comment"># Step 2: init Plane Game</span>
	plane <span class="token operator">=</span> game<span class="token punctuation">.</span>GameState<span class="token punctuation">(</span><span class="token punctuation">)</span>
	<span class="token comment"># Step 3: play game</span>
	<span class="token comment"># Step 3.1: obtain init state</span>
	action0 <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># [1,0,0]do nothing,[0,1,0]left,[0,0,1]right</span>
	observation0<span class="token punctuation">,</span> reward0<span class="token punctuation">,</span> terminal <span class="token operator">=</span> plane<span class="token punctuation">.</span>frame_step<span class="token punctuation">(</span>action0<span class="token punctuation">)</span>

	observation0 <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>resize<span class="token punctuation">(</span>observation0<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2GRAY<span class="token punctuation">)</span>
	ret<span class="token punctuation">,</span> observation0 <span class="token operator">=</span> cv2<span class="token punctuation">.</span>threshold<span class="token punctuation">(</span>observation0<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">255</span><span class="token punctuation">,</span>cv2<span class="token punctuation">.</span>THRESH_BINARY<span class="token punctuation">)</span>
	brain<span class="token punctuation">.</span>setInitState<span class="token punctuation">(</span>observation0<span class="token punctuation">)</span>

	<span class="token comment"># Step 3.2: run the game</span>
	<span class="token keyword">while</span> <span class="token number">1</span><span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
		action <span class="token operator">=</span> brain<span class="token punctuation">.</span>getAction<span class="token punctuation">(</span><span class="token punctuation">)</span>
		nextObservation<span class="token punctuation">,</span>reward<span class="token punctuation">,</span>terminal <span class="token operator">=</span> plane<span class="token punctuation">.</span>frame_step<span class="token punctuation">(</span>action<span class="token punctuation">)</span>
		nextObservation <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>nextObservation<span class="token punctuation">)</span>
		brain<span class="token punctuation">.</span>setPerception<span class="token punctuation">(</span>nextObservation<span class="token punctuation">,</span>action<span class="token punctuation">,</span>reward<span class="token punctuation">,</span>terminal<span class="token punctuation">)</span>
</code></pre> 
  <p>3.3 游戏类GameState和framestep<br> 通过pygame实现游戏界面的搭建，分别建立子弹类、玩家类、敌机类和游戏类，结构代码所示。</p> 
  <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Bullet</span><span class="token punctuation">(</span>pygame<span class="token punctuation">.</span>sprite<span class="token punctuation">.</span>Sprite<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> bullet_img<span class="token punctuation">,</span> init_pos<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">move</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token comment"># 我方飞机类</span>
<span class="token keyword">class</span> <span class="token class-name">Player</span><span class="token punctuation">(</span>pygame<span class="token punctuation">.</span>sprite<span class="token punctuation">.</span>Sprite<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> plane_img<span class="token punctuation">,</span> player_rect<span class="token punctuation">,</span> init_pos<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">shoot</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> bullet_img<span class="token punctuation">)</span><span class="token punctuation">:</span>           <span class="token keyword">def</span> <span class="token function">moveLeft</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        
    <span class="token keyword">def</span> <span class="token function">moveRight</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
<span class="token comment"># 敌方飞机类</span>
<span class="token keyword">class</span> <span class="token class-name">Enemy</span><span class="token punctuation">(</span>pygame<span class="token punctuation">.</span>sprite<span class="token punctuation">.</span>Sprite<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enemy_img<span class="token punctuation">,</span> enemy_down_imgs<span class="token punctuation">,</span> init_pos<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">move</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>

<span class="token keyword">class</span> <span class="token class-name">GameState</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>       
    <span class="token keyword">def</span> <span class="token function">frame_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_actions<span class="token punctuation">)</span><span class="token punctuation">:</span>       
        <span class="token keyword">if</span> input_actions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span> <span class="token operator">or</span> input_actions<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">==</span> <span class="token number">1</span> <span class="token operator">or</span> input_actions<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>  <span class="token comment"># 检查输入正常</span>
            <span class="token keyword">if</span> input_actions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">and</span> input_actions<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span> <span class="token operator">and</span> input_actions<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>player<span class="token punctuation">.</span>moveLeft<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> input_actions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">and</span> input_actions<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token operator">and</span> input_actions<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>player<span class="token punctuation">.</span>moveRight<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token keyword">pass</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">'Multiple input actions!'</span><span class="token punctuation">)</span>
        image_data <span class="token operator">=</span> pygame<span class="token punctuation">.</span>surfarray<span class="token punctuation">.</span>array3d<span class="token punctuation">(</span>pygame<span class="token punctuation">.</span>display<span class="token punctuation">.</span>get_surface<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        pygame<span class="token punctuation">.</span>display<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">)</span>
        clock <span class="token operator">=</span> pygame<span class="token punctuation">.</span>time<span class="token punctuation">.</span>Clock<span class="token punctuation">(</span><span class="token punctuation">)</span>
        clock<span class="token punctuation">.</span>tick<span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> image_data<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> terminal
</code></pre> 
  <p>其中GameState中的framestep()函数，是整个DQN运行一次使环境发生变化的基础函数，该函数运行一次，会根据inputaction进行动作实施，接着会在该时段对界面上的元素进行移动，并判断是否撞击。最后通过get_surface获取界面图像，最后返回环境的image_data，reward和游戏是否停止的terminal。本文游戏效果图为：<img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218192456578.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2lkX2lvdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 为提高模型收敛速度，在实际运行时将背景图片去掉。<br> 3.4 DQN模型类<br> 该部分为DQN模型的核心，主要有根据参数建立CNN网络的createQNetwork()，进行模型训练的trainQNetwork()，进行动作选择的getAction()。</p> 
  <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BrainDQN</span><span class="token punctuation">:</span>
   <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>actions<span class="token punctuation">)</span><span class="token punctuation">:</span>   
   <span class="token keyword">def</span> <span class="token function">createQNetwork</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">return</span> stateInput<span class="token punctuation">,</span>QValue<span class="token punctuation">,</span>W_conv1<span class="token punctuation">,</span>b_conv1<span class="token punctuation">,</span>W_conv2<span class="token punctuation">,</span>b_conv2<span class="token punctuation">,</span>W_conv3<span class="token punctuation">,</span>b_conv3<span class="token punctuation">,</span>W_fc1<span class="token punctuation">,</span>b_fc1<span class="token punctuation">,</span>W_fc2<span class="token punctuation">,</span>b_fc2
   <span class="token keyword">def</span> <span class="token function">copyTargetQNetwork</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
      self<span class="token punctuation">.</span>session<span class="token punctuation">.</span>run<span class="token punctuation">(</span>self<span class="token punctuation">.</span>copyTargetQNetworkOperation<span class="token punctuation">)</span>
   <span class="token keyword">def</span> <span class="token function">createTrainingMethod</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
   <span class="token keyword">def</span> <span class="token function">trainQNetwork</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>   
   <span class="token keyword">def</span> <span class="token function">getAction</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">return</span> action
   <span class="token keyword">def</span> <span class="token function">setInitState</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>observation<span class="token punctuation">)</span><span class="token punctuation">:</span>
      self<span class="token punctuation">.</span>currentState <span class="token operator">=</span> np<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>observation<span class="token punctuation">,</span> observation<span class="token punctuation">,</span> observation<span class="token punctuation">,</span> observation<span class="token punctuation">)</span><span class="token punctuation">,</span> axis <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span>
   <span class="token keyword">def</span> <span class="token function">weight_variable</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">return</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>initial<span class="token punctuation">)</span>
   <span class="token keyword">def</span> <span class="token function">bias_variable</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>shape<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">return</span> tf<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>initial<span class="token punctuation">)</span>
   <span class="token keyword">def</span> <span class="token function">conv2d</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">,</span> W<span class="token punctuation">,</span> stride<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> W<span class="token punctuation">,</span> strides <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> padding <span class="token operator">=</span> <span class="token string">"SAME"</span><span class="token punctuation">)</span>
   <span class="token keyword">def</span> <span class="token function">max_pool_2x2</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>max_pool<span class="token punctuation">(</span>x<span class="token punctuation">,</span> ksize <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> strides <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> paddin
</code></pre> 
  <p>3.5图像处理<br> 图像预处理调用cv2库函数，对图像进行大小和灰度处理。</p> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">preprocess</span><span class="token punctuation">(</span>observation<span class="token punctuation">)</span><span class="token punctuation">:</span>
   observation <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>cv2<span class="token punctuation">.</span>resize<span class="token punctuation">(</span>observation<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2GRAY<span class="token punctuation">)</span><span class="token comment">#灰度转化</span>
   ret<span class="token punctuation">,</span> observation <span class="token operator">=</span> cv2<span class="token punctuation">.</span>threshold<span class="token punctuation">(</span>observation<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">255</span><span class="token punctuation">,</span>cv2<span class="token punctuation">.</span>THRESH_BINARY<span class="token punctuation">)</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>observation<span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token number">80</span><span class="token punctuation">,</span><span class="token number">80</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <p><strong>/4/环境搭建</strong><br> 系统：Ubuntu16.04、win10</p> 
  <ul> 
   <li>Python3.5</li> 
   <li>pygame 1.9.4</li> 
   <li>TensorFlow1.11（GPU版）</li> 
   <li>OpenCV-Python</li> 
  </ul> 
  <p>关注公众号“1024程序开发者社区”回复“AI飞机”，获取代码，包含训练500000次的结果。本程序对硬件要求不高，显存2GB以上就可运行。<img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218193815902.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2lkX2lvdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 1024程序开发者社区的交流群已经建立，许多小伙伴已经加入其中，感谢大家的支持。大家可以在群里就技术问题进行交流，还没有加入的小伙伴可以扫描下方“社区物业”二维码，让管理员帮忙拉进群，期待大家的加入。<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218193836166.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2lkX2lvdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
