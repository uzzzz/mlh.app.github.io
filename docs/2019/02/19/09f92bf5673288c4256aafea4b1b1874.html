<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>ML-逻辑回归-公式推导- 多种实现 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="ML-逻辑回归-公式推导- 多种实现" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="如果你看完这个还不懂逻辑回归原理的话，请直接留言我，一定一对一免费教。-------良心码农 目录 1.简介 2.数学背景 3.推导 4.联合概率 5.求参、极大似然 &nbsp;6.参数求解： 7.牛顿法、拟牛顿法、梯度下降法等求参： 牛顿法： 梯度下降法 8.完整流程 9.正则化 10.多元逻辑回归 11.scikit-learn中逻辑回归 1.简介 逻辑回归并不是回归，是分类算法。通过函数映射，通常映射后的值＞0.5称为正例，反之反例，这样的学习称为二分类。 2.数学背景 定义映射函数： 求导性质： &nbsp; 3.推导 令z表示线性关系： 其中x是给定的数据，b是偏置，是需要学习到的参数，通过这样映射得到y∈[0,1]，记y为正例概率，1-y则是反例概率. 则： 记为概率形式,p1,p0有： 对正反比取对数发现： 结果就是z，也就是说z越大，正概率比反概率的比越大，越可能是正。这样的一个模型具有分类表示能力。&nbsp; 4.联合概率 记 联合0、1概率可写在一起，为： 取对数时： 所以对数联合概率记为： &nbsp; 5.求参、极大似然 极大似然估计w,b的值，记m个样本的联合模型： 极大化 等价于极小化： &nbsp; 最终，转为对m个数据集D求极小化： &nbsp; &nbsp;6.参数求解： 求得参数后，就可以利用： 进行预测了，当y&gt;0.5的时候意味着正的可能性比反的可能性大，既被预测为正例。&nbsp; 7.牛顿法、拟牛顿法、梯度下降法等求参： 1）牛顿法：https://blog.csdn.net/jiang425776024/article/details/87601854 2）拟牛顿法：https://blog.csdn.net/jiang425776024/article/details/87602847 3）梯度下降：https://blog.csdn.net/jiang425776024/article/details/87601506 牛顿法： 方便起见，在这里需要求的是参数w,b的整体，根据 5 中的损失函数： 一阶导数为： 根据 2 中 因为 所以，二阶导数为： &nbsp;因此，牛顿法参数的迭代形式： 当然，牛顿法也可以加上一维线性搜索（https://blog.csdn.net/jiang425776024/article/details/87600422），即，如下梯度法那样，在上面的导数式子上，加上学习率a。 梯度下降法 只需要一阶导数，a为步长，一般∈[0,1]，代入上面的 1 阶导数即可： &nbsp; 8.完整流程 输入： &nbsp; 过程：随机初始化参数Beita &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;while &nbsp;总体变化量&lt;某个阀值：对进行7中那样的迭代更新 9.正则化 和其它算法一样，可以对参数进行正则化，L1\L2等，通常都是在损失函数后面加上形如L2正则化：，其中a为正则化强度，theta为模型参数。这时参数更新需要加入对正则化这部分的求导。 10.多元逻辑回归 二分类中，形如： 多分类中假设有K&gt;2个类别，则有K-1个方程： 、,..., 则K元逻辑回归的概率分布如下： ，k = 1,2,...K-1 剩下的多元逻辑回归的损失函数推导以及优化方法和二元逻辑回归类似。 还有一种简单粗暴的办法就是，构造多个二分类，比如有A，B,C,D&nbsp;四个类，那么按照上面的，可以构造4个二分类：正A与反(B，C,D);正B与反(A,C,D)，.....。这样，就可以按照二分类的情况进行多分类了，只是判断的时候需要进行多个if判断。 11.scikit-learn中逻辑回归 主要是3个：LogisticRegression， LogisticRegressionCV 和logistic_regression_path。 其中LogisticRegression和 LogisticRegressionCV的主要区别是 LogisticRegressionCV使用了交叉验证来选择（9中正则化介绍的a）正则化系数C，而LogisticRegression需要自己每次指定一个正则化系数。 logistic_regression_path主要用在模型选择，不能直接来做预测，只能选择合适逻辑回归的系数和正则化系数。 使用： 这里不会详细介绍api的参数，点击可查看LogisticRegression， LogisticRegressionCV的参数说明， 或者：https://www.cnblogs.com/pinard/p/6035872.html。 import numpy as np from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.linear_model import LogisticRegressionCV &#39;&#39;&#39; https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV &#39;&#39;&#39; X, y = load_iris(return_X_y=True) #solver=&#39;lbfgs&#39;使用拟牛顿法迭代寻参 #random_state=0随机种子，这个很必要，数值随便 clf = LogisticRegression(random_state=0, C=2, solver=&#39;lbfgs&#39;, multi_class=&#39;multinomial&#39;).fit(X, y)#指定了正则化强度为2 clfcv = LogisticRegressionCV(random_state=0, solver=&#39;lbfgs&#39;, multi_class=&#39;multinomial&#39;).fit(X, y)#cv类自动寻找正则化强度 pd = clf.predict(X[:2, :]) pdcv = clf.predict(X[:2, :]) print(X[:2, :], &#39;预测类型：&#39;, pd) print(X[:2, :], &#39;预测类型：&#39;, pdcv) pbd = clf.predict_proba(X[:2, :]) pbdcv = clf.predict_proba(X[:2, :]) print(X[:2, :], &#39;预测类型的概率：&#39;, pbd) print(X[:2, :], &#39;预测类型的概率：&#39;, pbdcv) print(&#39;预测分数：&#39;, clf.score(X, y)) print(&#39;预测分数：&#39;, clfcv.score(X, y)) &#39;&#39;&#39; [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型： [0 0] [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型： [0 0] [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型的概率： [[9.89252266e-01 1.07477333e-02 2.22652613e-10] [9.82191282e-01 1.78087169e-02 6.46675701e-10]] [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型的概率： [[9.89252266e-01 1.07477333e-02 2.22652613e-10] [9.82191282e-01 1.78087169e-02 6.46675701e-10]] 预测分数： 0.9866666666666667 预测分数： 0.98 Process finished with exit code 0 &#39;&#39;&#39; &nbsp;" />
<meta property="og:description" content="如果你看完这个还不懂逻辑回归原理的话，请直接留言我，一定一对一免费教。-------良心码农 目录 1.简介 2.数学背景 3.推导 4.联合概率 5.求参、极大似然 &nbsp;6.参数求解： 7.牛顿法、拟牛顿法、梯度下降法等求参： 牛顿法： 梯度下降法 8.完整流程 9.正则化 10.多元逻辑回归 11.scikit-learn中逻辑回归 1.简介 逻辑回归并不是回归，是分类算法。通过函数映射，通常映射后的值＞0.5称为正例，反之反例，这样的学习称为二分类。 2.数学背景 定义映射函数： 求导性质： &nbsp; 3.推导 令z表示线性关系： 其中x是给定的数据，b是偏置，是需要学习到的参数，通过这样映射得到y∈[0,1]，记y为正例概率，1-y则是反例概率. 则： 记为概率形式,p1,p0有： 对正反比取对数发现： 结果就是z，也就是说z越大，正概率比反概率的比越大，越可能是正。这样的一个模型具有分类表示能力。&nbsp; 4.联合概率 记 联合0、1概率可写在一起，为： 取对数时： 所以对数联合概率记为： &nbsp; 5.求参、极大似然 极大似然估计w,b的值，记m个样本的联合模型： 极大化 等价于极小化： &nbsp; 最终，转为对m个数据集D求极小化： &nbsp; &nbsp;6.参数求解： 求得参数后，就可以利用： 进行预测了，当y&gt;0.5的时候意味着正的可能性比反的可能性大，既被预测为正例。&nbsp; 7.牛顿法、拟牛顿法、梯度下降法等求参： 1）牛顿法：https://blog.csdn.net/jiang425776024/article/details/87601854 2）拟牛顿法：https://blog.csdn.net/jiang425776024/article/details/87602847 3）梯度下降：https://blog.csdn.net/jiang425776024/article/details/87601506 牛顿法： 方便起见，在这里需要求的是参数w,b的整体，根据 5 中的损失函数： 一阶导数为： 根据 2 中 因为 所以，二阶导数为： &nbsp;因此，牛顿法参数的迭代形式： 当然，牛顿法也可以加上一维线性搜索（https://blog.csdn.net/jiang425776024/article/details/87600422），即，如下梯度法那样，在上面的导数式子上，加上学习率a。 梯度下降法 只需要一阶导数，a为步长，一般∈[0,1]，代入上面的 1 阶导数即可： &nbsp; 8.完整流程 输入： &nbsp; 过程：随机初始化参数Beita &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;while &nbsp;总体变化量&lt;某个阀值：对进行7中那样的迭代更新 9.正则化 和其它算法一样，可以对参数进行正则化，L1\L2等，通常都是在损失函数后面加上形如L2正则化：，其中a为正则化强度，theta为模型参数。这时参数更新需要加入对正则化这部分的求导。 10.多元逻辑回归 二分类中，形如： 多分类中假设有K&gt;2个类别，则有K-1个方程： 、,..., 则K元逻辑回归的概率分布如下： ，k = 1,2,...K-1 剩下的多元逻辑回归的损失函数推导以及优化方法和二元逻辑回归类似。 还有一种简单粗暴的办法就是，构造多个二分类，比如有A，B,C,D&nbsp;四个类，那么按照上面的，可以构造4个二分类：正A与反(B，C,D);正B与反(A,C,D)，.....。这样，就可以按照二分类的情况进行多分类了，只是判断的时候需要进行多个if判断。 11.scikit-learn中逻辑回归 主要是3个：LogisticRegression， LogisticRegressionCV 和logistic_regression_path。 其中LogisticRegression和 LogisticRegressionCV的主要区别是 LogisticRegressionCV使用了交叉验证来选择（9中正则化介绍的a）正则化系数C，而LogisticRegression需要自己每次指定一个正则化系数。 logistic_regression_path主要用在模型选择，不能直接来做预测，只能选择合适逻辑回归的系数和正则化系数。 使用： 这里不会详细介绍api的参数，点击可查看LogisticRegression， LogisticRegressionCV的参数说明， 或者：https://www.cnblogs.com/pinard/p/6035872.html。 import numpy as np from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.linear_model import LogisticRegressionCV &#39;&#39;&#39; https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV &#39;&#39;&#39; X, y = load_iris(return_X_y=True) #solver=&#39;lbfgs&#39;使用拟牛顿法迭代寻参 #random_state=0随机种子，这个很必要，数值随便 clf = LogisticRegression(random_state=0, C=2, solver=&#39;lbfgs&#39;, multi_class=&#39;multinomial&#39;).fit(X, y)#指定了正则化强度为2 clfcv = LogisticRegressionCV(random_state=0, solver=&#39;lbfgs&#39;, multi_class=&#39;multinomial&#39;).fit(X, y)#cv类自动寻找正则化强度 pd = clf.predict(X[:2, :]) pdcv = clf.predict(X[:2, :]) print(X[:2, :], &#39;预测类型：&#39;, pd) print(X[:2, :], &#39;预测类型：&#39;, pdcv) pbd = clf.predict_proba(X[:2, :]) pbdcv = clf.predict_proba(X[:2, :]) print(X[:2, :], &#39;预测类型的概率：&#39;, pbd) print(X[:2, :], &#39;预测类型的概率：&#39;, pbdcv) print(&#39;预测分数：&#39;, clf.score(X, y)) print(&#39;预测分数：&#39;, clfcv.score(X, y)) &#39;&#39;&#39; [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型： [0 0] [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型： [0 0] [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型的概率： [[9.89252266e-01 1.07477333e-02 2.22652613e-10] [9.82191282e-01 1.78087169e-02 6.46675701e-10]] [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型的概率： [[9.89252266e-01 1.07477333e-02 2.22652613e-10] [9.82191282e-01 1.78087169e-02 6.46675701e-10]] 预测分数： 0.9866666666666667 预测分数： 0.98 Process finished with exit code 0 &#39;&#39;&#39; &nbsp;" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-19T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"如果你看完这个还不懂逻辑回归原理的话，请直接留言我，一定一对一免费教。-------良心码农 目录 1.简介 2.数学背景 3.推导 4.联合概率 5.求参、极大似然 &nbsp;6.参数求解： 7.牛顿法、拟牛顿法、梯度下降法等求参： 牛顿法： 梯度下降法 8.完整流程 9.正则化 10.多元逻辑回归 11.scikit-learn中逻辑回归 1.简介 逻辑回归并不是回归，是分类算法。通过函数映射，通常映射后的值＞0.5称为正例，反之反例，这样的学习称为二分类。 2.数学背景 定义映射函数： 求导性质： &nbsp; 3.推导 令z表示线性关系： 其中x是给定的数据，b是偏置，是需要学习到的参数，通过这样映射得到y∈[0,1]，记y为正例概率，1-y则是反例概率. 则： 记为概率形式,p1,p0有： 对正反比取对数发现： 结果就是z，也就是说z越大，正概率比反概率的比越大，越可能是正。这样的一个模型具有分类表示能力。&nbsp; 4.联合概率 记 联合0、1概率可写在一起，为： 取对数时： 所以对数联合概率记为： &nbsp; 5.求参、极大似然 极大似然估计w,b的值，记m个样本的联合模型： 极大化 等价于极小化： &nbsp; 最终，转为对m个数据集D求极小化： &nbsp; &nbsp;6.参数求解： 求得参数后，就可以利用： 进行预测了，当y&gt;0.5的时候意味着正的可能性比反的可能性大，既被预测为正例。&nbsp; 7.牛顿法、拟牛顿法、梯度下降法等求参： 1）牛顿法：https://blog.csdn.net/jiang425776024/article/details/87601854 2）拟牛顿法：https://blog.csdn.net/jiang425776024/article/details/87602847 3）梯度下降：https://blog.csdn.net/jiang425776024/article/details/87601506 牛顿法： 方便起见，在这里需要求的是参数w,b的整体，根据 5 中的损失函数： 一阶导数为： 根据 2 中 因为 所以，二阶导数为： &nbsp;因此，牛顿法参数的迭代形式： 当然，牛顿法也可以加上一维线性搜索（https://blog.csdn.net/jiang425776024/article/details/87600422），即，如下梯度法那样，在上面的导数式子上，加上学习率a。 梯度下降法 只需要一阶导数，a为步长，一般∈[0,1]，代入上面的 1 阶导数即可： &nbsp; 8.完整流程 输入： &nbsp; 过程：随机初始化参数Beita &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;while &nbsp;总体变化量&lt;某个阀值：对进行7中那样的迭代更新 9.正则化 和其它算法一样，可以对参数进行正则化，L1\\L2等，通常都是在损失函数后面加上形如L2正则化：，其中a为正则化强度，theta为模型参数。这时参数更新需要加入对正则化这部分的求导。 10.多元逻辑回归 二分类中，形如： 多分类中假设有K&gt;2个类别，则有K-1个方程： 、,..., 则K元逻辑回归的概率分布如下： ，k = 1,2,...K-1 剩下的多元逻辑回归的损失函数推导以及优化方法和二元逻辑回归类似。 还有一种简单粗暴的办法就是，构造多个二分类，比如有A，B,C,D&nbsp;四个类，那么按照上面的，可以构造4个二分类：正A与反(B，C,D);正B与反(A,C,D)，.....。这样，就可以按照二分类的情况进行多分类了，只是判断的时候需要进行多个if判断。 11.scikit-learn中逻辑回归 主要是3个：LogisticRegression， LogisticRegressionCV 和logistic_regression_path。 其中LogisticRegression和 LogisticRegressionCV的主要区别是 LogisticRegressionCV使用了交叉验证来选择（9中正则化介绍的a）正则化系数C，而LogisticRegression需要自己每次指定一个正则化系数。 logistic_regression_path主要用在模型选择，不能直接来做预测，只能选择合适逻辑回归的系数和正则化系数。 使用： 这里不会详细介绍api的参数，点击可查看LogisticRegression， LogisticRegressionCV的参数说明， 或者：https://www.cnblogs.com/pinard/p/6035872.html。 import numpy as np from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression from sklearn.linear_model import LogisticRegressionCV &#39;&#39;&#39; https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV &#39;&#39;&#39; X, y = load_iris(return_X_y=True) #solver=&#39;lbfgs&#39;使用拟牛顿法迭代寻参 #random_state=0随机种子，这个很必要，数值随便 clf = LogisticRegression(random_state=0, C=2, solver=&#39;lbfgs&#39;, multi_class=&#39;multinomial&#39;).fit(X, y)#指定了正则化强度为2 clfcv = LogisticRegressionCV(random_state=0, solver=&#39;lbfgs&#39;, multi_class=&#39;multinomial&#39;).fit(X, y)#cv类自动寻找正则化强度 pd = clf.predict(X[:2, :]) pdcv = clf.predict(X[:2, :]) print(X[:2, :], &#39;预测类型：&#39;, pd) print(X[:2, :], &#39;预测类型：&#39;, pdcv) pbd = clf.predict_proba(X[:2, :]) pbdcv = clf.predict_proba(X[:2, :]) print(X[:2, :], &#39;预测类型的概率：&#39;, pbd) print(X[:2, :], &#39;预测类型的概率：&#39;, pbdcv) print(&#39;预测分数：&#39;, clf.score(X, y)) print(&#39;预测分数：&#39;, clfcv.score(X, y)) &#39;&#39;&#39; [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型： [0 0] [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型： [0 0] [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型的概率： [[9.89252266e-01 1.07477333e-02 2.22652613e-10] [9.82191282e-01 1.78087169e-02 6.46675701e-10]] [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2]] 预测类型的概率： [[9.89252266e-01 1.07477333e-02 2.22652613e-10] [9.82191282e-01 1.78087169e-02 6.46675701e-10]] 预测分数： 0.9866666666666667 预测分数： 0.98 Process finished with exit code 0 &#39;&#39;&#39; &nbsp;","@type":"BlogPosting","url":"/2019/02/19/09f92bf5673288c4256aafea4b1b1874.html","headline":"ML-逻辑回归-公式推导- 多种实现","dateModified":"2019-02-19T00:00:00+08:00","datePublished":"2019-02-19T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/02/19/09f92bf5673288c4256aafea4b1b1874.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>ML-逻辑回归-公式推导- 多种实现</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>如果你看完这个还不懂逻辑回归原理的话，请直接留言我，一定一对一免费教。-------良心码农</p> 
  <p id="main-toc"><strong>目录</strong></p> 
  <p id="1.%E7%AE%80%E4%BB%8B-toc" style="margin-left:0px;"><a href="#1.%E7%AE%80%E4%BB%8B" rel="nofollow">1.简介</a></p> 
  <p id="2.%E6%95%B0%E5%AD%A6%E8%83%8C%E6%99%AF-toc" style="margin-left:0px;"><a href="#2.%E6%95%B0%E5%AD%A6%E8%83%8C%E6%99%AF" rel="nofollow">2.数学背景</a></p> 
  <p id="3.%E6%8E%A8%E5%AF%BC-toc" style="margin-left:0px;"><a href="#3.%E6%8E%A8%E5%AF%BC" rel="nofollow">3.推导</a></p> 
  <p id="4.%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87-toc" style="margin-left:0px;"><a href="#4.%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87" rel="nofollow">4.联合概率</a></p> 
  <p id="5.%E6%B1%82%E5%8F%82%E3%80%81%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6-toc" style="margin-left:0px;"><a href="#5.%E6%B1%82%E5%8F%82%E3%80%81%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6" rel="nofollow">5.求参、极大似然</a></p> 
  <p id="%C2%A06.%E5%8F%82%E6%95%B0%E6%B1%82%E8%A7%A3%EF%BC%9A-toc" style="margin-left:0px;"><a href="#%C2%A06.%E5%8F%82%E6%95%B0%E6%B1%82%E8%A7%A3%EF%BC%9A" rel="nofollow">&nbsp;6.参数求解：</a></p> 
  <p id="7.%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%AD%89%E6%B1%82%E5%8F%82%EF%BC%9A-toc" style="margin-left:0px;"><a href="#7.%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%AD%89%E6%B1%82%E5%8F%82%EF%BC%9A" rel="nofollow">7.牛顿法、拟牛顿法、梯度下降法等求参：</a></p> 
  <p id="%E7%89%9B%E9%A1%BF%E6%B3%95%EF%BC%9A-toc" style="margin-left:40px;"><a href="#%E7%89%9B%E9%A1%BF%E6%B3%95%EF%BC%9A" rel="nofollow">牛顿法：</a></p> 
  <p id="%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-toc" style="margin-left:40px;"><a href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95" rel="nofollow">梯度下降法</a></p> 
  <p id="8.%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B-toc" style="margin-left:0px;"><a href="#8.%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B" rel="nofollow">8.完整流程</a></p> 
  <p id="9.%E6%AD%A3%E5%88%99%E5%8C%96-toc" style="margin-left:0px;"><a href="#9.%E6%AD%A3%E5%88%99%E5%8C%96" rel="nofollow">9.正则化</a></p> 
  <p id="10.%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-toc" style="margin-left:0px;"><a href="#10.%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92" rel="nofollow">10.多元逻辑回归</a></p> 
  <p id="11.scikit-learn%E4%B8%AD%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-toc" style="margin-left:0px;"><a href="#11.scikit-learn%E4%B8%AD%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92" rel="nofollow">11.scikit-learn中逻辑回归</a></p> 
  <hr id="hr-toc">
  <h1 id="1.%E7%AE%80%E4%BB%8B">1.简介</h1> 
  <p style="text-indent:50px;">逻辑回归并不是回归，是分类算法。通过函数映射，通常映射后的值＞0.5称为正例，反之反例，这样的学习称为二分类。</p> 
  <hr>
  <h1 id="2.%E6%95%B0%E5%AD%A6%E8%83%8C%E6%99%AF">2.数学背景</h1> 
  <p style="text-indent:50px;">定义映射函数：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="43" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194052888.png" width="160"></p> 
  <p style="text-indent:50px;">求导性质：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="140" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194201947.png" width="256"></p> 
  <hr>
  <p>&nbsp;</p> 
  <h1 id="3.%E6%8E%A8%E5%AF%BC">3.推导</h1> 
  <p style="text-indent:50px;">令z表示线性关系：<img alt="" class="has" height="20" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194248190.png" width="110"></p> 
  <p style="text-indent:50px;">其中x是给定的数据，b是偏置，<img alt="" class="has" height="24" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194304568.png" width="15">是需要学习到的参数，通过这样映射得到y∈[0,1]，记y为正例概率，1-y则是反例概率.</p> 
  <p style="text-align:center;"><img alt="" class="has" height="241" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194319827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="458"></p> 
  <p style="text-indent:50px;">则：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="64" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194400107.png" width="276"></p> 
  <p style="text-indent:50px;">记为概率形式,p1,p0有：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="77" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194453496.png" width="558"></p> 
  <p style="text-indent:50px;">对正反比取对数发现：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="60" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194517690.png" width="456"></p> 
  <p style="text-indent:50px;">结果就是z，也就是说z越大，正概率比反概率的比越大，越可能是正。<span style="color:#ff0000;">这样的一个模型具有分类表示能力</span>。&nbsp;</p> 
  <hr>
  <h1 id="4.%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87">4.联合概率</h1> 
  <p style="text-indent:50px;">记</p> 
  <p style="text-align:center;"><img alt="" class="has" height="58" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194648735.png" width="543"></p> 
  <p style="text-indent:50px;"><span style="color:#f33b45;"><strong>联合0、1概率可写在一起，为：</strong></span></p> 
  <p style="text-align:center;"><img alt="" class="has" height="62" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194742670.png" width="459"></p> 
  <p style="text-indent:50px;"><span style="color:#000000;">取对数时：</span></p> 
  <p style="text-align:center;"><img alt="" class="has" height="64" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194813549.png" width="399"></p> 
  <p style="text-indent:50px;"><span style="color:#000000;">所以</span><span style="color:#ff0000;">对数联合概率</span><span style="color:#000000;">记为：</span></p> 
  <p style="text-align:center;"><img alt="" class="has" height="42" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218194826884.png" width="443"></p> 
  <hr>
  <p>&nbsp;</p> 
  <h1 id="5.%E6%B1%82%E5%8F%82%E3%80%81%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6">5.求参、极大似然</h1> 
  <p style="text-indent:50px;">极大似然估计w,b的值，记m个样本的联合模型：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="76" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019021819490538.png" width="251"></p> 
  <p style="text-indent:50px;"><span style="color:#000000;">极大化</span></p> 
  <p style="text-align:center;"><img alt="" class="has" height="43" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195018817.png" width="455"></p> 
  <p style="text-indent:50px;"><span style="color:#000000;">等价于极小化：</span></p> 
  <p style="text-align:center;"><img alt="" class="has" height="51" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195041140.png" width="465"></p> 
  <p>&nbsp;</p> 
  <p style="text-indent:50px;"><span style="color:#ff0000;">最终，转为</span><span style="color:#ff0000;">对m个数据集D求极小化：</span></p> 
  <p style="text-align:center;"><img alt="" class="has" height="70" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195111876.png" width="322"></p> 
  <hr>
  <p>&nbsp;</p> 
  <h1 id="%C2%A06.%E5%8F%82%E6%95%B0%E6%B1%82%E8%A7%A3%EF%BC%9A">&nbsp;<strong><strong><span style="color:#ff0000;"><strong>6.</strong></span></strong><strong><span style="color:#ff0000;"><strong>参数求解：</strong></span></strong></strong></h1> 
  <p style="text-align:center;"><img alt="" class="has" height="61" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195150447.png" width="382"></p> 
  <p style="text-indent:50px;">求得参数后，就可以利用：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="60" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195215786.png" width="158"></p> 
  <p style="text-indent:50px;">进行预测了，当y&gt;0.5的时候意味着正的可能性比反的可能性大，既被预测为正例。&nbsp;</p> 
  <hr>
  <h1 id="7.%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95%E3%80%81%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%AD%89%E6%B1%82%E5%8F%82%EF%BC%9A">7.<strong><strong><strong>牛顿法、拟牛顿法、梯度下降法</strong></strong><strong><strong>等求参</strong></strong><strong><strong>：</strong></strong></strong></h1> 
  <p style="text-indent:50px;">1）牛顿法：<a href="https://blog.csdn.net/jiang425776024/article/details/87601854" rel="nofollow">https://blog.csdn.net/jiang425776024/article/details/87601854</a></p> 
  <p style="text-indent:50px;">2）拟牛顿法：<a href="https://blog.csdn.net/jiang425776024/article/details/87602847" rel="nofollow">https://blog.csdn.net/jiang425776024/article/details/87602847</a></p> 
  <p style="text-indent:50px;">3）梯度下降：<a href="https://blog.csdn.net/jiang425776024/article/details/87601506" rel="nofollow">https://blog.csdn.net/jiang425776024/article/details/87601506</a></p> 
  <h2 id="%E7%89%9B%E9%A1%BF%E6%B3%95%EF%BC%9A">牛顿法：</h2> 
  <p style="text-indent:50px;">方便起见，在这里需要求的是参数w,b的整体<img alt="" class="has" height="33" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019021819561028.png" width="90">，根据 5 中的损失函数：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="82" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195627578.png" width="358"></p> 
  <p style="text-indent:50px;"><span style="color:#4472c4;">一阶导数为：</span></p> 
  <p style="text-align:center;"><img alt="" class="has" height="83" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195709608.png" width="479"></p> 
  <p style="text-indent:50px;"><span style="color:#4472c4;">根据 2 中</span></p> 
  <p style="text-align:center;"><img alt="" class="has" height="140" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195737165.png" width="256"></p> 
  <p style="text-indent:50px;">因为</p> 
  <p style="text-align:center;"><img alt="" class="has" height="94" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195755388.png" width="432"></p> 
  <p style="text-indent:50px;"><span style="color:#4472c4;">所以，二阶导数为：</span></p> 
  <p style="text-align:center;"><img alt="" class="has" height="66" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195815184.png" width="568"></p> 
  <p style="text-indent:50px;">&nbsp;<span style="color:#4472c4;">因此，牛顿法参数的迭代形式：</span></p> 
  <p style="text-align:center;"><img alt="" class="has" height="123" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195840739.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="384"></p> 
  <p style="text-indent:50px;">当然，牛顿法也可以加上一维线性搜索（<a href="https://blog.csdn.net/jiang425776024/article/details/87600422" rel="nofollow">https://blog.csdn.net/jiang425776024/article/details/87600422</a>），即，如下梯度法那样，在上面的导数式子上，加上学习率a。</p> 
  <h2 id="%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95">梯度下降法</h2> 
  <p style="text-indent:50px;">只需要一阶导数，a为步长，一般∈[0,1]，代入上面的 1 阶导数即可：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="66" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218195908107.png" width="178"></p> 
  <p style="text-indent:0;">&nbsp;</p> 
  <hr>
  <h1 id="8.%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B">8.完整流程</h1> 
  <p style="text-indent:50px;">输入：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="140" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019021820001473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="483"></p> 
  <p style="margin-left:0pt;">&nbsp;</p> 
  <p style="text-indent:50px;">过程：随机初始化参数Beita</p> 
  <p style="text-align:center;"><img alt="" class="has" height="135" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218200043853.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="488"></p> 
  <p style="margin-left:0pt;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;while &nbsp;<img alt="" class="has" height="23" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218200135682.png" width="21">总体变化量&lt;某个阀值：对<img alt="" class="has" height="23" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190218200135682.png" width="21">进行7中那样的迭代更新</p> 
  <hr>
  <h1 id="9.%E6%AD%A3%E5%88%99%E5%8C%96">9.正则化</h1> 
  <p style="text-indent:50px;">和其它算法一样，可以对参数进行正则化，L1\L2等，通常都是在损失函数后面加上形如L2正则化：<img alt="\frac{1}{2}\alpha||\theta||_2^2" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7B2%7D%5Calpha%7C%7C%5Ctheta%7C%7C_2%5E2">，其中a为正则化强度，theta为模型参数。这时参数更新需要加入对正则化这部分的求导。</p> 
  <hr>
  <h1 id="10.%E5%A4%9A%E5%85%83%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92">10.多元逻辑回归</h1> 
  <p style="text-indent:50px;">二分类中，形如：<img alt="ln\frac{P(y=1|x,\theta )}{P(y=0|x,\theta)} = x\theta" class="mathcode" src="https://private.codecogs.com/gif.latex?ln%5Cfrac%7BP%28y%3D1%7Cx%2C%5Ctheta%20%29%7D%7BP%28y%3D0%7Cx%2C%5Ctheta%29%7D%20%3D%20x%5Ctheta"></p> 
  <p style="text-indent:50px;">多分类中假设有K&gt;2个类别，则有K-1个方程：</p> 
  <p style="text-indent:50px;"><img alt="ln\frac{P(y=1|x,\theta )}{P(y=K|x,\theta)} = x\theta_1" class="mathcode" src="https://private.codecogs.com/gif.latex?ln%5Cfrac%7BP%28y%3D1%7Cx%2C%5Ctheta%20%29%7D%7BP%28y%3DK%7Cx%2C%5Ctheta%29%7D%20%3D%20x%5Ctheta_1">、<img alt="ln\frac{P(y=2|x,\theta )}{P(y=K|x,\theta)} = x\theta_2" class="mathcode" src="https://private.codecogs.com/gif.latex?ln%5Cfrac%7BP%28y%3D2%7Cx%2C%5Ctheta%20%29%7D%7BP%28y%3DK%7Cx%2C%5Ctheta%29%7D%20%3D%20x%5Ctheta_2">,...,<img alt="ln\frac{P(y=K-1|x,\theta )}{P(y=K|x,\theta)} = x\theta_{K-1}" class="mathcode" src="https://private.codecogs.com/gif.latex?ln%5Cfrac%7BP%28y%3DK-1%7Cx%2C%5Ctheta%20%29%7D%7BP%28y%3DK%7Cx%2C%5Ctheta%29%7D%20%3D%20x%5Ctheta_%7BK-1%7D"></p> 
  <p style="text-indent:50px;">则K元逻辑回归的概率分布如下：</p> 
  <p style="text-indent:50px;"><img alt="P(y=k|x,\theta ) = e^{x\theta_k} \bigg/ 1+\sum\limits_{t=1}^{K-1}e^{x\theta_t}" class="mathcode" src="https://private.codecogs.com/gif.latex?P%28y%3Dk%7Cx%2C%5Ctheta%20%29%20%3D%20e%5E%7Bx%5Ctheta_k%7D%20%5Cbigg/%201&amp;plus;%5Csum%5Climits_%7Bt%3D1%7D%5E%7BK-1%7De%5E%7Bx%5Ctheta_t%7D">，k = 1,2,...K-1</p> 
  <p style="text-indent:50px;"><img alt="P(y=K|x,\theta ) = 1 \bigg/ 1+\sum\limits_{t=1}^{K-1}e^{x\theta_t}" class="mathcode" src="https://private.codecogs.com/gif.latex?P%28y%3DK%7Cx%2C%5Ctheta%20%29%20%3D%201%20%5Cbigg/%201&amp;plus;%5Csum%5Climits_%7Bt%3D1%7D%5E%7BK-1%7De%5E%7Bx%5Ctheta_t%7D"></p> 
  <p style="text-indent:50px;">剩下的多元逻辑回归的损失函数推导以及优化方法和二元逻辑回归类似。</p> 
  <p style="text-indent:50px;">还有一种简单粗暴的办法就是，构造多个二分类，比如有A，B,C,D&nbsp;四个类，那么按照上面的，可以构造4个二分类：正A与反(B，C,D);正B与反(A,C,D)，.....。这样，就可以按照二分类的情况进行多分类了，只是判断的时候需要进行多个if判断。</p> 
  <hr>
  <h1 id="11.scikit-learn%E4%B8%AD%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92">11.scikit-learn中逻辑回归</h1> 
  <p style="text-indent:50px;">主要是3个：<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" rel="nofollow">LogisticRegression</a>， <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" rel="nofollow">LogisticRegressionCV </a>和<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.logistic_regression_path.html#sklearn.linear_model.logistic_regression_path" rel="nofollow">logistic_regression_path</a>。</p> 
  <p style="text-indent:50px;">其中<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" rel="nofollow">LogisticRegression</a>和 <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" rel="nofollow">LogisticRegressionCV</a>的主要区别是 <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" rel="nofollow">LogisticRegressionCV</a>使用了<span style="color:#f33b45;">交叉验证来选择（9中正则化介绍的a）正则化系数C，</span>而<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" rel="nofollow">LogisticRegression</a>需要自己每次指定一个正则化系数。</p> 
  <p style="text-indent:50px;"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.logistic_regression_path.html#sklearn.linear_model.logistic_regression_path" rel="nofollow">logistic_regression_path</a>主要用在模型选择，不能直接来做预测，只能选择合适逻辑回归的系数和正则化系数。</p> 
  <p style="text-indent:0;">使用：</p> 
  <p style="text-indent:50px;">这里不会详细介绍api的参数，点击可查看<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" rel="nofollow">LogisticRegression</a>， <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" rel="nofollow">LogisticRegressionCV</a>的参数说明，</p> 
  <p style="text-indent:50px;">或者：<a href="https://www.cnblogs.com/pinard/p/6035872.html" rel="nofollow">https://www.cnblogs.com/pinard/p/6035872.html</a>。</p> 
  <pre class="has">
<code>import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV

'''
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV
'''

X, y = load_iris(return_X_y=True)

#solver='lbfgs'使用拟牛顿法迭代寻参
#random_state=0随机种子，这个很必要，数值随便
clf = LogisticRegression(random_state=0, C=2, solver='lbfgs', multi_class='multinomial').fit(X, y)#指定了正则化强度为2
clfcv = LogisticRegressionCV(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X, y)#cv类自动寻找正则化强度

pd = clf.predict(X[:2, :])
pdcv = clf.predict(X[:2, :])
print(X[:2, :], '预测类型：', pd)
print(X[:2, :], '预测类型：', pdcv)

pbd = clf.predict_proba(X[:2, :])
pbdcv = clf.predict_proba(X[:2, :])
print(X[:2, :], '预测类型的概率：', pbd)
print(X[:2, :], '预测类型的概率：', pbdcv)

print('预测分数：', clf.score(X, y))
print('预测分数：', clfcv.score(X, y))

'''
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]] 预测类型： [0 0]
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]] 预测类型： [0 0]
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]] 预测类型的概率： [[9.89252266e-01 1.07477333e-02 2.22652613e-10]
 [9.82191282e-01 1.78087169e-02 6.46675701e-10]]
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]] 预测类型的概率： [[9.89252266e-01 1.07477333e-02 2.22652613e-10]
 [9.82191282e-01 1.78087169e-02 6.46675701e-10]]
预测分数： 0.9866666666666667
预测分数： 0.98

Process finished with exit code 0

'''</code></pre> 
  <p style="text-indent:50px;">&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
