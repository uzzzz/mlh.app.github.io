<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>《Computer vision》笔记-MobileNet（7） | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="《Computer vision》笔记-MobileNet（7）" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp; &nbsp;作者：石文华&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 编辑：陈人和&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; 前 &nbsp;言 移动端和其他嵌入式系统通常是内存空间小，能耗要求低的，也就是计算资源有限。一般的模型，比如ImageNet数据集上训练的VGG,googlenet,resnet等，需要巨大的计算资源，所以很难用在嵌入式系统上。MobileNet是一种高效的模型，用于移动和嵌入式视觉应用。 章节目录 深度可分离卷积（depthwise separable convolution） 宽度因子和分辨率因子 改进（MobileNet V2) 01 深度可分离卷积（depthwise separable convolution) MobileNet使用技术之一是深度可分离卷积(depthwise separable convolution)代替传统的3D卷积操作，这样可以减少参数数量以及卷积过程中的计算量，但是也会导致特征丢失，使得精度下降。MobileNet其实就是Xception思想的应用。区别就是Xception文章重点在提高精度，而MobileNet重点在压缩模型。 假设输入特征图有M个，大小为DF，输出的特征图是N个，卷积核尺寸是Dk*Dk,那么传统的3D卷积的卷积核是立体的，也就是每一个卷积核都是Dk*Dk*M，总共有N个Dk*Dk*M的卷积核，如下图所示： 所以总的参数个数为Dk*Dk*M*N,假设输出使用的padding参数是same,则输出特征图大小也是DF，那么总的计算量为Dk*Dk*M*N*DF*DF。&nbsp;而MobileNet将普通卷积操作分为两部分，第一部分是逐通道卷积，使用M个通道数为1，大小为Dk*Dk的卷积核，每一个卷积核负责其中的一个通道。如下图所示： 逐通道卷积之后的参数个数为：Dk*Dk*M，同样假设padding为same,则计算量为：Dk*Dk*M*DF*DF&nbsp; 第二部分是点卷积，也就是采用3D的1*1卷积改变通道数，对深度方向上的特征信息进行组合，最终将输出通道数变为指定的数。如下图所示： 这部分的参数个数为：M*N，padding为same时的计算量为M*N*DF*DF.&nbsp;因此这种分离之后的总的计算量为：：Dk*Dk*M*DF*DF+M*N*DF*DF。&nbsp;深度可分离卷积跟传统3D卷积计算量的比例为： 如下图，左边是3D卷积常见结构，右边是深度可分离卷积的使用方式： 02 宽度因子和分辨率因子 MobileNet有两个简单的全局超参数，分别是宽度因子和分辨率因子，可有效的在延迟和准确率之间做折中。允许我们依据约束条件选择合适大小、低延迟、易满足嵌入式设备要求的模型。&nbsp; （1）宽度因子 上述的逐通道卷积的卷积核个数通常是M，也就是Dk*Dk*1的卷积核个数等于输入通道数，宽度因子是一个参数为（0，1]之间的参数，作用于通道数，可以理解为按照比例缩减输入通道数。同理，输出的通道数也可以通过这个参数进行按比例缩减。用α表示这个参数，则计算量为： 不同的α取值在ImageNet上的准确率，下图为准确率，参数数量和计算量之间的权衡情况： （2）分辨因子 上述的输入特征图大小为DF*DF,分辨率因子取值范围在(0,1]之间，可以理解为对特征图进行下采样，也就是按比例降低特征图的大小，使得输入数据以及由此在每一个模块产生的特征图都变小，用β表示这个参数，结合宽度因子α，则计算量为： 不同的β系数作用于标准MobileNet时，对精度和计算量以的影响（α固定）： 03 改进（MobileNet V2） 在 MobileNet-V1 基础上结合当下流行的残差思想而设计,V2 主要引入了两个改动：Linear Bottleneck 和 Inverted Residual Blocks。&nbsp; V1与V2的结构对比： 两者相同的地方在于都采用 Depth-wise (DW) 卷积搭配 Point-wise (PW) 卷积的方式来提特征。&nbsp; （1）改进一：&nbsp; V2在 DW 卷积之前新加了一个 PW 卷积，主要目的是为了提升维度数。相比V1直接在每个通道上单独提取特征，V2的这种做法能够先组合不同深度上的特征，并升维，使得特征更加丰富。比直接DW的话，特征提取的效果更好。&nbsp; （2）改进二：&nbsp; V2 去掉了第二个 PW 的激活函数。论文作者称其为 Linear Bottleneck。这么做的原因，是因为作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，由Relu的性质，Relu对于负的输入，输出全为零，所以relu会使得一部分特征失效。由于第二个 PW 的主要功能就是降维，再经过Relu的话，又要“损失”一部分特征，因此按照上面的理论，降维之后就不宜再使用 Relu了。如下图所示，一个原始螺旋形被利用随机矩阵T经过ReLU后嵌入到一个n维空间中，然后使用T−1投影到2维空间中。例子中，n=2,3导致信息损失，可以看到流形的中心点之间互相坍塌。同时n=15,30时信息变成高度非凸。 （3）使用短路连接: 倒残差(Inverted Residual)&nbsp;典的残差块是：1x1(压缩)-&gt;3x3(卷积)-&gt;1x1(升维)，而inverted residual顾名思义是颠倒的残差：1x1(升维)-&gt;3x3(dw conv+relu)-&gt;1x1(降维+线性变换)，skip-connection(跳过连接)是在低维的瓶颈层间发生的(如下图)，这对于移动端有限的宽带是有益的。连接情况如下图所示(shortcut只在stride==1时才使用)： （4）网络结构如下： 由于笔者水平有限，可参考如下github上的代码：https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py import torch.nn as nnimport math#传统的3D卷积def conv_bn(inp, oup, stride): &nbsp; &nbsp;return nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, oup, 3, stride, 1, bias=False),#卷积 &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp;#bn &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True)#relu6 &nbsp; &nbsp;)#1x1的点卷积def conv_1x1_bn(inp, oup): &nbsp; &nbsp;return nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, oup, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True) &nbsp; &nbsp;)#倒残差class InvertedResidual(nn.Module): &nbsp; &nbsp;def __init__(self, inp, oup, stride, expand_ratio):#参数分别是输入特征图数，输出特征图数，步长，扩展比例 &nbsp; &nbsp; &nbsp; &nbsp;super(InvertedResidual, self).__init__() &nbsp; &nbsp; &nbsp; &nbsp;self.stride = stride &nbsp; #步长 &nbsp; &nbsp; &nbsp; &nbsp;assert stride in [1, 2] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hidden_dim = round(inp * expand_ratio)#隐藏层层数，expand_ratio为拓展倍数 &nbsp; &nbsp; &nbsp; &nbsp;self.use_res_connect = (self.stride == 1 and inp == oup)#是否进跳跃链接 &nbsp; &nbsp; &nbsp; &nbsp;if expand_ratio == 1: &nbsp;#不进行扩展 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.conv = nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# dw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),#可分离卷积 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw-linear &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;) &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.conv = nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# dw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw-linear &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;) &nbsp; &nbsp;def forward(self, x): &nbsp; &nbsp; &nbsp; &nbsp;if self.use_res_connect: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return x + self.conv(x) &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return self.conv(x)class MobileNetV2(nn.Module): &nbsp; &nbsp;def __init__(self, n_class=1000, input_size=224, width_mult=1.): &nbsp; &nbsp; &nbsp; &nbsp;super(MobileNetV2, self).__init__() &nbsp; &nbsp; &nbsp; &nbsp;block = InvertedResidual#创建一个倒残差对象 &nbsp; &nbsp; &nbsp; &nbsp;input_channel = 32 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;last_channel = 1280 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#39;&#39;&#39; &nbsp; &nbsp; &nbsp; &nbsp;t :是输入通道的倍增系数（即中间部分的通道数是输入通道数的多少倍） &nbsp; &nbsp; &nbsp; &nbsp;n :是该模块重复次数 &nbsp; &nbsp; &nbsp; &nbsp;c :是输出通道数 &nbsp; &nbsp; &nbsp; &nbsp;s :是该模块第一次重复时的 stride（后面重复都是 stride 1） &nbsp; &nbsp; &nbsp; &nbsp;&#39;&#39;&#39; &nbsp; &nbsp; &nbsp; &nbsp;interverted_residual_setting = [ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# t, c, n, s &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[1, 16, 1, 1], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 24, 2, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 32, 3, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 64, 4, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 96, 3, 1], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 160, 3, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 320, 1, 1], &nbsp; &nbsp; &nbsp; &nbsp;] &nbsp; &nbsp; &nbsp; &nbsp;# building first layer &nbsp; &nbsp; &nbsp; &nbsp;assert input_size % 32 == 0 &nbsp; &nbsp; &nbsp; &nbsp;input_channel = int(input_channel * width_mult) &nbsp; &nbsp; &nbsp; &nbsp;self.last_channel = int(last_channel * width_mult) if width_mult &gt; 1.0 else last_channel &nbsp; &nbsp; &nbsp; &nbsp;self.features = [conv_bn(3, input_channel, 2)] &nbsp; &nbsp; &nbsp; &nbsp;# building inverted residual blocks &nbsp; &nbsp; &nbsp; &nbsp;for t, c, n, s in interverted_residual_setting: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;output_channel = int(c * width_mult) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for i in range(n): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if i == 0: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(block(input_channel, output_channel, s, expand_ratio=t)) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(block(input_channel, output_channel, 1, expand_ratio=t)) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;input_channel = output_channel &nbsp; &nbsp; &nbsp; &nbsp;# building last several layers &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(conv_1x1_bn(input_channel, self.last_channel)) &nbsp; &nbsp; &nbsp; &nbsp;# make it nn.Sequential &nbsp; &nbsp; &nbsp; &nbsp;self.features = nn.Sequential(*self.features) &nbsp; &nbsp; &nbsp; &nbsp;# building classifier &nbsp; &nbsp; &nbsp; &nbsp;self.classifier = nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Dropout(0.2), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Linear(self.last_channel, n_class), &nbsp; &nbsp; &nbsp; &nbsp;) &nbsp; &nbsp; &nbsp; &nbsp;self._initialize_weights() &nbsp; &nbsp;def forward(self, x): &nbsp; &nbsp; &nbsp; &nbsp;x = self.features(x) &nbsp; &nbsp; &nbsp; &nbsp;x = x.mean(3).mean(2) &nbsp; &nbsp; &nbsp; &nbsp;x = self.classifier(x) &nbsp; &nbsp; &nbsp; &nbsp;return x &nbsp; &nbsp;def _initialize_weights(self): &nbsp; &nbsp; &nbsp; &nbsp;for m in self.modules(): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if isinstance(m, nn.Conv2d): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.normal_(0, math.sqrt(2. / n)) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if m.bias is not None: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_() &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;elif isinstance(m, nn.BatchNorm2d): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.fill_(1) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_() &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;elif isinstance(m, nn.Linear): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n = m.weight.size(1) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.normal_(0, 0.01) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_()model= MobileNetV2()print(model) 参考文献：&nbsp;https://blog.csdn.net/t800ghb/article/details/78879612&nbsp;https://www.cnblogs.com/CodingML-1122/p/9043078.html&nbsp;https://blog.csdn.net/u011995719/article/details/79135818&nbsp;https://zhuanlan.zhihu.com/p/33075914&nbsp;https://zhuanlan.zhihu.com/p/39386719&nbsp;https://mp.weixin.qq.com/s/T6S1_cFXPEuhRAkJo2m8Ig&nbsp;https://blog.csdn.net/qq_31531635/article/details/80550412&nbsp;https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py &nbsp; END 往期回顾之作者石文华 【1】《Computer vision》笔记-SqueezeNet（6） 【2】《Computer vision》笔记-GoodLeNet（3） 【3】&nbsp;干货|（DL~2)一看就懂的卷积神经网络 【4】《Computer vision》笔记-Xception(5) 【5】&nbsp;干货|（DL~4）对象定位和检测 机器学习算法工程师 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 一个用心的公众号 长按，识别，加关注 进群，学习，得帮助 你的关注，我们的热度， 我们一定给你学习最大的帮助 你点的每个赞，我都认真当成了喜欢" />
<meta property="og:description" content="&nbsp; &nbsp;作者：石文华&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 编辑：陈人和&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; 前 &nbsp;言 移动端和其他嵌入式系统通常是内存空间小，能耗要求低的，也就是计算资源有限。一般的模型，比如ImageNet数据集上训练的VGG,googlenet,resnet等，需要巨大的计算资源，所以很难用在嵌入式系统上。MobileNet是一种高效的模型，用于移动和嵌入式视觉应用。 章节目录 深度可分离卷积（depthwise separable convolution） 宽度因子和分辨率因子 改进（MobileNet V2) 01 深度可分离卷积（depthwise separable convolution) MobileNet使用技术之一是深度可分离卷积(depthwise separable convolution)代替传统的3D卷积操作，这样可以减少参数数量以及卷积过程中的计算量，但是也会导致特征丢失，使得精度下降。MobileNet其实就是Xception思想的应用。区别就是Xception文章重点在提高精度，而MobileNet重点在压缩模型。 假设输入特征图有M个，大小为DF，输出的特征图是N个，卷积核尺寸是Dk*Dk,那么传统的3D卷积的卷积核是立体的，也就是每一个卷积核都是Dk*Dk*M，总共有N个Dk*Dk*M的卷积核，如下图所示： 所以总的参数个数为Dk*Dk*M*N,假设输出使用的padding参数是same,则输出特征图大小也是DF，那么总的计算量为Dk*Dk*M*N*DF*DF。&nbsp;而MobileNet将普通卷积操作分为两部分，第一部分是逐通道卷积，使用M个通道数为1，大小为Dk*Dk的卷积核，每一个卷积核负责其中的一个通道。如下图所示： 逐通道卷积之后的参数个数为：Dk*Dk*M，同样假设padding为same,则计算量为：Dk*Dk*M*DF*DF&nbsp; 第二部分是点卷积，也就是采用3D的1*1卷积改变通道数，对深度方向上的特征信息进行组合，最终将输出通道数变为指定的数。如下图所示： 这部分的参数个数为：M*N，padding为same时的计算量为M*N*DF*DF.&nbsp;因此这种分离之后的总的计算量为：：Dk*Dk*M*DF*DF+M*N*DF*DF。&nbsp;深度可分离卷积跟传统3D卷积计算量的比例为： 如下图，左边是3D卷积常见结构，右边是深度可分离卷积的使用方式： 02 宽度因子和分辨率因子 MobileNet有两个简单的全局超参数，分别是宽度因子和分辨率因子，可有效的在延迟和准确率之间做折中。允许我们依据约束条件选择合适大小、低延迟、易满足嵌入式设备要求的模型。&nbsp; （1）宽度因子 上述的逐通道卷积的卷积核个数通常是M，也就是Dk*Dk*1的卷积核个数等于输入通道数，宽度因子是一个参数为（0，1]之间的参数，作用于通道数，可以理解为按照比例缩减输入通道数。同理，输出的通道数也可以通过这个参数进行按比例缩减。用α表示这个参数，则计算量为： 不同的α取值在ImageNet上的准确率，下图为准确率，参数数量和计算量之间的权衡情况： （2）分辨因子 上述的输入特征图大小为DF*DF,分辨率因子取值范围在(0,1]之间，可以理解为对特征图进行下采样，也就是按比例降低特征图的大小，使得输入数据以及由此在每一个模块产生的特征图都变小，用β表示这个参数，结合宽度因子α，则计算量为： 不同的β系数作用于标准MobileNet时，对精度和计算量以的影响（α固定）： 03 改进（MobileNet V2） 在 MobileNet-V1 基础上结合当下流行的残差思想而设计,V2 主要引入了两个改动：Linear Bottleneck 和 Inverted Residual Blocks。&nbsp; V1与V2的结构对比： 两者相同的地方在于都采用 Depth-wise (DW) 卷积搭配 Point-wise (PW) 卷积的方式来提特征。&nbsp; （1）改进一：&nbsp; V2在 DW 卷积之前新加了一个 PW 卷积，主要目的是为了提升维度数。相比V1直接在每个通道上单独提取特征，V2的这种做法能够先组合不同深度上的特征，并升维，使得特征更加丰富。比直接DW的话，特征提取的效果更好。&nbsp; （2）改进二：&nbsp; V2 去掉了第二个 PW 的激活函数。论文作者称其为 Linear Bottleneck。这么做的原因，是因为作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，由Relu的性质，Relu对于负的输入，输出全为零，所以relu会使得一部分特征失效。由于第二个 PW 的主要功能就是降维，再经过Relu的话，又要“损失”一部分特征，因此按照上面的理论，降维之后就不宜再使用 Relu了。如下图所示，一个原始螺旋形被利用随机矩阵T经过ReLU后嵌入到一个n维空间中，然后使用T−1投影到2维空间中。例子中，n=2,3导致信息损失，可以看到流形的中心点之间互相坍塌。同时n=15,30时信息变成高度非凸。 （3）使用短路连接: 倒残差(Inverted Residual)&nbsp;典的残差块是：1x1(压缩)-&gt;3x3(卷积)-&gt;1x1(升维)，而inverted residual顾名思义是颠倒的残差：1x1(升维)-&gt;3x3(dw conv+relu)-&gt;1x1(降维+线性变换)，skip-connection(跳过连接)是在低维的瓶颈层间发生的(如下图)，这对于移动端有限的宽带是有益的。连接情况如下图所示(shortcut只在stride==1时才使用)： （4）网络结构如下： 由于笔者水平有限，可参考如下github上的代码：https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py import torch.nn as nnimport math#传统的3D卷积def conv_bn(inp, oup, stride): &nbsp; &nbsp;return nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, oup, 3, stride, 1, bias=False),#卷积 &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp;#bn &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True)#relu6 &nbsp; &nbsp;)#1x1的点卷积def conv_1x1_bn(inp, oup): &nbsp; &nbsp;return nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, oup, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True) &nbsp; &nbsp;)#倒残差class InvertedResidual(nn.Module): &nbsp; &nbsp;def __init__(self, inp, oup, stride, expand_ratio):#参数分别是输入特征图数，输出特征图数，步长，扩展比例 &nbsp; &nbsp; &nbsp; &nbsp;super(InvertedResidual, self).__init__() &nbsp; &nbsp; &nbsp; &nbsp;self.stride = stride &nbsp; #步长 &nbsp; &nbsp; &nbsp; &nbsp;assert stride in [1, 2] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hidden_dim = round(inp * expand_ratio)#隐藏层层数，expand_ratio为拓展倍数 &nbsp; &nbsp; &nbsp; &nbsp;self.use_res_connect = (self.stride == 1 and inp == oup)#是否进跳跃链接 &nbsp; &nbsp; &nbsp; &nbsp;if expand_ratio == 1: &nbsp;#不进行扩展 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.conv = nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# dw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),#可分离卷积 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw-linear &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;) &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.conv = nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# dw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw-linear &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;) &nbsp; &nbsp;def forward(self, x): &nbsp; &nbsp; &nbsp; &nbsp;if self.use_res_connect: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return x + self.conv(x) &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return self.conv(x)class MobileNetV2(nn.Module): &nbsp; &nbsp;def __init__(self, n_class=1000, input_size=224, width_mult=1.): &nbsp; &nbsp; &nbsp; &nbsp;super(MobileNetV2, self).__init__() &nbsp; &nbsp; &nbsp; &nbsp;block = InvertedResidual#创建一个倒残差对象 &nbsp; &nbsp; &nbsp; &nbsp;input_channel = 32 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;last_channel = 1280 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#39;&#39;&#39; &nbsp; &nbsp; &nbsp; &nbsp;t :是输入通道的倍增系数（即中间部分的通道数是输入通道数的多少倍） &nbsp; &nbsp; &nbsp; &nbsp;n :是该模块重复次数 &nbsp; &nbsp; &nbsp; &nbsp;c :是输出通道数 &nbsp; &nbsp; &nbsp; &nbsp;s :是该模块第一次重复时的 stride（后面重复都是 stride 1） &nbsp; &nbsp; &nbsp; &nbsp;&#39;&#39;&#39; &nbsp; &nbsp; &nbsp; &nbsp;interverted_residual_setting = [ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# t, c, n, s &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[1, 16, 1, 1], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 24, 2, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 32, 3, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 64, 4, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 96, 3, 1], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 160, 3, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 320, 1, 1], &nbsp; &nbsp; &nbsp; &nbsp;] &nbsp; &nbsp; &nbsp; &nbsp;# building first layer &nbsp; &nbsp; &nbsp; &nbsp;assert input_size % 32 == 0 &nbsp; &nbsp; &nbsp; &nbsp;input_channel = int(input_channel * width_mult) &nbsp; &nbsp; &nbsp; &nbsp;self.last_channel = int(last_channel * width_mult) if width_mult &gt; 1.0 else last_channel &nbsp; &nbsp; &nbsp; &nbsp;self.features = [conv_bn(3, input_channel, 2)] &nbsp; &nbsp; &nbsp; &nbsp;# building inverted residual blocks &nbsp; &nbsp; &nbsp; &nbsp;for t, c, n, s in interverted_residual_setting: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;output_channel = int(c * width_mult) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for i in range(n): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if i == 0: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(block(input_channel, output_channel, s, expand_ratio=t)) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(block(input_channel, output_channel, 1, expand_ratio=t)) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;input_channel = output_channel &nbsp; &nbsp; &nbsp; &nbsp;# building last several layers &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(conv_1x1_bn(input_channel, self.last_channel)) &nbsp; &nbsp; &nbsp; &nbsp;# make it nn.Sequential &nbsp; &nbsp; &nbsp; &nbsp;self.features = nn.Sequential(*self.features) &nbsp; &nbsp; &nbsp; &nbsp;# building classifier &nbsp; &nbsp; &nbsp; &nbsp;self.classifier = nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Dropout(0.2), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Linear(self.last_channel, n_class), &nbsp; &nbsp; &nbsp; &nbsp;) &nbsp; &nbsp; &nbsp; &nbsp;self._initialize_weights() &nbsp; &nbsp;def forward(self, x): &nbsp; &nbsp; &nbsp; &nbsp;x = self.features(x) &nbsp; &nbsp; &nbsp; &nbsp;x = x.mean(3).mean(2) &nbsp; &nbsp; &nbsp; &nbsp;x = self.classifier(x) &nbsp; &nbsp; &nbsp; &nbsp;return x &nbsp; &nbsp;def _initialize_weights(self): &nbsp; &nbsp; &nbsp; &nbsp;for m in self.modules(): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if isinstance(m, nn.Conv2d): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.normal_(0, math.sqrt(2. / n)) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if m.bias is not None: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_() &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;elif isinstance(m, nn.BatchNorm2d): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.fill_(1) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_() &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;elif isinstance(m, nn.Linear): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n = m.weight.size(1) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.normal_(0, 0.01) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_()model= MobileNetV2()print(model) 参考文献：&nbsp;https://blog.csdn.net/t800ghb/article/details/78879612&nbsp;https://www.cnblogs.com/CodingML-1122/p/9043078.html&nbsp;https://blog.csdn.net/u011995719/article/details/79135818&nbsp;https://zhuanlan.zhihu.com/p/33075914&nbsp;https://zhuanlan.zhihu.com/p/39386719&nbsp;https://mp.weixin.qq.com/s/T6S1_cFXPEuhRAkJo2m8Ig&nbsp;https://blog.csdn.net/qq_31531635/article/details/80550412&nbsp;https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py &nbsp; END 往期回顾之作者石文华 【1】《Computer vision》笔记-SqueezeNet（6） 【2】《Computer vision》笔记-GoodLeNet（3） 【3】&nbsp;干货|（DL~2)一看就懂的卷积神经网络 【4】《Computer vision》笔记-Xception(5) 【5】&nbsp;干货|（DL~4）对象定位和检测 机器学习算法工程师 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 一个用心的公众号 长按，识别，加关注 进群，学习，得帮助 你的关注，我们的热度， 我们一定给你学习最大的帮助 你点的每个赞，我都认真当成了喜欢" />
<link rel="canonical" href="https://mlh.app/2019/02/11/5cdbc3ca05f9b090887f8c567ea4bcf0.html" />
<meta property="og:url" content="https://mlh.app/2019/02/11/5cdbc3ca05f9b090887f8c567ea4bcf0.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-11T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp; &nbsp;作者：石文华&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 编辑：陈人和&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; 前 &nbsp;言 移动端和其他嵌入式系统通常是内存空间小，能耗要求低的，也就是计算资源有限。一般的模型，比如ImageNet数据集上训练的VGG,googlenet,resnet等，需要巨大的计算资源，所以很难用在嵌入式系统上。MobileNet是一种高效的模型，用于移动和嵌入式视觉应用。 章节目录 深度可分离卷积（depthwise separable convolution） 宽度因子和分辨率因子 改进（MobileNet V2) 01 深度可分离卷积（depthwise separable convolution) MobileNet使用技术之一是深度可分离卷积(depthwise separable convolution)代替传统的3D卷积操作，这样可以减少参数数量以及卷积过程中的计算量，但是也会导致特征丢失，使得精度下降。MobileNet其实就是Xception思想的应用。区别就是Xception文章重点在提高精度，而MobileNet重点在压缩模型。 假设输入特征图有M个，大小为DF，输出的特征图是N个，卷积核尺寸是Dk*Dk,那么传统的3D卷积的卷积核是立体的，也就是每一个卷积核都是Dk*Dk*M，总共有N个Dk*Dk*M的卷积核，如下图所示： 所以总的参数个数为Dk*Dk*M*N,假设输出使用的padding参数是same,则输出特征图大小也是DF，那么总的计算量为Dk*Dk*M*N*DF*DF。&nbsp;而MobileNet将普通卷积操作分为两部分，第一部分是逐通道卷积，使用M个通道数为1，大小为Dk*Dk的卷积核，每一个卷积核负责其中的一个通道。如下图所示： 逐通道卷积之后的参数个数为：Dk*Dk*M，同样假设padding为same,则计算量为：Dk*Dk*M*DF*DF&nbsp; 第二部分是点卷积，也就是采用3D的1*1卷积改变通道数，对深度方向上的特征信息进行组合，最终将输出通道数变为指定的数。如下图所示： 这部分的参数个数为：M*N，padding为same时的计算量为M*N*DF*DF.&nbsp;因此这种分离之后的总的计算量为：：Dk*Dk*M*DF*DF+M*N*DF*DF。&nbsp;深度可分离卷积跟传统3D卷积计算量的比例为： 如下图，左边是3D卷积常见结构，右边是深度可分离卷积的使用方式： 02 宽度因子和分辨率因子 MobileNet有两个简单的全局超参数，分别是宽度因子和分辨率因子，可有效的在延迟和准确率之间做折中。允许我们依据约束条件选择合适大小、低延迟、易满足嵌入式设备要求的模型。&nbsp; （1）宽度因子 上述的逐通道卷积的卷积核个数通常是M，也就是Dk*Dk*1的卷积核个数等于输入通道数，宽度因子是一个参数为（0，1]之间的参数，作用于通道数，可以理解为按照比例缩减输入通道数。同理，输出的通道数也可以通过这个参数进行按比例缩减。用α表示这个参数，则计算量为： 不同的α取值在ImageNet上的准确率，下图为准确率，参数数量和计算量之间的权衡情况： （2）分辨因子 上述的输入特征图大小为DF*DF,分辨率因子取值范围在(0,1]之间，可以理解为对特征图进行下采样，也就是按比例降低特征图的大小，使得输入数据以及由此在每一个模块产生的特征图都变小，用β表示这个参数，结合宽度因子α，则计算量为： 不同的β系数作用于标准MobileNet时，对精度和计算量以的影响（α固定）： 03 改进（MobileNet V2） 在 MobileNet-V1 基础上结合当下流行的残差思想而设计,V2 主要引入了两个改动：Linear Bottleneck 和 Inverted Residual Blocks。&nbsp; V1与V2的结构对比： 两者相同的地方在于都采用 Depth-wise (DW) 卷积搭配 Point-wise (PW) 卷积的方式来提特征。&nbsp; （1）改进一：&nbsp; V2在 DW 卷积之前新加了一个 PW 卷积，主要目的是为了提升维度数。相比V1直接在每个通道上单独提取特征，V2的这种做法能够先组合不同深度上的特征，并升维，使得特征更加丰富。比直接DW的话，特征提取的效果更好。&nbsp; （2）改进二：&nbsp; V2 去掉了第二个 PW 的激活函数。论文作者称其为 Linear Bottleneck。这么做的原因，是因为作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，由Relu的性质，Relu对于负的输入，输出全为零，所以relu会使得一部分特征失效。由于第二个 PW 的主要功能就是降维，再经过Relu的话，又要“损失”一部分特征，因此按照上面的理论，降维之后就不宜再使用 Relu了。如下图所示，一个原始螺旋形被利用随机矩阵T经过ReLU后嵌入到一个n维空间中，然后使用T−1投影到2维空间中。例子中，n=2,3导致信息损失，可以看到流形的中心点之间互相坍塌。同时n=15,30时信息变成高度非凸。 （3）使用短路连接: 倒残差(Inverted Residual)&nbsp;典的残差块是：1x1(压缩)-&gt;3x3(卷积)-&gt;1x1(升维)，而inverted residual顾名思义是颠倒的残差：1x1(升维)-&gt;3x3(dw conv+relu)-&gt;1x1(降维+线性变换)，skip-connection(跳过连接)是在低维的瓶颈层间发生的(如下图)，这对于移动端有限的宽带是有益的。连接情况如下图所示(shortcut只在stride==1时才使用)： （4）网络结构如下： 由于笔者水平有限，可参考如下github上的代码：https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py import torch.nn as nnimport math#传统的3D卷积def conv_bn(inp, oup, stride): &nbsp; &nbsp;return nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, oup, 3, stride, 1, bias=False),#卷积 &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp;#bn &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True)#relu6 &nbsp; &nbsp;)#1x1的点卷积def conv_1x1_bn(inp, oup): &nbsp; &nbsp;return nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, oup, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True) &nbsp; &nbsp;)#倒残差class InvertedResidual(nn.Module): &nbsp; &nbsp;def __init__(self, inp, oup, stride, expand_ratio):#参数分别是输入特征图数，输出特征图数，步长，扩展比例 &nbsp; &nbsp; &nbsp; &nbsp;super(InvertedResidual, self).__init__() &nbsp; &nbsp; &nbsp; &nbsp;self.stride = stride &nbsp; #步长 &nbsp; &nbsp; &nbsp; &nbsp;assert stride in [1, 2] &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hidden_dim = round(inp * expand_ratio)#隐藏层层数，expand_ratio为拓展倍数 &nbsp; &nbsp; &nbsp; &nbsp;self.use_res_connect = (self.stride == 1 and inp == oup)#是否进跳跃链接 &nbsp; &nbsp; &nbsp; &nbsp;if expand_ratio == 1: &nbsp;#不进行扩展 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.conv = nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# dw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),#可分离卷积 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw-linear &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;) &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.conv = nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# dw &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw-linear &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;) &nbsp; &nbsp;def forward(self, x): &nbsp; &nbsp; &nbsp; &nbsp;if self.use_res_connect: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return x + self.conv(x) &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return self.conv(x)class MobileNetV2(nn.Module): &nbsp; &nbsp;def __init__(self, n_class=1000, input_size=224, width_mult=1.): &nbsp; &nbsp; &nbsp; &nbsp;super(MobileNetV2, self).__init__() &nbsp; &nbsp; &nbsp; &nbsp;block = InvertedResidual#创建一个倒残差对象 &nbsp; &nbsp; &nbsp; &nbsp;input_channel = 32 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;last_channel = 1280 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&#39;&#39;&#39; &nbsp; &nbsp; &nbsp; &nbsp;t :是输入通道的倍增系数（即中间部分的通道数是输入通道数的多少倍） &nbsp; &nbsp; &nbsp; &nbsp;n :是该模块重复次数 &nbsp; &nbsp; &nbsp; &nbsp;c :是输出通道数 &nbsp; &nbsp; &nbsp; &nbsp;s :是该模块第一次重复时的 stride（后面重复都是 stride 1） &nbsp; &nbsp; &nbsp; &nbsp;&#39;&#39;&#39; &nbsp; &nbsp; &nbsp; &nbsp;interverted_residual_setting = [ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# t, c, n, s &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[1, 16, 1, 1], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 24, 2, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 32, 3, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 64, 4, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 96, 3, 1], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 160, 3, 2], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[6, 320, 1, 1], &nbsp; &nbsp; &nbsp; &nbsp;] &nbsp; &nbsp; &nbsp; &nbsp;# building first layer &nbsp; &nbsp; &nbsp; &nbsp;assert input_size % 32 == 0 &nbsp; &nbsp; &nbsp; &nbsp;input_channel = int(input_channel * width_mult) &nbsp; &nbsp; &nbsp; &nbsp;self.last_channel = int(last_channel * width_mult) if width_mult &gt; 1.0 else last_channel &nbsp; &nbsp; &nbsp; &nbsp;self.features = [conv_bn(3, input_channel, 2)] &nbsp; &nbsp; &nbsp; &nbsp;# building inverted residual blocks &nbsp; &nbsp; &nbsp; &nbsp;for t, c, n, s in interverted_residual_setting: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;output_channel = int(c * width_mult) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for i in range(n): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if i == 0: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(block(input_channel, output_channel, s, expand_ratio=t)) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;else: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(block(input_channel, output_channel, 1, expand_ratio=t)) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;input_channel = output_channel &nbsp; &nbsp; &nbsp; &nbsp;# building last several layers &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(conv_1x1_bn(input_channel, self.last_channel)) &nbsp; &nbsp; &nbsp; &nbsp;# make it nn.Sequential &nbsp; &nbsp; &nbsp; &nbsp;self.features = nn.Sequential(*self.features) &nbsp; &nbsp; &nbsp; &nbsp;# building classifier &nbsp; &nbsp; &nbsp; &nbsp;self.classifier = nn.Sequential( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Dropout(0.2), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Linear(self.last_channel, n_class), &nbsp; &nbsp; &nbsp; &nbsp;) &nbsp; &nbsp; &nbsp; &nbsp;self._initialize_weights() &nbsp; &nbsp;def forward(self, x): &nbsp; &nbsp; &nbsp; &nbsp;x = self.features(x) &nbsp; &nbsp; &nbsp; &nbsp;x = x.mean(3).mean(2) &nbsp; &nbsp; &nbsp; &nbsp;x = self.classifier(x) &nbsp; &nbsp; &nbsp; &nbsp;return x &nbsp; &nbsp;def _initialize_weights(self): &nbsp; &nbsp; &nbsp; &nbsp;for m in self.modules(): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if isinstance(m, nn.Conv2d): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.normal_(0, math.sqrt(2. / n)) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;if m.bias is not None: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_() &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;elif isinstance(m, nn.BatchNorm2d): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.fill_(1) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_() &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;elif isinstance(m, nn.Linear): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n = m.weight.size(1) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.normal_(0, 0.01) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_()model= MobileNetV2()print(model) 参考文献：&nbsp;https://blog.csdn.net/t800ghb/article/details/78879612&nbsp;https://www.cnblogs.com/CodingML-1122/p/9043078.html&nbsp;https://blog.csdn.net/u011995719/article/details/79135818&nbsp;https://zhuanlan.zhihu.com/p/33075914&nbsp;https://zhuanlan.zhihu.com/p/39386719&nbsp;https://mp.weixin.qq.com/s/T6S1_cFXPEuhRAkJo2m8Ig&nbsp;https://blog.csdn.net/qq_31531635/article/details/80550412&nbsp;https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py &nbsp; END 往期回顾之作者石文华 【1】《Computer vision》笔记-SqueezeNet（6） 【2】《Computer vision》笔记-GoodLeNet（3） 【3】&nbsp;干货|（DL~2)一看就懂的卷积神经网络 【4】《Computer vision》笔记-Xception(5) 【5】&nbsp;干货|（DL~4）对象定位和检测 机器学习算法工程师 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 一个用心的公众号 长按，识别，加关注 进群，学习，得帮助 你的关注，我们的热度， 我们一定给你学习最大的帮助 你点的每个赞，我都认真当成了喜欢","@type":"BlogPosting","url":"https://mlh.app/2019/02/11/5cdbc3ca05f9b090887f8c567ea4bcf0.html","headline":"《Computer vision》笔记-MobileNet（7）","dateModified":"2019-02-11T00:00:00+08:00","datePublished":"2019-02-11T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/02/11/5cdbc3ca05f9b090887f8c567ea4bcf0.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>《Computer vision》笔记-MobileNet（7）</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="rich_media_content" id="js_content"> 
   <p><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/LiaGhAsRNttuXh4oBW56OVg6Wia5gGODPwpcpqq6v5Tia4cyVibRoAbW2gvojzicpWibnOG99Diaic8bX7Mvsor9S6BiaLA/640?wx_fmt=gif" alt="640?wx_fmt=gif"></p>
   <p><br></p>
   <p style="color:rgb(108,105,105);font-size:14px;text-align:right;">&nbsp; &nbsp;作者：石文华&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br></p>
   <p style="color:rgb(108,105,105);font-size:14px;text-align:right;"><span style="color:rgb(89,89,89);">编辑：陈人和&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;<br></span></p>
   <p><span style="color:rgb(89,89,89);"><br></span></p>
   <p><strong class="135brush">前 &nbsp;言</strong></p>
   <br>
   <span>移动端和其他嵌入式系统通常是内存空间小，能耗要求低的，也就是计算资源有限。一般的模型，比如ImageNet数据集上训练的VGG,googlenet,resnet等，需要巨大的计算资源，所以很难用在嵌入式系统上。MobileNet是一种高效的模型，用于移动和嵌入式视觉应用。</span>
   <br>
   <strong>章节目录<br><br></strong>
   <ul class="list-paddingleft-2">
    <li><p>深度可分离卷积（depthwise separable convolution）</p></li>
    <li><p>宽度因子和分辨率因子</p></li>
    <li><p>改进（MobileNet V2)</p></li>
   </ul>
   <p><br></p>
   <p><br></p>
   <p style="min-height:1em;"><strong>01</strong></p>
   <h1 style="letter-spacing:1.5px;line-height:1.75em;text-align:center;"><span style="color:rgb(0,122,170);"><strong><span style="font-size:15px;">深度可分离卷积（depthwise separable convolution)</span></strong></span></h1>
   <p><span style="font-size:15px;color:rgb(68,68,68);"></span>MobileNet使用技术之一是深度可分离卷积(depthwise separable convolution)代替传统的3D卷积操作，这样可以减少参数数量以及卷积过程中的计算量，但是也会导致特征丢失，使得精度下降。MobileNet其实就是Xception思想的应用。区别就是Xception文章重点在提高精度，而MobileNet重点在压缩模型。</p>
   <p><span style="text-indent:0em;">假设输入特征图有M个，大小为DF，输出的特征图是N个，卷积核尺寸是Dk*Dk,那么传统的3D卷积的卷积核是立体的，也就是每一个卷积核都是Dk*Dk*M，总共有N个Dk*Dk*M的卷积核，如下图所示：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5gjwonVL3f3fRyPGtbgIsjayPueXqNrniaeN3ic7FdR97gCLqCdpYbYkcA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p><span style="text-indent:0em;"></span><span style="text-indent:0em;">所以总的参数个数为Dk*Dk*M*N,假设输出使用的padding参数是same,则输出特征图大小也是DF，那么总的计算量为Dk*Dk*M*N*DF*DF。&nbsp;<br style="color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;font-size:16px;line-height:27px;">而MobileNet将普通卷积操作分为两部分，第一部分是逐通道卷积，使用M个通道数为1，大小为Dk*Dk的卷积核，每一个卷积核负责其中的一个通道。如下图所示：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5gkiacGpZaLxRKO6VmEQB58icS9yrYsiazV6Oo2VAObe9DjFH2HR42z1npQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p><span style="text-indent:0em;">逐通道卷积之后的参数个数为：Dk*Dk*M，同样假设padding为same,则计算量为：Dk*Dk*M*DF*DF&nbsp;<br></span></p>
   <p><span style="text-indent:0em;">第二部分是点卷积，也就是采用3D的1*1卷积改变通道数，对深度方向上的特征信息进行组合，最终将输出通道数变为指定的数。如下图所示：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5gLSPTliaT6D5dv2IBb0fxVpWMSofcqzd8enhAVnl1Y7YIlT1CYicjC2tw/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p><span style="text-indent:0em;"></span><span style="text-indent:0em;">这部分的参数个数为：M*N，padding为same时的计算量为M*N*DF*DF.&nbsp;<br style="color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;font-size:16px;line-height:27px;">因此这种分离之后的总的计算量为：：Dk*Dk*M*DF*DF+M*N*DF*DF。&nbsp;<br style="color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;font-size:16px;line-height:27px;">深度可分离卷积跟传统3D卷积计算量的比例为</span><span style="text-indent:0em;">：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5g8Nb5FbSzEzIRaRKiaW0JorACtfpP5qXib4ochO0eHcg8ibzomptoMcl4Q/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p><span style="text-indent:0em;">如下图，左边是3D卷积常见结构，右边是深度可分离卷积的使用方式：</span></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5guw4oVribdotBAkVU7h0kbWbJDyFzaZdSibavBfDEfyKDaOI9UAEYcDicg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p><br><span style="text-indent:0em;"></span></p>
   <p style="min-height:1em;"><strong>02</strong></p>
   <h1 style="letter-spacing:1.5px;line-height:1.75em;text-align:center;"><span style="color:rgb(0,122,170);"><strong><span style="font-size:15px;">宽度因子和分辨率因子</span></strong></span></h1>
   <h1 style="letter-spacing:1.5px;line-height:1.75em;text-align:left;">MobileNet有两个简单的全局超参数，分别是宽度因子和分辨率因子，可有效的在延迟和准确率之间做折中。允许我们依据约束条件选择合适大小、低延迟、易满足嵌入式设备要求的模型。<span style="font-size:17px;text-align:justify;">&nbsp;</span></h1>
   <p>（1）宽度因子</p>
   <p>上述的逐通道卷积的卷积核个数通常是M，也就是Dk*Dk*1的卷积核个数等于输入通道数，宽度因子是一个参数为（0，1]之间的参数，作用于通道数，可以理解为按照比例缩减输入通道数。同理，输出的通道数也可以通过这个参数进行按比例缩减。用α表示这个参数，则计算量为：</p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5gM7Ir4qeosN6kQHSn8iaplRJfSbvSOsZsLYBBlrwhpibA3zKQI3KU9jKQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p>不同的α取值在ImageNet上的准确率，下图为准确率，参数数量和计算量之间的权衡情况：</p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5g8gPYVsVUcJhqc2evo0YG96nia4s0r11P8PlliccicRNc9hamGBSHyz08g/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p>（2）分辨因子</p>
   <p>上述的输入特征图大小为DF*DF,分辨率因子取值范围在(0,1]之间，可以理解为对特征图进行下采样，也就是按比例降低特征图的大小，使得输入数据以及由此在每一个模块产生的特征图都变小，用β表示这个参数，结合宽度因子α，则计算量为：</p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5g4kfTpQND2UUPwp4Fib8sDCVUkCPYfohDtAGdI3U6BfXHAgcP4AK6Vzg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p>不同的β系数作用于标准MobileNet时，对精度和计算量以的影响（α固定）：</p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5gyCjYujsX6fLzWuyia21PSBD6dtJuSwX8FibPVqxibs6DfPicOZFJYw9ibWw/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p><br></p>
   <p style="min-height:1em;"><strong>03</strong></p>
   <h1 style="letter-spacing:1.5px;line-height:1.75em;text-align:center;"><span style="color:rgb(0,122,170);"><strong><span style="font-size:15px;">改进（MobileNet V2）</span></strong></span></h1>
   <p><span style="color:rgb(0,122,170);"><span style="font-size:15px;"></span></span></p>
   <p style="margin-left:8px;min-height:1em;letter-spacing:1.5px;line-height:1.75em;"><span style="font-size:15px;color:rgb(68,68,68);"></span>在 MobileNet-V1 基础上结合当下流行的残差思想而设计,V2 主要引入了两个改动：Linear Bottleneck 和 Inverted Residual Blocks。&nbsp;</p>
   <p style="margin-left:8px;min-height:1em;letter-spacing:1.5px;line-height:1.75em;">V1与V2的结构对比：</p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5gqL4tsJXGkqAAhUJ7Rq8iaelHnShDFJtygqaZyIKNezBoUyePfTZV3Sw/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="min-height:1em;letter-spacing:1.5px;line-height:1.75em;">两者相同的地方在于都采用 Depth-wise (DW) 卷积搭配 Point-wise (PW) 卷积的方式来提特征。&nbsp;</p>
   <p style="min-height:1em;letter-spacing:1.5px;line-height:1.75em;">（1）改进一：&nbsp;</p>
   <p style="min-height:1em;letter-spacing:1.5px;line-height:1.75em;">V2在 DW 卷积之前新加了一个 PW 卷积，主要目的是为了提升维度数。相比V1直接在每个通道上单独提取特征，V2的这种做法能够先组合不同深度上的特征，并升维，使得特征更加丰富。比直接DW的话，特征提取的效果更好。&nbsp;</p>
   <p style="min-height:1em;letter-spacing:1.5px;line-height:1.75em;">（2）改进二：&nbsp;</p>
   <p style="min-height:1em;letter-spacing:1.5px;line-height:1.75em;">V2 去掉了第二个 PW 的激活函数。论文作者称其为 Linear Bottleneck。这么做的原因，是因为作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，由Relu的性质，Relu对于负的输入，输出全为零，所以relu会使得一部分特征失效。由于第二个 PW 的主要功能就是降维，再经过Relu的话，又要“损失”一部分特征，因此按照上面的理论，降维之后就不宜再使用 Relu了。如下图所示，一个原始螺旋形被利用随机矩阵T经过ReLU后嵌入到一个n维空间中，然后使用T−1投影到2维空间中。例子中，n=2,3导致信息损失，可以看到流形的中心点之间互相坍塌。同时n=15,30时信息变成高度非凸。</p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5gCT0iaG5cMTcMgOU6sqz88U0eAQKwviakbVQoJ5o3DAfLywt7hu4iayCgQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="min-height:1em;letter-spacing:1.5px;line-height:1.75em;">（3）使用短路连接: 倒残差(Inverted Residual)&nbsp;<br style="color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;font-size:16px;line-height:27px;">典的残差块是：1x1(压缩)-&gt;3x3(卷积)-&gt;1x1(升维)，而inverted residual顾名思义是颠倒的残差：1x1(升维)-&gt;3x3(dw conv+relu)-&gt;1x1(降维+线性变换)，skip-connection(跳过连接)是在低维的瓶颈层间发生的(如下图)，这对于移动端有限的宽带是有益的。连接情况如下图所示(shortcut只在stride==1时才使用)：</p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5gkY9zZBkYiase0YVCVuV4uwOBAnbt4lEyYfORYydViclaQa7hA42BrOfQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="min-height:1em;letter-spacing:1.5px;line-height:1.75em;">（4）网络结构如下：</p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDsRJnjnFkHH41mt0UUfn5glYvj4SZDvhRLzTibMxu4FjSRnobV3edVTI8vmsgaIhyQm8zQuQyMrfA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="min-height:1em;letter-spacing:1.5px;line-height:1.75em;">由于笔者水平有限，可参考如下github上的代码：https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py</p>
   <p style="min-height:1em;letter-spacing:1.5px;line-height:1.75em;"><br></p>
   <pre><code style="font-size:.85em;font-family:Consolas, Menlo, Courier, monospace;color:rgb(171,178,191);min-width:400px;background:rgb(40,44,52) none repeat scroll 0% 0%;font-weight:400;" class="c hljs cpp"><span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">import</span> torch.nn as nn<br><span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">import</span> math<br>#传统的<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">3</span>D卷积<br><span class="hljs-function" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">def <span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">conv_bn</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">(inp, oup, stride)</span>:<br> &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">return</span> nn.<span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">Sequential</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">(<br> &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, oup, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">3</span>, stride, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, bias=False)</span>,#卷积<br> &nbsp; &nbsp; &nbsp; &nbsp;nn.<span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">BatchNorm2d</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">(oup)</span>, &nbsp;<span class="hljs-meta" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">#bn</span><br> &nbsp; &nbsp; &nbsp; &nbsp;nn.<span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">ReLU6</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">(inplace=True)</span>#relu6<br> &nbsp; &nbsp;)<br>#1x1的点卷积<br>def <span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">conv_1x1_bn</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">(inp, oup)</span>:<br> &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">return</span> nn.<span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">Sequential</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">(<br> &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, oup, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">0</span>, bias=False)</span>,<br> &nbsp; &nbsp; &nbsp; &nbsp;nn.<span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">BatchNorm2d</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">(oup)</span>,<br> &nbsp; &nbsp; &nbsp; &nbsp;nn.<span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">ReLU6</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">(inplace=True)</span><br> &nbsp; &nbsp;)<br>#倒残差<br><span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">class</span> <span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">InvertedResidual</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">(nn.Module)</span>:<br> &nbsp; &nbsp;def __<span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">init__</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">(self, inp, oup, stride, expand_ratio)</span>:#参数分别是输入特征图数，输出特征图数，步长，扩展比例<br> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">super</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">(InvertedResidual, self)</span>.__<span class="hljs-title" style="color:rgb(97,174,238);font-weight:400;font-style:normal;">init__</span><span class="hljs-params" style="color:rgb(171,178,191);font-weight:400;font-style:normal;">()</span><br> &nbsp; &nbsp; &nbsp; &nbsp;self.stride </span>= stride &nbsp; #步长<br> &nbsp; &nbsp; &nbsp; &nbsp;assert stride in [<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">2</span>] &nbsp; <br> &nbsp; &nbsp; &nbsp; &nbsp;hidden_dim = round(inp * expand_ratio)#隐藏层层数，expand_ratio为拓展倍数<br> &nbsp; &nbsp; &nbsp; &nbsp;self.use_res_connect = (self.stride == <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span> and inp == oup)#是否进跳跃链接<br> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">if</span> expand_ratio == <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>: &nbsp;#不进行扩展<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.conv = nn.Sequential(<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# dw<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, hidden_dim, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">3</span>, stride, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, groups=hidden_dim, bias=False),#可分离卷积<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw-linear<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, oup, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">0</span>, bias=False), &nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)<br> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">else</span>:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.conv = nn.Sequential(<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(inp, hidden_dim, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">0</span>, bias=False),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# dw<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, hidden_dim, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">3</span>, stride, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, groups=hidden_dim, bias=False),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(hidden_dim),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.ReLU6(inplace=True),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# pw-linear<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Conv2d(hidden_dim, oup, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">0</span>, bias=False),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.BatchNorm2d(oup),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;)<br> &nbsp; &nbsp;def forward(self, x):<br> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">if</span> self.use_res_connect:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">return</span> x + self.conv(x)<br> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">else</span>:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">return</span> self.conv(x)<br><span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">class</span> MobileNetV2(nn.Module):<br> &nbsp; &nbsp;def __init__(self, n_class=<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1000</span>, input_size=<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">224</span>, width_mult=<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1.</span>):<br> &nbsp; &nbsp; &nbsp; &nbsp;super(MobileNetV2, self).__init__()<br> &nbsp; &nbsp; &nbsp; &nbsp;block = InvertedResidual#创建一个倒残差对象<br> &nbsp; &nbsp; &nbsp; &nbsp;input_channel = <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">32</span> &nbsp; <br> &nbsp; &nbsp; &nbsp; &nbsp;last_channel = <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1280</span> &nbsp; <br> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string" style="color:rgb(152,195,121);font-weight:400;font-style:normal;">'''</span><br> &nbsp; &nbsp; &nbsp; &nbsp;t :是输入通道的倍增系数（即中间部分的通道数是输入通道数的多少倍）<br> &nbsp; &nbsp; &nbsp; &nbsp;n :是该模块重复次数<br> &nbsp; &nbsp; &nbsp; &nbsp;c :是输出通道数<br> &nbsp; &nbsp; &nbsp; &nbsp;s :是该模块第一次重复时的 stride（后面重复都是 stride <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>）<br> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-string" style="color:rgb(152,195,121);font-weight:400;font-style:normal;">'''</span><br> &nbsp; &nbsp; &nbsp; &nbsp;interverted_residual_setting = [<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;# t, c, n, s<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">16</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>],<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">6</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">24</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">2</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">2</span>],<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">6</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">32</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">3</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">2</span>],<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">6</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">64</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">4</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">2</span>],<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">6</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">96</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">3</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>],<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">6</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">160</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">3</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">2</span>],<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;[<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">6</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">320</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>],<br> &nbsp; &nbsp; &nbsp; &nbsp;]<br> &nbsp; &nbsp; &nbsp; &nbsp;# building first layer<br> &nbsp; &nbsp; &nbsp; &nbsp;assert input_size % <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">32</span> == <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">0</span><br> &nbsp; &nbsp; &nbsp; &nbsp;input_channel = <span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">int</span>(input_channel * width_mult)<br> &nbsp; &nbsp; &nbsp; &nbsp;self.last_channel = <span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">int</span>(last_channel * width_mult) <span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">if</span> width_mult &gt; <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1.0</span> <span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">else</span> last_channel<br> &nbsp; &nbsp; &nbsp; &nbsp;self.features = [conv_bn(<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">3</span>, input_channel, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">2</span>)]<br> &nbsp; &nbsp; &nbsp; &nbsp;# building inverted residual blocks<br> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">for</span> t, c, n, s in interverted_residual_setting:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;output_channel = <span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">int</span>(c * width_mult)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">for</span> i in range(n):<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">if</span> i == <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">0</span>:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(block(input_channel, output_channel, s, expand_ratio=t))<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">else</span>:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(block(input_channel, output_channel, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>, expand_ratio=t))<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;input_channel = output_channel<br> &nbsp; &nbsp; &nbsp; &nbsp;# building last several layers<br> &nbsp; &nbsp; &nbsp; &nbsp;self.features.append(conv_1x1_bn(input_channel, self.last_channel))<br> &nbsp; &nbsp; &nbsp; &nbsp;# make it nn.Sequential<br> &nbsp; &nbsp; &nbsp; &nbsp;self.features = nn.Sequential(*self.features)<br> &nbsp; &nbsp; &nbsp; &nbsp;# building classifier<br> &nbsp; &nbsp; &nbsp; &nbsp;self.classifier = nn.Sequential(<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Dropout(<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">0.2</span>),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;nn.Linear(self.last_channel, n_class),<br> &nbsp; &nbsp; &nbsp; &nbsp;)<br> &nbsp; &nbsp; &nbsp; &nbsp;self._initialize_weights()<br> &nbsp; &nbsp;def forward(self, x):<br> &nbsp; &nbsp; &nbsp; &nbsp;x = self.features(x)<br> &nbsp; &nbsp; &nbsp; &nbsp;x = x.mean(<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">3</span>).mean(<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">2</span>)<br> &nbsp; &nbsp; &nbsp; &nbsp;x = self.classifier(x)<br> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">return</span> x<br> &nbsp; &nbsp;def _initialize_weights(self):<br> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">for</span> m in self.modules():<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">if</span> isinstance(m, nn.Conv2d):<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n = m.kernel_size[<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">0</span>] * m.kernel_size[<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>] * m.out_channels<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.normal_(<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">0</span>, math.<span class="hljs-built_in" style="color:rgb(230,192,123);font-weight:400;font-style:normal;">sqrt</span>(<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">2.</span> / n))<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-keyword" style="color:rgb(198,120,221);font-weight:400;font-style:normal;">if</span> m.bias is not None:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_()<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;elif isinstance(m, nn.BatchNorm2d):<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.fill_(<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_()<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;elif isinstance(m, nn.Linear):<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;n = m.weight.size(<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">1</span>)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.weight.data.normal_(<span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">0</span>, <span class="hljs-number" style="color:rgb(209,154,102);font-weight:400;font-style:normal;">0.01</span>)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;m.bias.data.zero_()<br>model= MobileNetV2()<br>print(model)</code></pre>
   <p><br></p>
   <p style="min-height:1em;letter-spacing:1.5px;line-height:1.75em;"><br></p>
   <p style="min-height:1em;letter-spacing:1.5px;line-height:1.75em;"><span style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">参考文献：&nbsp;</span><br style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">https://blog.csdn.net/t800ghb/article/details/78879612<span style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">&nbsp;</span><br style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">https://www.cnblogs.com/CodingML-1122/p/9043078.html<span style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">&nbsp;</span><br style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">https://blog.csdn.net/u011995719/article/details/79135818<span style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">&nbsp;</span><br style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">https://zhuanlan.zhihu.com/p/33075914<span style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">&nbsp;</span><br style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">https://zhuanlan.zhihu.com/p/39386719<span style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">&nbsp;</span><br style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;"><a href="https://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247487205&amp;idx=1&amp;sn=2d6c27aab0b4b2d56eec9b06d7468ea2&amp;scene=21#wechat_redirect" rel="nofollow" style="font-size:16px;color:rgb(0,136,204);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">https://mp.weixin.qq.com/s/T6S1_cFXPEuhRAkJo2m8Ig</a><span style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">&nbsp;</span><br style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">https://blog.csdn.net/qq_31531635/article/details/80550412<span style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">&nbsp;</span><br style="font-size:16px;color:rgb(44,62,80);font-family:'PingFang SC', 'Hiragino Sans GB', 'Helvetica Neue', 'Microsoft Yahei', 'WenQuanYi Micro Hei', sans-serif;">https://github.com/tonylins/pytorch-mobilenet-v2/blob/master/MobileNetV2.py</p>
   <p style="min-height:1em;text-align:center;"><br></p>
   <p style="min-height:1em;text-align:center;"><br></p>
   <p style="min-height:1em;text-align:center;"><br></p>
   <p style="min-height:1em;text-align:center;"><strong style="line-height:28px;"><span style="line-height:1.75em;font-size:15px;color:rgb(171,25,66);"><strong style="color:rgb(62,62,62);font-size:16px;line-height:28px;"><span style="line-height:1.75em;color:rgb(63,63,63);font-size:14px;letter-spacing:0px;text-align:justify;">&nbsp;</span></strong><span style="color:rgb(63,63,63);font-size:14px;letter-spacing:0px;text-align:justify;"><strong style="text-align:center;color:rgb(62,62,62);font-size:16px;line-height:28px;"><span style="line-height:1.75em;font-size:15px;color:rgb(171,25,66);"><img class="__bg_gif" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/TCHicQEF6XKANicUCsKbWsXv1yJgVCSSRGucMYaHPrsrDRFNbNUVibEic1qJC34XVssCm5k1NiaPULLZZOvuIWHn5eg/640?wx_fmt=gif" alt="640?wx_fmt=gif"></span></strong></span></span></strong></p>
   <p style="text-align:center;"><span style="color:rgb(0,122,170);"><span style="font-size:18px;letter-spacing:1.5px;"><strong>END</strong></span></span></p>
   <p><span style="color:rgb(0,122,170);"><span style="font-size:18px;letter-spacing:1.5px;"><strong><br></strong></span></span></p>
   <p><span style="color:rgb(0,122,170);"><span style="font-size:18px;letter-spacing:1.5px;"><strong><br></strong></span></span></p>
   <p><span style="color:rgb(0,122,170);"><span style="font-size:18px;letter-spacing:1.5px;"><strong><br></strong></span></span></p>
   <p><span style="color:rgb(0,122,170);"><span style="font-size:18px;letter-spacing:1.5px;"><strong><br></strong></span></span></p>
   <p><span style="color:rgb(0,122,170);"><span style="font-size:18px;letter-spacing:1.5px;"><strong><br></strong></span></span></p>
   <p><span style="color:rgb(0,122,170);"><span style="font-size:18px;letter-spacing:1.5px;"><strong><br></strong></span></span></p>
   <p><span style="color:rgb(0,122,170);"><span style="font-size:18px;letter-spacing:1.5px;"><strong><br></strong></span></span></p>
   <p style="min-height:1em;">往期回顾之作者石文华</p>
   <p style="min-height:1em;">【1】<a href="http://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247487394&amp;idx=1&amp;sn=64e2a4b096fbee0343a7a96fba591bc1&amp;chksm=f9d1513acea6d82c629e79e6a452841c783fa3c9aa5b105bf196f9189f692651aaacdb9564fd&amp;scene=21#wechat_redirect" rel="nofollow">《Computer vision》笔记-SqueezeNet（6）</a></p>
   <p style="min-height:1em;">【2】<a href="http://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247487349&amp;idx=2&amp;sn=06016388e317e06da8ccc581c59a9c08&amp;chksm=f9d151edcea6d8fb1c791a341adea98b45604caf6f508632632b4b39cf567ee85830e70457fc&amp;scene=21#wechat_redirect" rel="nofollow">《Computer vision》笔记-GoodLeNet（3）</a></p>
   <p style="min-height:1em;">【3】&nbsp;<a href="http://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247487327&amp;idx=2&amp;sn=62f75fd674cf7301ef2dced0375b93d2&amp;chksm=f9d151c7cea6d8d12d45fe67c48e1651ec450d59ee43fa0dc4a8bc033b6a2845d186d084f6e6&amp;scene=21#wechat_redirect" rel="nofollow">干货|（DL~2)一看就懂的卷积神经网络</a></p>
   <p style="min-height:1em;">【4】<a href="http://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247487233&amp;idx=1&amp;sn=49a46cdd741b9d961a7627ca65d17005&amp;chksm=f9d15199cea6d88fd9ae6b8217b128ecd80e3f3315fda1141df0428e9620de41add505f499e1&amp;scene=21#wechat_redirect" rel="nofollow">《Computer vision》笔记-Xception(5)</a></p>
   <p style="min-height:1em;">【5】&nbsp;<a href="http://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&amp;mid=2247487183&amp;idx=1&amp;sn=176face642faaed7656c72902e12490e&amp;chksm=f9d15057cea6d9419fffd22b1eee289f90e4f12a7111dccc86274457d0b96b307247dfca17ee&amp;scene=21#wechat_redirect" rel="nofollow">干货|（DL~4）对象定位和检测</a></p>
   <p style="min-height:1em;"><br></p>
   <br>
   <p style="min-height:1em;text-align:center;"><strong style="line-height:28px;"><span style="line-height:1.75em;font-size:15px;color:rgb(171,25,66);"><span style="color:rgb(63,63,63);font-size:14px;letter-spacing:0px;text-align:justify;"><strong style="text-align:center;color:rgb(62,62,62);font-size:16px;line-height:28px;"><span style="line-height:1.75em;font-size:15px;color:rgb(171,25,66);"></span></strong></span></span></strong><br></p>
   <br>
   <p><br></p>
   <p><br></p>
   <p><br></p>
   <p><br></p>
   <p style="min-height:1em;"><span style="color:rgb(3,3,3);font-size:20px;"><strong>机器学习算法工程师</strong></span></p>
   <hr style="border-color:rgb(33,33,34);">
   <p style="min-height:1em;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 一个用心的公众号</p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/iaTa8ut6HiawDdZMYspr4Sg6JgNEHRRRaZ7Bjjv4zo9GabzO4PkUILEGkyC7odlWMVEl6rsbfkr9PduYMbnQFZEA/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></p>
   <span style="color:rgb(12,12,12);">长按，识别，加关注<br></span>
   <p style="min-height:1em;"><span style="color:rgb(12,12,12);">进群，学习，得帮助</span></p>
   <p style="min-height:1em;"><span style="color:rgb(12,12,12);"></span>你的关注，我们的热度，</p>
   <p style="min-height:1em;">我们一定给你学习最大的帮助</p>
   <p style="clear:none;min-height:1em;"><br></p>
   <p style="min-height:1em;font-size:16px;letter-spacing:.544px;text-align:center;"><br></p>
   <img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/iaTa8ut6HiawDGYbtuibKFBo6VnucFVutzfOoMhtaKlbLRiaKvAz4APHHLxHoficEoG2oP0Cib6QLXibw9gshAibp1kVAA/640?wx_fmt=png" alt="640?wx_fmt=png">你点的每个赞，我都认真当成了喜欢
   <p><br></p> 
  </div> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
