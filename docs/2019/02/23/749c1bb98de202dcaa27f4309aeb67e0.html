<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>【PyTorch学习笔记】6：Broadcasting,对Tensor的合并与拆分,Tensor运算 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="【PyTorch学习笔记】6：Broadcasting,对Tensor的合并与拆分,Tensor运算" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="版权声明：本文为博主原创学习笔记，如需转载请注明来源。 https://blog.csdn.net/SHU15121856/article/details/87886885 Broadcasting Broadcasting也就和之前学MPI时候学的广播一样，能够实现自动维度扩展，有点像上节学的expand的功能，但是是自动完成的，而且不需要像repeat那样对数据进行拷贝，可以节省内存。 从最后面的维度开始匹配。 在前面插入若干维度。 将维度的size从1通过expand变到和某个Tensor相同的维度。 总之，Broadcasting也就是自动实现了若干unsqueeze和expand操作，以使两个Tensor的shape一致，从而完成某些操作（往往是加法）。 例如 [ b a t c h , c h a n n e l , h e i g h t , w i d t h ] = [ 4 , 3 , 32 , 32 ] [batch,channel,height,width]=[4,3,32,32] [batch,channel,height,width]=[4,3,32,32]：如果加上一个 [ 32 , 32 ] [32,32] [32,32]的Tensor，意思就是不管是4张图片中的哪一张，也不管是什么通道，都加上一个32*32的Feature Map；如果加上一个 [ 3 , 1 , 1 ] [3,1,1] [3,1,1]，意思就是对3个通道分别加上一个值；如果加上一个 [ 1 , 1 , 1 , 1 ] [1,1,1,1] [1,1,1,1]，这和加上一个 [ 1 ] [1] [1]一样，也就是对每个元素都加上一个相同的Bais值。 对Tensor的合并 维度合并(cat) 要保证其它维度的size是相同的。 import torch a = torch.rand(4, 32, 8) b = torch.rand(5, 32, 8) print(torch.cat([a, b], dim=0).shape) 运行结果： torch.Size([9, 32, 8] 合并新增(stack) stack需要保证两个Tensor的shape是一致的，这就像是有两类东西，它们的其它属性都是一样的（比如男的一张表，女的一张表）。使用stack时候要指定一个维度位置，在那个位置前会插入一个新的维度，因为是两类东西合并过来所以这个新的维度size是2，通过指定这个维度是0或者1来选择性别是男还是女。 c = torch.rand(4, 3, 32, 32) d = torch.rand(4, 3, 32, 32) print(torch.stack([c, d], dim=2).shape) print(torch.stack([c, d], dim=0).shape) 运行结果： torch.Size([4, 3, 2, 32, 32]) torch.Size([2, 4, 3, 32, 32]) 对Tensor的拆分 按照size的长度拆分(split) 对一个Tensor而言，要拆分的那个维度的size就是&quot;这个维度的总长度&quot;了，可以指定拆分后的几个Tensor各取多少长度，或者指定每个Tensor取多少长度。 import torch a = torch.rand(2, 4, 3, 32, 32) a1, a2 = a.split(1, dim=0) # 对0号维度拆分,拆分后每个Tensor取长度1 print(a1.shape, a2.shape) b = torch.rand(4, 3, 32, 32) b1, b2 = b.split([2, 1], dim=1) # 对1号维度拆分,拆分后第一个维度取2,第二个维度取1 print(b1.shape, b2.shape) 运行结果： torch.Size([1, 4, 3, 32, 32]) torch.Size([1, 4, 3, 32, 32]) torch.Size([4, 2, 32, 32]) torch.Size([4, 1, 32, 32]) 按照份数等量拆分(chunk) 给定在指定的维度上要拆分得的份数，就会按照指定的份数尽量等量地进行拆分。 c = torch.rand(7, 4) c1, c2, c3, c4 = c.chunk(4, dim=0) print(c1.shape, c2.shape, c3.shape, c4.shape) 运行结果： torch.Size([2, 4]) torch.Size([2, 4]) torch.Size([2, 4]) torch.Size([1, 4]) Tensor运算 相加 import torch # 这两个Tensor加减乘除会对b自动进行Broadcasting a = torch.rand(3, 4) b = torch.rand(4) c1 = a + b c2 = torch.add(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果： torch.Size([3, 4]) torch.Size([3, 4]) tensor(1, dtype=torch.uint8) 相减 a = torch.rand(3, 4) b = torch.rand(4) c1 = a - b c2 = torch.sub(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果：同上 哈达玛积(element wise) c1 = a * b c2 = torch.mul(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果：同上 相除 c1 = a / b c2 = torch.div(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果：同上 矩阵乘法(matrix mul) 使用Torch.mm（只能用于2d的Tensor）或者Torch.matmul（和符号@效果一样）。 dim=2 import torch a = torch.ones(2, 1) b = torch.ones(1, 2) print(torch.mm(a, b).shape) print(torch.matmul(a, b).shape) print((a @ b).shape) 运行结果： torch.Size([2, 2]) torch.Size([2, 2]) torch.Size([2, 2]) dim&gt;2 对于高维的Tensor，定义其矩阵乘法仅在最后的两个维度上，要求前面的维度必须保持一致，就像矩阵的索引一样。 c = torch.rand(4, 3, 28, 64) d = torch.rand(4, 3, 64, 32) print(torch.matmul(c, d).shape) 运行结果： torch.Size([4, 3, 28, 32]) 注意，在这种情形下的矩阵相乘，前面的&quot;矩阵索引维度&quot;如果符合Broadcasting机制，也会自动做广播，然后相乘。 c = torch.rand(4, 3, 28, 64) d = torch.rand(4, 1, 64, 32) print(torch.matmul(c, d).shape) 运行结果： torch.Size([4, 3, 28, 32]) 幂和开方 import torch a = torch.full([2, 2], 3) b = a.pow(2) # 也可以a**2 print(b) c = b.sqrt() # 也可以a**(0.5) print(c) d = b.rsqrt() # 平方根的倒数 print(d) 运行结果： tensor([[9., 9.], [9., 9.]]) tensor([[3., 3.], [3., 3.]]) tensor([[0.3333, 0.3333], [0.3333, 0.3333]]) 指数和取对数 注意log是以自然对数为底数的，以2为底的用log2，以10为底的用log10。 import torch a = torch.exp(torch.ones(2, 2)) # 得到2*2的全是e的Tensor print(a) print(torch.log(a)) # 取自然对数 运行结果： tensor([[2.7183, 2.7183], [2.7183, 2.7183]]) tensor([[1., 1.], [1., 1.]]) 近似值操作 import torch a = torch.tensor(3.14) print(a.floor(), a.ceil(), a.trunc(), a.frac()) # 取下,取上,取整数,取小数 b = torch.tensor(3.49) c = torch.tensor(3.5) print(b.round(), c.round()) # 四舍五入 运行结果： tensor(3.) tensor(4.) tensor(3.) tensor(0.1400) tensor(3.) tensor(4.) 裁剪 即对Tensor中的元素进行范围过滤，不符合条件的可以把它变换到范围内部（边界）上，常用于梯度裁剪（gradient clipping），即在发生梯度离散或者梯度爆炸时对梯度的处理。 实际使用时可以查看梯度的（L2范数）模来看看需不需要做处理：w.grad.norm(2)。 import torch grad = torch.rand(2, 3) * 15 # 0~15随机生成 print(grad.max(), grad.min(), grad.median()) # 最大值最小值平均值 print(grad) print(grad.clamp(10)) # 最小是10,小于10的都变成10 print(grad.clamp(3, 10)) # 最小是3,小于3的都变成3;最大是10,大于10的都变成10 运行结果： tensor(14.7400) tensor(1.8522) tensor(10.5734) tensor([[ 1.8522, 14.7400, 8.2445], [13.5520, 10.5734, 12.9756]]) tensor([[10.0000, 14.7400, 10.0000], [13.5520, 10.5734, 12.9756]]) tensor([[ 3.0000, 10.0000, 8.2445], [10.0000, 10.0000, 10.0000]])" />
<meta property="og:description" content="版权声明：本文为博主原创学习笔记，如需转载请注明来源。 https://blog.csdn.net/SHU15121856/article/details/87886885 Broadcasting Broadcasting也就和之前学MPI时候学的广播一样，能够实现自动维度扩展，有点像上节学的expand的功能，但是是自动完成的，而且不需要像repeat那样对数据进行拷贝，可以节省内存。 从最后面的维度开始匹配。 在前面插入若干维度。 将维度的size从1通过expand变到和某个Tensor相同的维度。 总之，Broadcasting也就是自动实现了若干unsqueeze和expand操作，以使两个Tensor的shape一致，从而完成某些操作（往往是加法）。 例如 [ b a t c h , c h a n n e l , h e i g h t , w i d t h ] = [ 4 , 3 , 32 , 32 ] [batch,channel,height,width]=[4,3,32,32] [batch,channel,height,width]=[4,3,32,32]：如果加上一个 [ 32 , 32 ] [32,32] [32,32]的Tensor，意思就是不管是4张图片中的哪一张，也不管是什么通道，都加上一个32*32的Feature Map；如果加上一个 [ 3 , 1 , 1 ] [3,1,1] [3,1,1]，意思就是对3个通道分别加上一个值；如果加上一个 [ 1 , 1 , 1 , 1 ] [1,1,1,1] [1,1,1,1]，这和加上一个 [ 1 ] [1] [1]一样，也就是对每个元素都加上一个相同的Bais值。 对Tensor的合并 维度合并(cat) 要保证其它维度的size是相同的。 import torch a = torch.rand(4, 32, 8) b = torch.rand(5, 32, 8) print(torch.cat([a, b], dim=0).shape) 运行结果： torch.Size([9, 32, 8] 合并新增(stack) stack需要保证两个Tensor的shape是一致的，这就像是有两类东西，它们的其它属性都是一样的（比如男的一张表，女的一张表）。使用stack时候要指定一个维度位置，在那个位置前会插入一个新的维度，因为是两类东西合并过来所以这个新的维度size是2，通过指定这个维度是0或者1来选择性别是男还是女。 c = torch.rand(4, 3, 32, 32) d = torch.rand(4, 3, 32, 32) print(torch.stack([c, d], dim=2).shape) print(torch.stack([c, d], dim=0).shape) 运行结果： torch.Size([4, 3, 2, 32, 32]) torch.Size([2, 4, 3, 32, 32]) 对Tensor的拆分 按照size的长度拆分(split) 对一个Tensor而言，要拆分的那个维度的size就是&quot;这个维度的总长度&quot;了，可以指定拆分后的几个Tensor各取多少长度，或者指定每个Tensor取多少长度。 import torch a = torch.rand(2, 4, 3, 32, 32) a1, a2 = a.split(1, dim=0) # 对0号维度拆分,拆分后每个Tensor取长度1 print(a1.shape, a2.shape) b = torch.rand(4, 3, 32, 32) b1, b2 = b.split([2, 1], dim=1) # 对1号维度拆分,拆分后第一个维度取2,第二个维度取1 print(b1.shape, b2.shape) 运行结果： torch.Size([1, 4, 3, 32, 32]) torch.Size([1, 4, 3, 32, 32]) torch.Size([4, 2, 32, 32]) torch.Size([4, 1, 32, 32]) 按照份数等量拆分(chunk) 给定在指定的维度上要拆分得的份数，就会按照指定的份数尽量等量地进行拆分。 c = torch.rand(7, 4) c1, c2, c3, c4 = c.chunk(4, dim=0) print(c1.shape, c2.shape, c3.shape, c4.shape) 运行结果： torch.Size([2, 4]) torch.Size([2, 4]) torch.Size([2, 4]) torch.Size([1, 4]) Tensor运算 相加 import torch # 这两个Tensor加减乘除会对b自动进行Broadcasting a = torch.rand(3, 4) b = torch.rand(4) c1 = a + b c2 = torch.add(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果： torch.Size([3, 4]) torch.Size([3, 4]) tensor(1, dtype=torch.uint8) 相减 a = torch.rand(3, 4) b = torch.rand(4) c1 = a - b c2 = torch.sub(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果：同上 哈达玛积(element wise) c1 = a * b c2 = torch.mul(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果：同上 相除 c1 = a / b c2 = torch.div(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果：同上 矩阵乘法(matrix mul) 使用Torch.mm（只能用于2d的Tensor）或者Torch.matmul（和符号@效果一样）。 dim=2 import torch a = torch.ones(2, 1) b = torch.ones(1, 2) print(torch.mm(a, b).shape) print(torch.matmul(a, b).shape) print((a @ b).shape) 运行结果： torch.Size([2, 2]) torch.Size([2, 2]) torch.Size([2, 2]) dim&gt;2 对于高维的Tensor，定义其矩阵乘法仅在最后的两个维度上，要求前面的维度必须保持一致，就像矩阵的索引一样。 c = torch.rand(4, 3, 28, 64) d = torch.rand(4, 3, 64, 32) print(torch.matmul(c, d).shape) 运行结果： torch.Size([4, 3, 28, 32]) 注意，在这种情形下的矩阵相乘，前面的&quot;矩阵索引维度&quot;如果符合Broadcasting机制，也会自动做广播，然后相乘。 c = torch.rand(4, 3, 28, 64) d = torch.rand(4, 1, 64, 32) print(torch.matmul(c, d).shape) 运行结果： torch.Size([4, 3, 28, 32]) 幂和开方 import torch a = torch.full([2, 2], 3) b = a.pow(2) # 也可以a**2 print(b) c = b.sqrt() # 也可以a**(0.5) print(c) d = b.rsqrt() # 平方根的倒数 print(d) 运行结果： tensor([[9., 9.], [9., 9.]]) tensor([[3., 3.], [3., 3.]]) tensor([[0.3333, 0.3333], [0.3333, 0.3333]]) 指数和取对数 注意log是以自然对数为底数的，以2为底的用log2，以10为底的用log10。 import torch a = torch.exp(torch.ones(2, 2)) # 得到2*2的全是e的Tensor print(a) print(torch.log(a)) # 取自然对数 运行结果： tensor([[2.7183, 2.7183], [2.7183, 2.7183]]) tensor([[1., 1.], [1., 1.]]) 近似值操作 import torch a = torch.tensor(3.14) print(a.floor(), a.ceil(), a.trunc(), a.frac()) # 取下,取上,取整数,取小数 b = torch.tensor(3.49) c = torch.tensor(3.5) print(b.round(), c.round()) # 四舍五入 运行结果： tensor(3.) tensor(4.) tensor(3.) tensor(0.1400) tensor(3.) tensor(4.) 裁剪 即对Tensor中的元素进行范围过滤，不符合条件的可以把它变换到范围内部（边界）上，常用于梯度裁剪（gradient clipping），即在发生梯度离散或者梯度爆炸时对梯度的处理。 实际使用时可以查看梯度的（L2范数）模来看看需不需要做处理：w.grad.norm(2)。 import torch grad = torch.rand(2, 3) * 15 # 0~15随机生成 print(grad.max(), grad.min(), grad.median()) # 最大值最小值平均值 print(grad) print(grad.clamp(10)) # 最小是10,小于10的都变成10 print(grad.clamp(3, 10)) # 最小是3,小于3的都变成3;最大是10,大于10的都变成10 运行结果： tensor(14.7400) tensor(1.8522) tensor(10.5734) tensor([[ 1.8522, 14.7400, 8.2445], [13.5520, 10.5734, 12.9756]]) tensor([[10.0000, 14.7400, 10.0000], [13.5520, 10.5734, 12.9756]]) tensor([[ 3.0000, 10.0000, 8.2445], [10.0000, 10.0000, 10.0000]])" />
<link rel="canonical" href="https://mlh.app/2019/02/23/749c1bb98de202dcaa27f4309aeb67e0.html" />
<meta property="og:url" content="https://mlh.app/2019/02/23/749c1bb98de202dcaa27f4309aeb67e0.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-23T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"版权声明：本文为博主原创学习笔记，如需转载请注明来源。 https://blog.csdn.net/SHU15121856/article/details/87886885 Broadcasting Broadcasting也就和之前学MPI时候学的广播一样，能够实现自动维度扩展，有点像上节学的expand的功能，但是是自动完成的，而且不需要像repeat那样对数据进行拷贝，可以节省内存。 从最后面的维度开始匹配。 在前面插入若干维度。 将维度的size从1通过expand变到和某个Tensor相同的维度。 总之，Broadcasting也就是自动实现了若干unsqueeze和expand操作，以使两个Tensor的shape一致，从而完成某些操作（往往是加法）。 例如 [ b a t c h , c h a n n e l , h e i g h t , w i d t h ] = [ 4 , 3 , 32 , 32 ] [batch,channel,height,width]=[4,3,32,32] [batch,channel,height,width]=[4,3,32,32]：如果加上一个 [ 32 , 32 ] [32,32] [32,32]的Tensor，意思就是不管是4张图片中的哪一张，也不管是什么通道，都加上一个32*32的Feature Map；如果加上一个 [ 3 , 1 , 1 ] [3,1,1] [3,1,1]，意思就是对3个通道分别加上一个值；如果加上一个 [ 1 , 1 , 1 , 1 ] [1,1,1,1] [1,1,1,1]，这和加上一个 [ 1 ] [1] [1]一样，也就是对每个元素都加上一个相同的Bais值。 对Tensor的合并 维度合并(cat) 要保证其它维度的size是相同的。 import torch a = torch.rand(4, 32, 8) b = torch.rand(5, 32, 8) print(torch.cat([a, b], dim=0).shape) 运行结果： torch.Size([9, 32, 8] 合并新增(stack) stack需要保证两个Tensor的shape是一致的，这就像是有两类东西，它们的其它属性都是一样的（比如男的一张表，女的一张表）。使用stack时候要指定一个维度位置，在那个位置前会插入一个新的维度，因为是两类东西合并过来所以这个新的维度size是2，通过指定这个维度是0或者1来选择性别是男还是女。 c = torch.rand(4, 3, 32, 32) d = torch.rand(4, 3, 32, 32) print(torch.stack([c, d], dim=2).shape) print(torch.stack([c, d], dim=0).shape) 运行结果： torch.Size([4, 3, 2, 32, 32]) torch.Size([2, 4, 3, 32, 32]) 对Tensor的拆分 按照size的长度拆分(split) 对一个Tensor而言，要拆分的那个维度的size就是&quot;这个维度的总长度&quot;了，可以指定拆分后的几个Tensor各取多少长度，或者指定每个Tensor取多少长度。 import torch a = torch.rand(2, 4, 3, 32, 32) a1, a2 = a.split(1, dim=0) # 对0号维度拆分,拆分后每个Tensor取长度1 print(a1.shape, a2.shape) b = torch.rand(4, 3, 32, 32) b1, b2 = b.split([2, 1], dim=1) # 对1号维度拆分,拆分后第一个维度取2,第二个维度取1 print(b1.shape, b2.shape) 运行结果： torch.Size([1, 4, 3, 32, 32]) torch.Size([1, 4, 3, 32, 32]) torch.Size([4, 2, 32, 32]) torch.Size([4, 1, 32, 32]) 按照份数等量拆分(chunk) 给定在指定的维度上要拆分得的份数，就会按照指定的份数尽量等量地进行拆分。 c = torch.rand(7, 4) c1, c2, c3, c4 = c.chunk(4, dim=0) print(c1.shape, c2.shape, c3.shape, c4.shape) 运行结果： torch.Size([2, 4]) torch.Size([2, 4]) torch.Size([2, 4]) torch.Size([1, 4]) Tensor运算 相加 import torch # 这两个Tensor加减乘除会对b自动进行Broadcasting a = torch.rand(3, 4) b = torch.rand(4) c1 = a + b c2 = torch.add(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果： torch.Size([3, 4]) torch.Size([3, 4]) tensor(1, dtype=torch.uint8) 相减 a = torch.rand(3, 4) b = torch.rand(4) c1 = a - b c2 = torch.sub(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果：同上 哈达玛积(element wise) c1 = a * b c2 = torch.mul(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果：同上 相除 c1 = a / b c2 = torch.div(a, b) print(c1.shape, c2.shape) print(torch.all(torch.eq(c1, c2))) 运行结果：同上 矩阵乘法(matrix mul) 使用Torch.mm（只能用于2d的Tensor）或者Torch.matmul（和符号@效果一样）。 dim=2 import torch a = torch.ones(2, 1) b = torch.ones(1, 2) print(torch.mm(a, b).shape) print(torch.matmul(a, b).shape) print((a @ b).shape) 运行结果： torch.Size([2, 2]) torch.Size([2, 2]) torch.Size([2, 2]) dim&gt;2 对于高维的Tensor，定义其矩阵乘法仅在最后的两个维度上，要求前面的维度必须保持一致，就像矩阵的索引一样。 c = torch.rand(4, 3, 28, 64) d = torch.rand(4, 3, 64, 32) print(torch.matmul(c, d).shape) 运行结果： torch.Size([4, 3, 28, 32]) 注意，在这种情形下的矩阵相乘，前面的&quot;矩阵索引维度&quot;如果符合Broadcasting机制，也会自动做广播，然后相乘。 c = torch.rand(4, 3, 28, 64) d = torch.rand(4, 1, 64, 32) print(torch.matmul(c, d).shape) 运行结果： torch.Size([4, 3, 28, 32]) 幂和开方 import torch a = torch.full([2, 2], 3) b = a.pow(2) # 也可以a**2 print(b) c = b.sqrt() # 也可以a**(0.5) print(c) d = b.rsqrt() # 平方根的倒数 print(d) 运行结果： tensor([[9., 9.], [9., 9.]]) tensor([[3., 3.], [3., 3.]]) tensor([[0.3333, 0.3333], [0.3333, 0.3333]]) 指数和取对数 注意log是以自然对数为底数的，以2为底的用log2，以10为底的用log10。 import torch a = torch.exp(torch.ones(2, 2)) # 得到2*2的全是e的Tensor print(a) print(torch.log(a)) # 取自然对数 运行结果： tensor([[2.7183, 2.7183], [2.7183, 2.7183]]) tensor([[1., 1.], [1., 1.]]) 近似值操作 import torch a = torch.tensor(3.14) print(a.floor(), a.ceil(), a.trunc(), a.frac()) # 取下,取上,取整数,取小数 b = torch.tensor(3.49) c = torch.tensor(3.5) print(b.round(), c.round()) # 四舍五入 运行结果： tensor(3.) tensor(4.) tensor(3.) tensor(0.1400) tensor(3.) tensor(4.) 裁剪 即对Tensor中的元素进行范围过滤，不符合条件的可以把它变换到范围内部（边界）上，常用于梯度裁剪（gradient clipping），即在发生梯度离散或者梯度爆炸时对梯度的处理。 实际使用时可以查看梯度的（L2范数）模来看看需不需要做处理：w.grad.norm(2)。 import torch grad = torch.rand(2, 3) * 15 # 0~15随机生成 print(grad.max(), grad.min(), grad.median()) # 最大值最小值平均值 print(grad) print(grad.clamp(10)) # 最小是10,小于10的都变成10 print(grad.clamp(3, 10)) # 最小是3,小于3的都变成3;最大是10,大于10的都变成10 运行结果： tensor(14.7400) tensor(1.8522) tensor(10.5734) tensor([[ 1.8522, 14.7400, 8.2445], [13.5520, 10.5734, 12.9756]]) tensor([[10.0000, 14.7400, 10.0000], [13.5520, 10.5734, 12.9756]]) tensor([[ 3.0000, 10.0000, 8.2445], [10.0000, 10.0000, 10.0000]])","@type":"BlogPosting","url":"https://mlh.app/2019/02/23/749c1bb98de202dcaa27f4309aeb67e0.html","headline":"【PyTorch学习笔记】6：Broadcasting,对Tensor的合并与拆分,Tensor运算","dateModified":"2019-02-23T00:00:00+08:00","datePublished":"2019-02-23T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/02/23/749c1bb98de202dcaa27f4309aeb67e0.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>【PyTorch学习笔记】6：Broadcasting,对Tensor的合并与拆分,Tensor运算</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div class="article-copyright">
   版权声明：本文为博主原创学习笔记，如需转载请注明来源。 https://blog.csdn.net/SHU15121856/article/details/87886885 
 </div> 
 <div id="content_views" class="markdown_views prism-tomorrow-night"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <h3><a id="Broadcasting_0"></a>Broadcasting</h3> 
  <p>Broadcasting也就和之前学MPI时候学的广播一样，能够实现自动维度扩展，有点像上节学的<code>expand</code>的功能，但是是自动完成的，而且不需要像<code>repeat</code>那样对数据进行拷贝，可以节省内存。</p> 
  <ul> 
   <li>从最后面的维度开始匹配。</li> 
   <li>在前面插入若干维度。</li> 
   <li>将维度的size从1通过expand变到和某个Tensor相同的维度。</li> 
  </ul> 
  <p>总之，Broadcasting也就是自动实现了若干<code>unsqueeze</code>和<code>expand</code>操作，以使两个Tensor的shape一致，从而完成某些操作（往往是加法）。</p> 
  <p>例如<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mo>
          [
         </mo>
         <mi>
          b
         </mi>
         <mi>
          a
         </mi>
         <mi>
          t
         </mi>
         <mi>
          c
         </mi>
         <mi>
          h
         </mi>
         <mo separator="true">
          ,
         </mo>
         <mi>
          c
         </mi>
         <mi>
          h
         </mi>
         <mi>
          a
         </mi>
         <mi>
          n
         </mi>
         <mi>
          n
         </mi>
         <mi>
          e
         </mi>
         <mi>
          l
         </mi>
         <mo separator="true">
          ,
         </mo>
         <mi>
          h
         </mi>
         <mi>
          e
         </mi>
         <mi>
          i
         </mi>
         <mi>
          g
         </mi>
         <mi>
          h
         </mi>
         <mi>
          t
         </mi>
         <mo separator="true">
          ,
         </mo>
         <mi>
          w
         </mi>
         <mi>
          i
         </mi>
         <mi>
          d
         </mi>
         <mi>
          t
         </mi>
         <mi>
          h
         </mi>
         <mo>
          ]
         </mo>
         <mo>
          =
         </mo>
         <mo>
          [
         </mo>
         <mn>
          4
         </mn>
         <mo separator="true">
          ,
         </mo>
         <mn>
          3
         </mn>
         <mo separator="true">
          ,
         </mo>
         <mn>
          32
         </mn>
         <mo separator="true">
          ,
         </mo>
         <mn>
          32
         </mn>
         <mo>
          ]
         </mo>
        </mrow>
        <annotation encoding="application/x-tex">
         [batch,channel,height,width]=[4,3,32,32]
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord mathit">b</span><span class="mord mathit">a</span><span class="mord mathit">t</span><span class="mord mathit">c</span><span class="mord mathit">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit">c</span><span class="mord mathit">h</span><span class="mord mathit">a</span><span class="mord mathit">n</span><span class="mord mathit">n</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right: 0.01968em;">l</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit">h</span><span class="mord mathit">e</span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right: 0.03588em;">g</span><span class="mord mathit">h</span><span class="mord mathit">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.02691em;">w</span><span class="mord mathit">i</span><span class="mord mathit">d</span><span class="mord mathit">t</span><span class="mord mathit">h</span><span class="mclose">]</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">3</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">3</span><span class="mord">2</span><span class="mclose">]</span></span></span></span></span>：如果加上一个<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mo>
          [
         </mo>
         <mn>
          32
         </mn>
         <mo separator="true">
          ,
         </mo>
         <mn>
          32
         </mn>
         <mo>
          ]
         </mo>
        </mrow>
        <annotation encoding="application/x-tex">
         [32,32]
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">3</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">3</span><span class="mord">2</span><span class="mclose">]</span></span></span></span></span>的Tensor，意思就是不管是4张图片中的哪一张，也不管是什么通道，都加上一个32*32的Feature Map；如果加上一个<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mo>
          [
         </mo>
         <mn>
          3
         </mn>
         <mo separator="true">
          ,
         </mo>
         <mn>
          1
         </mn>
         <mo separator="true">
          ,
         </mo>
         <mn>
          1
         </mn>
         <mo>
          ]
         </mo>
        </mrow>
        <annotation encoding="application/x-tex">
         [3,1,1]
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">3</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span>，意思就是对3个通道分别加上一个值；如果加上一个<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mo>
          [
         </mo>
         <mn>
          1
         </mn>
         <mo separator="true">
          ,
         </mo>
         <mn>
          1
         </mn>
         <mo separator="true">
          ,
         </mo>
         <mn>
          1
         </mn>
         <mo separator="true">
          ,
         </mo>
         <mn>
          1
         </mn>
         <mo>
          ]
         </mo>
        </mrow>
        <annotation encoding="application/x-tex">
         [1,1,1,1]
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span>，这和加上一个<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mo>
          [
         </mo>
         <mn>
          1
         </mn>
         <mo>
          ]
         </mo>
        </mrow>
        <annotation encoding="application/x-tex">
         [1]
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span>一样，也就是对每个元素都加上一个相同的Bais值。</p> 
  <h3><a id="Tensor_10"></a>对Tensor的合并</h3> 
  <h4><a id="cat_11"></a>维度合并(cat)</h4> 
  <p>要保证其它维度的size是相同的。</p> 
  <pre><code class="prism language-py"><span class="token keyword">import</span> torch

a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>torch.Size([9, 32, 8]
</code></pre> 
  <h4><a id="stack_24"></a>合并新增(stack)</h4> 
  <p>stack需要保证两个Tensor的shape是一致的，这就像是有两类东西，它们的其它属性都是一样的（比如男的一张表，女的一张表）。使用<code>stack</code>时候要指定一个维度位置，在那个位置前会插入一个新的维度，因为是两类东西合并过来所以这个新的维度size是2，通过指定这个维度是0或者1来选择性别是男还是女。</p> 
  <pre><code class="prism language-py">c <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>
d <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>c<span class="token punctuation">,</span> d<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>c<span class="token punctuation">,</span> d<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>torch.Size([4, 3, 2, 32, 32])
torch.Size([2, 4, 3, 32, 32])
</code></pre> 
  <h3><a id="Tensor_37"></a>对Tensor的拆分</h3> 
  <h4><a id="sizesplit_38"></a>按照size的长度拆分(split)</h4> 
  <p>对一个Tensor而言，要拆分的那个维度的size就是"这个维度的总长度"了，可以指定拆分后的几个Tensor各取多少长度，或者指定每个Tensor取多少长度。</p> 
  <pre><code class="prism language-py"><span class="token keyword">import</span> torch

a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>
a1<span class="token punctuation">,</span> a2 <span class="token operator">=</span> a<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 对0号维度拆分,拆分后每个Tensor取长度1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a1<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> a2<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>
b1<span class="token punctuation">,</span> b2 <span class="token operator">=</span> b<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 对1号维度拆分,拆分后第一个维度取2,第二个维度取1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b1<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> b2<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>torch.Size([1, 4, 3, 32, 32]) torch.Size([1, 4, 3, 32, 32])
torch.Size([4, 2, 32, 32]) torch.Size([4, 1, 32, 32])
</code></pre> 
  <h4><a id="chunk_56"></a>按照份数等量拆分(chunk)</h4> 
  <p>给定在指定的维度上要拆分得的份数，就会按照指定的份数尽量等量地进行拆分。</p> 
  <pre><code class="prism language-py">c <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
c1<span class="token punctuation">,</span> c2<span class="token punctuation">,</span> c3<span class="token punctuation">,</span> c4 <span class="token operator">=</span> c<span class="token punctuation">.</span>chunk<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>c1<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> c2<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> c3<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> c4<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>torch.Size([2, 4]) torch.Size([2, 4]) torch.Size([2, 4]) torch.Size([1, 4])
</code></pre> 
  <h3><a id="Tensor_67"></a>Tensor运算</h3> 
  <h4><a id="_68"></a>相加</h4> 
  <pre><code class="prism language-py"><span class="token keyword">import</span> torch

<span class="token comment"># 这两个Tensor加减乘除会对b自动进行Broadcasting</span>
a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>

c1 <span class="token operator">=</span> a <span class="token operator">+</span> b
c2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>c1<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> c2<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>c1<span class="token punctuation">,</span> c2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>torch.Size([3, 4]) torch.Size([3, 4])
tensor(1, dtype=torch.uint8)
</code></pre> 
  <h4><a id="_86"></a>相减</h4> 
  <pre><code class="prism language-py">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>

c1 <span class="token operator">=</span> a <span class="token operator">-</span> b
c2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>sub<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>c1<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> c2<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>c1<span class="token punctuation">,</span> c2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：同上</p> 
  <h4><a id="element_wise_97"></a>哈达玛积(element wise)</h4> 
  <pre><code class="prism language-py">c1 <span class="token operator">=</span> a <span class="token operator">*</span> b
c2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>c1<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> c2<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>c1<span class="token punctuation">,</span> c2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：同上</p> 
  <h4><a id="_105"></a>相除</h4> 
  <pre><code class="prism language-py">c1 <span class="token operator">=</span> a <span class="token operator">/</span> b
c2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>div<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>c1<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> c2<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>c1<span class="token punctuation">,</span> c2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：同上</p> 
  <h4><a id="matrix_mul_113"></a>矩阵乘法(matrix mul)</h4> 
  <p>使用<code>Torch.mm</code>（只能用于2d的Tensor）或者<code>Torch.matmul</code>（和符号<code>@</code>效果一样）。</p> 
  <h5><a id="dim2_115"></a>dim=2</h5> 
  <pre><code class="prism language-py"><span class="token keyword">import</span> torch

a <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">(</span>a @ b<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>torch.Size([2, 2])
torch.Size([2, 2])
torch.Size([2, 2])
</code></pre> 
  <h5><a id="dim2_131"></a>dim&gt;2</h5> 
  <p>对于高维的Tensor，定义其矩阵乘法仅在最后的两个维度上，要求前面的维度必须保持一致，就像矩阵的索引一样。</p> 
  <pre><code class="prism language-py">c <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
d <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>c<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>torch.Size([4, 3, 28, 32])
</code></pre> 
  <hr> 
  <p>注意，在这种情形下的矩阵相乘，前面的"矩阵索引维度"如果符合Broadcasting机制，也会自动做广播，然后相乘。</p> 
  <pre><code class="prism language-py">c <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
d <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>c<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>torch.Size([4, 3, 28, 32])
</code></pre> 
  <h4><a id="_154"></a>幂和开方</h4> 
  <pre><code class="prism language-py"><span class="token keyword">import</span> torch

a <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

b <span class="token operator">=</span> a<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 也可以a**2</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span>

c <span class="token operator">=</span> b<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 也可以a**(0.5)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span>

d <span class="token operator">=</span> b<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 平方根的倒数</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>tensor([[9., 9.],
        [9., 9.]])
tensor([[3., 3.],
        [3., 3.]])
tensor([[0.3333, 0.3333],
        [0.3333, 0.3333]])
</code></pre> 
  <h4><a id="_178"></a>指数和取对数</h4> 
  <p>注意<code>log</code>是以自然对数为底数的，以2为底的用<code>log2</code>，以10为底的用<code>log10</code>。</p> 
  <pre><code class="prism language-py"><span class="token keyword">import</span> torch

a <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 得到2*2的全是e的Tensor</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 取自然对数</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>tensor([[2.7183, 2.7183],
        [2.7183, 2.7183]])
tensor([[1., 1.],
        [1., 1.]])
</code></pre> 
  <h4><a id="_194"></a>近似值操作</h4> 
  <pre><code class="prism language-py"><span class="token keyword">import</span> torch

a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.14</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>floor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>trunc<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> a<span class="token punctuation">.</span>frac<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 取下,取上,取整数,取小数</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.49</span><span class="token punctuation">)</span>
c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.5</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 四舍五入</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>tensor(3.) tensor(4.) tensor(3.) tensor(0.1400)
tensor(3.) tensor(4.)
</code></pre> 
  <h4><a id="_209"></a>裁剪</h4> 
  <p>即对Tensor中的元素进行范围过滤，不符合条件的可以把它变换到范围内部（边界）上，常用于<strong>梯度裁剪</strong>（gradient clipping），即在发生梯度离散或者梯度爆炸时对梯度的处理。</p> 
  <blockquote> 
   <p>实际使用时可以查看梯度的（L2范数）模来看看需不需要做处理：<code>w.grad.norm(2)</code>。</p> 
  </blockquote> 
  <pre><code class="prism language-py"><span class="token keyword">import</span> torch

grad <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">15</span>  <span class="token comment"># 0~15随机生成</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>grad<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> grad<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> grad<span class="token punctuation">.</span>median<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 最大值最小值平均值</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>grad<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>grad<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 最小是10,小于10的都变成10</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>grad<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 最小是3,小于3的都变成3;最大是10,大于10的都变成10</span>
</code></pre> 
  <p>运行结果：</p> 
  <pre><code>tensor(14.7400) tensor(1.8522) tensor(10.5734)
tensor([[ 1.8522, 14.7400,  8.2445],
        [13.5520, 10.5734, 12.9756]])
tensor([[10.0000, 14.7400, 10.0000],
        [13.5520, 10.5734, 12.9756]])
tensor([[ 3.0000, 10.0000,  8.2445],
        [10.0000, 10.0000, 10.0000]])
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
