<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>情感分析中文本数据预处理 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="情感分析中文本数据预处理" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="版权声明：我是小仙女 转载要告诉小仙女哦 https://blog.csdn.net/qq_40210472/article/details/87896970 读数据 直接获取文件内容 # 获取文件内容 一个文件中有很多行信息，每一行是一个序列 def getData(file): f = open(file,&#39;r&#39;) raw_data = f.readlines() return raw_data # Read the file and split into lines 以换行符来分开和readlines 相似 lines = open(&#39;data/%s-%s.txt&#39; % (lang1, lang2), encoding=&#39;utf-8&#39;).\ read().strip().split(&#39;\n&#39;) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #一个文件夹中有很多个文件，每一个文件是一个序列 files = os.listdir(os.path.join(path,seg,label)) for file in files: with open(os.path.join(path,seg,label,file),&#39;r&#39;,encoding = &#39;utf-8&#39;) as rf: review = rf.read().replace(&#39;\n&#39;,&#39;&#39;) &nbsp; 获取标签 对标签进行数字化 每一个文件中的每一行前面是文本，后面是标签 # Split every line into pairs and normalize pairs = [[normalizeString(s) for s in l.split(&#39;\t&#39;)] for l in lines] #normalize函数是一个正则化的函数，也就是使数据更加标准化的 def normalizeString(s): s = unicodeToAscii(s.lower().strip()) s = re.sub(r&quot;([.!?])&quot;, r&quot; \1&quot;, s) s = re.sub(r&quot;[^a-zA-Z.!?]+&quot;, r&quot; &quot;, s) return s #如果只需要将text 和 label 分开 则可以 pairs = [[l for l in l.split(&#39;\t&#39;)] for l in lines] &nbsp; 构建label map直接后面添加 变成二分类问题 if label == &#39;pos&#39;: data.append([review,1]) else : data.append([review,0]) &nbsp; 构建labelmap label_map = {&#39;pos&#39;:0,&#39;neg&#39;:1} #转换 y = label_map[d[&#39;type&#39;]] #根据label_map将label转换为数字表示 &nbsp; 观察数据分布，找到合适的长度作为截断长度。 train_text = [] for line in train_data: d = eval(line) t = jieba.cut(d[&#39;text&#39;]) train_text.append(t) sentence_length = [len(x) for x in train_text] #train_text是train.csv中每一行分词之后的数据 %matplotlib notebook import matplotlib.pyplot as plt plt.hist(sentence_length,1000,normed=1,cumulative=True) plt.xlim(0,1000) plt.show() &nbsp; 由文本得到训练用的mini-batch数据 分词 #英文分词 只分空格即可 vocab = [v.lower() for v in l.strip().split(&#39; &#39;)] #Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。 #注意：该方法只能删除开头或是结尾的字符，不能删除中间部分的字符。 #中文分词 def tokenizer(x): res = [w for w in jieba.cut(x)] return res #注意去掉标点符号，一般使用正则表达式re &nbsp; &nbsp; 去除停用词 def stopwords_filter(filename,list_words_lemmatizer): list_filter_stopwords=[] #声明一个停用词过滤后的词列表 with open(filename,&#39;r&#39;) as fr: stop_words=list(fr.read().split(&#39;\n&#39;)) #将停用词读取到列表里 for i in range(len(list_words_lemmatizer)): word_list = [] for j in list_words_lemmatizer[i]: if j not in stop_words: word_list.append(j.lower()) #将词变为小写加入词列表 list_filter_stopwords.append(word_list) return list_filter_stopwords #建立一个停用词表 def stopwords(filepath): stopword=[] with open(filepath,&#39;r&#39;) as f: for l in f.readllines(): stopword.append(l.strip()) return stopword &nbsp; 建立词汇表 vocab = set(chain(*train_tokenized)) word_to_idx = {word:idx for idx,word in enumerate(vocab)} word_to_idx[&#39;&lt;unk&gt;&#39;] = 0 idx_to_word = {idx:word for idx,word in enumerate(vocab)} idx_to_word[0] = &#39;&lt;unk&gt;&#39; &nbsp; 将分词去除停用词后的数据转换成下标数据，也就是转换成index。 def indexesFromSentence( sentence): return [word2index[word] for word in sentence.split(&#39; &#39;)] def tensorFromSentence(lang, sentence): indexes = indexesFromSentence(lang, sentence) indexes.append(EOS_token) result = torch.LongTensor(indexes) return result &nbsp; 将数据分成mini-batch 第一种是重写Dataset类，然后利用dataloader分batch from torch.utils.data import Dataset class TextDataset(Dataset): def __init__(self, dataload=prepareData, lang=[&#39;eng&#39;, &#39;fra&#39;]): self.input_lang, self.output_lang, self.pairs = dataload( lang[0], lang[1], reverse=True) self.input_lang_words = self.input_lang.n_words self.output_lang_words = self.output_lang.n_words def __getitem__(self, index): return tensorFromPair(self.input_lang, self.output_lang, self.pairs[index]) def __len__(self): return len(self.pairs) #class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=&lt;function default_collate&gt;, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None) #第二种，如果text和label都是Tensor的话，可以直接调用TensorDataset函数，然后再用dataloader读数据 train_set = torch.utils.data.TensorDataset(train_features, train_labels) test_set = torch.utils.data.TensorDataset(test_features, test_labels) train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True) test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False) &nbsp; 根据mini-batch中内个index对应的向量得到最终输入（一般在网络里，也就是embedding） &nbsp; 直接用torchtext来进行上述步骤 TEXT = data.Field(sequential=True, tokenize=tokenizer,fix_length=1000,stop_words=stop_words) LABEL = data.Field(sequential=False,use_vocab=False) torchtext 的组件&nbsp; Field :主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等 Dataset :继承自pytorch的Dataset，用于加载数据，提供了TabularDataset可以指点路径，格式，Field信息就可以方便的完成数据加载。同时torchtext还提供预先构建的常用数据集的Dataset对象，可以直接加载使用，splits方法可以同时加载训练集，验证集和测试集。 Iterator : 主要是数据输出的模型的迭代器，可以支持batch定制。 &nbsp; Field Field 包含一写文本处理的通用参数的设置，同时还包含一个词典对象，可以把文本数据表示成数字类型，（即转换成index形式）进而可以把文本表示成需要的tensor类型完成了分词，固定长度，去掉停用词等 以下是Field对象包含的参数： sequential: 是否把数据表示成序列，如果是False, 不能使用分词 默认值: True. use_vocab: 是否使用词典对象. 如果是False 数据的类型必须已经是数值类型. 默认值: True. init_token: 每一条数据的起始字符 默认值: None. eos_token: 每条数据的结尾字符 默认值: None. fix_length: 修改每条数据的长度为该值，不够的用pad_token补全. 默认值: None.为None则按每个Batch内的最大长度进行动态padding。 tensor_type: 把数据转换成的tensor类型 默认值: torch.LongTensor. preprocessing:在分词之后和数值化之前使用的管道 默认值: None. postprocessing: 数值化之后和转化成tensor之前使用的管道默认值: None. lower: 是否把数据转化为小写 默认值: False. tokenize: 分词函数. 默认值: str.split. include_lengths: 是否返回一个已经补全的最小batch的元组和和一个包含每条数据长度的列表 . 默认值: False. batch_first: Whether to produce tensors with the batch dimension first. 默认值: False. pad_token: 用于补全的字符. 默认值: &quot;&lt;pad&gt;&quot;. unk_token: 不存在词典里的字符. 默认值: &quot;&lt;unk&gt;&quot;. pad_first: 是否补全第一个字符. 默认值: False. TEXT = data.Field(tokenize=data.get_tokenizer(&#39;spacy&#39;), init_token=&#39;&lt;SOS&gt;&#39;, eos_token=&#39;&lt;EOS&gt;&#39;,lower=True) Lable = data.Field(squential = False,use_vocab = False) 2.Dataset torchtext的Dataset是继承自pytorch的Dataset，提供了一个可以下载压缩数据并解压的方法（支持.zip, .gz, .tgz）完成了读取数据问题 splits方法可以同时读取训练集，验证集，测试集 TabularDataset可以很方便的读取CSV, TSV, or JSON格式的文件，例子如下： train, val, test = data.TabularDataset.splits( path=&#39;./data/&#39;, train=&#39;train.tsv&#39;, validation=&#39;val.tsv&#39;, test=&#39;test.tsv&#39;, format=&#39;tsv&#39;, fields=[(&#39;Text&#39;, TEXT), (&#39;Label&#39;, LABEL)]) 加载数据后可以建立词典，建立词典的时候可以使用与训练的word vector TEXT.build_vocab(train, vectors=&quot;glove.6B.100d&quot;) 3. Iterator&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 完成了分batch的问题 Iterator是torchtext到模型的输出，它提供了我们对数据的一般处理方式，比如打乱，排序，等等，可以动态修改batch大小，这里也有splits方法 可以同时输出训练集，验证集，测试集,类似dataloader 参数如下： dataset: 加载的数据集 batch_size: Batch 大小. batch_size_fn: 产生动态的batch大小 的函数 sort_key: 排序的key train: 是否是一个训练集 repeat: 是否在不同epoch中重复迭代 shuffle: 是否打乱数据 sort: 是否对数据进行排序 sort_within_batch: batch内部是否排序 device: 建立batch的设备 -1:CPU ；0,1 ...：对应的GPU &nbsp; train_iter, val_iter, test_iter = data.Iterator.splits( (train, val, test), sort_key=lambda x: len(x.Text), batch_sizes=(32, 256, 256), device=-1) 4.其他 torchtext提供常用文本数据集，并可以直接加载使用： train,val,test = datasets.WikiText2.splits(text_field=TEXT) 现在包含的数据集包括： &nbsp; Sentiment analysis: SST and IMDb Question classification: TREC Entailment: SNLI Language modeling: WikiText-2 Machine translation: Multi30k, IWSLT, WMT14 import spacy import torch from torchtext import data, datasets spacy_en = spacy.load(&#39;en&#39;) def tokenizer(text): # create a tokenizer function return [tok.text for tok in spacy_en.tokenizer(text)] TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=150) LABEL = data.Field(sequential=False, use_vocab=False) train, val, test = data.TabularDataset.splits( path=&#39;./data/&#39;, train=&#39;train.tsv&#39;, validation=&#39;val.tsv&#39;, test=&#39;test.tsv&#39;, format=&#39;tsv&#39;, fields=[(&#39;Text&#39;, TEXT), (&#39;Label&#39;, LABEL)]) TEXT.build_vocab(train, vectors=&quot;glove.6B.100d&quot;) train_iter, val_iter, test_iter = data.Iterator.splits( (train, val, test), sort_key=lambda x: len(x.Text), batch_sizes=(32, 256, 256), device=-1) vocab = TEXT.vocab &nbsp;" />
<meta property="og:description" content="版权声明：我是小仙女 转载要告诉小仙女哦 https://blog.csdn.net/qq_40210472/article/details/87896970 读数据 直接获取文件内容 # 获取文件内容 一个文件中有很多行信息，每一行是一个序列 def getData(file): f = open(file,&#39;r&#39;) raw_data = f.readlines() return raw_data # Read the file and split into lines 以换行符来分开和readlines 相似 lines = open(&#39;data/%s-%s.txt&#39; % (lang1, lang2), encoding=&#39;utf-8&#39;).\ read().strip().split(&#39;\n&#39;) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #一个文件夹中有很多个文件，每一个文件是一个序列 files = os.listdir(os.path.join(path,seg,label)) for file in files: with open(os.path.join(path,seg,label,file),&#39;r&#39;,encoding = &#39;utf-8&#39;) as rf: review = rf.read().replace(&#39;\n&#39;,&#39;&#39;) &nbsp; 获取标签 对标签进行数字化 每一个文件中的每一行前面是文本，后面是标签 # Split every line into pairs and normalize pairs = [[normalizeString(s) for s in l.split(&#39;\t&#39;)] for l in lines] #normalize函数是一个正则化的函数，也就是使数据更加标准化的 def normalizeString(s): s = unicodeToAscii(s.lower().strip()) s = re.sub(r&quot;([.!?])&quot;, r&quot; \1&quot;, s) s = re.sub(r&quot;[^a-zA-Z.!?]+&quot;, r&quot; &quot;, s) return s #如果只需要将text 和 label 分开 则可以 pairs = [[l for l in l.split(&#39;\t&#39;)] for l in lines] &nbsp; 构建label map直接后面添加 变成二分类问题 if label == &#39;pos&#39;: data.append([review,1]) else : data.append([review,0]) &nbsp; 构建labelmap label_map = {&#39;pos&#39;:0,&#39;neg&#39;:1} #转换 y = label_map[d[&#39;type&#39;]] #根据label_map将label转换为数字表示 &nbsp; 观察数据分布，找到合适的长度作为截断长度。 train_text = [] for line in train_data: d = eval(line) t = jieba.cut(d[&#39;text&#39;]) train_text.append(t) sentence_length = [len(x) for x in train_text] #train_text是train.csv中每一行分词之后的数据 %matplotlib notebook import matplotlib.pyplot as plt plt.hist(sentence_length,1000,normed=1,cumulative=True) plt.xlim(0,1000) plt.show() &nbsp; 由文本得到训练用的mini-batch数据 分词 #英文分词 只分空格即可 vocab = [v.lower() for v in l.strip().split(&#39; &#39;)] #Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。 #注意：该方法只能删除开头或是结尾的字符，不能删除中间部分的字符。 #中文分词 def tokenizer(x): res = [w for w in jieba.cut(x)] return res #注意去掉标点符号，一般使用正则表达式re &nbsp; &nbsp; 去除停用词 def stopwords_filter(filename,list_words_lemmatizer): list_filter_stopwords=[] #声明一个停用词过滤后的词列表 with open(filename,&#39;r&#39;) as fr: stop_words=list(fr.read().split(&#39;\n&#39;)) #将停用词读取到列表里 for i in range(len(list_words_lemmatizer)): word_list = [] for j in list_words_lemmatizer[i]: if j not in stop_words: word_list.append(j.lower()) #将词变为小写加入词列表 list_filter_stopwords.append(word_list) return list_filter_stopwords #建立一个停用词表 def stopwords(filepath): stopword=[] with open(filepath,&#39;r&#39;) as f: for l in f.readllines(): stopword.append(l.strip()) return stopword &nbsp; 建立词汇表 vocab = set(chain(*train_tokenized)) word_to_idx = {word:idx for idx,word in enumerate(vocab)} word_to_idx[&#39;&lt;unk&gt;&#39;] = 0 idx_to_word = {idx:word for idx,word in enumerate(vocab)} idx_to_word[0] = &#39;&lt;unk&gt;&#39; &nbsp; 将分词去除停用词后的数据转换成下标数据，也就是转换成index。 def indexesFromSentence( sentence): return [word2index[word] for word in sentence.split(&#39; &#39;)] def tensorFromSentence(lang, sentence): indexes = indexesFromSentence(lang, sentence) indexes.append(EOS_token) result = torch.LongTensor(indexes) return result &nbsp; 将数据分成mini-batch 第一种是重写Dataset类，然后利用dataloader分batch from torch.utils.data import Dataset class TextDataset(Dataset): def __init__(self, dataload=prepareData, lang=[&#39;eng&#39;, &#39;fra&#39;]): self.input_lang, self.output_lang, self.pairs = dataload( lang[0], lang[1], reverse=True) self.input_lang_words = self.input_lang.n_words self.output_lang_words = self.output_lang.n_words def __getitem__(self, index): return tensorFromPair(self.input_lang, self.output_lang, self.pairs[index]) def __len__(self): return len(self.pairs) #class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=&lt;function default_collate&gt;, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None) #第二种，如果text和label都是Tensor的话，可以直接调用TensorDataset函数，然后再用dataloader读数据 train_set = torch.utils.data.TensorDataset(train_features, train_labels) test_set = torch.utils.data.TensorDataset(test_features, test_labels) train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True) test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False) &nbsp; 根据mini-batch中内个index对应的向量得到最终输入（一般在网络里，也就是embedding） &nbsp; 直接用torchtext来进行上述步骤 TEXT = data.Field(sequential=True, tokenize=tokenizer,fix_length=1000,stop_words=stop_words) LABEL = data.Field(sequential=False,use_vocab=False) torchtext 的组件&nbsp; Field :主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等 Dataset :继承自pytorch的Dataset，用于加载数据，提供了TabularDataset可以指点路径，格式，Field信息就可以方便的完成数据加载。同时torchtext还提供预先构建的常用数据集的Dataset对象，可以直接加载使用，splits方法可以同时加载训练集，验证集和测试集。 Iterator : 主要是数据输出的模型的迭代器，可以支持batch定制。 &nbsp; Field Field 包含一写文本处理的通用参数的设置，同时还包含一个词典对象，可以把文本数据表示成数字类型，（即转换成index形式）进而可以把文本表示成需要的tensor类型完成了分词，固定长度，去掉停用词等 以下是Field对象包含的参数： sequential: 是否把数据表示成序列，如果是False, 不能使用分词 默认值: True. use_vocab: 是否使用词典对象. 如果是False 数据的类型必须已经是数值类型. 默认值: True. init_token: 每一条数据的起始字符 默认值: None. eos_token: 每条数据的结尾字符 默认值: None. fix_length: 修改每条数据的长度为该值，不够的用pad_token补全. 默认值: None.为None则按每个Batch内的最大长度进行动态padding。 tensor_type: 把数据转换成的tensor类型 默认值: torch.LongTensor. preprocessing:在分词之后和数值化之前使用的管道 默认值: None. postprocessing: 数值化之后和转化成tensor之前使用的管道默认值: None. lower: 是否把数据转化为小写 默认值: False. tokenize: 分词函数. 默认值: str.split. include_lengths: 是否返回一个已经补全的最小batch的元组和和一个包含每条数据长度的列表 . 默认值: False. batch_first: Whether to produce tensors with the batch dimension first. 默认值: False. pad_token: 用于补全的字符. 默认值: &quot;&lt;pad&gt;&quot;. unk_token: 不存在词典里的字符. 默认值: &quot;&lt;unk&gt;&quot;. pad_first: 是否补全第一个字符. 默认值: False. TEXT = data.Field(tokenize=data.get_tokenizer(&#39;spacy&#39;), init_token=&#39;&lt;SOS&gt;&#39;, eos_token=&#39;&lt;EOS&gt;&#39;,lower=True) Lable = data.Field(squential = False,use_vocab = False) 2.Dataset torchtext的Dataset是继承自pytorch的Dataset，提供了一个可以下载压缩数据并解压的方法（支持.zip, .gz, .tgz）完成了读取数据问题 splits方法可以同时读取训练集，验证集，测试集 TabularDataset可以很方便的读取CSV, TSV, or JSON格式的文件，例子如下： train, val, test = data.TabularDataset.splits( path=&#39;./data/&#39;, train=&#39;train.tsv&#39;, validation=&#39;val.tsv&#39;, test=&#39;test.tsv&#39;, format=&#39;tsv&#39;, fields=[(&#39;Text&#39;, TEXT), (&#39;Label&#39;, LABEL)]) 加载数据后可以建立词典，建立词典的时候可以使用与训练的word vector TEXT.build_vocab(train, vectors=&quot;glove.6B.100d&quot;) 3. Iterator&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 完成了分batch的问题 Iterator是torchtext到模型的输出，它提供了我们对数据的一般处理方式，比如打乱，排序，等等，可以动态修改batch大小，这里也有splits方法 可以同时输出训练集，验证集，测试集,类似dataloader 参数如下： dataset: 加载的数据集 batch_size: Batch 大小. batch_size_fn: 产生动态的batch大小 的函数 sort_key: 排序的key train: 是否是一个训练集 repeat: 是否在不同epoch中重复迭代 shuffle: 是否打乱数据 sort: 是否对数据进行排序 sort_within_batch: batch内部是否排序 device: 建立batch的设备 -1:CPU ；0,1 ...：对应的GPU &nbsp; train_iter, val_iter, test_iter = data.Iterator.splits( (train, val, test), sort_key=lambda x: len(x.Text), batch_sizes=(32, 256, 256), device=-1) 4.其他 torchtext提供常用文本数据集，并可以直接加载使用： train,val,test = datasets.WikiText2.splits(text_field=TEXT) 现在包含的数据集包括： &nbsp; Sentiment analysis: SST and IMDb Question classification: TREC Entailment: SNLI Language modeling: WikiText-2 Machine translation: Multi30k, IWSLT, WMT14 import spacy import torch from torchtext import data, datasets spacy_en = spacy.load(&#39;en&#39;) def tokenizer(text): # create a tokenizer function return [tok.text for tok in spacy_en.tokenizer(text)] TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=150) LABEL = data.Field(sequential=False, use_vocab=False) train, val, test = data.TabularDataset.splits( path=&#39;./data/&#39;, train=&#39;train.tsv&#39;, validation=&#39;val.tsv&#39;, test=&#39;test.tsv&#39;, format=&#39;tsv&#39;, fields=[(&#39;Text&#39;, TEXT), (&#39;Label&#39;, LABEL)]) TEXT.build_vocab(train, vectors=&quot;glove.6B.100d&quot;) train_iter, val_iter, test_iter = data.Iterator.splits( (train, val, test), sort_key=lambda x: len(x.Text), batch_sizes=(32, 256, 256), device=-1) vocab = TEXT.vocab &nbsp;" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-23T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"版权声明：我是小仙女 转载要告诉小仙女哦 https://blog.csdn.net/qq_40210472/article/details/87896970 读数据 直接获取文件内容 # 获取文件内容 一个文件中有很多行信息，每一行是一个序列 def getData(file): f = open(file,&#39;r&#39;) raw_data = f.readlines() return raw_data # Read the file and split into lines 以换行符来分开和readlines 相似 lines = open(&#39;data/%s-%s.txt&#39; % (lang1, lang2), encoding=&#39;utf-8&#39;).\\ read().strip().split(&#39;\\n&#39;) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #一个文件夹中有很多个文件，每一个文件是一个序列 files = os.listdir(os.path.join(path,seg,label)) for file in files: with open(os.path.join(path,seg,label,file),&#39;r&#39;,encoding = &#39;utf-8&#39;) as rf: review = rf.read().replace(&#39;\\n&#39;,&#39;&#39;) &nbsp; 获取标签 对标签进行数字化 每一个文件中的每一行前面是文本，后面是标签 # Split every line into pairs and normalize pairs = [[normalizeString(s) for s in l.split(&#39;\\t&#39;)] for l in lines] #normalize函数是一个正则化的函数，也就是使数据更加标准化的 def normalizeString(s): s = unicodeToAscii(s.lower().strip()) s = re.sub(r&quot;([.!?])&quot;, r&quot; \\1&quot;, s) s = re.sub(r&quot;[^a-zA-Z.!?]+&quot;, r&quot; &quot;, s) return s #如果只需要将text 和 label 分开 则可以 pairs = [[l for l in l.split(&#39;\\t&#39;)] for l in lines] &nbsp; 构建label map直接后面添加 变成二分类问题 if label == &#39;pos&#39;: data.append([review,1]) else : data.append([review,0]) &nbsp; 构建labelmap label_map = {&#39;pos&#39;:0,&#39;neg&#39;:1} #转换 y = label_map[d[&#39;type&#39;]] #根据label_map将label转换为数字表示 &nbsp; 观察数据分布，找到合适的长度作为截断长度。 train_text = [] for line in train_data: d = eval(line) t = jieba.cut(d[&#39;text&#39;]) train_text.append(t) sentence_length = [len(x) for x in train_text] #train_text是train.csv中每一行分词之后的数据 %matplotlib notebook import matplotlib.pyplot as plt plt.hist(sentence_length,1000,normed=1,cumulative=True) plt.xlim(0,1000) plt.show() &nbsp; 由文本得到训练用的mini-batch数据 分词 #英文分词 只分空格即可 vocab = [v.lower() for v in l.strip().split(&#39; &#39;)] #Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。 #注意：该方法只能删除开头或是结尾的字符，不能删除中间部分的字符。 #中文分词 def tokenizer(x): res = [w for w in jieba.cut(x)] return res #注意去掉标点符号，一般使用正则表达式re &nbsp; &nbsp; 去除停用词 def stopwords_filter(filename,list_words_lemmatizer): list_filter_stopwords=[] #声明一个停用词过滤后的词列表 with open(filename,&#39;r&#39;) as fr: stop_words=list(fr.read().split(&#39;\\n&#39;)) #将停用词读取到列表里 for i in range(len(list_words_lemmatizer)): word_list = [] for j in list_words_lemmatizer[i]: if j not in stop_words: word_list.append(j.lower()) #将词变为小写加入词列表 list_filter_stopwords.append(word_list) return list_filter_stopwords #建立一个停用词表 def stopwords(filepath): stopword=[] with open(filepath,&#39;r&#39;) as f: for l in f.readllines(): stopword.append(l.strip()) return stopword &nbsp; 建立词汇表 vocab = set(chain(*train_tokenized)) word_to_idx = {word:idx for idx,word in enumerate(vocab)} word_to_idx[&#39;&lt;unk&gt;&#39;] = 0 idx_to_word = {idx:word for idx,word in enumerate(vocab)} idx_to_word[0] = &#39;&lt;unk&gt;&#39; &nbsp; 将分词去除停用词后的数据转换成下标数据，也就是转换成index。 def indexesFromSentence( sentence): return [word2index[word] for word in sentence.split(&#39; &#39;)] def tensorFromSentence(lang, sentence): indexes = indexesFromSentence(lang, sentence) indexes.append(EOS_token) result = torch.LongTensor(indexes) return result &nbsp; 将数据分成mini-batch 第一种是重写Dataset类，然后利用dataloader分batch from torch.utils.data import Dataset class TextDataset(Dataset): def __init__(self, dataload=prepareData, lang=[&#39;eng&#39;, &#39;fra&#39;]): self.input_lang, self.output_lang, self.pairs = dataload( lang[0], lang[1], reverse=True) self.input_lang_words = self.input_lang.n_words self.output_lang_words = self.output_lang.n_words def __getitem__(self, index): return tensorFromPair(self.input_lang, self.output_lang, self.pairs[index]) def __len__(self): return len(self.pairs) #class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=&lt;function default_collate&gt;, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None) #第二种，如果text和label都是Tensor的话，可以直接调用TensorDataset函数，然后再用dataloader读数据 train_set = torch.utils.data.TensorDataset(train_features, train_labels) test_set = torch.utils.data.TensorDataset(test_features, test_labels) train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True) test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False) &nbsp; 根据mini-batch中内个index对应的向量得到最终输入（一般在网络里，也就是embedding） &nbsp; 直接用torchtext来进行上述步骤 TEXT = data.Field(sequential=True, tokenize=tokenizer,fix_length=1000,stop_words=stop_words) LABEL = data.Field(sequential=False,use_vocab=False) torchtext 的组件&nbsp; Field :主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等 Dataset :继承自pytorch的Dataset，用于加载数据，提供了TabularDataset可以指点路径，格式，Field信息就可以方便的完成数据加载。同时torchtext还提供预先构建的常用数据集的Dataset对象，可以直接加载使用，splits方法可以同时加载训练集，验证集和测试集。 Iterator : 主要是数据输出的模型的迭代器，可以支持batch定制。 &nbsp; Field Field 包含一写文本处理的通用参数的设置，同时还包含一个词典对象，可以把文本数据表示成数字类型，（即转换成index形式）进而可以把文本表示成需要的tensor类型完成了分词，固定长度，去掉停用词等 以下是Field对象包含的参数： sequential: 是否把数据表示成序列，如果是False, 不能使用分词 默认值: True. use_vocab: 是否使用词典对象. 如果是False 数据的类型必须已经是数值类型. 默认值: True. init_token: 每一条数据的起始字符 默认值: None. eos_token: 每条数据的结尾字符 默认值: None. fix_length: 修改每条数据的长度为该值，不够的用pad_token补全. 默认值: None.为None则按每个Batch内的最大长度进行动态padding。 tensor_type: 把数据转换成的tensor类型 默认值: torch.LongTensor. preprocessing:在分词之后和数值化之前使用的管道 默认值: None. postprocessing: 数值化之后和转化成tensor之前使用的管道默认值: None. lower: 是否把数据转化为小写 默认值: False. tokenize: 分词函数. 默认值: str.split. include_lengths: 是否返回一个已经补全的最小batch的元组和和一个包含每条数据长度的列表 . 默认值: False. batch_first: Whether to produce tensors with the batch dimension first. 默认值: False. pad_token: 用于补全的字符. 默认值: &quot;&lt;pad&gt;&quot;. unk_token: 不存在词典里的字符. 默认值: &quot;&lt;unk&gt;&quot;. pad_first: 是否补全第一个字符. 默认值: False. TEXT = data.Field(tokenize=data.get_tokenizer(&#39;spacy&#39;), init_token=&#39;&lt;SOS&gt;&#39;, eos_token=&#39;&lt;EOS&gt;&#39;,lower=True) Lable = data.Field(squential = False,use_vocab = False) 2.Dataset torchtext的Dataset是继承自pytorch的Dataset，提供了一个可以下载压缩数据并解压的方法（支持.zip, .gz, .tgz）完成了读取数据问题 splits方法可以同时读取训练集，验证集，测试集 TabularDataset可以很方便的读取CSV, TSV, or JSON格式的文件，例子如下： train, val, test = data.TabularDataset.splits( path=&#39;./data/&#39;, train=&#39;train.tsv&#39;, validation=&#39;val.tsv&#39;, test=&#39;test.tsv&#39;, format=&#39;tsv&#39;, fields=[(&#39;Text&#39;, TEXT), (&#39;Label&#39;, LABEL)]) 加载数据后可以建立词典，建立词典的时候可以使用与训练的word vector TEXT.build_vocab(train, vectors=&quot;glove.6B.100d&quot;) 3. Iterator&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 完成了分batch的问题 Iterator是torchtext到模型的输出，它提供了我们对数据的一般处理方式，比如打乱，排序，等等，可以动态修改batch大小，这里也有splits方法 可以同时输出训练集，验证集，测试集,类似dataloader 参数如下： dataset: 加载的数据集 batch_size: Batch 大小. batch_size_fn: 产生动态的batch大小 的函数 sort_key: 排序的key train: 是否是一个训练集 repeat: 是否在不同epoch中重复迭代 shuffle: 是否打乱数据 sort: 是否对数据进行排序 sort_within_batch: batch内部是否排序 device: 建立batch的设备 -1:CPU ；0,1 ...：对应的GPU &nbsp; train_iter, val_iter, test_iter = data.Iterator.splits( (train, val, test), sort_key=lambda x: len(x.Text), batch_sizes=(32, 256, 256), device=-1) 4.其他 torchtext提供常用文本数据集，并可以直接加载使用： train,val,test = datasets.WikiText2.splits(text_field=TEXT) 现在包含的数据集包括： &nbsp; Sentiment analysis: SST and IMDb Question classification: TREC Entailment: SNLI Language modeling: WikiText-2 Machine translation: Multi30k, IWSLT, WMT14 import spacy import torch from torchtext import data, datasets spacy_en = spacy.load(&#39;en&#39;) def tokenizer(text): # create a tokenizer function return [tok.text for tok in spacy_en.tokenizer(text)] TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=150) LABEL = data.Field(sequential=False, use_vocab=False) train, val, test = data.TabularDataset.splits( path=&#39;./data/&#39;, train=&#39;train.tsv&#39;, validation=&#39;val.tsv&#39;, test=&#39;test.tsv&#39;, format=&#39;tsv&#39;, fields=[(&#39;Text&#39;, TEXT), (&#39;Label&#39;, LABEL)]) TEXT.build_vocab(train, vectors=&quot;glove.6B.100d&quot;) train_iter, val_iter, test_iter = data.Iterator.splits( (train, val, test), sort_key=lambda x: len(x.Text), batch_sizes=(32, 256, 256), device=-1) vocab = TEXT.vocab &nbsp;","@type":"BlogPosting","url":"/2019/02/23/36dcb4bda5614a2c88870bb113bb5a12.html","headline":"情感分析中文本数据预处理","dateModified":"2019-02-23T00:00:00+08:00","datePublished":"2019-02-23T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/02/23/36dcb4bda5614a2c88870bb113bb5a12.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>情感分析中文本数据预处理</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div class="article-copyright">
   版权声明：我是小仙女 转载要告诉小仙女哦 https://blog.csdn.net/qq_40210472/article/details/87896970 
 </div> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h1>读数据</h1> 
  <ul>
   <li> <h2>直接获取文件内容</h2> <pre class="has">
<code># 获取文件内容  一个文件中有很多行信息，每一行是一个序列
def getData(file):
    f = open(file,'r')
    raw_data = f.readlines()
    return raw_data

 
# Read the file and split into lines   以换行符来分开和readlines 相似
    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\
        read().strip().split('\n')
</code></pre> <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p> </li> 
   <li> <pre class="has">
<code>#一个文件夹中有很多个文件，每一个文件是一个序列
files = os.listdir(os.path.join(path,seg,label))
        for file in files:
            with open(os.path.join(path,seg,label,file),'r',encoding = 'utf-8') as rf:
                review = rf.read().replace('\n','')</code></pre> <p>&nbsp;</p> </li> 
   <li> <h2>获取标签 对标签进行数字化</h2> </li> 
   <li>每一个文件中的每一行前面是文本，后面是标签 <pre class="has">
<code># Split every line into pairs and normalize

    pairs = [[normalizeString(s) for s in l.split('\t')] for l in lines]

#normalize函数是一个正则化的函数，也就是使数据更加标准化的

def normalizeString(s):
    s = unicodeToAscii(s.lower().strip())
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s


#如果只需要将text 和 label 分开 则可以 
pairs = [[l for l in l.split('\t')] for l in lines]
 </code></pre> <p>&nbsp;</p> </li> 
   <li>构建label map直接后面添加 变成二分类问题 <pre class="has">
<code>if label == 'pos':

    data.append([review,1])
else :
    data.append([review,0])</code></pre> <p>&nbsp;</p> </li> 
   <li> <p>构建labelmap</p> <pre class="has">
<code>label_map = {'pos':0,'neg':1}

#转换
y = label_map[d['type']] #根据label_map将label转换为数字表示</code></pre> <p>&nbsp;</p> </li> 
   <li> <h2>观察数据分布，找到合适的长度作为截断长度。</h2> </li> 
   <li> <pre class="has">
<code class="language-python">train_text = []
for line in train_data: 
    d = eval(line) 
t = jieba.cut(d['text'])
train_text.append(t) 
sentence_length = [len(x) for x in train_text] #train_text是train.csv中每一行分词之后的数据 %matplotlib notebook 
import matplotlib.pyplot as plt 
plt.hist(sentence_length,1000,normed=1,cumulative=True) 
plt.xlim(0,1000) 
plt.show()
</code></pre> <p>&nbsp;</p> </li> 
   <li> <h2>由文本得到训练用的mini-batch数据</h2> </li> 
   <li>分词 <pre class="has">
<code>#英文分词 只分空格即可

vocab = [v.lower() for v in l.strip().split(' ')]

#Python strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。

#注意：该方法只能删除开头或是结尾的字符，不能删除中间部分的字符。


#中文分词

def tokenizer(x):
    res = [w for w in jieba.cut(x)]
    return res
#注意去掉标点符号，一般使用正则表达式re</code></pre> <p>&nbsp;</p> </li> 
   <li>&nbsp;</li> 
   <li>去除停用词</li> 
   <li> <pre class="has">
<code class="language-python">def stopwords_filter(filename,list_words_lemmatizer):
    list_filter_stopwords=[]  #声明一个停用词过滤后的词列表
    with open(filename,'r') as fr:
        stop_words=list(fr.read().split('\n')) #将停用词读取到列表里
        for i in range(len(list_words_lemmatizer)):
            word_list = []
            for j in list_words_lemmatizer[i]:
                if j not in stop_words:
                    word_list.append(j.lower()) #将词变为小写加入词列表
            list_filter_stopwords.append(word_list)
        return list_filter_stopwords


#建立一个停用词表

def stopwords(filepath):
    stopword=[]
    with open(filepath,'r') as f:
        
        for l in f.readllines():
            stopword.append(l.strip())

    return stopword     
</code></pre> <p>&nbsp;</p> </li> 
   <li>建立词汇表</li> 
   <li> <pre class="has">
<code>vocab = set(chain(*train_tokenized))
word_to_idx = {word:idx for idx,word in enumerate(vocab)}
word_to_idx['&lt;unk&gt;'] = 0
idx_to_word = {idx:word for idx,word in enumerate(vocab)}
idx_to_word[0] = '&lt;unk&gt;'</code></pre> <p>&nbsp;</p> </li> 
   <li>将分词去除停用词后的数据转换成下标数据，也就是转换成index。</li> 
   <li> <pre class="has">
<code>def indexesFromSentence( sentence):
    return [word2index[word] for word in sentence.split(' ')]


def tensorFromSentence(lang, sentence):
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    result = torch.LongTensor(indexes)
    return result
</code></pre> <p>&nbsp;</p> </li> 
   <li>将数据分成mini-batch</li> 
   <li> <pre class="has">
<code>第一种是重写Dataset类，然后利用dataloader分batch
from torch.utils.data import Dataset

class TextDataset(Dataset):
    def __init__(self, dataload=prepareData, lang=['eng', 'fra']):
        self.input_lang, self.output_lang, self.pairs = dataload(
            lang[0], lang[1], reverse=True)
        self.input_lang_words = self.input_lang.n_words
        self.output_lang_words = self.output_lang.n_words

    def __getitem__(self, index):
        return tensorFromPair(self.input_lang, self.output_lang,
                              self.pairs[index])

    def __len__(self):
        return len(self.pairs)
#class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=&lt;function default_collate&gt;, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None)


#第二种，如果text和label都是Tensor的话，可以直接调用TensorDataset函数，然后再用dataloader读数据
train_set = torch.utils.data.TensorDataset(train_features, train_labels)
test_set = torch.utils.data.TensorDataset(test_features, test_labels)

train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,
                                         shuffle=True)
test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,
                                        shuffle=False)</code></pre> <p>&nbsp;</p> </li> 
   <li>根据mini-batch中内个index对应的向量得到最终输入（一般在网络里，也就是embedding）</li> 
  </ul>
  <p>&nbsp;</p> 
  <h2>直接用torchtext来进行上述步骤</h2> 
  <pre class="has">
<code>TEXT = data.Field(sequential=True, tokenize=tokenizer,fix_length=1000,stop_words=stop_words)
LABEL = data.Field(sequential=False,use_vocab=False)
</code></pre> 
  <p>torchtext 的组件&nbsp;</p> 
  <p><strong>Field</strong> :主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等</p> 
  <p><strong>Dataset</strong> :继承自pytorch的Dataset，用于加载数据，提供了TabularDataset可以指点路径，格式，Field信息就可以方便的完成数据加载。同时torchtext还提供预先构建的常用数据集的Dataset对象，可以直接加载使用，splits方法可以同时加载训练集，验证集和测试集。</p> 
  <p><strong>Iterator</strong> : 主要是数据输出的模型的迭代器，可以支持batch定制。</p> 
  <p>&nbsp;</p> 
  <ul>
   <li> <h3>Field</h3> </li> 
   <li>Field 包含一写文本处理的通用参数的设置，同时还包含一个词典对象，可以把文本数据表示成数字类型，（即转换成index形式）进而可以把文本表示成需要的tensor类型<strong>完成了分词，固定长度，去掉停用词等</strong></li> 
   <li> <p>以下是Field对象包含的参数：</p> <p>sequential: 是否把数据表示成序列，如果是False, 不能使用分词 默认值: True.</p> <p>use_vocab: 是否使用词典对象. 如果是False 数据的类型必须已经是数值类型. 默认值: True.</p> <p>init_token: 每一条数据的起始字符 默认值: None.</p> <p>eos_token: 每条数据的结尾字符 默认值: None.</p> <p>fix_length: 修改每条数据的长度为该值，不够的用pad_token补全. 默认值: None.<strong>为None则按每个Batch内的最大长度进行动态padding。</strong></p> <p>tensor_type: 把数据转换成的tensor类型 默认值: torch.LongTensor.</p> <p>preprocessing:在分词之后和数值化之前使用的管道 默认值: None.</p> <p>postprocessing: 数值化之后和转化成tensor之前使用的管道默认值: None.</p> <p>lower: 是否把数据转化为小写 默认值: False.</p> <p>tokenize: 分词函数. 默认值: str.split.</p> <p>include_lengths: 是否返回一个已经补全的最小batch的元组和和一个包含每条数据长度的列表 . 默认值: False.</p> <p>batch_first: Whether to produce tensors with the batch dimension first. 默认值: False.</p> <p>pad_token: 用于补全的字符. 默认值: "&lt;pad&gt;".</p> <p>unk_token: 不存在词典里的字符. 默认值: "&lt;unk&gt;".</p> <p>pad_first: 是否补全第一个字符. 默认值: False.</p> </li> 
  </ul>
  <pre class="has">
<code>TEXT = data.Field(tokenize=data.get_tokenizer('spacy'), init_token='&lt;SOS&gt;', eos_token='&lt;EOS&gt;',lower=True)
Lable = data.Field(squential = False,use_vocab = False)
</code></pre> 
  <p><strong>2.Dataset</strong></p> 
  <p>torchtext的Dataset是继承自pytorch的Dataset，提供了一个可以下载压缩数据并解压的方法（支持.zip, .gz, .tgz）<strong>完成了读取数据问题</strong></p> 
  <p>splits方法可以同时读取训练集，验证集，测试集</p> 
  <p>TabularDataset可以很方便的读取CSV, TSV, or JSON格式的文件，例子如下：</p> 
  <pre class="has">
<code>train, val, test = data.TabularDataset.splits( path='./data/', 
    train='train.tsv', validation='val.tsv', test='test.tsv', 
    format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])</code></pre> 
  <p>加载数据后可以建立词典，建立词典的时候可以使用与训练的word vector</p> 
  <pre class="has">
<code>TEXT.build_vocab(train, vectors="glove.6B.100d")</code></pre> 
  <p>3. Iterator&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>完成了分batch的问题</strong></p> 
  <p>Iterator是torchtext到模型的输出，它提供了我们对数据的一般处理方式，比如打乱，排序，等等，可以动态修改batch大小，这里也有splits方法 可以同时输出训练集，验证集，测试集,类似dataloader</p> 
  <p>参数如下：</p> 
  <p>dataset: 加载的数据集</p> 
  <p>batch_size: Batch 大小.</p> 
  <p>batch_size_fn: 产生动态的batch大小 的函数</p> 
  <p>sort_key: 排序的key</p> 
  <p>train: 是否是一个训练集</p> 
  <p>repeat: 是否在不同epoch中重复迭代</p> 
  <p>shuffle: 是否打乱数据</p> 
  <p>sort: 是否对数据进行排序</p> 
  <p>sort_within_batch: batch内部是否排序</p> 
  <p>device: 建立batch的设备 -1:CPU ；0,1 ...：对应的GPU</p> 
  <p>&nbsp;</p> 
  <pre class="has">
<code>train_iter, val_iter, test_iter = data.Iterator.splits( (train, val, test),
    sort_key=lambda x: len(x.Text), batch_sizes=(32, 256, 256), device=-1)</code></pre> 
  <p><strong>4.其他</strong></p> 
  <p>torchtext提供常用文本数据集，并可以直接加载使用：</p> 
  <pre class="has">
<code>train,val,test = datasets.WikiText2.splits(text_field=TEXT)</code></pre> 
  <p>现在包含的数据集包括：</p> 
  <p>&nbsp;</p> 
  <ul>
   <li>Sentiment analysis: SST and IMDb</li> 
   <li>Question classification: TREC</li> 
   <li>Entailment: SNLI</li> 
   <li>Language modeling: WikiText-2</li> 
   <li>Machine translation: Multi30k, IWSLT, WMT14</li> 
   <li> <pre class="has">
<code>import spacy
import torch 
from torchtext import data, datasets 
spacy_en = spacy.load('en') 
def tokenizer(text): # create a tokenizer function 
    return [tok.text for tok in spacy_en.tokenizer(text)] 
TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=150) 
LABEL = data.Field(sequential=False, use_vocab=False) 
train, val, test = data.TabularDataset.splits( path='./data/', train='train.tsv', validation='val.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])
TEXT.build_vocab(train, vectors="glove.6B.100d")
train_iter, val_iter, test_iter = data.Iterator.splits( (train, val, test), sort_key=lambda x: len(x.Text), batch_sizes=(32, 256, 256), device=-1)
vocab = TEXT.vocab</code></pre> <p>&nbsp;</p> </li> 
  </ul> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
