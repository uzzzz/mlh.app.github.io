<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>深度学习入门之第四章 神经网络的学习 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="深度学习入门之第四章 神经网络的学习" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="版权声明：版权归世界上所有无产阶级所有---------------------转载需要标明出处 https://blog.csdn.net/qq_41776781/article/details/87885038 前言：在第三章的时候，咱们讲了咱们在神经网络之中设置相关的参数，可以使数据通过神经网络前向传播，最后输出一个类别信息。但是我们怎么知道我们输出的类别信息是否正确？&nbsp; 或者我和正确的信息差距是多少？ 以及知道了差距之后我怎么告诉我们的模型调节对应W（权重）和B（偏置）的参数？本章节主要就是讲的就是这三件事。 本章介绍的梯度其本质就是将 使用梯度方向进行方向传播 调节神经网络中的参数 但是现在这种方法计算速度比较慢 所以很少有用的 所以在本章就关注前两个问题。 第三个问题再下一章继续。 &nbsp; 对于前两个问题的解决方案是： 均方误差：&nbsp;&nbsp; 深度学习之中我们一般会使用交叉熵， 这个了解一下就可以了 # 当然神经网络的目标是使均方误差最小化 越小表示我预测的精确 import numpy as np y =[0.1, 0.1, 0.2, 0.3, 0.2, 0.1] t =[0, 0, 0, 1, 0, 0 ] def mean_squared_error(y,t): return 0.5 * np.sum((y-t)**2) result = mean_squared_error(np.array(y),np.array(t)) print(result) # mean_squared_error 的结果是 0.3 &nbsp; &nbsp; 交叉熵误差：&nbsp;&nbsp; &nbsp; &nbsp; 公式 1&nbsp;&nbsp; &nbsp; tf.nn.softmax_cross_entropy_with_logits() 函数讲解 使用softmax的进行优化的时候 应用于1：n的情况 输出的结果只能属于某一属性 tensorflow定义的softmax交叉熵操作是指首先将原始数据经过softmax函数得&nbsp; &nbsp;预测数据logist 然后将预测数据logist 和 标签数据 按照公式1&nbsp; 求交叉熵误差 验证代码如下： import numpy as np import tensorflow as tf # 直接定义softmax激活函数 def softmax(x): c = np.max(x) exp = np.exp(x-c) sum_exp = np.sum(exp) class_y = exp / sum_exp return class_y Raw_data = np.array([1, 1, 6, 2]) label = np.array([0, 0, 1, 0]) my_softmax = softmax(Raw_data) print(&quot;原始数据经过softmax之后输出的结果是：&quot;, my_softmax) # 定义交叉熵函数 def cross_entropy_error(t,y): delta = 1e-7 return -np.sum(t*np.log(y+delta)) # 将得到的softmax和label交叉熵计算 交叉熵之后前者表示对应的标签 后者表示对应的softmax输出的数据 my_cross_entropy_error = cross_entropy_error(label, my_softmax) print(&quot;the result of my_cross_entropy_error difined by myself is: &quot;, my_cross_entropy_error) Raw_data = tf.convert_to_tensor(Raw_data,dtype=tf.float64) tf_result = tf.nn.softmax_cross_entropy_with_logits(labels=label,logits=Raw_data) sess = tf.Session() print(&quot;the result of softmax_cross_entropy_with_logits difined by tensorflow is: &quot;, sess.run(tf_result)) 实验结果：从实验结果上可以看出两则之间有些许差距，原因有二。但是理解原理即可 不用太较真 第一个是：softmax函数我采用防止出现nan数据的方案 第二个是：防止除数为0 使用了一个delta = 1e-7&nbsp; &#39;&#39;&#39; 原始数据经过softmax之后输出的结果是： [0.00653034 0.00653034 0.96918803 0.0177513 ] the result of my_cross_entropy_error difined by myself is: 0.03129654045830098 the result of softmax_cross_entropy_with_logits difined by tensorflow is: 0.03129664363744888 &#39;&#39;&#39; tf.nn.sigmoid_cross_entropy_with_logits()函数讲解， 应用于1：n的情况 输出的结果同时可以属于任何一个属性 计算步骤首先是将原始数据经过sigmoid函数 然后将数据经过交叉熵函数（这里面的交叉熵和上面定义的交叉熵不一致） # 定义tf.nn.sigmoid_cross_entropy_with_logits() import numpy as np import tensorflow as tf # 直接定义sigmoid激活函数 def sigmoid(x): return 1/(1 + np.exp(-x)) Raw_data = np.array([1, 1, 6, 2]) label = np.array([0, 0, 1, 0]) my_sigmoid = sigmoid(Raw_data) print(&quot;原始数据经过softmax之后输出的结果是：&quot;, my_sigmoid) # 定义交叉熵函数 def cross_entropy_error(y,t): return -y * np.log(t) - (1 - y) * np.log(1 - t) # 将得到的softmax和label交叉熵计算 交叉熵之后前者表示对应的标签 后者表示对应的softmax输出的数据 my_cross_entropy_error = cross_entropy_error(label, my_sigmoid) print(&quot;the result of my_cross_entropy_error difined by myself is: &quot;, my_cross_entropy_error) Raw_data = tf.convert_to_tensor(Raw_data,dtype=tf.float64) label = tf.convert_to_tensor(label,dtype=tf.float64) tf_result = tf.nn.sigmoid_cross_entropy_with_logits(labels=label,logits=Raw_data) sess = tf.Session() print(&quot;the result of sigmoid_cross_entropy_with_logits difined by tensorflow is: &quot;, sess.run(tf_result)) 实验结果：两则完全一致 验证通过 &#39;&#39;&#39; 原始数据经过softmax之后输出的结果是： [0.73105858 0.73105858 0.99752738 0.88079708] the result of my_cross_entropy_error difined by myself is: [1.31326169 1.31326169 0.00247569 2.12692801] the result of sigmoid_cross_entropy_with_logits difined by tensorflow is: [1.31326169 1.31326169 0.00247569 2.12692801] &#39;&#39;&#39; OK&nbsp; &nbsp;本章结束。。。。。。。。。。。。。。。。。。。。。。。" />
<meta property="og:description" content="版权声明：版权归世界上所有无产阶级所有---------------------转载需要标明出处 https://blog.csdn.net/qq_41776781/article/details/87885038 前言：在第三章的时候，咱们讲了咱们在神经网络之中设置相关的参数，可以使数据通过神经网络前向传播，最后输出一个类别信息。但是我们怎么知道我们输出的类别信息是否正确？&nbsp; 或者我和正确的信息差距是多少？ 以及知道了差距之后我怎么告诉我们的模型调节对应W（权重）和B（偏置）的参数？本章节主要就是讲的就是这三件事。 本章介绍的梯度其本质就是将 使用梯度方向进行方向传播 调节神经网络中的参数 但是现在这种方法计算速度比较慢 所以很少有用的 所以在本章就关注前两个问题。 第三个问题再下一章继续。 &nbsp; 对于前两个问题的解决方案是： 均方误差：&nbsp;&nbsp; 深度学习之中我们一般会使用交叉熵， 这个了解一下就可以了 # 当然神经网络的目标是使均方误差最小化 越小表示我预测的精确 import numpy as np y =[0.1, 0.1, 0.2, 0.3, 0.2, 0.1] t =[0, 0, 0, 1, 0, 0 ] def mean_squared_error(y,t): return 0.5 * np.sum((y-t)**2) result = mean_squared_error(np.array(y),np.array(t)) print(result) # mean_squared_error 的结果是 0.3 &nbsp; &nbsp; 交叉熵误差：&nbsp;&nbsp; &nbsp; &nbsp; 公式 1&nbsp;&nbsp; &nbsp; tf.nn.softmax_cross_entropy_with_logits() 函数讲解 使用softmax的进行优化的时候 应用于1：n的情况 输出的结果只能属于某一属性 tensorflow定义的softmax交叉熵操作是指首先将原始数据经过softmax函数得&nbsp; &nbsp;预测数据logist 然后将预测数据logist 和 标签数据 按照公式1&nbsp; 求交叉熵误差 验证代码如下： import numpy as np import tensorflow as tf # 直接定义softmax激活函数 def softmax(x): c = np.max(x) exp = np.exp(x-c) sum_exp = np.sum(exp) class_y = exp / sum_exp return class_y Raw_data = np.array([1, 1, 6, 2]) label = np.array([0, 0, 1, 0]) my_softmax = softmax(Raw_data) print(&quot;原始数据经过softmax之后输出的结果是：&quot;, my_softmax) # 定义交叉熵函数 def cross_entropy_error(t,y): delta = 1e-7 return -np.sum(t*np.log(y+delta)) # 将得到的softmax和label交叉熵计算 交叉熵之后前者表示对应的标签 后者表示对应的softmax输出的数据 my_cross_entropy_error = cross_entropy_error(label, my_softmax) print(&quot;the result of my_cross_entropy_error difined by myself is: &quot;, my_cross_entropy_error) Raw_data = tf.convert_to_tensor(Raw_data,dtype=tf.float64) tf_result = tf.nn.softmax_cross_entropy_with_logits(labels=label,logits=Raw_data) sess = tf.Session() print(&quot;the result of softmax_cross_entropy_with_logits difined by tensorflow is: &quot;, sess.run(tf_result)) 实验结果：从实验结果上可以看出两则之间有些许差距，原因有二。但是理解原理即可 不用太较真 第一个是：softmax函数我采用防止出现nan数据的方案 第二个是：防止除数为0 使用了一个delta = 1e-7&nbsp; &#39;&#39;&#39; 原始数据经过softmax之后输出的结果是： [0.00653034 0.00653034 0.96918803 0.0177513 ] the result of my_cross_entropy_error difined by myself is: 0.03129654045830098 the result of softmax_cross_entropy_with_logits difined by tensorflow is: 0.03129664363744888 &#39;&#39;&#39; tf.nn.sigmoid_cross_entropy_with_logits()函数讲解， 应用于1：n的情况 输出的结果同时可以属于任何一个属性 计算步骤首先是将原始数据经过sigmoid函数 然后将数据经过交叉熵函数（这里面的交叉熵和上面定义的交叉熵不一致） # 定义tf.nn.sigmoid_cross_entropy_with_logits() import numpy as np import tensorflow as tf # 直接定义sigmoid激活函数 def sigmoid(x): return 1/(1 + np.exp(-x)) Raw_data = np.array([1, 1, 6, 2]) label = np.array([0, 0, 1, 0]) my_sigmoid = sigmoid(Raw_data) print(&quot;原始数据经过softmax之后输出的结果是：&quot;, my_sigmoid) # 定义交叉熵函数 def cross_entropy_error(y,t): return -y * np.log(t) - (1 - y) * np.log(1 - t) # 将得到的softmax和label交叉熵计算 交叉熵之后前者表示对应的标签 后者表示对应的softmax输出的数据 my_cross_entropy_error = cross_entropy_error(label, my_sigmoid) print(&quot;the result of my_cross_entropy_error difined by myself is: &quot;, my_cross_entropy_error) Raw_data = tf.convert_to_tensor(Raw_data,dtype=tf.float64) label = tf.convert_to_tensor(label,dtype=tf.float64) tf_result = tf.nn.sigmoid_cross_entropy_with_logits(labels=label,logits=Raw_data) sess = tf.Session() print(&quot;the result of sigmoid_cross_entropy_with_logits difined by tensorflow is: &quot;, sess.run(tf_result)) 实验结果：两则完全一致 验证通过 &#39;&#39;&#39; 原始数据经过softmax之后输出的结果是： [0.73105858 0.73105858 0.99752738 0.88079708] the result of my_cross_entropy_error difined by myself is: [1.31326169 1.31326169 0.00247569 2.12692801] the result of sigmoid_cross_entropy_with_logits difined by tensorflow is: [1.31326169 1.31326169 0.00247569 2.12692801] &#39;&#39;&#39; OK&nbsp; &nbsp;本章结束。。。。。。。。。。。。。。。。。。。。。。。" />
<link rel="canonical" href="https://mlh.app/2019/02/23/3bb173a3124aee4901f668aa8292addd.html" />
<meta property="og:url" content="https://mlh.app/2019/02/23/3bb173a3124aee4901f668aa8292addd.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-23T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"版权声明：版权归世界上所有无产阶级所有---------------------转载需要标明出处 https://blog.csdn.net/qq_41776781/article/details/87885038 前言：在第三章的时候，咱们讲了咱们在神经网络之中设置相关的参数，可以使数据通过神经网络前向传播，最后输出一个类别信息。但是我们怎么知道我们输出的类别信息是否正确？&nbsp; 或者我和正确的信息差距是多少？ 以及知道了差距之后我怎么告诉我们的模型调节对应W（权重）和B（偏置）的参数？本章节主要就是讲的就是这三件事。 本章介绍的梯度其本质就是将 使用梯度方向进行方向传播 调节神经网络中的参数 但是现在这种方法计算速度比较慢 所以很少有用的 所以在本章就关注前两个问题。 第三个问题再下一章继续。 &nbsp; 对于前两个问题的解决方案是： 均方误差：&nbsp;&nbsp; 深度学习之中我们一般会使用交叉熵， 这个了解一下就可以了 # 当然神经网络的目标是使均方误差最小化 越小表示我预测的精确 import numpy as np y =[0.1, 0.1, 0.2, 0.3, 0.2, 0.1] t =[0, 0, 0, 1, 0, 0 ] def mean_squared_error(y,t): return 0.5 * np.sum((y-t)**2) result = mean_squared_error(np.array(y),np.array(t)) print(result) # mean_squared_error 的结果是 0.3 &nbsp; &nbsp; 交叉熵误差：&nbsp;&nbsp; &nbsp; &nbsp; 公式 1&nbsp;&nbsp; &nbsp; tf.nn.softmax_cross_entropy_with_logits() 函数讲解 使用softmax的进行优化的时候 应用于1：n的情况 输出的结果只能属于某一属性 tensorflow定义的softmax交叉熵操作是指首先将原始数据经过softmax函数得&nbsp; &nbsp;预测数据logist 然后将预测数据logist 和 标签数据 按照公式1&nbsp; 求交叉熵误差 验证代码如下： import numpy as np import tensorflow as tf # 直接定义softmax激活函数 def softmax(x): c = np.max(x) exp = np.exp(x-c) sum_exp = np.sum(exp) class_y = exp / sum_exp return class_y Raw_data = np.array([1, 1, 6, 2]) label = np.array([0, 0, 1, 0]) my_softmax = softmax(Raw_data) print(&quot;原始数据经过softmax之后输出的结果是：&quot;, my_softmax) # 定义交叉熵函数 def cross_entropy_error(t,y): delta = 1e-7 return -np.sum(t*np.log(y+delta)) # 将得到的softmax和label交叉熵计算 交叉熵之后前者表示对应的标签 后者表示对应的softmax输出的数据 my_cross_entropy_error = cross_entropy_error(label, my_softmax) print(&quot;the result of my_cross_entropy_error difined by myself is: &quot;, my_cross_entropy_error) Raw_data = tf.convert_to_tensor(Raw_data,dtype=tf.float64) tf_result = tf.nn.softmax_cross_entropy_with_logits(labels=label,logits=Raw_data) sess = tf.Session() print(&quot;the result of softmax_cross_entropy_with_logits difined by tensorflow is: &quot;, sess.run(tf_result)) 实验结果：从实验结果上可以看出两则之间有些许差距，原因有二。但是理解原理即可 不用太较真 第一个是：softmax函数我采用防止出现nan数据的方案 第二个是：防止除数为0 使用了一个delta = 1e-7&nbsp; &#39;&#39;&#39; 原始数据经过softmax之后输出的结果是： [0.00653034 0.00653034 0.96918803 0.0177513 ] the result of my_cross_entropy_error difined by myself is: 0.03129654045830098 the result of softmax_cross_entropy_with_logits difined by tensorflow is: 0.03129664363744888 &#39;&#39;&#39; tf.nn.sigmoid_cross_entropy_with_logits()函数讲解， 应用于1：n的情况 输出的结果同时可以属于任何一个属性 计算步骤首先是将原始数据经过sigmoid函数 然后将数据经过交叉熵函数（这里面的交叉熵和上面定义的交叉熵不一致） # 定义tf.nn.sigmoid_cross_entropy_with_logits() import numpy as np import tensorflow as tf # 直接定义sigmoid激活函数 def sigmoid(x): return 1/(1 + np.exp(-x)) Raw_data = np.array([1, 1, 6, 2]) label = np.array([0, 0, 1, 0]) my_sigmoid = sigmoid(Raw_data) print(&quot;原始数据经过softmax之后输出的结果是：&quot;, my_sigmoid) # 定义交叉熵函数 def cross_entropy_error(y,t): return -y * np.log(t) - (1 - y) * np.log(1 - t) # 将得到的softmax和label交叉熵计算 交叉熵之后前者表示对应的标签 后者表示对应的softmax输出的数据 my_cross_entropy_error = cross_entropy_error(label, my_sigmoid) print(&quot;the result of my_cross_entropy_error difined by myself is: &quot;, my_cross_entropy_error) Raw_data = tf.convert_to_tensor(Raw_data,dtype=tf.float64) label = tf.convert_to_tensor(label,dtype=tf.float64) tf_result = tf.nn.sigmoid_cross_entropy_with_logits(labels=label,logits=Raw_data) sess = tf.Session() print(&quot;the result of sigmoid_cross_entropy_with_logits difined by tensorflow is: &quot;, sess.run(tf_result)) 实验结果：两则完全一致 验证通过 &#39;&#39;&#39; 原始数据经过softmax之后输出的结果是： [0.73105858 0.73105858 0.99752738 0.88079708] the result of my_cross_entropy_error difined by myself is: [1.31326169 1.31326169 0.00247569 2.12692801] the result of sigmoid_cross_entropy_with_logits difined by tensorflow is: [1.31326169 1.31326169 0.00247569 2.12692801] &#39;&#39;&#39; OK&nbsp; &nbsp;本章结束。。。。。。。。。。。。。。。。。。。。。。。","@type":"BlogPosting","url":"https://mlh.app/2019/02/23/3bb173a3124aee4901f668aa8292addd.html","headline":"深度学习入门之第四章 神经网络的学习","dateModified":"2019-02-23T00:00:00+08:00","datePublished":"2019-02-23T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/02/23/3bb173a3124aee4901f668aa8292addd.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>深度学习入门之第四章 神经网络的学习</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div class="article-copyright">
   版权声明：版权归世界上所有无产阶级所有---------------------转载需要标明出处 https://blog.csdn.net/qq_41776781/article/details/87885038 
 </div> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>前言：在第三章的时候，咱们讲了咱们在神经网络之中设置相关的参数，可以使数据通过神经网络前向传播，最后输出一个类别信息。但是我们怎么知道我们输出的类别信息是否正确？&nbsp; 或者我和正确的信息差距是多少？ 以及知道了差距之后我怎么告诉我们的模型调节对应W（权重）和B（偏置）的参数？本章节主要就是讲的就是这三件事。</p> 
  <p>本章介绍的梯度其本质就是将 使用梯度方向进行方向传播 调节神经网络中的参数 但是现在这种方法计算速度比较慢 所以很少有用的 所以在本章就关注前两个问题。 第三个问题再下一章继续。</p> 
  <p>&nbsp;</p> 
  <p>对于前两个问题的解决方案是：</p> 
  <p>均方误差：&nbsp;<img alt="E=1/2\sum (Y_{k}-T_{k})^{2}" class="mathcode" src="https://private.codecogs.com/gif.latex?E%3D1/2%5Csum%20%28Y_%7Bk%7D-T_%7Bk%7D%29%5E%7B2%7D">&nbsp; 深度学习之中我们一般会使用交叉熵， 这个了解一下就可以了</p> 
  <pre class="has">
<code class="language-python"># 当然神经网络的目标是使均方误差最小化 越小表示我预测的精确
import numpy as np
y =[0.1, 0.1, 0.2, 0.3, 0.2, 0.1]
t =[0,   0,    0,   1,   0,   0 ]

def mean_squared_error(y,t):
    return 0.5 * np.sum((y-t)**2)

result = mean_squared_error(np.array(y),np.array(t))
print(result)

# mean_squared_error 的结果是 0.3</code></pre> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>交叉熵误差：&nbsp;<img alt="E=-\sum_{K}^{N}(T_{k} * logY_{k})" class="mathcode" src="https://private.codecogs.com/gif.latex?E%3D-%5Csum_%7BK%7D%5E%7BN%7D%28T_%7Bk%7D%20*%20logY_%7Bk%7D%29">&nbsp; &nbsp; &nbsp; 公式 1&nbsp;&nbsp; &nbsp;</p> 
  <p><span style="color:#f33b45;">tf.nn.softmax_cross_entropy_with_logits() 函数讲解 使用softmax的进行优化的时候 </span>应用于1：n的情况 输出的结果只能属于某一属性</p> 
  <p>tensorflow定义的softmax交叉熵操作是指首先将原始数据经过softmax函数得&nbsp; &nbsp;预测数据logist</p> 
  <p>然后将预测数据logist 和 标签数据 按照公式1&nbsp; 求交叉熵误差</p> 
  <p>验证代码如下：</p> 
  <pre class="has">
<code class="language-python">import numpy as np
import tensorflow as tf

# 直接定义softmax激活函数
def softmax(x):
    c = np.max(x)
    exp = np.exp(x-c)
    sum_exp = np.sum(exp)
    class_y = exp / sum_exp
    return class_y

Raw_data = np.array([1, 1, 6, 2])
label = np.array([0, 0, 1, 0])

my_softmax = softmax(Raw_data)
print("原始数据经过softmax之后输出的结果是：", my_softmax)

# 定义交叉熵函数
def cross_entropy_error(t,y):
    delta = 1e-7
    return -np.sum(t*np.log(y+delta))

# 将得到的softmax和label交叉熵计算 交叉熵之后前者表示对应的标签 后者表示对应的softmax输出的数据
my_cross_entropy_error = cross_entropy_error(label, my_softmax)
print("the result of my_cross_entropy_error difined by myself is: ", my_cross_entropy_error)



Raw_data = tf.convert_to_tensor(Raw_data,dtype=tf.float64)
tf_result = tf.nn.softmax_cross_entropy_with_logits(labels=label,logits=Raw_data)
sess = tf.Session()
print("the result of softmax_cross_entropy_with_logits difined by tensorflow is: ", sess.run(tf_result))
</code></pre> 
  <p>实验结果：从实验结果上可以看出两则之间有些许差距，原因有二。但是理解原理即可 不用太较真</p> 
  <p>第一个是：softmax函数我采用防止出现nan数据的方案</p> 
  <p>第二个是：防止除数为0 使用了一个delta = 1e-7&nbsp;</p> 
  <pre class="has">
<code class="language-python">'''
原始数据经过softmax之后输出的结果是： [0.00653034 0.00653034 0.96918803 0.0177513 ]
the result of my_cross_entropy_error difined by myself is:  0.03129654045830098
the result of softmax_cross_entropy_with_logits difined by tensorflow is:  0.03129664363744888
'''</code></pre> 
  <pre>

tf.nn.sigmoid_cross_entropy_with_logits()函数讲解， 应用于1：n的情况 输出的结果同时可以属于任何一个属性</pre> 
  <p>计算步骤首先是将原始数据经过sigmoid函数</p> 
  <p>然后将数据经过交叉熵函数（这里面的交叉熵和上面定义的交叉熵不一致）</p> 
  <pre class="has">
<code class="language-python"># 定义tf.nn.sigmoid_cross_entropy_with_logits()
import numpy as np
import tensorflow as tf

# 直接定义sigmoid激活函数
def sigmoid(x):
    return 1/(1 + np.exp(-x))

Raw_data = np.array([1, 1, 6, 2])
label = np.array([0, 0, 1, 0])

my_sigmoid = sigmoid(Raw_data)
print("原始数据经过softmax之后输出的结果是：", my_sigmoid)

# 定义交叉熵函数
def cross_entropy_error(y,t):

    return -y * np.log(t) - (1 - y) * np.log(1 - t)

# 将得到的softmax和label交叉熵计算 交叉熵之后前者表示对应的标签 后者表示对应的softmax输出的数据
my_cross_entropy_error = cross_entropy_error(label, my_sigmoid)
print("the result of my_cross_entropy_error difined by myself is: ", my_cross_entropy_error)

Raw_data = tf.convert_to_tensor(Raw_data,dtype=tf.float64)
label = tf.convert_to_tensor(label,dtype=tf.float64)

tf_result = tf.nn.sigmoid_cross_entropy_with_logits(labels=label,logits=Raw_data)
sess = tf.Session()
print("the result of sigmoid_cross_entropy_with_logits difined by tensorflow is: ", sess.run(tf_result))
</code></pre> 
  <p>实验结果：两则完全一致 验证通过</p> 
  <pre class="has">
<code class="language-python">'''
原始数据经过softmax之后输出的结果是： [0.73105858 0.73105858 0.99752738 0.88079708]
the result of my_cross_entropy_error difined by myself is:  [1.31326169 1.31326169 0.00247569 2.12692801]
the result of sigmoid_cross_entropy_with_logits difined by tensorflow is:  [1.31326169 1.31326169 0.00247569 2.12692801]
'''</code></pre> 
  <p>OK&nbsp; &nbsp;本章结束。。。。。。。。。。。。。。。。。。。。。。。</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
