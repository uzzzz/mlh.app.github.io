<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>scipy.optimize优化器的各种使用 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="scipy.optimize优化器的各种使用" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="目录 0.scipy.optimize.minimize 1.无约束最小化多元标量函数 1.1Nelder-Mead（单纯形法） &nbsp;1.2拟牛顿法：BFGS算法 1.3牛顿 - 共轭梯度法：Newton-CG 2&nbsp;约束最小化多元标量函数 2.1SLSQP(Sequential Least SQuares Programming optimization algorithm) &nbsp;2.2最小二乘最小化Least-squares minimization 3.单变量函数最小化器 4.有界最小化 5.定制自己的最小化器 6.找根 1）f(x)=0的根 2）两个等式的根&nbsp; 3）寻找大问题的根源 scipy api：https://docs.scipy.org/doc/scipy-0.18.1/reference/index.html 优化器optimize的参数设置：https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.html#module-scipy.optimize&nbsp; 优化器optimize的使用：https://docs.scipy.org/doc/scipy-0.18.1/reference/tutorial/optimize.html#constrained-minimization-of-multivariate-scalar-functions-minimize 0.scipy.optimize.minimize scipy.optimize.minimize(fun,&nbsp;x0,&nbsp;args=(),&nbsp;method=None,&nbsp;jac=None,&nbsp;hess=None,&nbsp;hessp=None,&nbsp;bounds=None,&nbsp;constraints=(),&nbsp;tol=None,&nbsp;callback=None,&nbsp;options=None) fun：目标函数，返回单值， x0：初始迭代点， method：求解方法 ‘Nelder-Mead’&nbsp;(see here) ‘Powell’&nbsp;(see here) ‘CG’&nbsp;(see here) ‘BFGS’&nbsp;(see here) ‘Newton-CG’&nbsp;(see here) ‘L-BFGS-B’&nbsp;(see here) ‘TNC’&nbsp;(see here) ‘COBYLA’&nbsp;(see here) ‘SLSQP’&nbsp;(see here) ‘dogleg’&nbsp;(see here) ‘trust-ncg’&nbsp;(see here) jac:目标函数的雅可比矩阵。可选的。仅适用于CG，BFGS，Newton-CG，L-BFGS-B，TNC，SLSQP，dogleg，trust-ncg。如果jac是布尔值并且为True，则假定fun会返回梯度以及目标函数。如果为False，将以数字方式估计梯度。jac也可以是一个可调用的，返回目标的梯度。在这种情况下，它必须接受与fun相同的参数。 hess:可选的，目标函数的Hessian（二阶导数矩阵）或目标函数的Hessian乘以任意向量p。仅适用于Newton-CG，dogleg，trust-ncg。只需要给出hessp或hess中的一个。如果提供了hess，那么hessp将被忽略。如果没有提供hess和hessp，那么将使用jac上的有限差分来近似Hessian乘积。hessp必须将Hessian时间计算为任意向量。 bounds:序列，可选变量的界限（仅适用于L-BFGS-B，TNC和SLSQP）。（min，max）对x中每个元素的对，定义该参数的边界。 constraints:约束，类型有: ‘eq’ for equality, ‘ineq’ for inequality，如：constraints=cons cons = ({&#39;type&#39;: &#39;eq&#39;, &#39;fun&#39; : lambda x: np.array([x[0]**3 - x[1]]), &#39;jac&#39; : lambda x: np.array([3.0*(x[0]**2.0), -1.0])}, {&#39;type&#39;: &#39;ineq&#39;, &#39;fun&#39; : lambda x: np.array([x[1] - 1]), &#39;jac&#39; : lambda x: np.array([0.0, 1.0])}) callback(xk)：每次迭代召唤函数，需要有参数xk. options&nbsp;:&nbsp;字典可选， #容忍精度，是否打印， options={&#39;xtol&#39;: 1e-2, &#39;disp&#39;: True} &nbsp; 1.无约束最小化多元标量函数 1.1Nelder-Mead（单纯形法） 函数Rosenbrock&nbsp;： def rosen(x): &quot;&quot;&quot;The Rosenbrock function&quot;&quot;&quot; return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0) 求解：&nbsp; import numpy as np from scipy.optimize import minimize def callback(xk): print(xk) # 初始迭代点 x0 = np.random.rand(10) * 2 # 最小化优化器，方法：Nelder-Mead（单纯形法） res = minimize(rosen, x0, method=&#39;nelder-mead&#39;, options={&#39;xtol&#39;: 1e-2, &#39;disp&#39;: True}, callback=callback) print(res.x) # Optimization terminated successfully. Current function value: 0.036794 Iterations: 521 Function evaluations: 769 [1.00107996 1.00207144 1.00207692 1.00489278 1.00985838 1.0192085 1.03896708 1.08001283 1.16669137 1.36167574] &nbsp; &nbsp;1.2拟牛顿法：BFGS算法 拟牛顿法的核心思想是构造目标函数二阶导数矩阵黑塞矩阵的逆的近似矩阵，避免了解线性方程组求逆的大量计算，更加高效。介绍：https://blog.csdn.net/jiang425776024/article/details/87602847 函数Rosenbrock&nbsp;： def rosen(x): &quot;&quot;&quot;The Rosenbrock function&quot;&quot;&quot; return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0) 导数： def rosen_der(x): # rosen函数的雅可比矩阵 xm = x[1:-1] xm_m1 = x[:-2] xm_p1 = x[2:] der = np.zeros_like(x) der[1:-1] = 200 * (xm - xm_m1 ** 2) - 400 * (xm_p1 - xm ** 2) * xm - 2 * (1 - xm) der[0] = -400 * x[0] * (x[1] - x[0] ** 2) - 2 * (1 - x[0]) der[-1] = 200 * (x[-1] - x[-2] ** 2) return der 求解：&nbsp; import numpy as np from scipy.optimize import minimize # 初始迭代点 x0 = np.random.rand(10) * 2 res = minimize(rosen, x0, method=&#39;BFGS&#39;, jac=rosen_der, options={&#39;disp&#39;: True}) print(res.x) 1.3牛顿法：Newton-CG 利用黑塞矩阵和梯度来优化，介绍：https://blog.csdn.net/jiang425776024/article/details/87601854 函数Rosenbrock&nbsp;： 构造目标函数的近似二次型（泰勒展开）： 利用黑塞矩阵H和梯度做迭代 黑塞矩阵： def rosen_hess(x): x = np.asarray(x) H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1) diagonal = np.zeros_like(x) diagonal[0] = 1200*x[0]**2-400*x[1]+2 diagonal[-1] = 200 diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:] H = H + np.diag(diagonal) return H 实现： import numpy as np from scipy.optimize import minimize def rosen_hess(x): x = np.asarray(x) H = np.diag(-400 * x[:-1], 1) - np.diag(400 * x[:-1], -1) diagonal = np.zeros_like(x) diagonal[0] = 1200 * x[0] ** 2 - 400 * x[1] + 2 diagonal[-1] = 200 diagonal[1:-1] = 202 + 1200 * x[1:-1] ** 2 - 400 * x[2:] H = H + np.diag(diagonal) return H # 初始迭代点 x0 = np.random.rand(10) * 2 res = minimize(rosen, x0, method=&#39;Newton-CG&#39;, jac=rosen_der, hess=rosen_hess, options={&#39;xtol&#39;: 1e-8, &#39;disp&#39;: True}) print(res.x) 2&nbsp;约束最小化多元标量函数 &nbsp; 2.1SLSQP(Sequential Least SQuares Programming optimization algorithm) 形如： 例子： def func(x, sign=1.0): &quot;&quot;&quot; Objective function &quot;&quot;&quot; return sign * (2 * x[0] * x[1] + 2 * x[0] - x[0] ** 2 - 2 * x[1] ** 2) # 导数，可有可无，可选的 def func_deriv(x, sign=1.0): &quot;&quot;&quot; Derivative of objective function &quot;&quot;&quot; dfdx0 = sign * (-2 * x[0] + 2 * x[1] + 2) dfdx1 = sign * (2 * x[0] - 4 * x[1]) return np.array([dfdx0, dfdx1]) # 约束 cons = ({&#39;type&#39;: &#39;eq&#39;, &#39;fun&#39;: lambda x: np.array([x[0] ** 3 - x[1]]), &#39;jac&#39;: lambda x: np.array([3.0 * (x[0] ** 2.0), -1.0])}, {&#39;type&#39;: &#39;ineq&#39;, &#39;fun&#39;: lambda x: np.array([x[1] - 1]), &#39;jac&#39;: lambda x: np.array([0.0, 1.0])}) 实现： import numpy as np from scipy.optimize import minimize res = minimize(func, [-1.0, 5.0], args=(-1.0,), jac=func_deriv, constraints=cons, method=&#39;SLSQP&#39;, options={&#39;disp&#39;: True}) print(res.x) &nbsp;2.2最小二乘最小化Least-squares、leastsq leastsq（最小化一组方程的平方和）：https://blog.csdn.net/jiang425776024/article/details/86801232 Least-squares（求解带变量边界的非线性最小二乘问题）： 形如： 例： def model(x, u): return x[0] * (u ** 2 + x[1] * u) / (u ** 2 + x[2] * u + x[3]) &nbsp; 导数： 拟合显示： from scipy.optimize import least_squares import numpy as np #原函数 def model(x, u): return x[0] * (u ** 2 + x[1] * u) / (u ** 2 + x[2] * u + x[3]) #损失函数 def fun(x, u, y): return model(x, u) - y #原函数的一阶导数，雅可比矩阵，可选 def jac(x, u, y): J = np.empty((u.size, x.size)) den = u ** 2 + x[2] * u + x[3] num = u ** 2 + x[1] * u J[:, 0] = num / den J[:, 1] = x[0] * u / den J[:, 2] = -x[0] * num * u / den ** 2 J[:, 3] = -x[0] * num / den ** 2 return J #训练数据x轴 u = np.array([4.0, 2.0, 1.0, 5.0e-1, 2.5e-1, 1.67e-1, 1.25e-1, 1.0e-1, 8.33e-2, 7.14e-2, 6.25e-2]) #参考真实值y y = np.array([1.957e-1, 1.947e-1, 1.735e-1, 1.6e-1, 8.44e-2, 6.27e-2, 4.56e-2, 3.42e-2, 3.23e-2, 2.35e-2, 2.46e-2]) #初始点，系数 x0 = np.array([2.5, 3.9, 4.15, 3.9]) #边界量0-100 res = least_squares(fun, x0, jac=jac, bounds=(0, 100), args=(u, y), verbose=1) print(res.x) import matplotlib.pyplot as plt #测试数据x u_test = np.linspace(0, 5) #测试结果y y_test = model(res.x, u_test) #原数据 plt.plot(u, y, &#39;o&#39;, markersize=4, label=&#39;data&#39;) #预测拟合结果 plt.plot(u_test, y_test, label=&#39;fitted model&#39;) plt.xlabel(&quot;u&quot;) plt.ylabel(&quot;y&quot;) plt.legend(loc=&#39;lower right&#39;) plt.show() 3.单变量函数最小化器 from scipy.optimize import minimize_scalar f = lambda x: (x - 2) * (x + 1)**2 res = minimize_scalar(f, method=&#39;brent&#39;) print(res.x) 4.有界最小化 from scipy.special import j1 res = minimize_scalar(j1, bounds=(4, 7), method=&#39;bounded&#39;) res.x 5.定制自己的最小化器 from scipy.optimize import OptimizeResult from scipy.optimize import minimize import numpy as np def rosen(x): &quot;&quot;&quot;The Rosenbrock function&quot;&quot;&quot; return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0) def custmin(fun, x0, args=(), maxfev=None, stepsize=0.1, maxiter=100, callback=None, **options): bestx = x0 besty = fun(x0) funcalls = 1 niter = 0 improved = True stop = False while improved and not stop and niter &lt; maxiter: improved = False niter += 1 for dim in range(np.size(x0)): for s in [bestx[dim] - stepsize, bestx[dim] + stepsize]: testx = np.copy(bestx) testx[dim] = s testy = fun(testx, *args) funcalls += 1 if testy &lt; besty: besty = testy bestx = testx improved = True if callback is not None: callback(bestx) if maxfev is not None and funcalls &gt;= maxfev: stop = True break return OptimizeResult(fun=besty, x=bestx, nit=niter, nfev=funcalls, success=(niter &gt; 1)) x0 = [1.35, 0.9, 0.8, 1.1, 1.2] res = minimize(rosen, x0, method=custmin, options=dict(stepsize=0.05)) print(res.x) 6.找根 1）f(x)=0的根 import numpy as np from scipy.optimize import root def func(x): return x + 2 * np.cos(x) sol = root(func, 0.3) print(sol.x, sol.fun) 2）两个等式的根&nbsp; &nbsp; import numpy as np from scipy.optimize import root def func2(x): f = [x[0] * np.cos(x[1]) - 4, x[1]*x[0] - x[1] - 5] df = np.array([[np.cos(x[1]), -x[0] * np.sin(x[1])], [x[1], x[0] - 1]]) return f, df sol = root(func2, [1, 1], jac=True, method=&#39;lm&#39;) print(sol.x, sol.fun) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<meta property="og:description" content="目录 0.scipy.optimize.minimize 1.无约束最小化多元标量函数 1.1Nelder-Mead（单纯形法） &nbsp;1.2拟牛顿法：BFGS算法 1.3牛顿 - 共轭梯度法：Newton-CG 2&nbsp;约束最小化多元标量函数 2.1SLSQP(Sequential Least SQuares Programming optimization algorithm) &nbsp;2.2最小二乘最小化Least-squares minimization 3.单变量函数最小化器 4.有界最小化 5.定制自己的最小化器 6.找根 1）f(x)=0的根 2）两个等式的根&nbsp; 3）寻找大问题的根源 scipy api：https://docs.scipy.org/doc/scipy-0.18.1/reference/index.html 优化器optimize的参数设置：https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.html#module-scipy.optimize&nbsp; 优化器optimize的使用：https://docs.scipy.org/doc/scipy-0.18.1/reference/tutorial/optimize.html#constrained-minimization-of-multivariate-scalar-functions-minimize 0.scipy.optimize.minimize scipy.optimize.minimize(fun,&nbsp;x0,&nbsp;args=(),&nbsp;method=None,&nbsp;jac=None,&nbsp;hess=None,&nbsp;hessp=None,&nbsp;bounds=None,&nbsp;constraints=(),&nbsp;tol=None,&nbsp;callback=None,&nbsp;options=None) fun：目标函数，返回单值， x0：初始迭代点， method：求解方法 ‘Nelder-Mead’&nbsp;(see here) ‘Powell’&nbsp;(see here) ‘CG’&nbsp;(see here) ‘BFGS’&nbsp;(see here) ‘Newton-CG’&nbsp;(see here) ‘L-BFGS-B’&nbsp;(see here) ‘TNC’&nbsp;(see here) ‘COBYLA’&nbsp;(see here) ‘SLSQP’&nbsp;(see here) ‘dogleg’&nbsp;(see here) ‘trust-ncg’&nbsp;(see here) jac:目标函数的雅可比矩阵。可选的。仅适用于CG，BFGS，Newton-CG，L-BFGS-B，TNC，SLSQP，dogleg，trust-ncg。如果jac是布尔值并且为True，则假定fun会返回梯度以及目标函数。如果为False，将以数字方式估计梯度。jac也可以是一个可调用的，返回目标的梯度。在这种情况下，它必须接受与fun相同的参数。 hess:可选的，目标函数的Hessian（二阶导数矩阵）或目标函数的Hessian乘以任意向量p。仅适用于Newton-CG，dogleg，trust-ncg。只需要给出hessp或hess中的一个。如果提供了hess，那么hessp将被忽略。如果没有提供hess和hessp，那么将使用jac上的有限差分来近似Hessian乘积。hessp必须将Hessian时间计算为任意向量。 bounds:序列，可选变量的界限（仅适用于L-BFGS-B，TNC和SLSQP）。（min，max）对x中每个元素的对，定义该参数的边界。 constraints:约束，类型有: ‘eq’ for equality, ‘ineq’ for inequality，如：constraints=cons cons = ({&#39;type&#39;: &#39;eq&#39;, &#39;fun&#39; : lambda x: np.array([x[0]**3 - x[1]]), &#39;jac&#39; : lambda x: np.array([3.0*(x[0]**2.0), -1.0])}, {&#39;type&#39;: &#39;ineq&#39;, &#39;fun&#39; : lambda x: np.array([x[1] - 1]), &#39;jac&#39; : lambda x: np.array([0.0, 1.0])}) callback(xk)：每次迭代召唤函数，需要有参数xk. options&nbsp;:&nbsp;字典可选， #容忍精度，是否打印， options={&#39;xtol&#39;: 1e-2, &#39;disp&#39;: True} &nbsp; 1.无约束最小化多元标量函数 1.1Nelder-Mead（单纯形法） 函数Rosenbrock&nbsp;： def rosen(x): &quot;&quot;&quot;The Rosenbrock function&quot;&quot;&quot; return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0) 求解：&nbsp; import numpy as np from scipy.optimize import minimize def callback(xk): print(xk) # 初始迭代点 x0 = np.random.rand(10) * 2 # 最小化优化器，方法：Nelder-Mead（单纯形法） res = minimize(rosen, x0, method=&#39;nelder-mead&#39;, options={&#39;xtol&#39;: 1e-2, &#39;disp&#39;: True}, callback=callback) print(res.x) # Optimization terminated successfully. Current function value: 0.036794 Iterations: 521 Function evaluations: 769 [1.00107996 1.00207144 1.00207692 1.00489278 1.00985838 1.0192085 1.03896708 1.08001283 1.16669137 1.36167574] &nbsp; &nbsp;1.2拟牛顿法：BFGS算法 拟牛顿法的核心思想是构造目标函数二阶导数矩阵黑塞矩阵的逆的近似矩阵，避免了解线性方程组求逆的大量计算，更加高效。介绍：https://blog.csdn.net/jiang425776024/article/details/87602847 函数Rosenbrock&nbsp;： def rosen(x): &quot;&quot;&quot;The Rosenbrock function&quot;&quot;&quot; return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0) 导数： def rosen_der(x): # rosen函数的雅可比矩阵 xm = x[1:-1] xm_m1 = x[:-2] xm_p1 = x[2:] der = np.zeros_like(x) der[1:-1] = 200 * (xm - xm_m1 ** 2) - 400 * (xm_p1 - xm ** 2) * xm - 2 * (1 - xm) der[0] = -400 * x[0] * (x[1] - x[0] ** 2) - 2 * (1 - x[0]) der[-1] = 200 * (x[-1] - x[-2] ** 2) return der 求解：&nbsp; import numpy as np from scipy.optimize import minimize # 初始迭代点 x0 = np.random.rand(10) * 2 res = minimize(rosen, x0, method=&#39;BFGS&#39;, jac=rosen_der, options={&#39;disp&#39;: True}) print(res.x) 1.3牛顿法：Newton-CG 利用黑塞矩阵和梯度来优化，介绍：https://blog.csdn.net/jiang425776024/article/details/87601854 函数Rosenbrock&nbsp;： 构造目标函数的近似二次型（泰勒展开）： 利用黑塞矩阵H和梯度做迭代 黑塞矩阵： def rosen_hess(x): x = np.asarray(x) H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1) diagonal = np.zeros_like(x) diagonal[0] = 1200*x[0]**2-400*x[1]+2 diagonal[-1] = 200 diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:] H = H + np.diag(diagonal) return H 实现： import numpy as np from scipy.optimize import minimize def rosen_hess(x): x = np.asarray(x) H = np.diag(-400 * x[:-1], 1) - np.diag(400 * x[:-1], -1) diagonal = np.zeros_like(x) diagonal[0] = 1200 * x[0] ** 2 - 400 * x[1] + 2 diagonal[-1] = 200 diagonal[1:-1] = 202 + 1200 * x[1:-1] ** 2 - 400 * x[2:] H = H + np.diag(diagonal) return H # 初始迭代点 x0 = np.random.rand(10) * 2 res = minimize(rosen, x0, method=&#39;Newton-CG&#39;, jac=rosen_der, hess=rosen_hess, options={&#39;xtol&#39;: 1e-8, &#39;disp&#39;: True}) print(res.x) 2&nbsp;约束最小化多元标量函数 &nbsp; 2.1SLSQP(Sequential Least SQuares Programming optimization algorithm) 形如： 例子： def func(x, sign=1.0): &quot;&quot;&quot; Objective function &quot;&quot;&quot; return sign * (2 * x[0] * x[1] + 2 * x[0] - x[0] ** 2 - 2 * x[1] ** 2) # 导数，可有可无，可选的 def func_deriv(x, sign=1.0): &quot;&quot;&quot; Derivative of objective function &quot;&quot;&quot; dfdx0 = sign * (-2 * x[0] + 2 * x[1] + 2) dfdx1 = sign * (2 * x[0] - 4 * x[1]) return np.array([dfdx0, dfdx1]) # 约束 cons = ({&#39;type&#39;: &#39;eq&#39;, &#39;fun&#39;: lambda x: np.array([x[0] ** 3 - x[1]]), &#39;jac&#39;: lambda x: np.array([3.0 * (x[0] ** 2.0), -1.0])}, {&#39;type&#39;: &#39;ineq&#39;, &#39;fun&#39;: lambda x: np.array([x[1] - 1]), &#39;jac&#39;: lambda x: np.array([0.0, 1.0])}) 实现： import numpy as np from scipy.optimize import minimize res = minimize(func, [-1.0, 5.0], args=(-1.0,), jac=func_deriv, constraints=cons, method=&#39;SLSQP&#39;, options={&#39;disp&#39;: True}) print(res.x) &nbsp;2.2最小二乘最小化Least-squares、leastsq leastsq（最小化一组方程的平方和）：https://blog.csdn.net/jiang425776024/article/details/86801232 Least-squares（求解带变量边界的非线性最小二乘问题）： 形如： 例： def model(x, u): return x[0] * (u ** 2 + x[1] * u) / (u ** 2 + x[2] * u + x[3]) &nbsp; 导数： 拟合显示： from scipy.optimize import least_squares import numpy as np #原函数 def model(x, u): return x[0] * (u ** 2 + x[1] * u) / (u ** 2 + x[2] * u + x[3]) #损失函数 def fun(x, u, y): return model(x, u) - y #原函数的一阶导数，雅可比矩阵，可选 def jac(x, u, y): J = np.empty((u.size, x.size)) den = u ** 2 + x[2] * u + x[3] num = u ** 2 + x[1] * u J[:, 0] = num / den J[:, 1] = x[0] * u / den J[:, 2] = -x[0] * num * u / den ** 2 J[:, 3] = -x[0] * num / den ** 2 return J #训练数据x轴 u = np.array([4.0, 2.0, 1.0, 5.0e-1, 2.5e-1, 1.67e-1, 1.25e-1, 1.0e-1, 8.33e-2, 7.14e-2, 6.25e-2]) #参考真实值y y = np.array([1.957e-1, 1.947e-1, 1.735e-1, 1.6e-1, 8.44e-2, 6.27e-2, 4.56e-2, 3.42e-2, 3.23e-2, 2.35e-2, 2.46e-2]) #初始点，系数 x0 = np.array([2.5, 3.9, 4.15, 3.9]) #边界量0-100 res = least_squares(fun, x0, jac=jac, bounds=(0, 100), args=(u, y), verbose=1) print(res.x) import matplotlib.pyplot as plt #测试数据x u_test = np.linspace(0, 5) #测试结果y y_test = model(res.x, u_test) #原数据 plt.plot(u, y, &#39;o&#39;, markersize=4, label=&#39;data&#39;) #预测拟合结果 plt.plot(u_test, y_test, label=&#39;fitted model&#39;) plt.xlabel(&quot;u&quot;) plt.ylabel(&quot;y&quot;) plt.legend(loc=&#39;lower right&#39;) plt.show() 3.单变量函数最小化器 from scipy.optimize import minimize_scalar f = lambda x: (x - 2) * (x + 1)**2 res = minimize_scalar(f, method=&#39;brent&#39;) print(res.x) 4.有界最小化 from scipy.special import j1 res = minimize_scalar(j1, bounds=(4, 7), method=&#39;bounded&#39;) res.x 5.定制自己的最小化器 from scipy.optimize import OptimizeResult from scipy.optimize import minimize import numpy as np def rosen(x): &quot;&quot;&quot;The Rosenbrock function&quot;&quot;&quot; return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0) def custmin(fun, x0, args=(), maxfev=None, stepsize=0.1, maxiter=100, callback=None, **options): bestx = x0 besty = fun(x0) funcalls = 1 niter = 0 improved = True stop = False while improved and not stop and niter &lt; maxiter: improved = False niter += 1 for dim in range(np.size(x0)): for s in [bestx[dim] - stepsize, bestx[dim] + stepsize]: testx = np.copy(bestx) testx[dim] = s testy = fun(testx, *args) funcalls += 1 if testy &lt; besty: besty = testy bestx = testx improved = True if callback is not None: callback(bestx) if maxfev is not None and funcalls &gt;= maxfev: stop = True break return OptimizeResult(fun=besty, x=bestx, nit=niter, nfev=funcalls, success=(niter &gt; 1)) x0 = [1.35, 0.9, 0.8, 1.1, 1.2] res = minimize(rosen, x0, method=custmin, options=dict(stepsize=0.05)) print(res.x) 6.找根 1）f(x)=0的根 import numpy as np from scipy.optimize import root def func(x): return x + 2 * np.cos(x) sol = root(func, 0.3) print(sol.x, sol.fun) 2）两个等式的根&nbsp; &nbsp; import numpy as np from scipy.optimize import root def func2(x): f = [x[0] * np.cos(x[1]) - 4, x[1]*x[0] - x[1] - 5] df = np.array([[np.cos(x[1]), -x[0] * np.sin(x[1])], [x[1], x[0] - 1]]) return f, df sol = root(func2, [1, 1], jac=True, method=&#39;lm&#39;) print(sol.x, sol.fun) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/02/22/b592e5e57adaf33462667cde4fa1fff1.html" />
<meta property="og:url" content="https://mlh.app/2019/02/22/b592e5e57adaf33462667cde4fa1fff1.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-22T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"目录 0.scipy.optimize.minimize 1.无约束最小化多元标量函数 1.1Nelder-Mead（单纯形法） &nbsp;1.2拟牛顿法：BFGS算法 1.3牛顿 - 共轭梯度法：Newton-CG 2&nbsp;约束最小化多元标量函数 2.1SLSQP(Sequential Least SQuares Programming optimization algorithm) &nbsp;2.2最小二乘最小化Least-squares minimization 3.单变量函数最小化器 4.有界最小化 5.定制自己的最小化器 6.找根 1）f(x)=0的根 2）两个等式的根&nbsp; 3）寻找大问题的根源 scipy api：https://docs.scipy.org/doc/scipy-0.18.1/reference/index.html 优化器optimize的参数设置：https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.html#module-scipy.optimize&nbsp; 优化器optimize的使用：https://docs.scipy.org/doc/scipy-0.18.1/reference/tutorial/optimize.html#constrained-minimization-of-multivariate-scalar-functions-minimize 0.scipy.optimize.minimize scipy.optimize.minimize(fun,&nbsp;x0,&nbsp;args=(),&nbsp;method=None,&nbsp;jac=None,&nbsp;hess=None,&nbsp;hessp=None,&nbsp;bounds=None,&nbsp;constraints=(),&nbsp;tol=None,&nbsp;callback=None,&nbsp;options=None) fun：目标函数，返回单值， x0：初始迭代点， method：求解方法 ‘Nelder-Mead’&nbsp;(see here) ‘Powell’&nbsp;(see here) ‘CG’&nbsp;(see here) ‘BFGS’&nbsp;(see here) ‘Newton-CG’&nbsp;(see here) ‘L-BFGS-B’&nbsp;(see here) ‘TNC’&nbsp;(see here) ‘COBYLA’&nbsp;(see here) ‘SLSQP’&nbsp;(see here) ‘dogleg’&nbsp;(see here) ‘trust-ncg’&nbsp;(see here) jac:目标函数的雅可比矩阵。可选的。仅适用于CG，BFGS，Newton-CG，L-BFGS-B，TNC，SLSQP，dogleg，trust-ncg。如果jac是布尔值并且为True，则假定fun会返回梯度以及目标函数。如果为False，将以数字方式估计梯度。jac也可以是一个可调用的，返回目标的梯度。在这种情况下，它必须接受与fun相同的参数。 hess:可选的，目标函数的Hessian（二阶导数矩阵）或目标函数的Hessian乘以任意向量p。仅适用于Newton-CG，dogleg，trust-ncg。只需要给出hessp或hess中的一个。如果提供了hess，那么hessp将被忽略。如果没有提供hess和hessp，那么将使用jac上的有限差分来近似Hessian乘积。hessp必须将Hessian时间计算为任意向量。 bounds:序列，可选变量的界限（仅适用于L-BFGS-B，TNC和SLSQP）。（min，max）对x中每个元素的对，定义该参数的边界。 constraints:约束，类型有: ‘eq’ for equality, ‘ineq’ for inequality，如：constraints=cons cons = ({&#39;type&#39;: &#39;eq&#39;, &#39;fun&#39; : lambda x: np.array([x[0]**3 - x[1]]), &#39;jac&#39; : lambda x: np.array([3.0*(x[0]**2.0), -1.0])}, {&#39;type&#39;: &#39;ineq&#39;, &#39;fun&#39; : lambda x: np.array([x[1] - 1]), &#39;jac&#39; : lambda x: np.array([0.0, 1.0])}) callback(xk)：每次迭代召唤函数，需要有参数xk. options&nbsp;:&nbsp;字典可选， #容忍精度，是否打印， options={&#39;xtol&#39;: 1e-2, &#39;disp&#39;: True} &nbsp; 1.无约束最小化多元标量函数 1.1Nelder-Mead（单纯形法） 函数Rosenbrock&nbsp;： def rosen(x): &quot;&quot;&quot;The Rosenbrock function&quot;&quot;&quot; return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0) 求解：&nbsp; import numpy as np from scipy.optimize import minimize def callback(xk): print(xk) # 初始迭代点 x0 = np.random.rand(10) * 2 # 最小化优化器，方法：Nelder-Mead（单纯形法） res = minimize(rosen, x0, method=&#39;nelder-mead&#39;, options={&#39;xtol&#39;: 1e-2, &#39;disp&#39;: True}, callback=callback) print(res.x) # Optimization terminated successfully. Current function value: 0.036794 Iterations: 521 Function evaluations: 769 [1.00107996 1.00207144 1.00207692 1.00489278 1.00985838 1.0192085 1.03896708 1.08001283 1.16669137 1.36167574] &nbsp; &nbsp;1.2拟牛顿法：BFGS算法 拟牛顿法的核心思想是构造目标函数二阶导数矩阵黑塞矩阵的逆的近似矩阵，避免了解线性方程组求逆的大量计算，更加高效。介绍：https://blog.csdn.net/jiang425776024/article/details/87602847 函数Rosenbrock&nbsp;： def rosen(x): &quot;&quot;&quot;The Rosenbrock function&quot;&quot;&quot; return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0) 导数： def rosen_der(x): # rosen函数的雅可比矩阵 xm = x[1:-1] xm_m1 = x[:-2] xm_p1 = x[2:] der = np.zeros_like(x) der[1:-1] = 200 * (xm - xm_m1 ** 2) - 400 * (xm_p1 - xm ** 2) * xm - 2 * (1 - xm) der[0] = -400 * x[0] * (x[1] - x[0] ** 2) - 2 * (1 - x[0]) der[-1] = 200 * (x[-1] - x[-2] ** 2) return der 求解：&nbsp; import numpy as np from scipy.optimize import minimize # 初始迭代点 x0 = np.random.rand(10) * 2 res = minimize(rosen, x0, method=&#39;BFGS&#39;, jac=rosen_der, options={&#39;disp&#39;: True}) print(res.x) 1.3牛顿法：Newton-CG 利用黑塞矩阵和梯度来优化，介绍：https://blog.csdn.net/jiang425776024/article/details/87601854 函数Rosenbrock&nbsp;： 构造目标函数的近似二次型（泰勒展开）： 利用黑塞矩阵H和梯度做迭代 黑塞矩阵： def rosen_hess(x): x = np.asarray(x) H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1) diagonal = np.zeros_like(x) diagonal[0] = 1200*x[0]**2-400*x[1]+2 diagonal[-1] = 200 diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:] H = H + np.diag(diagonal) return H 实现： import numpy as np from scipy.optimize import minimize def rosen_hess(x): x = np.asarray(x) H = np.diag(-400 * x[:-1], 1) - np.diag(400 * x[:-1], -1) diagonal = np.zeros_like(x) diagonal[0] = 1200 * x[0] ** 2 - 400 * x[1] + 2 diagonal[-1] = 200 diagonal[1:-1] = 202 + 1200 * x[1:-1] ** 2 - 400 * x[2:] H = H + np.diag(diagonal) return H # 初始迭代点 x0 = np.random.rand(10) * 2 res = minimize(rosen, x0, method=&#39;Newton-CG&#39;, jac=rosen_der, hess=rosen_hess, options={&#39;xtol&#39;: 1e-8, &#39;disp&#39;: True}) print(res.x) 2&nbsp;约束最小化多元标量函数 &nbsp; 2.1SLSQP(Sequential Least SQuares Programming optimization algorithm) 形如： 例子： def func(x, sign=1.0): &quot;&quot;&quot; Objective function &quot;&quot;&quot; return sign * (2 * x[0] * x[1] + 2 * x[0] - x[0] ** 2 - 2 * x[1] ** 2) # 导数，可有可无，可选的 def func_deriv(x, sign=1.0): &quot;&quot;&quot; Derivative of objective function &quot;&quot;&quot; dfdx0 = sign * (-2 * x[0] + 2 * x[1] + 2) dfdx1 = sign * (2 * x[0] - 4 * x[1]) return np.array([dfdx0, dfdx1]) # 约束 cons = ({&#39;type&#39;: &#39;eq&#39;, &#39;fun&#39;: lambda x: np.array([x[0] ** 3 - x[1]]), &#39;jac&#39;: lambda x: np.array([3.0 * (x[0] ** 2.0), -1.0])}, {&#39;type&#39;: &#39;ineq&#39;, &#39;fun&#39;: lambda x: np.array([x[1] - 1]), &#39;jac&#39;: lambda x: np.array([0.0, 1.0])}) 实现： import numpy as np from scipy.optimize import minimize res = minimize(func, [-1.0, 5.0], args=(-1.0,), jac=func_deriv, constraints=cons, method=&#39;SLSQP&#39;, options={&#39;disp&#39;: True}) print(res.x) &nbsp;2.2最小二乘最小化Least-squares、leastsq leastsq（最小化一组方程的平方和）：https://blog.csdn.net/jiang425776024/article/details/86801232 Least-squares（求解带变量边界的非线性最小二乘问题）： 形如： 例： def model(x, u): return x[0] * (u ** 2 + x[1] * u) / (u ** 2 + x[2] * u + x[3]) &nbsp; 导数： 拟合显示： from scipy.optimize import least_squares import numpy as np #原函数 def model(x, u): return x[0] * (u ** 2 + x[1] * u) / (u ** 2 + x[2] * u + x[3]) #损失函数 def fun(x, u, y): return model(x, u) - y #原函数的一阶导数，雅可比矩阵，可选 def jac(x, u, y): J = np.empty((u.size, x.size)) den = u ** 2 + x[2] * u + x[3] num = u ** 2 + x[1] * u J[:, 0] = num / den J[:, 1] = x[0] * u / den J[:, 2] = -x[0] * num * u / den ** 2 J[:, 3] = -x[0] * num / den ** 2 return J #训练数据x轴 u = np.array([4.0, 2.0, 1.0, 5.0e-1, 2.5e-1, 1.67e-1, 1.25e-1, 1.0e-1, 8.33e-2, 7.14e-2, 6.25e-2]) #参考真实值y y = np.array([1.957e-1, 1.947e-1, 1.735e-1, 1.6e-1, 8.44e-2, 6.27e-2, 4.56e-2, 3.42e-2, 3.23e-2, 2.35e-2, 2.46e-2]) #初始点，系数 x0 = np.array([2.5, 3.9, 4.15, 3.9]) #边界量0-100 res = least_squares(fun, x0, jac=jac, bounds=(0, 100), args=(u, y), verbose=1) print(res.x) import matplotlib.pyplot as plt #测试数据x u_test = np.linspace(0, 5) #测试结果y y_test = model(res.x, u_test) #原数据 plt.plot(u, y, &#39;o&#39;, markersize=4, label=&#39;data&#39;) #预测拟合结果 plt.plot(u_test, y_test, label=&#39;fitted model&#39;) plt.xlabel(&quot;u&quot;) plt.ylabel(&quot;y&quot;) plt.legend(loc=&#39;lower right&#39;) plt.show() 3.单变量函数最小化器 from scipy.optimize import minimize_scalar f = lambda x: (x - 2) * (x + 1)**2 res = minimize_scalar(f, method=&#39;brent&#39;) print(res.x) 4.有界最小化 from scipy.special import j1 res = minimize_scalar(j1, bounds=(4, 7), method=&#39;bounded&#39;) res.x 5.定制自己的最小化器 from scipy.optimize import OptimizeResult from scipy.optimize import minimize import numpy as np def rosen(x): &quot;&quot;&quot;The Rosenbrock function&quot;&quot;&quot; return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0) def custmin(fun, x0, args=(), maxfev=None, stepsize=0.1, maxiter=100, callback=None, **options): bestx = x0 besty = fun(x0) funcalls = 1 niter = 0 improved = True stop = False while improved and not stop and niter &lt; maxiter: improved = False niter += 1 for dim in range(np.size(x0)): for s in [bestx[dim] - stepsize, bestx[dim] + stepsize]: testx = np.copy(bestx) testx[dim] = s testy = fun(testx, *args) funcalls += 1 if testy &lt; besty: besty = testy bestx = testx improved = True if callback is not None: callback(bestx) if maxfev is not None and funcalls &gt;= maxfev: stop = True break return OptimizeResult(fun=besty, x=bestx, nit=niter, nfev=funcalls, success=(niter &gt; 1)) x0 = [1.35, 0.9, 0.8, 1.1, 1.2] res = minimize(rosen, x0, method=custmin, options=dict(stepsize=0.05)) print(res.x) 6.找根 1）f(x)=0的根 import numpy as np from scipy.optimize import root def func(x): return x + 2 * np.cos(x) sol = root(func, 0.3) print(sol.x, sol.fun) 2）两个等式的根&nbsp; &nbsp; import numpy as np from scipy.optimize import root def func2(x): f = [x[0] * np.cos(x[1]) - 4, x[1]*x[0] - x[1] - 5] df = np.array([[np.cos(x[1]), -x[0] * np.sin(x[1])], [x[1], x[0] - 1]]) return f, df sol = root(func2, [1, 1], jac=True, method=&#39;lm&#39;) print(sol.x, sol.fun) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/02/22/b592e5e57adaf33462667cde4fa1fff1.html","headline":"scipy.optimize优化器的各种使用","dateModified":"2019-02-22T00:00:00+08:00","datePublished":"2019-02-22T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/02/22/b592e5e57adaf33462667cde4fa1fff1.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>scipy.optimize优化器的各种使用</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p id="main-toc"><strong>目录</strong></p> 
  <p id="0.scipy.optimize.minimize-toc" style="margin-left:0px;"><a href="#0.scipy.optimize.minimize" rel="nofollow">0.scipy.optimize.minimize</a></p> 
  <p id="1.%E6%97%A0%E7%BA%A6%E6%9D%9F%E6%9C%80%E5%B0%8F%E5%8C%96%E5%A4%9A%E5%85%83%E6%A0%87%E9%87%8F%E5%87%BD%E6%95%B0-toc" style="margin-left:0px;"><a href="#1.%E6%97%A0%E7%BA%A6%E6%9D%9F%E6%9C%80%E5%B0%8F%E5%8C%96%E5%A4%9A%E5%85%83%E6%A0%87%E9%87%8F%E5%87%BD%E6%95%B0" rel="nofollow">1.无约束最小化多元标量函数</a></p> 
  <p id="1.1Nelder-Mead%EF%BC%88%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95%EF%BC%89-toc" style="margin-left:40px;"><a href="#1.1Nelder-Mead%EF%BC%88%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95%EF%BC%89" rel="nofollow">1.1Nelder-Mead（单纯形法）</a></p> 
  <p id="%C2%A01.2%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95%EF%BC%9ABFGS%E7%AE%97%E6%B3%95-toc" style="margin-left:40px;"><a href="#%C2%A01.2%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95%EF%BC%9ABFGS%E7%AE%97%E6%B3%95" rel="nofollow">&nbsp;1.2拟牛顿法：BFGS算法</a></p> 
  <p id="1.3%E7%89%9B%E9%A1%BF%20-%20%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%EF%BC%9ANewton-CG-toc" style="margin-left:40px;"><a href="#1.3%E7%89%9B%E9%A1%BF%20-%20%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%EF%BC%9ANewton-CG" rel="nofollow">1.3牛顿 - 共轭梯度法：Newton-CG</a></p> 
  <p id="2%C2%A0%E7%BA%A6%E6%9D%9F%E6%9C%80%E5%B0%8F%E5%8C%96%E5%A4%9A%E5%85%83%E6%A0%87%E9%87%8F%E5%87%BD%E6%95%B0-toc" style="margin-left:0px;"><a href="#2%C2%A0%E7%BA%A6%E6%9D%9F%E6%9C%80%E5%B0%8F%E5%8C%96%E5%A4%9A%E5%85%83%E6%A0%87%E9%87%8F%E5%87%BD%E6%95%B0" rel="nofollow">2&nbsp;约束最小化多元标量函数</a></p> 
  <p id="2.1SLSQP(Sequential%20Least%20SQuares%20Programming%20optimization%20algorithm)-toc" style="margin-left:40px;"><a href="#2.1SLSQP(Sequential%20Least%20SQuares%20Programming%20optimization%20algorithm)" rel="nofollow">2.1SLSQP(Sequential Least SQuares Programming optimization algorithm)</a></p> 
  <p id="%C2%A02.2%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%9C%80%E5%B0%8F%E5%8C%96Least-squares%20minimization-toc" style="margin-left:40px;"><a href="#%C2%A02.2%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%9C%80%E5%B0%8F%E5%8C%96Least-squares%20minimization" rel="nofollow">&nbsp;2.2最小二乘最小化Least-squares minimization</a></p> 
  <p id="3.%E5%8D%95%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E6%9C%80%E5%B0%8F%E5%8C%96%E5%99%A8-toc" style="margin-left:0px;"><a href="#3.%E5%8D%95%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E6%9C%80%E5%B0%8F%E5%8C%96%E5%99%A8" rel="nofollow">3.单变量函数最小化器</a></p> 
  <p id="%C2%A04.%E6%9C%89%E7%95%8C%E6%9C%80%E5%B0%8F%E5%8C%96-toc" style="margin-left:0px;"><a href="#%C2%A04.%E6%9C%89%E7%95%8C%E6%9C%80%E5%B0%8F%E5%8C%96" rel="nofollow">4.有界最小化</a></p> 
  <p id="5.%E5%AE%9A%E5%88%B6%E6%9C%80%E5%B0%8F%E5%8C%96%E5%99%A8-toc" style="margin-left:0px;"><a href="#5.%E5%AE%9A%E5%88%B6%E6%9C%80%E5%B0%8F%E5%8C%96%E5%99%A8" rel="nofollow">5.定制自己的最小化器</a></p> 
  <p id="6.%E6%89%BE%E6%A0%B9-toc" style="margin-left:0px;"><a href="#6.%E6%89%BE%E6%A0%B9" rel="nofollow">6.找根</a></p> 
  <p id="1%EF%BC%89f(x)%3D0%E7%9A%84%E6%A0%B9-toc" style="margin-left:40px;"><a href="#1%EF%BC%89f(x)%3D0%E7%9A%84%E6%A0%B9" rel="nofollow">1）f(x)=0的根</a></p> 
  <p id="2%EF%BC%89%E4%B8%A4%E4%B8%AA%E7%AD%89%E5%BC%8F%E7%9A%84%E6%A0%B9%C2%A0-toc" style="margin-left:40px;"><a href="#2%EF%BC%89%E4%B8%A4%E4%B8%AA%E7%AD%89%E5%BC%8F%E7%9A%84%E6%A0%B9%C2%A0" rel="nofollow">2）两个等式的根&nbsp;</a></p> 
  <p id="3%EF%BC%89%E5%AF%BB%E6%89%BE%E5%A4%A7%E9%97%AE%E9%A2%98%E7%9A%84%E6%A0%B9%E6%BA%90-toc" style="margin-left:40px;"><a href="#3%EF%BC%89%E5%AF%BB%E6%89%BE%E5%A4%A7%E9%97%AE%E9%A2%98%E7%9A%84%E6%A0%B9%E6%BA%90" rel="nofollow">3）寻找大问题的根源</a></p> 
  <hr id="hr-toc">
  <p>scipy api：<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/index.html" rel="nofollow">https://docs.scipy.org/doc/scipy-0.18.1/reference/index.html</a></p> 
  <p>优化器optimize的参数设置：<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.html#module-scipy.optimize" rel="nofollow">https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.html#module-scipy.optimize</a>&nbsp;</p> 
  <p>优化器optimize的使用：<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/tutorial/optimize.html#constrained-minimization-of-multivariate-scalar-functions-minimize" rel="nofollow">https://docs.scipy.org/doc/scipy-0.18.1/reference/tutorial/optimize.html#constrained-minimization-of-multivariate-scalar-functions-minimize</a></p> 
  <hr>
  <h1 id="0.scipy.optimize.minimize">0.scipy.optimize.minimize</h1> 
  <p style="text-indent:50px;">scipy.optimize.minimize(<em>fun</em>,&nbsp;<em>x0</em>,&nbsp;<em>args=()</em>,&nbsp;<em>method=None</em>,&nbsp;<em>jac=None</em>,&nbsp;<em>hess=None</em>,&nbsp;<em>hessp=None</em>,&nbsp;<em>bounds=None</em>,&nbsp;<em>constraints=()</em>,&nbsp;<em>tol=None</em>,&nbsp;<em>callback=None</em>,&nbsp;<em>options=None</em>)</p> 
  <p><em>fun：目标函数，返回单值，</em></p> 
  <p><em>x0：初始迭代点，</em></p> 
  <p><em>method：求解方法</em></p> 
  <blockquote> 
   <blockquote> 
    <ul>
     <li>‘Nelder-Mead’&nbsp;<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.minimize-neldermead.html#optimize-minimize-neldermead" rel="nofollow"><em>(see here)</em></a></li> 
     <li>‘Powell’&nbsp;<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.minimize-powell.html#optimize-minimize-powell" rel="nofollow"><em>(see here)</em></a></li> 
     <li>‘CG’&nbsp;<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.minimize-cg.html#optimize-minimize-cg" rel="nofollow"><em>(see here)</em></a></li> 
     <li>‘BFGS’&nbsp;<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.minimize-bfgs.html#optimize-minimize-bfgs" rel="nofollow"><em>(see here)</em></a></li> 
     <li>‘Newton-CG’&nbsp;<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.minimize-newtoncg.html#optimize-minimize-newtoncg" rel="nofollow"><em>(see here)</em></a></li> 
     <li>‘L-BFGS-B’&nbsp;<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb" rel="nofollow"><em>(see here)</em></a></li> 
     <li>‘TNC’&nbsp;<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.minimize-tnc.html#optimize-minimize-tnc" rel="nofollow"><em>(see here)</em></a></li> 
     <li>‘COBYLA’&nbsp;<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.minimize-cobyla.html#optimize-minimize-cobyla" rel="nofollow"><em>(see here)</em></a></li> 
     <li>‘SLSQP’&nbsp;<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp" rel="nofollow"><em>(see here)</em></a></li> 
     <li>‘dogleg’&nbsp;<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.minimize-dogleg.html#optimize-minimize-dogleg" rel="nofollow"><em>(see here)</em></a></li> 
     <li>‘trust-ncg’&nbsp;<a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/optimize.minimize-trustncg.html#optimize-minimize-trustncg" rel="nofollow"><em>(see here)</em></a></li> 
    </ul>
   </blockquote> 
  </blockquote> 
  <p>jac:目标函数的雅可比矩阵。可选的。仅适用于CG，BFGS，Newton-CG，L-BFGS-B，TNC，SLSQP，dogleg，trust-ncg。如果jac是布尔值并且为True，则假定fun会返回梯度以及目标函数。如果为False，将以数字方式估计梯度。jac也可以是一个可调用的，返回目标的梯度。在这种情况下，它必须接受与fun相同的参数。</p> 
  <p>hess:可选的，目标函数的Hessian（二阶导数矩阵）或目标函数的Hessian乘以任意向量p。仅适用于Newton-CG，dogleg，trust-ncg。只需要给出hessp或hess中的一个。如果提供了hess，那么hessp将被忽略。如果没有提供hess和hessp，那么将使用jac上的有限差分来近似Hessian乘积。hessp必须将Hessian时间计算为任意向量。</p> 
  <p>bounds:序列，可选变量的界限（仅适用于L-BFGS-B，TNC和SLSQP）。（min，max）对x中每个元素的对，定义该参数的边界。</p> 
  <p>constraints:约束，类型有: ‘eq’ for equality, ‘ineq’ for inequality，如：constraints=cons</p> 
  <p><img alt="\large x^3 - y= 0,y - 1\geq 0" class="mathcode" src="https://private.codecogs.com/gif.latex?%5Clarge%20x%5E3%20-%20y%3D%200%2Cy%20-%201%5Cgeq%200"></p> 
  <pre class="has">
<code>cons = ({'type': 'eq',
         'fun' : lambda x: np.array([x[0]**3 - x[1]]),
         'jac' : lambda x: np.array([3.0*(x[0]**2.0), -1.0])},
        {'type': 'ineq',
         'fun' : lambda x: np.array([x[1] - 1]),
         'jac' : lambda x: np.array([0.0, 1.0])})</code></pre> 
  <p>callback(xk)：每次迭代召唤函数，需要有参数xk.</p> 
  <p><strong>options</strong>&nbsp;:&nbsp;字典可选，</p> 
  <pre class="has">
<code>#容忍精度，是否打印，
options={'xtol': 1e-2, 'disp': True}</code></pre> 
  <hr>
  <p>&nbsp;</p> 
  <p>1.无约束最小化多元标量函数</p> 
  <h2 id="1.1Nelder-Mead%EF%BC%88%E5%8D%95%E7%BA%AF%E5%BD%A2%E6%B3%95%EF%BC%89">1.1Nelder-Mead（单纯形法）</h2> 
  <p>函数Rosenbrock&nbsp;：</p> 
  <p><img alt="\large f\left(\mathbf{x}\right)=\sum_{i=1}^{N-1}100\left(x_{i}-x_{i-1}^{2}\right)^{2}+\left(1-x_{i-1}\right)^{2}." class="mathcode" src="https://private.codecogs.com/gif.latex?%5Clarge%20f%5Cleft%28%5Cmathbf%7Bx%7D%5Cright%29%3D%5Csum_%7Bi%3D1%7D%5E%7BN-1%7D100%5Cleft%28x_%7Bi%7D-x_%7Bi-1%7D%5E%7B2%7D%5Cright%29%5E%7B2%7D&amp;plus;%5Cleft%281-x_%7Bi-1%7D%5Cright%29%5E%7B2%7D."></p> 
  <pre class="has">
<code>def rosen(x):
    """The Rosenbrock function"""
    return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0)</code></pre> 
  <p>求解：&nbsp;</p> 
  <pre class="has">
<code>import numpy as np
from scipy.optimize import minimize


def callback(xk):
    print(xk)


# 初始迭代点
x0 = np.random.rand(10) * 2

# 最小化优化器，方法：Nelder-Mead（单纯形法）
res = minimize(rosen, x0, method='nelder-mead',
               options={'xtol': 1e-2, 'disp': True}, callback=callback)

print(res.x)

#
Optimization terminated successfully.
         Current function value: 0.036794
         Iterations: 521
         Function evaluations: 769
[1.00107996 1.00207144 1.00207692 1.00489278 1.00985838 1.0192085
 1.03896708 1.08001283 1.16669137 1.36167574]</code></pre> 
  <p>&nbsp;</p> 
  <hr>
  <h2 id="%C2%A01.2%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95%EF%BC%9ABFGS%E7%AE%97%E6%B3%95"><span style="color:#f33b45;">&nbsp;1.2拟牛顿法：BFGS算法</span></h2> 
  <p style="text-indent:50px;">拟牛顿法的核心思想是构造目标函数二阶导数矩阵黑塞矩阵的逆的近似矩阵，避免了解线性方程组求逆的大量计算，更加高效。介绍：<a href="https://blog.csdn.net/jiang425776024/article/details/87602847" rel="nofollow">https://blog.csdn.net/jiang425776024/article/details/87602847</a></p> 
  <p>函数Rosenbrock&nbsp;：</p> 
  <p><img alt="\large f\left(\mathbf{x}\right)=\sum_{i=1}^{N-1}100\left(x_{i}-x_{i-1}^{2}\right)^{2}+\left(1-x_{i-1}\right)^{2}." class="mathcode" src="https://private.codecogs.com/gif.latex?%5Clarge%20f%5Cleft%28%5Cmathbf%7Bx%7D%5Cright%29%3D%5Csum_%7Bi%3D1%7D%5E%7BN-1%7D100%5Cleft%28x_%7Bi%7D-x_%7Bi-1%7D%5E%7B2%7D%5Cright%29%5E%7B2%7D&amp;plus;%5Cleft%281-x_%7Bi-1%7D%5Cright%29%5E%7B2%7D."></p> 
  <pre class="has">
<code>def rosen(x):
    """The Rosenbrock function"""
    return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0)</code></pre> 
  <p>导数：</p> 
  <p><img alt="" class="has" height="88" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222220518616.png" width="490"></p> 
  <p><img alt="" class="has" height="102" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019022222053264.png" width="347"></p> 
  <pre class="has">
<code>def rosen_der(x):
    # rosen函数的雅可比矩阵
    xm = x[1:-1]
    xm_m1 = x[:-2]
    xm_p1 = x[2:]
    der = np.zeros_like(x)
    der[1:-1] = 200 * (xm - xm_m1 ** 2) - 400 * (xm_p1 - xm ** 2) * xm - 2 * (1 - xm)
    der[0] = -400 * x[0] * (x[1] - x[0] ** 2) - 2 * (1 - x[0])
    der[-1] = 200 * (x[-1] - x[-2] ** 2)
    return der</code></pre> 
  <p>求解：&nbsp;</p> 
  <pre class="has">
<code>import numpy as np
from scipy.optimize import minimize


# 初始迭代点
x0 = np.random.rand(10) * 2

res = minimize(rosen, x0, method='BFGS', jac=rosen_der,
               options={'disp': True})

print(res.x)
</code></pre> 
  <hr>
  <h2 id="1.3%E7%89%9B%E9%A1%BF%20-%20%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%EF%BC%9ANewton-CG">1.3牛顿法：Newton-CG</h2> 
  <p>利用黑塞矩阵和梯度来优化，介绍：<a href="https://blog.csdn.net/jiang425776024/article/details/87601854" rel="nofollow">https://blog.csdn.net/jiang425776024/article/details/87601854</a></p> 
  <p>函数Rosenbrock&nbsp;：</p> 
  <p><img alt="\large f\left(\mathbf{x}\right)=\sum_{i=1}^{N-1}100\left(x_{i}-x_{i-1}^{2}\right)^{2}+\left(1-x_{i-1}\right)^{2}." class="mathcode" src="https://private.codecogs.com/gif.latex?%5Clarge%20f%5Cleft%28%5Cmathbf%7Bx%7D%5Cright%29%3D%5Csum_%7Bi%3D1%7D%5E%7BN-1%7D100%5Cleft%28x_%7Bi%7D-x_%7Bi-1%7D%5E%7B2%7D%5Cright%29%5E%7B2%7D&amp;plus;%5Cleft%281-x_%7Bi-1%7D%5Cright%29%5E%7B2%7D."></p> 
  <p>构造目标函数的近似二次型（泰勒展开）：</p> 
  <p><img alt="\large f\left(\mathbf{x}\right)\approx f\left(\mathbf{x}_{0}\right)+\nabla f\left(\mathbf{x}_{0}\right)\cdot\left(\mathbf{x}-\mathbf{x}_{0}\right)+\frac{1}{2}\left(\mathbf{x}-\mathbf{x}_{0}\right)^{T}\mathbf{H}\left(\mathbf{x}_{0}\right)\left(\mathbf{x}-\mathbf{x}_{0}\right)." class="mathcode" src="https://private.codecogs.com/gif.latex?%5Clarge%20f%5Cleft%28%5Cmathbf%7Bx%7D%5Cright%29%5Capprox%20f%5Cleft%28%5Cmathbf%7Bx%7D_%7B0%7D%5Cright%29&amp;plus;%5Cnabla%20f%5Cleft%28%5Cmathbf%7Bx%7D_%7B0%7D%5Cright%29%5Ccdot%5Cleft%28%5Cmathbf%7Bx%7D-%5Cmathbf%7Bx%7D_%7B0%7D%5Cright%29&amp;plus;%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Cmathbf%7Bx%7D-%5Cmathbf%7Bx%7D_%7B0%7D%5Cright%29%5E%7BT%7D%5Cmathbf%7BH%7D%5Cleft%28%5Cmathbf%7Bx%7D_%7B0%7D%5Cright%29%5Cleft%28%5Cmathbf%7Bx%7D-%5Cmathbf%7Bx%7D_%7B0%7D%5Cright%29."></p> 
  <p>利用黑塞矩阵H和梯度做迭代</p> 
  <p><img alt="\large \mathbf{x}_{\textrm{opt}}=\mathbf{x}_{0}-\mathbf{H}^{-1}\nabla f." class="mathcode" src="https://private.codecogs.com/gif.latex?%5Clarge%20%5Cmathbf%7Bx%7D_%7B%5Ctextrm%7Bopt%7D%7D%3D%5Cmathbf%7Bx%7D_%7B0%7D-%5Cmathbf%7BH%7D%5E%7B-1%7D%5Cnabla%20f."></p> 
  <p>黑塞矩阵：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="326" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222221227957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="802"></p> 
  <pre class="has">
<code>def rosen_hess(x):
    x = np.asarray(x)
    H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)
    diagonal = np.zeros_like(x)
    diagonal[0] = 1200*x[0]**2-400*x[1]+2
    diagonal[-1] = 200
    diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]
    H = H + np.diag(diagonal)
    return H</code></pre> 
  <p>实现：</p> 
  <pre class="has">
<code>import numpy as np
from scipy.optimize import minimize


def rosen_hess(x):
    x = np.asarray(x)
    H = np.diag(-400 * x[:-1], 1) - np.diag(400 * x[:-1], -1)
    diagonal = np.zeros_like(x)
    diagonal[0] = 1200 * x[0] ** 2 - 400 * x[1] + 2
    diagonal[-1] = 200
    diagonal[1:-1] = 202 + 1200 * x[1:-1] ** 2 - 400 * x[2:]
    H = H + np.diag(diagonal)
    return H


# 初始迭代点
x0 = np.random.rand(10) * 2

res = minimize(rosen, x0, method='Newton-CG',
               jac=rosen_der, hess=rosen_hess,
               options={'xtol': 1e-8, 'disp': True})

print(res.x)
</code></pre> 
  <hr>
  <h1 id="2%C2%A0%E7%BA%A6%E6%9D%9F%E6%9C%80%E5%B0%8F%E5%8C%96%E5%A4%9A%E5%85%83%E6%A0%87%E9%87%8F%E5%87%BD%E6%95%B0">2&nbsp;约束最小化多元标量函数</h1> 
  <p>&nbsp;</p> 
  <h2 id="2.1SLSQP(Sequential%20Least%20SQuares%20Programming%20optimization%20algorithm)">2.1<em>SLSQP</em>(Sequential Least SQuares Programming optimization algorithm)</h2> 
  <p>形如：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="103" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222221613772.png" width="363"></p> 
  <p>例子：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="114" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222221631315.png" width="234"></p> 
  <pre class="has">
<code>def func(x, sign=1.0):
    """ Objective function """
    return sign * (2 * x[0] * x[1] + 2 * x[0] - x[0] ** 2 - 2 * x[1] ** 2)

# 导数，可有可无，可选的
def func_deriv(x, sign=1.0):
    """ Derivative of objective function """
    dfdx0 = sign * (-2 * x[0] + 2 * x[1] + 2)
    dfdx1 = sign * (2 * x[0] - 4 * x[1])
    return np.array([dfdx0, dfdx1])

# 约束
cons = ({'type': 'eq',
         'fun': lambda x: np.array([x[0] ** 3 - x[1]]),
         'jac': lambda x: np.array([3.0 * (x[0] ** 2.0), -1.0])},
        {'type': 'ineq',
         'fun': lambda x: np.array([x[1] - 1]),
         'jac': lambda x: np.array([0.0, 1.0])})</code></pre> 
  <p>实现：</p> 
  <pre class="has">
<code>import numpy as np
from scipy.optimize import minimize


res = minimize(func, [-1.0, 5.0], args=(-1.0,), jac=func_deriv,
               constraints=cons, method='SLSQP', options={'disp': True})

print(res.x)
</code></pre> 
  <hr>
  <h2 id="%C2%A02.2%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%9C%80%E5%B0%8F%E5%8C%96Least-squares%20minimization">&nbsp;2.2最小二乘最小化Least-squares、leastsq</h2> 
  <p>leastsq（最小化一组方程的平方和）：<a href="https://blog.csdn.net/jiang425776024/article/details/86801232" rel="nofollow">https://blog.csdn.net/jiang425776024/article/details/86801232</a></p> 
  <p><img alt="è¿éåå¾çæè¿°" class="has" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20170806194837026?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc3V6eXUxMjM0NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p> 
  <p>Least-squares（求解带变量边界的非线性最小二乘问题）：</p> 
  <p>形如：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="80" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019022222222862.png" width="211"></p> 
  <p>例：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="60" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222222307494.png" width="342"></p> 
  <pre class="has">
<code>def model(x, u):
    return x[0] * (u ** 2 + x[1] * u) / (u ** 2 + x[2] * u + x[3])</code></pre> 
  <p>&nbsp;</p> 
  <p>导数：</p> 
  <p style="text-align:center;"><img alt="" class="has" height="208" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222222323914.png" width="264"></p> 
  <p>拟合显示：</p> 
  <pre class="has">
<code>from scipy.optimize import least_squares
import numpy as np

#原函数
def model(x, u):
    return x[0] * (u ** 2 + x[1] * u) / (u ** 2 + x[2] * u + x[3])

#损失函数
def fun(x, u, y):
    return model(x, u) - y

#原函数的一阶导数，雅可比矩阵，可选
def jac(x, u, y):
    J = np.empty((u.size, x.size))
    den = u ** 2 + x[2] * u + x[3]
    num = u ** 2 + x[1] * u
    J[:, 0] = num / den
    J[:, 1] = x[0] * u / den
    J[:, 2] = -x[0] * num * u / den ** 2
    J[:, 3] = -x[0] * num / den ** 2
    return J

#训练数据x轴
u = np.array([4.0, 2.0, 1.0, 5.0e-1, 2.5e-1, 1.67e-1, 1.25e-1, 1.0e-1,
              8.33e-2, 7.14e-2, 6.25e-2])
#参考真实值y
y = np.array([1.957e-1, 1.947e-1, 1.735e-1, 1.6e-1, 8.44e-2, 6.27e-2,
              4.56e-2, 3.42e-2, 3.23e-2, 2.35e-2, 2.46e-2])
#初始点，系数
x0 = np.array([2.5, 3.9, 4.15, 3.9])

#边界量0-100
res = least_squares(fun, x0, jac=jac, bounds=(0, 100), args=(u, y), verbose=1)

print(res.x)


import matplotlib.pyplot as plt
#测试数据x
u_test = np.linspace(0, 5)
#测试结果y
y_test = model(res.x, u_test)
#原数据
plt.plot(u, y, 'o', markersize=4, label='data')
#预测拟合结果
plt.plot(u_test, y_test, label='fitted model')
plt.xlabel("u")
plt.ylabel("y")
plt.legend(loc='lower right')
plt.show()</code></pre> 
  <p style="text-align:center;"><img alt="" class="has" height="320" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222222613774.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5nNDI1Nzc2MDI0,size_16,color_FFFFFF,t_70" width="439"></p> 
  <hr>
  <h1 id="3.%E5%8D%95%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E6%9C%80%E5%B0%8F%E5%8C%96%E5%99%A8">3.单变量函数最小化器</h1> 
  <pre class="has">
<code>from scipy.optimize import minimize_scalar
f = lambda x: (x - 2) * (x + 1)**2
res = minimize_scalar(f, method='brent')
print(res.x)
</code></pre> 
  <hr>
  <h1 id="%C2%A04.%E6%9C%89%E7%95%8C%E6%9C%80%E5%B0%8F%E5%8C%96">4.有界最小化</h1> 
  <pre class="has">
<code>from scipy.special import j1
res = minimize_scalar(j1, bounds=(4, 7), method='bounded')
res.x
</code></pre> 
  <hr>
  <h1 id="5.%E5%AE%9A%E5%88%B6%E6%9C%80%E5%B0%8F%E5%8C%96%E5%99%A8">5.定制自己的最小化器</h1> 
  <pre class="has">
<code>from scipy.optimize import OptimizeResult
from scipy.optimize import minimize
import numpy as np


def rosen(x):
    """The Rosenbrock function"""
    return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0)


def custmin(fun, x0, args=(), maxfev=None, stepsize=0.1,
        maxiter=100, callback=None, **options):
    bestx = x0
    besty = fun(x0)
    funcalls = 1
    niter = 0
    improved = True
    stop = False

    while improved and not stop and niter &lt; maxiter:
        improved = False
        niter += 1
        for dim in range(np.size(x0)):
            for s in [bestx[dim] - stepsize, bestx[dim] + stepsize]:
                testx = np.copy(bestx)
                testx[dim] = s
                testy = fun(testx, *args)
                funcalls += 1
                if testy &lt; besty:
                    besty = testy
                    bestx = testx
                    improved = True
            if callback is not None:
                callback(bestx)
            if maxfev is not None and funcalls &gt;= maxfev:
                stop = True
                break

    return OptimizeResult(fun=besty, x=bestx, nit=niter,
                          nfev=funcalls, success=(niter &gt; 1))
x0 = [1.35, 0.9, 0.8, 1.1, 1.2]
res = minimize(rosen, x0, method=custmin, options=dict(stepsize=0.05))
print(res.x)
</code></pre> 
  <hr>
  <h1 id="6.%E6%89%BE%E6%A0%B9">6.找根</h1> 
  <h2 id="1%EF%BC%89f(x)%3D0%E7%9A%84%E6%A0%B9">1）f(x)=0的根</h2> 
  <p style="text-align:center;"><img alt="" class="has" height="42" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222223023453.png" width="149"></p> 
  <pre class="has">
<code>import numpy as np
from scipy.optimize import root


def func(x):
    return x + 2 * np.cos(x)


sol = root(func, 0.3)
print(sol.x, sol.fun)
</code></pre> 
  <h2 id="2%EF%BC%89%E4%B8%A4%E4%B8%AA%E7%AD%89%E5%BC%8F%E7%9A%84%E6%A0%B9%C2%A0">2）两个等式的根&nbsp;</h2> 
  <p style="text-align:center;"><img alt="" class="has" height="58" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222223259229.png" width="166"></p> 
  <p>&nbsp;</p> 
  <pre class="has">
<code>import numpy as np
from scipy.optimize import root


def func2(x):
    f = [x[0] * np.cos(x[1]) - 4,
         x[1]*x[0] - x[1] - 5]
    df = np.array([[np.cos(x[1]), -x[0] * np.sin(x[1])],
                   [x[1], x[0] - 1]])
    return f, df
sol = root(func2, [1, 1], jac=True, method='lm')

print(sol.x, sol.fun)
</code></pre> 
  <h2 id="3%EF%BC%89%E5%AF%BB%E6%89%BE%E5%A4%A7%E9%97%AE%E9%A2%98%E7%9A%84%E6%A0%B9%E6%BA%90">&nbsp;</h2> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
