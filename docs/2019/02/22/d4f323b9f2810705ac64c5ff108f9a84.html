<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>ML–支持向量机SVM | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="ML–支持向量机SVM" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="ML–支持向量机SVM SVM算法专门解决线性不可分 主要涉及的知识点有： 支持向量机的基本原理和构造 支持向量机的核函数 支持向量机的参数调节 支持向量机实例–对波士顿房价进行回归分析 一.支持向量机SVM基本概念 1.支持向量机SVM的原理 # 导入numpy import numpy as np # 导入画图工具 import matplotlib.pyplot as plt # 导入支持向量机SVM from sklearn import svm # 导入数据集生成工具 from sklearn.datasets import make_blobs # 先创建50个数据点，让它们分为两类 X,y=make_blobs(n_samples=50,centers=2,random_state=6) # 创建一个线性内核的支持向量机模型 clf=svm.SVC(kernel=&#39;linear&#39;,C=1000) clf.fit(X,y) # 把数据点画出来 plt.scatter(X[:,0],X[:,1],c=y,s=30,cmap=plt.cm.Paired) # 建立图像坐标 ax=plt.gca() xlim=ax.get_xlim() ylim=ax.get_ylim() # 生成两个等差数列 xx=np.linspace(xlim[0],xlim[1],30) yy=np.linspace(ylim[0],ylim[1],30) YY,XX=np.meshgrid(yy,xx) xy=np.vstack([XX.ravel(),YY.ravel()]).T Z=clf.decision_function(xy).reshape(XX.shape) # 把分类的决定边界画出来 ax.contour(XX,YY,Z,colors=&#39;k&#39;,levels=[-1,0,1],alpha=0.5,linestyles=[&#39;--&#39;,&#39;-&#39;,&#39;--&#39;]) ax.scatter(clf.support_vectors_[:,0],clf.support_vectors_[:,1],s=100,linewidth=1,facecolors=&#39;none&#39;) plt.show() [结果分析] 从图中，可以清晰地看到，在分类器两侧分别有两条虚线，那些正好压在虚线上的数据点，就是我们刚刚提到的支持向量。而上例使用的这种方法称为最大边界间隔超平面(Maximum Margin Separating Hyperplane)。指的是说中间这条实线(在高维数据中是一个超平面)，和所有支持向量之间的距离，都是最大的 如果我们把SVM的内核换成是RBF，看下结果 # 创建一个RBF内核的支持向量机模型 clf_rbf=svm.SVC(kernel=&#39;rbf&#39;,C=1000) clf_rbf.fit(X,y) # 把数据点画出来 plt.scatter(X[:,0],X[:,1],c=y,s=30,cmap=plt.cm.Paired) # 建立图像坐标 ax=plt.gca() xlim=ax.get_xlim() ylim=ax.get_ylim() # 生成两个等差数列 xx=np.linspace(xlim[0],xlim[1],30) yy=np.linspace(ylim[0],ylim[1],30) YY,XX=np.meshgrid(yy,xx) xy=np.vstack([XX.ravel(),YY.ravel()]).T Z=clf_rbf.decision_function(xy).reshape(XX.shape) # 把分类的决定边界画出来 ax.contour(XX,YY,Z,colors=&#39;k&#39;,levels=[-1,0,1],alpha=0.5,linestyles=[&#39;--&#39;,&#39;-&#39;,&#39;--&#39;]) ax.scatter(clf_rbf.support_vectors_[:,0],clf_rbf.support_vectors_[:,1],s=100,linewidth=1,facecolors=&#39;none&#39;) plt.show() [结果分析] 我们看到分类器的样子变得完全不一样了，这是因为当我们使用RBF内核的时候，数据点之间的距离是用如下公式来计算的：公式中的x1和x2代表两个不同的数据点，而|||x1-x2|代表两个点之间的欧几里得距离。γ(gamma)是用来控制RBF内核宽度的参数，也就是图中实线距离两条虚线的距离 二.SVM的核函数与参数选择 1.不同核函数的SVM对比 前面提到的linearSVM就是一种使用了线性内核的SVM算法。不过linearSVM不支持对核函数进行修改，因为它默认只能使用线性内核。为了让大家能够直观体验不同内核的SVM算法在分类中的表现，我们画个图像进行展示 # 导入红酒数据集 from sklearn.datasets import load_wine # 定义一个函数用来画图 def make_meshgrid(x,y,h=.02): x_min,x_max=x.min()-1,x.max()+1 y_min,y_max=y.min()-1,y.max()+1 xx,yy=np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h)) return xx,yy # 定义一个绘制等高线的函数 def plot_contours(ax,clf,xx,yy,**params): Z=clf.predict(np.c_[xx.ravel(),yy.ravel()]) Z=Z.reshape(xx.shape) out=ax.contourf(xx,yy,Z,**params) return out # 使用酒的数据集 wine=load_wine() # 选取数据集的前两个特征 X=wine.data[:,:2] y=wine.target C=1.0 # 选取数据集的前两个特征 models=(svm.SVC(kernel=&#39;linear&#39;,C=C),svm.LinearSVC(C=C),svm.SVC(kernel=&#39;rbf&#39;,gamma=0.7,C=C),svm.SVC(kernel=&#39;poly&#39;,degree=3,C=C)) models=(clf.fit(X,y) for clf in models) # 设定图题 titles=(&#39;SVC with linear kernel&#39;,&#39;LinearSVC (linear kernel)&#39;,&#39;SVC with RBF kernel&#39;,&#39;SVC with polynomial (degree 3) kernel&#39;) # 设定一个字图形的个数和排列方式 fit,sub=plt.subplots(2,2) plt.subplots_adjust(wspace=0.4,hspace=0.4) # 使用前面定义的函数进行画图 X0,X1=X[:,0],X[:,1] xx,yy=make_meshgrid(X0,X1) for clf,title,ax in zip(models,titles,sub.flatten()): plot_contours(ax,clf,xx,yy,cmap=plt.cm.plasma,alpha=0.8) ax.scatter(X0,X1,c=y,cmap=plt.cm.plasma,s=20,edgecolors=&#39;k&#39;) ax.set_xlim(xx.min(),xx.max()) ax.set_ylim(yy.min(),yy.max()) ax.set_xlabel(&#39;Feature 0&#39;) ax.set_ylabel(&#39;Feature 1&#39;) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(title) plt.show() [结果分析] 我们可以看到线性内核的SVC与linearSVC得到的结果非常近似，但仍然有一点点差别。其中一个原因是linearSVC对L2范数进行最小化，而线性内核的SVC是对L1范数进行最小化。无论如何，linearSVC和线性内核的SVC生成的决定边界都是线性的，在更高维数据集中将会是相交的超平面。而RBF内核的SVC和polynomial内核的SVC分类器的决定边界则完全不是线性的，它们更加弹性。而决定了它们决定边界形状的，就是它们的参数。在polynomial内核的SVC中，起决定性作用的参数就是degree和正则化参数C，在本例中我们使用的degree为3，也就是对原始数据集的特征进行乘3次方操作。而在RBF内核的SVC中，其起决定作用的是正则化参数C和参数gamma 接下来我们重点介绍一下RBF内核SVC的gamma参数调节 2.支持向量机的gamma参数调节 首先让我们看一下不同的gamma值对于RBF内核的SVC分类器有什么影响 C=1.0 # SVM正则化参数 models=(svm.SVC(kernel=&#39;rbf&#39;,gamma=0.1,C=C),svm.SVC(kernel=&#39;rbf&#39;,gamma=1,C=C),svm.SVC(kernel=&#39;rbf&#39;,gamma=10,C=C)) models=(clf.fit(X,y) for clf in models) # 设定图题 titles=(&#39;gamma=0.1&#39;,&#39;gamma=1&#39;,&#39;gamma=10&#39;) # 设置子图形个数和排列 fig,sub=plt.subplots(1,3,figsize=(10,3)) X0,X1=X[:,0],X[:,1] xx,yy=make_meshgrid(X0,X1) # 使用定义好的函数进行画图 for clf,title,ax in zip(models,titles,sub.flatten()): plot_contours(ax,clf,xx,yy,cmap=plt.cm.plasma,alpha=0.8) ax.scatter(X0,X1,c=y,cmap=plt.cm.plasma,s=20,edgecolors=&#39;k&#39;) ax.set_xlim(xx.min(),xx.max()) ax.set_ylim(yy.min(),yy.max()) ax.set_xlabel(&#39;Feature 0&#39;) ax.set_ylabel(&#39;Feature 1&#39;) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(title) plt.show() [结果分析] 从图中可以看出，自左到右gamma值从0.1增加到10，gamma值越小，则RBF内核的直径越大，这样就会有更多的点被模型圈进决定边界中，所以决定边界也就越平滑，这时的模型也就越简单；而随着参数的增加，模型则更倾向于把每个点都放到相应的决定边界中，这时模型的复杂度也相应提高了。所以gamma值越小模型越倾向于欠拟合，而gamma值越大，则模型越倾向于出现过拟合的问题 而至于正则化参数C，C值越小，模型就越受限，也就是说单个数据点对模型的影响越小，模型就越简单；而C值越大，每个数据点对模型的影响就越大，模型也会更加复杂 3.SVM算法的优势与不足 SVM应对高维数据集和低维数据集都还算是得心应手。但是，前提条件是数据集的规模不太大。如果数据集中的样本数量在1万以内，SVM都能驾驭得了，但如果样本数量超过10万的话，SVM就会非常耗费时间和内存 SVM还有一个短板，就是对于数据预处理和参数调节要求非常高 在SVM算法中，有3个参数是比较重要的：第一个是核函数的选择；第二个是核函数的参数，例如RBF的gamma值；第三个是正则化参数C。RBF内核的gamma值是用来调节内核宽度的，gamma值和C值一起控制模型的复杂度，数值越大模型越复杂，而数值越小模型越简单 三.SVM实例–波士顿房价回归分析 在scikit-learn中，内置了一个非常适合做回归分析的数据集–波士顿房价数据集。我们将使用该数据集讲解SVM中用于回归分析的SVR的用法 1.初步了解数据集 # 导入波士顿房价数据集 from sklearn.datasets import load_boston boston=load_boston() # 打印数据集中的键 print(boston.keys()) dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;feature_names&#39;, &#39;DESCR&#39;, &#39;filename&#39;]) [结果分析] 从结果中可以看出，波士顿房价数据集中有5个键，分别是数据，目标，特征名称，描述和文件名。我们可能有疑问，波士顿房价数据集比红酒数据集少了一个键，就是目标名称(target_names)，这是为什么？我们查看一下 print(boston[&#39;DESCR&#39;]) .. _boston_dataset: Boston house prices dataset --------------------------- **Data Set Characteristics:** :Number of Instances: 506 :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. :Attribute Information (in order): - CRIM per capita crime rate by town - ZN proportion of residential land zoned for lots over 25,000 sq.ft. - INDUS proportion of non-retail business acres per town - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) - NOX nitric oxides concentration (parts per 10 million) - RM average number of rooms per dwelling - AGE proportion of owner-occupied units built prior to 1940 - DIS weighted distances to five Boston employment centres - RAD index of accessibility to radial highways - TAX full-value property-tax rate per $10,000 - PTRATIO pupil-teacher ratio by town - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town - LSTAT % lower status of the population - MEDV Median value of owner-occupied homes in $1000&#39;s :Missing Attribute Values: None :Creator: Harrison, D. and Rubinfeld, D.L. This is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/ This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &#39;Hedonic prices and the demand for clean air&#39;, J. Environ. Economics &amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics ...&#39;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. The Boston house-price data has been used in many machine learning papers that address regression problems. .. topic:: References - Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&#39;, Wiley, 1980. 244-261. - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. [结果分析] 从上面的这段描述可以看出，数据集中共有506个样本，每个样本有13个特征变量。而后面还有一个叫作中位数的第14个变量，这个变量就是该数据集中的target 2.使用SVR进行建模 我们要先制作训练数据集和测试数据集，代码如下： # 导入数据集拆分工具 from sklearn.model_selection import train_test_split # 建立训练数据集和测试数据集 X,y=boston.data,boston.target X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=8) print(X_train.shape) print(X_test.shape) (379, 13) (127, 13) 下面开始用SVR进行建模，我们在前面介绍了SVM的两种核函数：Linear和rbf，不过我们不知道这两种核函数哪一个会让模型表现得更好，那么分别尝试一下： # 导入支持向量机回归模型 from sklearn.svm import SVR # 分别测试sklearn核函数和rbf核函数 for kernel in [&#39;linear&#39;,&#39;rbf&#39;]: svr=SVR(kernel=kernel) svr.fit(X_train,y_train) print(kernel,&#39;核函数的模型训练集得分：{:.3f}&#39;.format(svr.score(X_train,y_train))) print(kernel,&#39;核函数的模型测试集得分：{:.3f}&#39;.format(svr.score(X_test,y_test))) linear 核函数的模型训练集得分：0.709 linear 核函数的模型测试集得分：0.696 rbf 核函数的模型训练集得分：0.145 rbf 核函数的模型测试集得分：0.001 [结果分析] 两种核函数的模型得分都不能令人满意。使用rbf核函数的模型糟糕透了，在训练数据集的分只有0.145，而在测试数据集的得分完全可以用&quot;灾难&quot;来形容了—居然只有0.001分 这是什么原因呢？想想，会不会是数据集的各个特征之间的量级差的比较远？由于SVM算法对于数据预处理的要求是比较高的，如果数据特征量差异较大，我们需要对数据进行预处理。所以我们用可视化的方法看一看数据集中各个特征的数量级是什么情况 # 将特征数值中的最小值和最大值用散点图画出来 plt.plot(X.min(axis=0),&#39;v&#39;,label=&#39;min&#39;) plt.plot(X.max(axis=0),&#39;^&#39;,label=&#39;max&#39;) # 设定纵坐标为对数形式 plt.yscale(&#39;log&#39;) # 设置图注位置为最佳 plt.legend(loc=&#39;best&#39;) # 设定横纵轴标题 plt.xlabel(&#39;features&#39;) plt.ylabel(&#39;feature magnitude&#39;) # 显示图形 plt.show() 看来为了能够让SVM算法能够更好地对数据进行拟合，我们必须对数据集进行预处理 # 导入数据预处理工具 from sklearn.preprocessing import StandardScaler # 对训练集和测试集进行数据预处理 scaler=StandardScaler() scaler.fit(X_train) X_train_scaled=scaler.transform(X_train) X_test_scaled=scaler.transform(X_test) # 将预处理后的数据特征最大值和最小值用散点图表示出来 plt.plot(X_train_scaled.min(axis=0),&#39;v&#39;,label=&#39;train set min&#39;) plt.plot(X_train_scaled.max(axis=0),&#39;^&#39;,label=&#39;train set max&#39;) plt.plot(X_test_scaled.min(axis=0),&#39;v&#39;,label=&#39;test set min&#39;) plt.plot(X_test_scaled.max(axis=0),&#39;^&#39;,label=&#39;test set max&#39;) plt.yscale(&#39;log&#39;) # 设置图注位置 plt.legend(loc=&#39;best&#39;) # 设置横纵轴标题 plt.xlabel(&#39;scaled features&#39;) plt.ylabel(&#39;scaled feature magnitude&#39;) plt.show() [结果分析] 经过了我们的预处理，不管是训练集还是测试集，基本上所有的特征最大值都不会超过10，而最小值也都趋于0，以至于在图中我们看不到它们了 现在我们在试试用经过预处理的数据来训练模型，看看结果会有什么不同 # 导入支持向量机回归模型 from sklearn.svm import SVR # 分别测试sklearn核函数和rbf核函数 for kernel in [&#39;linear&#39;,&#39;rbf&#39;]: svr=SVR(kernel=kernel) svr.fit(X_train_scaled,y_train) print(kernel,&#39;核函数的模型训练集得分：{:.3f}&#39;.format(svr.score(X_train_scaled,y_train))) print(kernel,&#39;核函数的模型测试集得分：{:.3f}&#39;.format(svr.score(X_test_scaled,y_test))) linear 核函数的模型训练集得分：0.706 linear 核函数的模型测试集得分：0.698 rbf 核函数的模型训练集得分：0.665 rbf 核函数的模型测试集得分：0.695 [结果分析] 经过预处理之后，linear内核的SVR得分变化不大，而rbf内核的SVR得分有了巨大的提升 和SVC一样，SVR模型也有gamma和C两个参数，接下来我们试着对两个参数进行修改 # 设置模型的C参数和gamma参数 svr=SVR(C=100,gamma=0.1) svr.fit(X_train_scaled,y_train) print(&#39;调节参数后的模型在训练集得分：{:.3f}&#39;.format(svr.score(X_train_scaled,y_train))) print(&#39;调节参数后的模型在测试集得分：{:.3f}&#39;.format(svr.score(X_test_scaled,y_test))) 调节参数后的模型在训练集得分：0.966 调节参数后的模型在测试集得分：0.894 [结果分析] 这是一个比较不错的结果，我们看到通过参数调节，rbf内核的SVR模型在训练集的得分已经高达0.966，而在测试数据集的得分也达到了0.894，可以说现在模型的表现已经是可以接受的" />
<meta property="og:description" content="ML–支持向量机SVM SVM算法专门解决线性不可分 主要涉及的知识点有： 支持向量机的基本原理和构造 支持向量机的核函数 支持向量机的参数调节 支持向量机实例–对波士顿房价进行回归分析 一.支持向量机SVM基本概念 1.支持向量机SVM的原理 # 导入numpy import numpy as np # 导入画图工具 import matplotlib.pyplot as plt # 导入支持向量机SVM from sklearn import svm # 导入数据集生成工具 from sklearn.datasets import make_blobs # 先创建50个数据点，让它们分为两类 X,y=make_blobs(n_samples=50,centers=2,random_state=6) # 创建一个线性内核的支持向量机模型 clf=svm.SVC(kernel=&#39;linear&#39;,C=1000) clf.fit(X,y) # 把数据点画出来 plt.scatter(X[:,0],X[:,1],c=y,s=30,cmap=plt.cm.Paired) # 建立图像坐标 ax=plt.gca() xlim=ax.get_xlim() ylim=ax.get_ylim() # 生成两个等差数列 xx=np.linspace(xlim[0],xlim[1],30) yy=np.linspace(ylim[0],ylim[1],30) YY,XX=np.meshgrid(yy,xx) xy=np.vstack([XX.ravel(),YY.ravel()]).T Z=clf.decision_function(xy).reshape(XX.shape) # 把分类的决定边界画出来 ax.contour(XX,YY,Z,colors=&#39;k&#39;,levels=[-1,0,1],alpha=0.5,linestyles=[&#39;--&#39;,&#39;-&#39;,&#39;--&#39;]) ax.scatter(clf.support_vectors_[:,0],clf.support_vectors_[:,1],s=100,linewidth=1,facecolors=&#39;none&#39;) plt.show() [结果分析] 从图中，可以清晰地看到，在分类器两侧分别有两条虚线，那些正好压在虚线上的数据点，就是我们刚刚提到的支持向量。而上例使用的这种方法称为最大边界间隔超平面(Maximum Margin Separating Hyperplane)。指的是说中间这条实线(在高维数据中是一个超平面)，和所有支持向量之间的距离，都是最大的 如果我们把SVM的内核换成是RBF，看下结果 # 创建一个RBF内核的支持向量机模型 clf_rbf=svm.SVC(kernel=&#39;rbf&#39;,C=1000) clf_rbf.fit(X,y) # 把数据点画出来 plt.scatter(X[:,0],X[:,1],c=y,s=30,cmap=plt.cm.Paired) # 建立图像坐标 ax=plt.gca() xlim=ax.get_xlim() ylim=ax.get_ylim() # 生成两个等差数列 xx=np.linspace(xlim[0],xlim[1],30) yy=np.linspace(ylim[0],ylim[1],30) YY,XX=np.meshgrid(yy,xx) xy=np.vstack([XX.ravel(),YY.ravel()]).T Z=clf_rbf.decision_function(xy).reshape(XX.shape) # 把分类的决定边界画出来 ax.contour(XX,YY,Z,colors=&#39;k&#39;,levels=[-1,0,1],alpha=0.5,linestyles=[&#39;--&#39;,&#39;-&#39;,&#39;--&#39;]) ax.scatter(clf_rbf.support_vectors_[:,0],clf_rbf.support_vectors_[:,1],s=100,linewidth=1,facecolors=&#39;none&#39;) plt.show() [结果分析] 我们看到分类器的样子变得完全不一样了，这是因为当我们使用RBF内核的时候，数据点之间的距离是用如下公式来计算的：公式中的x1和x2代表两个不同的数据点，而|||x1-x2|代表两个点之间的欧几里得距离。γ(gamma)是用来控制RBF内核宽度的参数，也就是图中实线距离两条虚线的距离 二.SVM的核函数与参数选择 1.不同核函数的SVM对比 前面提到的linearSVM就是一种使用了线性内核的SVM算法。不过linearSVM不支持对核函数进行修改，因为它默认只能使用线性内核。为了让大家能够直观体验不同内核的SVM算法在分类中的表现，我们画个图像进行展示 # 导入红酒数据集 from sklearn.datasets import load_wine # 定义一个函数用来画图 def make_meshgrid(x,y,h=.02): x_min,x_max=x.min()-1,x.max()+1 y_min,y_max=y.min()-1,y.max()+1 xx,yy=np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h)) return xx,yy # 定义一个绘制等高线的函数 def plot_contours(ax,clf,xx,yy,**params): Z=clf.predict(np.c_[xx.ravel(),yy.ravel()]) Z=Z.reshape(xx.shape) out=ax.contourf(xx,yy,Z,**params) return out # 使用酒的数据集 wine=load_wine() # 选取数据集的前两个特征 X=wine.data[:,:2] y=wine.target C=1.0 # 选取数据集的前两个特征 models=(svm.SVC(kernel=&#39;linear&#39;,C=C),svm.LinearSVC(C=C),svm.SVC(kernel=&#39;rbf&#39;,gamma=0.7,C=C),svm.SVC(kernel=&#39;poly&#39;,degree=3,C=C)) models=(clf.fit(X,y) for clf in models) # 设定图题 titles=(&#39;SVC with linear kernel&#39;,&#39;LinearSVC (linear kernel)&#39;,&#39;SVC with RBF kernel&#39;,&#39;SVC with polynomial (degree 3) kernel&#39;) # 设定一个字图形的个数和排列方式 fit,sub=plt.subplots(2,2) plt.subplots_adjust(wspace=0.4,hspace=0.4) # 使用前面定义的函数进行画图 X0,X1=X[:,0],X[:,1] xx,yy=make_meshgrid(X0,X1) for clf,title,ax in zip(models,titles,sub.flatten()): plot_contours(ax,clf,xx,yy,cmap=plt.cm.plasma,alpha=0.8) ax.scatter(X0,X1,c=y,cmap=plt.cm.plasma,s=20,edgecolors=&#39;k&#39;) ax.set_xlim(xx.min(),xx.max()) ax.set_ylim(yy.min(),yy.max()) ax.set_xlabel(&#39;Feature 0&#39;) ax.set_ylabel(&#39;Feature 1&#39;) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(title) plt.show() [结果分析] 我们可以看到线性内核的SVC与linearSVC得到的结果非常近似，但仍然有一点点差别。其中一个原因是linearSVC对L2范数进行最小化，而线性内核的SVC是对L1范数进行最小化。无论如何，linearSVC和线性内核的SVC生成的决定边界都是线性的，在更高维数据集中将会是相交的超平面。而RBF内核的SVC和polynomial内核的SVC分类器的决定边界则完全不是线性的，它们更加弹性。而决定了它们决定边界形状的，就是它们的参数。在polynomial内核的SVC中，起决定性作用的参数就是degree和正则化参数C，在本例中我们使用的degree为3，也就是对原始数据集的特征进行乘3次方操作。而在RBF内核的SVC中，其起决定作用的是正则化参数C和参数gamma 接下来我们重点介绍一下RBF内核SVC的gamma参数调节 2.支持向量机的gamma参数调节 首先让我们看一下不同的gamma值对于RBF内核的SVC分类器有什么影响 C=1.0 # SVM正则化参数 models=(svm.SVC(kernel=&#39;rbf&#39;,gamma=0.1,C=C),svm.SVC(kernel=&#39;rbf&#39;,gamma=1,C=C),svm.SVC(kernel=&#39;rbf&#39;,gamma=10,C=C)) models=(clf.fit(X,y) for clf in models) # 设定图题 titles=(&#39;gamma=0.1&#39;,&#39;gamma=1&#39;,&#39;gamma=10&#39;) # 设置子图形个数和排列 fig,sub=plt.subplots(1,3,figsize=(10,3)) X0,X1=X[:,0],X[:,1] xx,yy=make_meshgrid(X0,X1) # 使用定义好的函数进行画图 for clf,title,ax in zip(models,titles,sub.flatten()): plot_contours(ax,clf,xx,yy,cmap=plt.cm.plasma,alpha=0.8) ax.scatter(X0,X1,c=y,cmap=plt.cm.plasma,s=20,edgecolors=&#39;k&#39;) ax.set_xlim(xx.min(),xx.max()) ax.set_ylim(yy.min(),yy.max()) ax.set_xlabel(&#39;Feature 0&#39;) ax.set_ylabel(&#39;Feature 1&#39;) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(title) plt.show() [结果分析] 从图中可以看出，自左到右gamma值从0.1增加到10，gamma值越小，则RBF内核的直径越大，这样就会有更多的点被模型圈进决定边界中，所以决定边界也就越平滑，这时的模型也就越简单；而随着参数的增加，模型则更倾向于把每个点都放到相应的决定边界中，这时模型的复杂度也相应提高了。所以gamma值越小模型越倾向于欠拟合，而gamma值越大，则模型越倾向于出现过拟合的问题 而至于正则化参数C，C值越小，模型就越受限，也就是说单个数据点对模型的影响越小，模型就越简单；而C值越大，每个数据点对模型的影响就越大，模型也会更加复杂 3.SVM算法的优势与不足 SVM应对高维数据集和低维数据集都还算是得心应手。但是，前提条件是数据集的规模不太大。如果数据集中的样本数量在1万以内，SVM都能驾驭得了，但如果样本数量超过10万的话，SVM就会非常耗费时间和内存 SVM还有一个短板，就是对于数据预处理和参数调节要求非常高 在SVM算法中，有3个参数是比较重要的：第一个是核函数的选择；第二个是核函数的参数，例如RBF的gamma值；第三个是正则化参数C。RBF内核的gamma值是用来调节内核宽度的，gamma值和C值一起控制模型的复杂度，数值越大模型越复杂，而数值越小模型越简单 三.SVM实例–波士顿房价回归分析 在scikit-learn中，内置了一个非常适合做回归分析的数据集–波士顿房价数据集。我们将使用该数据集讲解SVM中用于回归分析的SVR的用法 1.初步了解数据集 # 导入波士顿房价数据集 from sklearn.datasets import load_boston boston=load_boston() # 打印数据集中的键 print(boston.keys()) dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;feature_names&#39;, &#39;DESCR&#39;, &#39;filename&#39;]) [结果分析] 从结果中可以看出，波士顿房价数据集中有5个键，分别是数据，目标，特征名称，描述和文件名。我们可能有疑问，波士顿房价数据集比红酒数据集少了一个键，就是目标名称(target_names)，这是为什么？我们查看一下 print(boston[&#39;DESCR&#39;]) .. _boston_dataset: Boston house prices dataset --------------------------- **Data Set Characteristics:** :Number of Instances: 506 :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. :Attribute Information (in order): - CRIM per capita crime rate by town - ZN proportion of residential land zoned for lots over 25,000 sq.ft. - INDUS proportion of non-retail business acres per town - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) - NOX nitric oxides concentration (parts per 10 million) - RM average number of rooms per dwelling - AGE proportion of owner-occupied units built prior to 1940 - DIS weighted distances to five Boston employment centres - RAD index of accessibility to radial highways - TAX full-value property-tax rate per $10,000 - PTRATIO pupil-teacher ratio by town - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town - LSTAT % lower status of the population - MEDV Median value of owner-occupied homes in $1000&#39;s :Missing Attribute Values: None :Creator: Harrison, D. and Rubinfeld, D.L. This is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/ This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &#39;Hedonic prices and the demand for clean air&#39;, J. Environ. Economics &amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics ...&#39;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. The Boston house-price data has been used in many machine learning papers that address regression problems. .. topic:: References - Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&#39;, Wiley, 1980. 244-261. - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. [结果分析] 从上面的这段描述可以看出，数据集中共有506个样本，每个样本有13个特征变量。而后面还有一个叫作中位数的第14个变量，这个变量就是该数据集中的target 2.使用SVR进行建模 我们要先制作训练数据集和测试数据集，代码如下： # 导入数据集拆分工具 from sklearn.model_selection import train_test_split # 建立训练数据集和测试数据集 X,y=boston.data,boston.target X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=8) print(X_train.shape) print(X_test.shape) (379, 13) (127, 13) 下面开始用SVR进行建模，我们在前面介绍了SVM的两种核函数：Linear和rbf，不过我们不知道这两种核函数哪一个会让模型表现得更好，那么分别尝试一下： # 导入支持向量机回归模型 from sklearn.svm import SVR # 分别测试sklearn核函数和rbf核函数 for kernel in [&#39;linear&#39;,&#39;rbf&#39;]: svr=SVR(kernel=kernel) svr.fit(X_train,y_train) print(kernel,&#39;核函数的模型训练集得分：{:.3f}&#39;.format(svr.score(X_train,y_train))) print(kernel,&#39;核函数的模型测试集得分：{:.3f}&#39;.format(svr.score(X_test,y_test))) linear 核函数的模型训练集得分：0.709 linear 核函数的模型测试集得分：0.696 rbf 核函数的模型训练集得分：0.145 rbf 核函数的模型测试集得分：0.001 [结果分析] 两种核函数的模型得分都不能令人满意。使用rbf核函数的模型糟糕透了，在训练数据集的分只有0.145，而在测试数据集的得分完全可以用&quot;灾难&quot;来形容了—居然只有0.001分 这是什么原因呢？想想，会不会是数据集的各个特征之间的量级差的比较远？由于SVM算法对于数据预处理的要求是比较高的，如果数据特征量差异较大，我们需要对数据进行预处理。所以我们用可视化的方法看一看数据集中各个特征的数量级是什么情况 # 将特征数值中的最小值和最大值用散点图画出来 plt.plot(X.min(axis=0),&#39;v&#39;,label=&#39;min&#39;) plt.plot(X.max(axis=0),&#39;^&#39;,label=&#39;max&#39;) # 设定纵坐标为对数形式 plt.yscale(&#39;log&#39;) # 设置图注位置为最佳 plt.legend(loc=&#39;best&#39;) # 设定横纵轴标题 plt.xlabel(&#39;features&#39;) plt.ylabel(&#39;feature magnitude&#39;) # 显示图形 plt.show() 看来为了能够让SVM算法能够更好地对数据进行拟合，我们必须对数据集进行预处理 # 导入数据预处理工具 from sklearn.preprocessing import StandardScaler # 对训练集和测试集进行数据预处理 scaler=StandardScaler() scaler.fit(X_train) X_train_scaled=scaler.transform(X_train) X_test_scaled=scaler.transform(X_test) # 将预处理后的数据特征最大值和最小值用散点图表示出来 plt.plot(X_train_scaled.min(axis=0),&#39;v&#39;,label=&#39;train set min&#39;) plt.plot(X_train_scaled.max(axis=0),&#39;^&#39;,label=&#39;train set max&#39;) plt.plot(X_test_scaled.min(axis=0),&#39;v&#39;,label=&#39;test set min&#39;) plt.plot(X_test_scaled.max(axis=0),&#39;^&#39;,label=&#39;test set max&#39;) plt.yscale(&#39;log&#39;) # 设置图注位置 plt.legend(loc=&#39;best&#39;) # 设置横纵轴标题 plt.xlabel(&#39;scaled features&#39;) plt.ylabel(&#39;scaled feature magnitude&#39;) plt.show() [结果分析] 经过了我们的预处理，不管是训练集还是测试集，基本上所有的特征最大值都不会超过10，而最小值也都趋于0，以至于在图中我们看不到它们了 现在我们在试试用经过预处理的数据来训练模型，看看结果会有什么不同 # 导入支持向量机回归模型 from sklearn.svm import SVR # 分别测试sklearn核函数和rbf核函数 for kernel in [&#39;linear&#39;,&#39;rbf&#39;]: svr=SVR(kernel=kernel) svr.fit(X_train_scaled,y_train) print(kernel,&#39;核函数的模型训练集得分：{:.3f}&#39;.format(svr.score(X_train_scaled,y_train))) print(kernel,&#39;核函数的模型测试集得分：{:.3f}&#39;.format(svr.score(X_test_scaled,y_test))) linear 核函数的模型训练集得分：0.706 linear 核函数的模型测试集得分：0.698 rbf 核函数的模型训练集得分：0.665 rbf 核函数的模型测试集得分：0.695 [结果分析] 经过预处理之后，linear内核的SVR得分变化不大，而rbf内核的SVR得分有了巨大的提升 和SVC一样，SVR模型也有gamma和C两个参数，接下来我们试着对两个参数进行修改 # 设置模型的C参数和gamma参数 svr=SVR(C=100,gamma=0.1) svr.fit(X_train_scaled,y_train) print(&#39;调节参数后的模型在训练集得分：{:.3f}&#39;.format(svr.score(X_train_scaled,y_train))) print(&#39;调节参数后的模型在测试集得分：{:.3f}&#39;.format(svr.score(X_test_scaled,y_test))) 调节参数后的模型在训练集得分：0.966 调节参数后的模型在测试集得分：0.894 [结果分析] 这是一个比较不错的结果，我们看到通过参数调节，rbf内核的SVR模型在训练集的得分已经高达0.966，而在测试数据集的得分也达到了0.894，可以说现在模型的表现已经是可以接受的" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-22T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"ML–支持向量机SVM SVM算法专门解决线性不可分 主要涉及的知识点有： 支持向量机的基本原理和构造 支持向量机的核函数 支持向量机的参数调节 支持向量机实例–对波士顿房价进行回归分析 一.支持向量机SVM基本概念 1.支持向量机SVM的原理 # 导入numpy import numpy as np # 导入画图工具 import matplotlib.pyplot as plt # 导入支持向量机SVM from sklearn import svm # 导入数据集生成工具 from sklearn.datasets import make_blobs # 先创建50个数据点，让它们分为两类 X,y=make_blobs(n_samples=50,centers=2,random_state=6) # 创建一个线性内核的支持向量机模型 clf=svm.SVC(kernel=&#39;linear&#39;,C=1000) clf.fit(X,y) # 把数据点画出来 plt.scatter(X[:,0],X[:,1],c=y,s=30,cmap=plt.cm.Paired) # 建立图像坐标 ax=plt.gca() xlim=ax.get_xlim() ylim=ax.get_ylim() # 生成两个等差数列 xx=np.linspace(xlim[0],xlim[1],30) yy=np.linspace(ylim[0],ylim[1],30) YY,XX=np.meshgrid(yy,xx) xy=np.vstack([XX.ravel(),YY.ravel()]).T Z=clf.decision_function(xy).reshape(XX.shape) # 把分类的决定边界画出来 ax.contour(XX,YY,Z,colors=&#39;k&#39;,levels=[-1,0,1],alpha=0.5,linestyles=[&#39;--&#39;,&#39;-&#39;,&#39;--&#39;]) ax.scatter(clf.support_vectors_[:,0],clf.support_vectors_[:,1],s=100,linewidth=1,facecolors=&#39;none&#39;) plt.show() [结果分析] 从图中，可以清晰地看到，在分类器两侧分别有两条虚线，那些正好压在虚线上的数据点，就是我们刚刚提到的支持向量。而上例使用的这种方法称为最大边界间隔超平面(Maximum Margin Separating Hyperplane)。指的是说中间这条实线(在高维数据中是一个超平面)，和所有支持向量之间的距离，都是最大的 如果我们把SVM的内核换成是RBF，看下结果 # 创建一个RBF内核的支持向量机模型 clf_rbf=svm.SVC(kernel=&#39;rbf&#39;,C=1000) clf_rbf.fit(X,y) # 把数据点画出来 plt.scatter(X[:,0],X[:,1],c=y,s=30,cmap=plt.cm.Paired) # 建立图像坐标 ax=plt.gca() xlim=ax.get_xlim() ylim=ax.get_ylim() # 生成两个等差数列 xx=np.linspace(xlim[0],xlim[1],30) yy=np.linspace(ylim[0],ylim[1],30) YY,XX=np.meshgrid(yy,xx) xy=np.vstack([XX.ravel(),YY.ravel()]).T Z=clf_rbf.decision_function(xy).reshape(XX.shape) # 把分类的决定边界画出来 ax.contour(XX,YY,Z,colors=&#39;k&#39;,levels=[-1,0,1],alpha=0.5,linestyles=[&#39;--&#39;,&#39;-&#39;,&#39;--&#39;]) ax.scatter(clf_rbf.support_vectors_[:,0],clf_rbf.support_vectors_[:,1],s=100,linewidth=1,facecolors=&#39;none&#39;) plt.show() [结果分析] 我们看到分类器的样子变得完全不一样了，这是因为当我们使用RBF内核的时候，数据点之间的距离是用如下公式来计算的：公式中的x1和x2代表两个不同的数据点，而|||x1-x2|代表两个点之间的欧几里得距离。γ(gamma)是用来控制RBF内核宽度的参数，也就是图中实线距离两条虚线的距离 二.SVM的核函数与参数选择 1.不同核函数的SVM对比 前面提到的linearSVM就是一种使用了线性内核的SVM算法。不过linearSVM不支持对核函数进行修改，因为它默认只能使用线性内核。为了让大家能够直观体验不同内核的SVM算法在分类中的表现，我们画个图像进行展示 # 导入红酒数据集 from sklearn.datasets import load_wine # 定义一个函数用来画图 def make_meshgrid(x,y,h=.02): x_min,x_max=x.min()-1,x.max()+1 y_min,y_max=y.min()-1,y.max()+1 xx,yy=np.meshgrid(np.arange(x_min,x_max,h),np.arange(y_min,y_max,h)) return xx,yy # 定义一个绘制等高线的函数 def plot_contours(ax,clf,xx,yy,**params): Z=clf.predict(np.c_[xx.ravel(),yy.ravel()]) Z=Z.reshape(xx.shape) out=ax.contourf(xx,yy,Z,**params) return out # 使用酒的数据集 wine=load_wine() # 选取数据集的前两个特征 X=wine.data[:,:2] y=wine.target C=1.0 # 选取数据集的前两个特征 models=(svm.SVC(kernel=&#39;linear&#39;,C=C),svm.LinearSVC(C=C),svm.SVC(kernel=&#39;rbf&#39;,gamma=0.7,C=C),svm.SVC(kernel=&#39;poly&#39;,degree=3,C=C)) models=(clf.fit(X,y) for clf in models) # 设定图题 titles=(&#39;SVC with linear kernel&#39;,&#39;LinearSVC (linear kernel)&#39;,&#39;SVC with RBF kernel&#39;,&#39;SVC with polynomial (degree 3) kernel&#39;) # 设定一个字图形的个数和排列方式 fit,sub=plt.subplots(2,2) plt.subplots_adjust(wspace=0.4,hspace=0.4) # 使用前面定义的函数进行画图 X0,X1=X[:,0],X[:,1] xx,yy=make_meshgrid(X0,X1) for clf,title,ax in zip(models,titles,sub.flatten()): plot_contours(ax,clf,xx,yy,cmap=plt.cm.plasma,alpha=0.8) ax.scatter(X0,X1,c=y,cmap=plt.cm.plasma,s=20,edgecolors=&#39;k&#39;) ax.set_xlim(xx.min(),xx.max()) ax.set_ylim(yy.min(),yy.max()) ax.set_xlabel(&#39;Feature 0&#39;) ax.set_ylabel(&#39;Feature 1&#39;) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(title) plt.show() [结果分析] 我们可以看到线性内核的SVC与linearSVC得到的结果非常近似，但仍然有一点点差别。其中一个原因是linearSVC对L2范数进行最小化，而线性内核的SVC是对L1范数进行最小化。无论如何，linearSVC和线性内核的SVC生成的决定边界都是线性的，在更高维数据集中将会是相交的超平面。而RBF内核的SVC和polynomial内核的SVC分类器的决定边界则完全不是线性的，它们更加弹性。而决定了它们决定边界形状的，就是它们的参数。在polynomial内核的SVC中，起决定性作用的参数就是degree和正则化参数C，在本例中我们使用的degree为3，也就是对原始数据集的特征进行乘3次方操作。而在RBF内核的SVC中，其起决定作用的是正则化参数C和参数gamma 接下来我们重点介绍一下RBF内核SVC的gamma参数调节 2.支持向量机的gamma参数调节 首先让我们看一下不同的gamma值对于RBF内核的SVC分类器有什么影响 C=1.0 # SVM正则化参数 models=(svm.SVC(kernel=&#39;rbf&#39;,gamma=0.1,C=C),svm.SVC(kernel=&#39;rbf&#39;,gamma=1,C=C),svm.SVC(kernel=&#39;rbf&#39;,gamma=10,C=C)) models=(clf.fit(X,y) for clf in models) # 设定图题 titles=(&#39;gamma=0.1&#39;,&#39;gamma=1&#39;,&#39;gamma=10&#39;) # 设置子图形个数和排列 fig,sub=plt.subplots(1,3,figsize=(10,3)) X0,X1=X[:,0],X[:,1] xx,yy=make_meshgrid(X0,X1) # 使用定义好的函数进行画图 for clf,title,ax in zip(models,titles,sub.flatten()): plot_contours(ax,clf,xx,yy,cmap=plt.cm.plasma,alpha=0.8) ax.scatter(X0,X1,c=y,cmap=plt.cm.plasma,s=20,edgecolors=&#39;k&#39;) ax.set_xlim(xx.min(),xx.max()) ax.set_ylim(yy.min(),yy.max()) ax.set_xlabel(&#39;Feature 0&#39;) ax.set_ylabel(&#39;Feature 1&#39;) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(title) plt.show() [结果分析] 从图中可以看出，自左到右gamma值从0.1增加到10，gamma值越小，则RBF内核的直径越大，这样就会有更多的点被模型圈进决定边界中，所以决定边界也就越平滑，这时的模型也就越简单；而随着参数的增加，模型则更倾向于把每个点都放到相应的决定边界中，这时模型的复杂度也相应提高了。所以gamma值越小模型越倾向于欠拟合，而gamma值越大，则模型越倾向于出现过拟合的问题 而至于正则化参数C，C值越小，模型就越受限，也就是说单个数据点对模型的影响越小，模型就越简单；而C值越大，每个数据点对模型的影响就越大，模型也会更加复杂 3.SVM算法的优势与不足 SVM应对高维数据集和低维数据集都还算是得心应手。但是，前提条件是数据集的规模不太大。如果数据集中的样本数量在1万以内，SVM都能驾驭得了，但如果样本数量超过10万的话，SVM就会非常耗费时间和内存 SVM还有一个短板，就是对于数据预处理和参数调节要求非常高 在SVM算法中，有3个参数是比较重要的：第一个是核函数的选择；第二个是核函数的参数，例如RBF的gamma值；第三个是正则化参数C。RBF内核的gamma值是用来调节内核宽度的，gamma值和C值一起控制模型的复杂度，数值越大模型越复杂，而数值越小模型越简单 三.SVM实例–波士顿房价回归分析 在scikit-learn中，内置了一个非常适合做回归分析的数据集–波士顿房价数据集。我们将使用该数据集讲解SVM中用于回归分析的SVR的用法 1.初步了解数据集 # 导入波士顿房价数据集 from sklearn.datasets import load_boston boston=load_boston() # 打印数据集中的键 print(boston.keys()) dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;feature_names&#39;, &#39;DESCR&#39;, &#39;filename&#39;]) [结果分析] 从结果中可以看出，波士顿房价数据集中有5个键，分别是数据，目标，特征名称，描述和文件名。我们可能有疑问，波士顿房价数据集比红酒数据集少了一个键，就是目标名称(target_names)，这是为什么？我们查看一下 print(boston[&#39;DESCR&#39;]) .. _boston_dataset: Boston house prices dataset --------------------------- **Data Set Characteristics:** :Number of Instances: 506 :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target. :Attribute Information (in order): - CRIM per capita crime rate by town - ZN proportion of residential land zoned for lots over 25,000 sq.ft. - INDUS proportion of non-retail business acres per town - CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) - NOX nitric oxides concentration (parts per 10 million) - RM average number of rooms per dwelling - AGE proportion of owner-occupied units built prior to 1940 - DIS weighted distances to five Boston employment centres - RAD index of accessibility to radial highways - TAX full-value property-tax rate per $10,000 - PTRATIO pupil-teacher ratio by town - B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town - LSTAT % lower status of the population - MEDV Median value of owner-occupied homes in $1000&#39;s :Missing Attribute Values: None :Creator: Harrison, D. and Rubinfeld, D.L. This is a copy of UCI ML housing dataset. https://archive.ics.uci.edu/ml/machine-learning-databases/housing/ This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The Boston house-price data of Harrison, D. and Rubinfeld, D.L. &#39;Hedonic prices and the demand for clean air&#39;, J. Environ. Economics &amp; Management, vol.5, 81-102, 1978. Used in Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics ...&#39;, Wiley, 1980. N.B. Various transformations are used in the table on pages 244-261 of the latter. The Boston house-price data has been used in many machine learning papers that address regression problems. .. topic:: References - Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&#39;, Wiley, 1980. 244-261. - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann. [结果分析] 从上面的这段描述可以看出，数据集中共有506个样本，每个样本有13个特征变量。而后面还有一个叫作中位数的第14个变量，这个变量就是该数据集中的target 2.使用SVR进行建模 我们要先制作训练数据集和测试数据集，代码如下： # 导入数据集拆分工具 from sklearn.model_selection import train_test_split # 建立训练数据集和测试数据集 X,y=boston.data,boston.target X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=8) print(X_train.shape) print(X_test.shape) (379, 13) (127, 13) 下面开始用SVR进行建模，我们在前面介绍了SVM的两种核函数：Linear和rbf，不过我们不知道这两种核函数哪一个会让模型表现得更好，那么分别尝试一下： # 导入支持向量机回归模型 from sklearn.svm import SVR # 分别测试sklearn核函数和rbf核函数 for kernel in [&#39;linear&#39;,&#39;rbf&#39;]: svr=SVR(kernel=kernel) svr.fit(X_train,y_train) print(kernel,&#39;核函数的模型训练集得分：{:.3f}&#39;.format(svr.score(X_train,y_train))) print(kernel,&#39;核函数的模型测试集得分：{:.3f}&#39;.format(svr.score(X_test,y_test))) linear 核函数的模型训练集得分：0.709 linear 核函数的模型测试集得分：0.696 rbf 核函数的模型训练集得分：0.145 rbf 核函数的模型测试集得分：0.001 [结果分析] 两种核函数的模型得分都不能令人满意。使用rbf核函数的模型糟糕透了，在训练数据集的分只有0.145，而在测试数据集的得分完全可以用&quot;灾难&quot;来形容了—居然只有0.001分 这是什么原因呢？想想，会不会是数据集的各个特征之间的量级差的比较远？由于SVM算法对于数据预处理的要求是比较高的，如果数据特征量差异较大，我们需要对数据进行预处理。所以我们用可视化的方法看一看数据集中各个特征的数量级是什么情况 # 将特征数值中的最小值和最大值用散点图画出来 plt.plot(X.min(axis=0),&#39;v&#39;,label=&#39;min&#39;) plt.plot(X.max(axis=0),&#39;^&#39;,label=&#39;max&#39;) # 设定纵坐标为对数形式 plt.yscale(&#39;log&#39;) # 设置图注位置为最佳 plt.legend(loc=&#39;best&#39;) # 设定横纵轴标题 plt.xlabel(&#39;features&#39;) plt.ylabel(&#39;feature magnitude&#39;) # 显示图形 plt.show() 看来为了能够让SVM算法能够更好地对数据进行拟合，我们必须对数据集进行预处理 # 导入数据预处理工具 from sklearn.preprocessing import StandardScaler # 对训练集和测试集进行数据预处理 scaler=StandardScaler() scaler.fit(X_train) X_train_scaled=scaler.transform(X_train) X_test_scaled=scaler.transform(X_test) # 将预处理后的数据特征最大值和最小值用散点图表示出来 plt.plot(X_train_scaled.min(axis=0),&#39;v&#39;,label=&#39;train set min&#39;) plt.plot(X_train_scaled.max(axis=0),&#39;^&#39;,label=&#39;train set max&#39;) plt.plot(X_test_scaled.min(axis=0),&#39;v&#39;,label=&#39;test set min&#39;) plt.plot(X_test_scaled.max(axis=0),&#39;^&#39;,label=&#39;test set max&#39;) plt.yscale(&#39;log&#39;) # 设置图注位置 plt.legend(loc=&#39;best&#39;) # 设置横纵轴标题 plt.xlabel(&#39;scaled features&#39;) plt.ylabel(&#39;scaled feature magnitude&#39;) plt.show() [结果分析] 经过了我们的预处理，不管是训练集还是测试集，基本上所有的特征最大值都不会超过10，而最小值也都趋于0，以至于在图中我们看不到它们了 现在我们在试试用经过预处理的数据来训练模型，看看结果会有什么不同 # 导入支持向量机回归模型 from sklearn.svm import SVR # 分别测试sklearn核函数和rbf核函数 for kernel in [&#39;linear&#39;,&#39;rbf&#39;]: svr=SVR(kernel=kernel) svr.fit(X_train_scaled,y_train) print(kernel,&#39;核函数的模型训练集得分：{:.3f}&#39;.format(svr.score(X_train_scaled,y_train))) print(kernel,&#39;核函数的模型测试集得分：{:.3f}&#39;.format(svr.score(X_test_scaled,y_test))) linear 核函数的模型训练集得分：0.706 linear 核函数的模型测试集得分：0.698 rbf 核函数的模型训练集得分：0.665 rbf 核函数的模型测试集得分：0.695 [结果分析] 经过预处理之后，linear内核的SVR得分变化不大，而rbf内核的SVR得分有了巨大的提升 和SVC一样，SVR模型也有gamma和C两个参数，接下来我们试着对两个参数进行修改 # 设置模型的C参数和gamma参数 svr=SVR(C=100,gamma=0.1) svr.fit(X_train_scaled,y_train) print(&#39;调节参数后的模型在训练集得分：{:.3f}&#39;.format(svr.score(X_train_scaled,y_train))) print(&#39;调节参数后的模型在测试集得分：{:.3f}&#39;.format(svr.score(X_test_scaled,y_test))) 调节参数后的模型在训练集得分：0.966 调节参数后的模型在测试集得分：0.894 [结果分析] 这是一个比较不错的结果，我们看到通过参数调节，rbf内核的SVR模型在训练集的得分已经高达0.966，而在测试数据集的得分也达到了0.894，可以说现在模型的表现已经是可以接受的","@type":"BlogPosting","url":"/2019/02/22/d4f323b9f2810705ac64c5ff108f9a84.html","headline":"ML–支持向量机SVM","dateModified":"2019-02-22T00:00:00+08:00","datePublished":"2019-02-22T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/02/22/d4f323b9f2810705ac64c5ff108f9a84.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>ML--支持向量机SVM</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div id="content_views" class="markdown_views prism-dracula"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <h1><a id="MLSVM_0"></a>ML–支持向量机SVM</h1> 
  <p><code>SVM算法</code>专门解决线性不可分</p> 
  <p>主要涉及的知识点有：</p> 
  <ul> 
   <li>支持向量机的基本原理和构造</li> 
   <li>支持向量机的核函数</li> 
   <li>支持向量机的参数调节</li> 
   <li>支持向量机实例–对波士顿房价进行回归分析</li> 
  </ul> 
  <h2><a id="SVM_11"></a>一.支持向量机SVM基本概念</h2> 
  <h3><a id="1SVM_13"></a>1.支持向量机SVM的原理</h3> 
  <pre><code class="prism language-python"><span class="token comment"># 导入numpy</span>
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token comment"># 导入画图工具</span>
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token comment"># 导入支持向量机SVM</span>
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> svm
<span class="token comment"># 导入数据集生成工具</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_blobs

<span class="token comment"># 先创建50个数据点，让它们分为两类</span>
X<span class="token punctuation">,</span>y<span class="token operator">=</span>make_blobs<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>centers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">)</span>

<span class="token comment"># 创建一个线性内核的支持向量机模型</span>
clf<span class="token operator">=</span>svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span><span class="token string">'linear'</span><span class="token punctuation">,</span>C<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>
clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span>y<span class="token punctuation">)</span>

<span class="token comment"># 把数据点画出来</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c<span class="token operator">=</span>y<span class="token punctuation">,</span>s<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span>cmap<span class="token operator">=</span>plt<span class="token punctuation">.</span>cm<span class="token punctuation">.</span>Paired<span class="token punctuation">)</span>

<span class="token comment"># 建立图像坐标</span>
ax<span class="token operator">=</span>plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span><span class="token punctuation">)</span>
xlim<span class="token operator">=</span>ax<span class="token punctuation">.</span>get_xlim<span class="token punctuation">(</span><span class="token punctuation">)</span>
ylim<span class="token operator">=</span>ax<span class="token punctuation">.</span>get_ylim<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 生成两个等差数列</span>
xx<span class="token operator">=</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span>xlim<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>xlim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">30</span><span class="token punctuation">)</span>
yy<span class="token operator">=</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span>ylim<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>ylim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">30</span><span class="token punctuation">)</span>
YY<span class="token punctuation">,</span>XX<span class="token operator">=</span>np<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>yy<span class="token punctuation">,</span>xx<span class="token punctuation">)</span>
xy<span class="token operator">=</span>np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span><span class="token punctuation">[</span>XX<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>YY<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T
Z<span class="token operator">=</span>clf<span class="token punctuation">.</span>decision_function<span class="token punctuation">(</span>xy<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>XX<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

<span class="token comment"># 把分类的决定边界画出来</span>
ax<span class="token punctuation">.</span>contour<span class="token punctuation">(</span>XX<span class="token punctuation">,</span>YY<span class="token punctuation">,</span>Z<span class="token punctuation">,</span>colors<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span>levels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>alpha<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>linestyles<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'--'</span><span class="token punctuation">,</span><span class="token string">'-'</span><span class="token punctuation">,</span><span class="token string">'--'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>clf<span class="token punctuation">.</span>support_vectors_<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>clf<span class="token punctuation">.</span>support_vectors_<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>s<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>linewidth<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>facecolors<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p><img src="https://cdn.nlark.com/yuque/0/2019/png/214438/1550838574259-d7485efc-24af-4773-b5dc-acf3b5313a6a.png#align=left&amp;display=inline&amp;height=252&amp;linkTarget=_blank&amp;name=output_5_0.png&amp;originHeight=252&amp;originWidth=380&amp;size=15304&amp;width=380" alt="output_5_0.png"></p> 
  <p><strong>[结果分析]</strong> 从图中，可以清晰地看到，在分类器两侧分别有两条虚线，那些正好压在虚线上的数据点，就是我们刚刚提到的支持向量。而上例使用的这种方法称为<code>最大边界间隔超平面(Maximum Margin Separating Hyperplane)</code>。指的是说中间这条实线(在高维数据中是一个超平面)，和所有支持向量之间的距离，都是最大的</p> 
  <p>如果我们把<code>SVM</code>的内核换成是<code>RBF</code>，看下结果</p> 
  <pre><code class="prism language-python"><span class="token comment"># 创建一个RBF内核的支持向量机模型</span>
clf_rbf<span class="token operator">=</span>svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span><span class="token string">'rbf'</span><span class="token punctuation">,</span>C<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>
clf_rbf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span>y<span class="token punctuation">)</span>

<span class="token comment"># 把数据点画出来</span>
plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c<span class="token operator">=</span>y<span class="token punctuation">,</span>s<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span>cmap<span class="token operator">=</span>plt<span class="token punctuation">.</span>cm<span class="token punctuation">.</span>Paired<span class="token punctuation">)</span>

<span class="token comment"># 建立图像坐标</span>
ax<span class="token operator">=</span>plt<span class="token punctuation">.</span>gca<span class="token punctuation">(</span><span class="token punctuation">)</span>
xlim<span class="token operator">=</span>ax<span class="token punctuation">.</span>get_xlim<span class="token punctuation">(</span><span class="token punctuation">)</span>
ylim<span class="token operator">=</span>ax<span class="token punctuation">.</span>get_ylim<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 生成两个等差数列</span>
xx<span class="token operator">=</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span>xlim<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>xlim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">30</span><span class="token punctuation">)</span>
yy<span class="token operator">=</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span>ylim<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>ylim<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token number">30</span><span class="token punctuation">)</span>
YY<span class="token punctuation">,</span>XX<span class="token operator">=</span>np<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>yy<span class="token punctuation">,</span>xx<span class="token punctuation">)</span>
xy<span class="token operator">=</span>np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span><span class="token punctuation">[</span>XX<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>YY<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>T
Z<span class="token operator">=</span>clf_rbf<span class="token punctuation">.</span>decision_function<span class="token punctuation">(</span>xy<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>XX<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

<span class="token comment"># 把分类的决定边界画出来</span>
ax<span class="token punctuation">.</span>contour<span class="token punctuation">(</span>XX<span class="token punctuation">,</span>YY<span class="token punctuation">,</span>Z<span class="token punctuation">,</span>colors<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">,</span>levels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>alpha<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>linestyles<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'--'</span><span class="token punctuation">,</span><span class="token string">'-'</span><span class="token punctuation">,</span><span class="token string">'--'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>clf_rbf<span class="token punctuation">.</span>support_vectors_<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>clf_rbf<span class="token punctuation">.</span>support_vectors_<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>s<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>linewidth<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>facecolors<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p><img src="https://cdn.nlark.com/yuque/0/2019/png/214438/1550838589415-c6986fd1-1da0-4e3e-8208-d5c9f6215993.png#align=left&amp;display=inline&amp;height=252&amp;linkTarget=_blank&amp;name=output_8_1.png&amp;originHeight=252&amp;originWidth=380&amp;size=23534&amp;width=380" alt="output_8_1.png"></p> 
  <p><strong>[结果分析]</strong> 我们看到分类器的样子变得完全不一样了，这是因为当我们使用<code>RBF内核</code>的时候，数据点之间的距离是用如下公式来计算的：<br><img src="https://cdn.nlark.com/yuque/__latex/f3a253ccadac119cc2243a80197ec40e.svg#card=math&amp;code=k_%7Brbf%7D%28x1%2Cx2%29%3Dexp%28%CE%B3%7C%7Cx1-x2%7C%7C%5E%7B2%7D%29&amp;height=25&amp;width=229" alt=""><br>公式中的x1和x2代表两个不同的数据点，而|||x1-x2|代表两个点之间的欧几里得距离。<code>γ(gamma)</code>是用来控制<code>RBF</code>内核宽度的参数，也就是图中实线距离两条虚线的距离</p> 
  <h2><a id="SVM_93"></a>二.SVM的核函数与参数选择</h2> 
  <h3><a id="1SVM_95"></a>1.不同核函数的SVM对比</h3> 
  <p>前面提到的<code>linearSVM</code>就是一种使用了线性内核的<code>SVM</code>算法。不过<code>linearSVM</code>不支持对核函数进行修改，因为它默认只能使用线性内核。为了让大家能够直观体验不同内核的<code>SVM</code>算法在分类中的表现，我们画个图像进行展示</p> 
  <pre><code class="prism language-python"><span class="token comment"># 导入红酒数据集</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_wine

<span class="token comment"># 定义一个函数用来画图</span>
<span class="token keyword">def</span> <span class="token function">make_meshgrid</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">,</span>h<span class="token operator">=</span><span class="token number">.02</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    x_min<span class="token punctuation">,</span>x_max<span class="token operator">=</span>x<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>x<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span>
    y_min<span class="token punctuation">,</span>y_max<span class="token operator">=</span>y<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>y<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span>
    xx<span class="token punctuation">,</span>yy<span class="token operator">=</span>np<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>x_min<span class="token punctuation">,</span>x_max<span class="token punctuation">,</span>h<span class="token punctuation">)</span><span class="token punctuation">,</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>y_min<span class="token punctuation">,</span>y_max<span class="token punctuation">,</span>h<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> xx<span class="token punctuation">,</span>yy

<span class="token comment"># 定义一个绘制等高线的函数</span>
<span class="token keyword">def</span> <span class="token function">plot_contours</span><span class="token punctuation">(</span>ax<span class="token punctuation">,</span>clf<span class="token punctuation">,</span>xx<span class="token punctuation">,</span>yy<span class="token punctuation">,</span><span class="token operator">**</span>params<span class="token punctuation">)</span><span class="token punctuation">:</span>
    Z<span class="token operator">=</span>clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>np<span class="token punctuation">.</span>c_<span class="token punctuation">[</span>xx<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>yy<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    Z<span class="token operator">=</span>Z<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>xx<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
    out<span class="token operator">=</span>ax<span class="token punctuation">.</span>contourf<span class="token punctuation">(</span>xx<span class="token punctuation">,</span>yy<span class="token punctuation">,</span>Z<span class="token punctuation">,</span><span class="token operator">**</span>params<span class="token punctuation">)</span>
    <span class="token keyword">return</span> out

<span class="token comment"># 使用酒的数据集</span>
wine<span class="token operator">=</span>load_wine<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 选取数据集的前两个特征</span>
X<span class="token operator">=</span>wine<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>
y<span class="token operator">=</span>wine<span class="token punctuation">.</span>target

C<span class="token operator">=</span><span class="token number">1.0</span>   <span class="token comment"># 选取数据集的前两个特征</span>
models<span class="token operator">=</span><span class="token punctuation">(</span>svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span><span class="token string">'linear'</span><span class="token punctuation">,</span>C<span class="token operator">=</span>C<span class="token punctuation">)</span><span class="token punctuation">,</span>svm<span class="token punctuation">.</span>LinearSVC<span class="token punctuation">(</span>C<span class="token operator">=</span>C<span class="token punctuation">)</span><span class="token punctuation">,</span>svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span><span class="token string">'rbf'</span><span class="token punctuation">,</span>gamma<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span>C<span class="token operator">=</span>C<span class="token punctuation">)</span><span class="token punctuation">,</span>svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span><span class="token string">'poly'</span><span class="token punctuation">,</span>degree<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>C<span class="token operator">=</span>C<span class="token punctuation">)</span><span class="token punctuation">)</span>

models<span class="token operator">=</span><span class="token punctuation">(</span>clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span>y<span class="token punctuation">)</span> <span class="token keyword">for</span> clf <span class="token keyword">in</span> models<span class="token punctuation">)</span>

<span class="token comment"># 设定图题</span>
titles<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'SVC with linear kernel'</span><span class="token punctuation">,</span><span class="token string">'LinearSVC (linear kernel)'</span><span class="token punctuation">,</span><span class="token string">'SVC with RBF kernel'</span><span class="token punctuation">,</span><span class="token string">'SVC with polynomial (degree 3) kernel'</span><span class="token punctuation">)</span>

<span class="token comment"># 设定一个字图形的个数和排列方式</span>
fit<span class="token punctuation">,</span>sub<span class="token operator">=</span>plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>subplots_adjust<span class="token punctuation">(</span>wspace<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span>hspace<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">)</span>

<span class="token comment"># 使用前面定义的函数进行画图</span>
X0<span class="token punctuation">,</span>X1<span class="token operator">=</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span>
xx<span class="token punctuation">,</span>yy<span class="token operator">=</span>make_meshgrid<span class="token punctuation">(</span>X0<span class="token punctuation">,</span>X1<span class="token punctuation">)</span>

<span class="token keyword">for</span> clf<span class="token punctuation">,</span>title<span class="token punctuation">,</span>ax <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>models<span class="token punctuation">,</span>titles<span class="token punctuation">,</span>sub<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    plot_contours<span class="token punctuation">(</span>ax<span class="token punctuation">,</span>clf<span class="token punctuation">,</span>xx<span class="token punctuation">,</span>yy<span class="token punctuation">,</span>cmap<span class="token operator">=</span>plt<span class="token punctuation">.</span>cm<span class="token punctuation">.</span>plasma<span class="token punctuation">,</span>alpha<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>X0<span class="token punctuation">,</span>X1<span class="token punctuation">,</span>c<span class="token operator">=</span>y<span class="token punctuation">,</span>cmap<span class="token operator">=</span>plt<span class="token punctuation">.</span>cm<span class="token punctuation">.</span>plasma<span class="token punctuation">,</span>s<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>edgecolors<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>set_xlim<span class="token punctuation">(</span>xx<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>xx<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>set_ylim<span class="token punctuation">(</span>yy<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>yy<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    ax<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'Feature 0'</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Feature 1'</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>set_xticks<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>set_yticks<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span>title<span class="token punctuation">)</span>
    
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p><img src="https://cdn.nlark.com/yuque/0/2019/png/214438/1550838651636-5b6e859d-8035-41d1-baaf-888b2d487195.png#align=left&amp;display=inline&amp;height=261&amp;linkTarget=_blank&amp;name=output_13_2.png&amp;originHeight=261&amp;originWidth=409&amp;size=54535&amp;width=409" alt="output_13_2.png"></p> 
  <p><strong>[结果分析]</strong> 我们可以看到线性内核的<code>SVC</code>与<code>linearSVC</code>得到的结果非常近似，但仍然有一点点差别。其中一个原因是<code>linearSVC</code>对L2范数进行最小化，而线性内核的<code>SVC</code>是对L1范数进行最小化。无论如何，<code>linearSVC</code>和线性内核的<code>SVC</code>生成的决定边界都是线性的，在更高维数据集中将会是相交的超平面。而<code>RBF</code>内核的<code>SVC</code>和<code>polynomial</code>内核的<code>SVC</code>分类器的决定边界则完全不是线性的，它们更加弹性。而决定了它们决定边界形状的，就是它们的参数。在<code>polynomial</code>内核的<code>SVC</code>中，起决定性作用的参数就是<code>degree</code>和正则化参数<code>C</code>，在本例中我们使用的<code>degree</code>为3，也就是对原始数据集的特征进行乘3次方操作。而在<code>RBF</code>内核的<code>SVC</code>中，其起决定作用的是正则化参数<code>C</code>和参数<code>gamma</code><br><br> 接下来我们重点介绍一下<code>RBF</code>内核<code>SVC</code>的<code>gamma</code>参数调节</p> 
  <h3><a id="2gamma_162"></a>2.支持向量机的gamma参数调节</h3> 
  <p>首先让我们看一下不同的<code>gamma</code>值对于<code>RBF</code>内核的<code>SVC</code>分类器有什么影响</p> 
  <pre><code class="prism language-python">C<span class="token operator">=</span><span class="token number">1.0</span> <span class="token comment"># SVM正则化参数</span>
models<span class="token operator">=</span><span class="token punctuation">(</span>svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span><span class="token string">'rbf'</span><span class="token punctuation">,</span>gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>C<span class="token operator">=</span>C<span class="token punctuation">)</span><span class="token punctuation">,</span>svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span><span class="token string">'rbf'</span><span class="token punctuation">,</span>gamma<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>C<span class="token operator">=</span>C<span class="token punctuation">)</span><span class="token punctuation">,</span>svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span>kernel<span class="token operator">=</span><span class="token string">'rbf'</span><span class="token punctuation">,</span>gamma<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>C<span class="token operator">=</span>C<span class="token punctuation">)</span><span class="token punctuation">)</span>

models<span class="token operator">=</span><span class="token punctuation">(</span>clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span>y<span class="token punctuation">)</span> <span class="token keyword">for</span> clf <span class="token keyword">in</span> models<span class="token punctuation">)</span>

<span class="token comment"># 设定图题</span>
titles<span class="token operator">=</span><span class="token punctuation">(</span><span class="token string">'gamma=0.1'</span><span class="token punctuation">,</span><span class="token string">'gamma=1'</span><span class="token punctuation">,</span><span class="token string">'gamma=10'</span><span class="token punctuation">)</span>

<span class="token comment"># 设置子图形个数和排列</span>
fig<span class="token punctuation">,</span>sub<span class="token operator">=</span>plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

X0<span class="token punctuation">,</span>X1<span class="token operator">=</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span>
xx<span class="token punctuation">,</span>yy<span class="token operator">=</span>make_meshgrid<span class="token punctuation">(</span>X0<span class="token punctuation">,</span>X1<span class="token punctuation">)</span>

<span class="token comment"># 使用定义好的函数进行画图</span>
<span class="token keyword">for</span> clf<span class="token punctuation">,</span>title<span class="token punctuation">,</span>ax <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>models<span class="token punctuation">,</span>titles<span class="token punctuation">,</span>sub<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    plot_contours<span class="token punctuation">(</span>ax<span class="token punctuation">,</span>clf<span class="token punctuation">,</span>xx<span class="token punctuation">,</span>yy<span class="token punctuation">,</span>cmap<span class="token operator">=</span>plt<span class="token punctuation">.</span>cm<span class="token punctuation">.</span>plasma<span class="token punctuation">,</span>alpha<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">)</span>
    
    ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>X0<span class="token punctuation">,</span>X1<span class="token punctuation">,</span>c<span class="token operator">=</span>y<span class="token punctuation">,</span>cmap<span class="token operator">=</span>plt<span class="token punctuation">.</span>cm<span class="token punctuation">.</span>plasma<span class="token punctuation">,</span>s<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>edgecolors<span class="token operator">=</span><span class="token string">'k'</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>set_xlim<span class="token punctuation">(</span>xx<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>xx<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>set_ylim<span class="token punctuation">(</span>yy<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>yy<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    ax<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'Feature 0'</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Feature 1'</span><span class="token punctuation">)</span>
    
    ax<span class="token punctuation">.</span>set_xticks<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>set_yticks<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span>title<span class="token punctuation">)</span>
    
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p><img src="https://cdn.nlark.com/yuque/0/2019/png/214438/1550838661994-33efe995-42c1-45bd-9e0c-4e917459d266.png#align=left&amp;display=inline&amp;height=207&amp;linkTarget=_blank&amp;name=output_17_0.png&amp;originHeight=207&amp;originWidth=589&amp;size=51973&amp;width=589" alt="output_17_0.png"></p> 
  <p><strong>[结果分析]</strong> 从图中可以看出，自左到右<code>gamma</code>值从0.1增加到10，<code>gamma</code>值越小，则<code>RBF</code>内核的直径越大，这样就会有更多的点被模型圈进决定边界中，所以决定边界也就越平滑，这时的模型也就越简单；而随着参数的增加，模型则更倾向于把每个点都放到相应的决定边界中，这时模型的复杂度也相应提高了。所以<code>gamma</code>值越小模型越倾向于欠拟合，而<code>gamma</code>值越大，则模型越倾向于出现过拟合的问题</p> 
  <p>而至于正则化参数<code>C</code>，<code>C</code>值越小，模型就越受限，也就是说单个数据点对模型的影响越小，模型就越简单；而<code>C</code>值越大，每个数据点对模型的影响就越大，模型也会更加复杂</p> 
  <h3><a id="3SVM_205"></a>3.SVM算法的优势与不足</h3> 
  <p><code>SVM</code>应对高维数据集和低维数据集都还算是得心应手。但是，前提条件是数据集的规模不太大。如果数据集中的样本数量在1万以内，<code>SVM</code>都能驾驭得了，但如果样本数量超过10万的话，<code>SVM</code>就会非常耗费时间和内存</p> 
  <p><code>SVM</code>还有一个短板，就是对于数据预处理和参数调节要求非常高</p> 
  <p>在<code>SVM</code>算法中，有3个参数是比较重要的：第一个是核函数的选择；第二个是核函数的参数，例如<code>RBF</code>的<code>gamma</code>值；第三个是正则化参数<code>C</code>。<code>RBF</code>内核的<code>gamma</code>值是用来调节内核宽度的，<code>gamma</code>值和<code>C</code>值一起控制模型的复杂度，数值越大模型越复杂，而数值越小模型越简单</p> 
  <h2><a id="SVM_213"></a>三.SVM实例–波士顿房价回归分析</h2> 
  <p>在<code>scikit-learn</code>中，内置了一个非常适合做回归分析的数据集–波士顿房价数据集。我们将使用该数据集讲解<code>SVM</code>中用于回归分析的<code>SVR</code>的用法</p> 
  <h3><a id="1_217"></a>1.初步了解数据集</h3> 
  <pre><code class="prism language-python"><span class="token comment"># 导入波士顿房价数据集</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_boston
boston<span class="token operator">=</span>load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 打印数据集中的键</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>boston<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <pre><code>dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])
</code></pre> 
  <p><strong>[结果分析]</strong> 从结果中可以看出，波士顿房价数据集中有5个键，分别是数据，目标，特征名称，描述和文件名。我们可能有疑问，波士顿房价数据集比红酒数据集少了一个键，就是目标名称(target_names)，这是为什么？我们查看一下</p> 
  <pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>boston<span class="token punctuation">[</span><span class="token string">'DESCR'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
  <pre><code>.. _boston_dataset:

Boston house prices dataset
---------------------------

**Data Set Characteristics:**  

    :Number of Instances: 506 

    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.

    :Attribute Information (in order):
        - CRIM     per capita crime rate by town
        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
        - INDUS    proportion of non-retail business acres per town
        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
        - NOX      nitric oxides concentration (parts per 10 million)
        - RM       average number of rooms per dwelling
        - AGE      proportion of owner-occupied units built prior to 1940
        - DIS      weighted distances to five Boston employment centres
        - RAD      index of accessibility to radial highways
        - TAX      full-value property-tax rate per $10,000
        - PTRATIO  pupil-teacher ratio by town
        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
        - LSTAT    % lower status of the population
        - MEDV     Median value of owner-occupied homes in $1000's

    :Missing Attribute Values: None

    :Creator: Harrison, D. and Rubinfeld, D.L.

This is a copy of UCI ML housing dataset.
https://archive.ics.uci.edu/ml/machine-learning-databases/housing/


This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.

The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic
prices and the demand for clean air', J. Environ. Economics &amp; Management,
vol.5, 81-102, 1978.   Used in Belsley, Kuh &amp; Welsch, 'Regression diagnostics
...', Wiley, 1980.   N.B. Various transformations are used in the table on
pages 244-261 of the latter.

The Boston house-price data has been used in many machine learning papers that address regression
problems.   
     
.. topic:: References

   - Belsley, Kuh &amp; Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.
   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.
</code></pre> 
  <p><strong>[结果分析]</strong> 从上面的这段描述可以看出，数据集中共有506个样本，每个样本有13个特征变量。而后面还有一个叫作中位数的第14个变量，这个变量就是该数据集中的target</p> 
  <h3><a id="2SVR_293"></a>2.使用SVR进行建模</h3> 
  <p>我们要先制作训练数据集和测试数据集，代码如下：</p> 
  <pre><code class="prism language-python"><span class="token comment"># 导入数据集拆分工具</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token comment"># 建立训练数据集和测试数据集</span>
X<span class="token punctuation">,</span>y<span class="token operator">=</span>boston<span class="token punctuation">.</span>data<span class="token punctuation">,</span>boston<span class="token punctuation">.</span>target
X_train<span class="token punctuation">,</span>X_test<span class="token punctuation">,</span>y_train<span class="token punctuation">,</span>y_test<span class="token operator">=</span>train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">,</span>y<span class="token punctuation">,</span>random_state<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>X_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X_test<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre> 
  <pre><code>(379, 13)
(127, 13)
</code></pre> 
  <p>下面开始用<code>SVR</code>进行建模，我们在前面介绍了<code>SVM</code>的两种核函数：<code>Linear</code>和<code>rbf</code>，不过我们不知道这两种核函数哪一个会让模型表现得更好，那么分别尝试一下：</p> 
  <pre><code class="prism language-python"><span class="token comment"># 导入支持向量机回归模型</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>svm <span class="token keyword">import</span> SVR
<span class="token comment"># 分别测试sklearn核函数和rbf核函数</span>
<span class="token keyword">for</span> kernel <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'linear'</span><span class="token punctuation">,</span><span class="token string">'rbf'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    svr<span class="token operator">=</span>SVR<span class="token punctuation">(</span>kernel<span class="token operator">=</span>kernel<span class="token punctuation">)</span>
    svr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>kernel<span class="token punctuation">,</span><span class="token string">'核函数的模型训练集得分：{:.3f}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>svr<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>kernel<span class="token punctuation">,</span><span class="token string">'核函数的模型测试集得分：{:.3f}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>svr<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <pre><code>linear 核函数的模型训练集得分：0.709
linear 核函数的模型测试集得分：0.696
rbf 核函数的模型训练集得分：0.145
rbf 核函数的模型测试集得分：0.001
</code></pre> 
  <p><strong>[结果分析]</strong> 两种核函数的模型得分都不能令人满意。使用<code>rbf</code>核函数的模型糟糕透了，在训练数据集的分只有0.145，而在测试数据集的得分完全可以用"灾难"来形容了—居然只有0.001分</p> 
  <p>这是什么原因呢？想想，会不会是数据集的各个特征之间的量级差的比较远？由于<code>SVM</code>算法对于数据预处理的要求是比较高的，如果数据特征量差异较大，我们需要对数据进行预处理。所以我们用可视化的方法看一看数据集中各个特征的数量级是什么情况</p> 
  <pre><code class="prism language-python"><span class="token comment"># 将特征数值中的最小值和最大值用散点图画出来</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'v'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'min'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'^'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'max'</span><span class="token punctuation">)</span>

<span class="token comment"># 设定纵坐标为对数形式</span>
plt<span class="token punctuation">.</span>yscale<span class="token punctuation">(</span><span class="token string">'log'</span><span class="token punctuation">)</span>

<span class="token comment"># 设置图注位置为最佳</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'best'</span><span class="token punctuation">)</span>

<span class="token comment"># 设定横纵轴标题</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'features'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'feature magnitude'</span><span class="token punctuation">)</span>

<span class="token comment"># 显示图形</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p><img src="https://cdn.nlark.com/yuque/0/2019/png/214438/1550838704832-16adb28f-9242-44b9-95ec-636ba7680665.png#align=left&amp;display=inline&amp;height=266&amp;linkTarget=_blank&amp;name=output_37_0.png&amp;originHeight=266&amp;originWidth=398&amp;size=8179&amp;width=398" alt="output_37_0.png"></p> 
  <p>看来为了能够让<code>SVM</code>算法能够更好地对数据进行拟合，我们必须对数据集进行预处理</p> 
  <pre><code class="prism language-python"><span class="token comment"># 导入数据预处理工具</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> StandardScaler
<span class="token comment"># 对训练集和测试集进行数据预处理</span>
scaler<span class="token operator">=</span>StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
scaler<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span>
X_train_scaled<span class="token operator">=</span>scaler<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span>
X_test_scaled<span class="token operator">=</span>scaler<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span>

<span class="token comment"># 将预处理后的数据特征最大值和最小值用散点图表示出来</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X_train_scaled<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'v'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'train set min'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X_train_scaled<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'^'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'train set max'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X_test_scaled<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'v'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'test set min'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X_test_scaled<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'^'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'test set max'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>yscale<span class="token punctuation">(</span><span class="token string">'log'</span><span class="token punctuation">)</span>

<span class="token comment"># 设置图注位置</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'best'</span><span class="token punctuation">)</span>

<span class="token comment"># 设置横纵轴标题</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'scaled features'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'scaled feature magnitude'</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <p><img src="https://cdn.nlark.com/yuque/0/2019/png/214438/1550838715115-02e917c7-fa5d-484d-b1e7-e9e1d97c95a1.png#align=left&amp;display=inline&amp;height=266&amp;linkTarget=_blank&amp;name=output_39_0.png&amp;originHeight=266&amp;originWidth=392&amp;size=10627&amp;width=392" alt="output_39_0.png"></p> 
  <p><strong>[结果分析]</strong> 经过了我们的预处理，不管是训练集还是测试集，基本上所有的特征最大值都不会超过10，而最小值也都趋于0，以至于在图中我们看不到它们了</p> 
  <p>现在我们在试试用经过预处理的数据来训练模型，看看结果会有什么不同</p> 
  <pre><code class="prism language-python"><span class="token comment"># 导入支持向量机回归模型</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>svm <span class="token keyword">import</span> SVR
<span class="token comment"># 分别测试sklearn核函数和rbf核函数</span>
<span class="token keyword">for</span> kernel <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'linear'</span><span class="token punctuation">,</span><span class="token string">'rbf'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    svr<span class="token operator">=</span>SVR<span class="token punctuation">(</span>kernel<span class="token operator">=</span>kernel<span class="token punctuation">)</span>
    svr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train_scaled<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>kernel<span class="token punctuation">,</span><span class="token string">'核函数的模型训练集得分：{:.3f}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>svr<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_train_scaled<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>kernel<span class="token punctuation">,</span><span class="token string">'核函数的模型测试集得分：{:.3f}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>svr<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test_scaled<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <pre><code>linear 核函数的模型训练集得分：0.706
linear 核函数的模型测试集得分：0.698
rbf 核函数的模型训练集得分：0.665
rbf 核函数的模型测试集得分：0.695
</code></pre> 
  <p><strong>[结果分析]</strong> 经过预处理之后，<code>linear</code>内核的<code>SVR</code>得分变化不大，而<code>rbf</code>内核的<code>SVR</code>得分有了巨大的提升</p> 
  <p>和<code>SVC</code>一样，<code>SVR</code>模型也有<code>gamma</code>和<code>C</code>两个参数，接下来我们试着对两个参数进行修改</p> 
  <pre><code class="prism language-python"><span class="token comment"># 设置模型的C参数和gamma参数</span>
svr<span class="token operator">=</span>SVR<span class="token punctuation">(</span>C<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
svr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train_scaled<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'调节参数后的模型在训练集得分：{:.3f}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>svr<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_train_scaled<span class="token punctuation">,</span>y_train<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'调节参数后的模型在测试集得分：{:.3f}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>svr<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X_test_scaled<span class="token punctuation">,</span>y_test<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <pre><code>调节参数后的模型在训练集得分：0.966
调节参数后的模型在测试集得分：0.894
</code></pre> 
  <p><strong>[结果分析]</strong> 这是一个比较不错的结果，我们看到通过参数调节，<code>rbf</code>内核的<code>SVR</code>模型在训练集的得分已经高达0.966，而在测试数据集的得分也达到了0.894，可以说现在模型的表现已经是可以接受的</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
