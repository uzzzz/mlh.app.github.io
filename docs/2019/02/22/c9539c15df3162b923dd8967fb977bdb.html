<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>è‡ªç„¶è¯­è¨€å¤„ç†ä¹‹â€”â€“Word2Vec | æœ‰ç»„ç»‡åœ¨ï¼</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="è‡ªç„¶è¯­è¨€å¤„ç†ä¹‹â€”â€“Word2Vec" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A Beginnerâ€™s Guide to Word2Vec and Neural Word Embeddings Introduction to Word2Vec Word2vecæ˜¯ä¸€ä¸ªå¤„ç†æ–‡æœ¬çš„åŒå±‚ç¥ç»ç½‘ç»œã€‚å®ƒçš„è¾“å…¥æ˜¯ä¸€ä¸ªæ–‡æœ¬è¯­æ–™åº“ï¼Œå®ƒçš„è¾“å‡ºæ˜¯ä¸€ç»„å‘é‡ï¼šè¯¥è¯­æ–™åº“ä¸­å•è¯çš„ç‰¹å¾å‘é‡ã€‚è™½ç„¶Word2vecä¸æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œï¼Œä½†å®ƒå°†æ–‡æœ¬è½¬æ¢ä¸ºæ·±ç½‘å¯ä»¥ç†è§£çš„æ•°å­—å½¢å¼ã€‚ Deeplearning4jå®ç°äº†ä¸€ä¸ªåˆ†å¸ƒå¼çš„Word2vec for Javaå’ŒScalaï¼Œå®ƒå¯ä»¥åœ¨Sparkä¸Šè¿è¡ŒGPUã€‚ Word2vecçš„åº”ç”¨ç¨‹åºä¸ä»…ä»…æ˜¯è§£æé‡å¤–çš„å¥å­ã€‚å®ƒä¹Ÿå¯ä»¥åº”ç”¨äºåŸºå› ï¼Œä»£ç ï¼Œå–œæ¬¢ï¼Œæ’­æ”¾åˆ—è¡¨ï¼Œç¤¾äº¤åª’ä½“å›¾å’Œå…¶ä»–å¯ä»¥è¾¨åˆ«æ¨¡å¼çš„è¯­è¨€æˆ–ç¬¦å·ç³»åˆ—ã€‚ ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºå•è¯å°±åƒä¸Šé¢æåˆ°çš„å…¶ä»–æ•°æ®ä¸€æ ·åªæ˜¯ç¦»æ•£çŠ¶æ€ï¼Œæˆ‘ä»¬åªæ˜¯åœ¨å¯»æ‰¾è¿™äº›çŠ¶æ€ä¹‹é—´çš„è¿‡æ¸¡æ¦‚ç‡ï¼šå®ƒä»¬å…±åŒå‘ç”Ÿçš„å¯èƒ½æ€§ã€‚æ‰€ä»¥gene2vecï¼Œlike2vecå’Œfollower2vecéƒ½æ˜¯å¯èƒ½çš„ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œä¸‹é¢çš„æ•™ç¨‹å°†å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¸ºä»»ä½•ç¦»æ•£å’Œå…±ç°çŠ¶æ€ç»„åˆ›å»ºç¥ç»åµŒå…¥ã€‚ Word2Vecçš„ç›®çš„å’Œç”¨å¤„æ˜¯å°†ç›¸ä¼¼å•è¯çš„å‘é‡ç»„åˆåœ¨å‘é‡ç©ºé—´ä¸­ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¥æ•°å­¦æ–¹å¼æ£€æµ‹ç›¸ä¼¼æ€§ã€‚ Word2Vecåˆ›å»ºçš„å‘é‡æ˜¯å•è¯ç‰¹å¾çš„åˆ†å¸ƒå¼æ•°å­—è¡¨ç¤ºï¼Œè¯¸å¦‚å•ä¸ªå•è¯çš„ä¸Šä¸‹æ–‡ä¹‹ç±»çš„ç‰¹å¾ã€‚å®ƒæ²¡æœ‰äººä¸ºå¹²é¢„å°±è¿™æ ·åšäº†ã€‚ æœ‰äº†è¶³å¤Ÿçš„æ•°æ®ï¼Œç”¨æ³•å’Œä¸Šä¸‹æ–‡ï¼ŒWord2Vecå¯ä»¥æ ¹æ®è¿‡å»çš„å¤–è§‚å¯¹å•è¯çš„å«ä¹‰è¿›è¡Œé«˜åº¦å‡†ç¡®çš„çŒœæµ‹ã€‚è¿™äº›çŒœæµ‹å¯ç”¨äºå»ºç«‹å•è¯ä¸å…¶ä»–å•è¯çš„å…³è”ï¼ˆä¾‹å¦‚â€œç”·äººâ€æ˜¯â€œç”·å­©â€ï¼Œâ€œå¥³äººâ€æ˜¯â€œå¥³å­©â€ï¼‰ï¼Œæˆ–é›†ç¾¤æ–‡æ¡£å¹¶æŒ‰ä¸»é¢˜å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚è¿™äº›é›†ç¾¤å¯ä»¥æ„æˆæœç´¢ï¼Œæƒ…æ„Ÿåˆ†æå’Œç§‘å­¦ç ”ç©¶ï¼Œæ³•å¾‹å‘ç°ï¼Œç”µå­å•†åŠ¡å’Œå®¢æˆ·å…³ç³»ç®¡ç†ç­‰å¤šä¸ªé¢†åŸŸçš„å»ºè®®çš„åŸºç¡€ã€‚ Word2Vecç¥ç»ç½‘ç»œçš„è¾“å‡ºæ˜¯ä¸€ä¸ªè¯æ±‡è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®éƒ½é™„æœ‰ä¸€ä¸ªå‘é‡ï¼Œå¯ä»¥å°†å…¶è¾“å…¥æ·±åº¦å­¦ä¹ ç½‘ç»œæˆ–ç®€å•åœ°æŸ¥è¯¢ä»¥æ£€æµ‹å•è¯ä¹‹é—´çš„å…³ç³»ã€‚ æµ‹é‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ²¡æœ‰ç›¸ä¼¼æ€§è¡¨ç¤ºä¸º90åº¦è§’ï¼Œè€Œ1çš„æ€»ç›¸ä¼¼åº¦æ˜¯0åº¦è§’ï¼Œå®Œå…¨é‡å ;ç‘å…¸ç­‰äºç‘å…¸ï¼Œè€ŒæŒªå¨ä¸ç‘å…¸çš„ä½™å¼¦è·ç¦»ä¸º0.760124ï¼Œæ˜¯å…¶ä»–ä»»ä½•å›½å®¶ä¸­æœ€é«˜çš„ã€‚ ä»¥ä¸‹æ˜¯ä½¿ç”¨Word2vecä¸â€œç‘å…¸â€ç›¸å…³è”çš„å•è¯åˆ—è¡¨ï¼ŒæŒ‰ç…§æ¥è¿‘é¡ºåºæ’åˆ—ï¼š æ–¯å ªçš„çº³ç»´äºšå›½å®¶å’Œå‡ ä¸ªå¯Œè£•ï¼ŒåŒ—æ¬§ï¼Œæ—¥è€³æ›¼å›½å®¶éƒ½ä½åˆ—å‰ä¹ã€‚ Neural Word Embeddings The vectors we use to represent words are called neural word embeddings, and representations are strange. One thing describes another, even though those two things are radically different. As Elvis Costello said: â€œWriting about music is like dancing about architecture.â€ Word2vec â€œvectorizesâ€ about words, and by doing so it makes natural language computer-readable â€“ we can start to perform powerful mathematical operations on words to detect their similarities. So a neural word embedding represents a word with numbers. Itâ€™s a simple, yet unlikely, translation. Word2vec is similar to an autoencoder, encoding each word in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, word2vec trains words against other words that neighbor them in the input corpus. It does so in one of two ways, either using context to predict a target word (a method known as continuous bag of words, or CBOW), or using a word to predict a target context, which is called skip-gram. We use the latter method because it produces more accurate results on large datasets. When the feature vector assigned to a word cannot be used to accurately predict that wordâ€™s context, the components of the vector are adjusted. Each wordâ€™s context in the corpus is the teacher sending error signals back to adjust the feature vector. The vectors of words judged similar by their context are nudged closer together by adjusting the numbers in the vector. Just as Van Goghâ€™s painting of sunflowers is a two-dimensional mixture of oil on canvas that represents vegetable matter in a three-dimensional space in Paris in the late 1880s, so 500 numbers arranged in a vector can represent a word or group of words. Those numbers locate each word as a point in 500-dimensional vectorspace. Spaces of more than three dimensions are difficult to visualize. (Geoff Hinton, teaching people to imagine 13-dimensional space, suggests that students first picture 3-dimensional space and then say to themselves: â€œThirteen, thirteen, thirteen.â€ ğŸ˜ƒ A well trained set of word vectors will place similar words close to each other in that space. The words oak, elm and birch might cluster in one corner, while war, conflict and strife huddle together in another. Similar things and ideas are shown to be â€œcloseâ€. Their relative meanings have been translated to measurable distances. Qualities become quantities, and algorithms can do their work. But similarity is just the basis of many associations that Word2vec can learn. For example, it can gauge relations between words of one language, and map them to another. These vectors are the basis of a more comprehensive geometry of words. Not only will Rome, Paris, Berlin and Beijing cluster near each other, but they will each have similar distances in vectorspace to the countries whose capitals they are; i.e. Rome - Italy = Beijing - China. And if you only knew that Rome was the capital of Italy, and were wondering about the capital of China, then the equation Rome -Italy + China would return Beijing. No kidding. Amusing Word2Vec Results Letâ€™s look at some other associations Word2vec can produce. Instead of the pluses, minus and equals signs, weâ€™ll give you the results in the notation of logical analogies, where : means â€œis toâ€ and :: means â€œasâ€; e.g. â€œRome is to Italy as Beijing is to Chinaâ€ = Rome:Italy::Beijing:China. In the last spot, rather than supplying the â€œanswerâ€, weâ€™ll give you the list of words that a Word2vec model proposes, when given the first three elements: king:queen:ğŸ‘¨[woman, Attempted abduction, teenager, girl] //Weird, but you can kind of see it China:Taiwan::Russia:[Ukraine, Moscow, Moldova, Armenia] //Two large countries and their small, estranged neighbors house:roof::castle:[dome, bell_tower, spire, crenellations, turrets] knee:leg::elbow:[forearm, arm, ulna_bone] New York Times:Sulzberger::Fox:[Murdoch, Chernin, Bancroft, Ailes] //The Sulzberger-Ochs family owns and runs the NYT. //The Murdoch family owns News Corp., which owns Fox News. //Peter Chernin was News Corp.&#39;s COO for 13 yrs. //Roger Ailes is president of Fox News. //The Bancroft family sold the Wall St. Journal to News Corp. love:indifference::fear:[apathy, callousness, timidity, helplessness, inaction] //the poetry of this single array is simply amazingâ€¦ Donald Trump:Republican::Barack Obama:[Democratic, GOP, Democrats, McCain] //Itâ€™s interesting to note that, just as Obama and McCain were rivals, //so too, Word2vec thinks Trump has a rivalry with the idea Republican. monkey:human::dinosaur:[fossil, fossilized, Ice_Age_mammals, fossilization] //Humans are fossilized monkeys? Humans are whatâ€™s left //over from monkeys? Humans are the species that beat monkeys //just as Ice Age mammals beat dinosaurs? Plausible. building:architect::software:[programmer, SecurityCenter, WinPcap] This model was trained on the Google News vocab, which you can import and play with. Contemplate, for a moment, that the Word2vec algorithm has never been taught a single rule of English syntax. It knows nothing about the world, and is unassociated with any rules-based symbolic logic or knowledge graph. And yet it learns more, in a flexible and automated fashion, than most knowledge graphs will learn after many years of human labor. It comes to the Google News documents as a blank slate, and by the end of training, it can compute complex analogies that mean something to humans. You can also query a Word2vec model for other assocations. Not everything has to be two analogies that mirror each other. Geopolitics: Iraq - Violence = Jordan Distinction: Human - Animal = Ethics President - Power = Prime Minister Library - Books = Hall Analogy: Stock Market â‰ˆ Thermometer By building a sense of one wordâ€™s proximity to other similar words, which do not necessarily contain the same letters, we have moved beyond hard tokens to a smoother and more general sense of meaning. N-grams &amp; Skip-grams Words are read into the vector one at a time, and scanned back and forth within a certain range. Those ranges are n-grams, and an n-gram is a contiguous sequence of n items from a given linguistic sequence; it is the nth version of unigram, bigram, trigram, four-gram or five-gram. A skip-gram simply drops items from the n-gram. The skip-gram representation popularized by Mikolov and used in the DL4J implementation has proven to be more accurate than other models, such as continuous bag of words, due to the more generalizable contexts generated. This n-gram is then fed into a neural network to learn the significance of a given word vector; i.e. significance is defined as its usefulness as an indicator of certain larger meanings, or labels. Advances in NLP: ElMO, BERT and GPT-2 Word vectors form the basis of most recent advances in natural-language processing, including language models such as ElMO, ULMFit and BERT. But those language models change how they represent words; that is, that which the vectors represent changes. Word2vec is an algorithm used to produce distributed representations of words, and by that we mean word types; i.e. any given word in a vocabulary, such as get or grab or go has its own word vector, and those vectors are effectively stored in a lookup table or dictionary. Unfortunately, this approach to word representation does not addres polysemy, or the co-existence of many possible meanings for a given word or phrase. For example, go is a verb and it is also a board game; get is a verb and it is also an animalâ€™s offspring. The meaning of a given word type such as go or get varies according to its context; i.e. the words that surround it. One thing that ElMO and BERT demonstrate is that by encoding the context of a given word, by including information about preceding and succeeding words in the vector that represents a given instance of a word, we can obtain much better results in natural language processing tasks. BERT owes its performance to the attention mechanism. Tested on the SWAG benchmark, which measures commonsense reasoning, ELMo was found to produce a 5% error reduction relaitve to non-contextual word vectors, while BERT showed an additional 66% error reduction past ELMo. More recently, OpenAIâ€™s work with GPT-2 showed surprisingly good results in generating natural language in response to a prompt." />
<meta property="og:description" content="A Beginnerâ€™s Guide to Word2Vec and Neural Word Embeddings Introduction to Word2Vec Word2vecæ˜¯ä¸€ä¸ªå¤„ç†æ–‡æœ¬çš„åŒå±‚ç¥ç»ç½‘ç»œã€‚å®ƒçš„è¾“å…¥æ˜¯ä¸€ä¸ªæ–‡æœ¬è¯­æ–™åº“ï¼Œå®ƒçš„è¾“å‡ºæ˜¯ä¸€ç»„å‘é‡ï¼šè¯¥è¯­æ–™åº“ä¸­å•è¯çš„ç‰¹å¾å‘é‡ã€‚è™½ç„¶Word2vecä¸æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œï¼Œä½†å®ƒå°†æ–‡æœ¬è½¬æ¢ä¸ºæ·±ç½‘å¯ä»¥ç†è§£çš„æ•°å­—å½¢å¼ã€‚ Deeplearning4jå®ç°äº†ä¸€ä¸ªåˆ†å¸ƒå¼çš„Word2vec for Javaå’ŒScalaï¼Œå®ƒå¯ä»¥åœ¨Sparkä¸Šè¿è¡ŒGPUã€‚ Word2vecçš„åº”ç”¨ç¨‹åºä¸ä»…ä»…æ˜¯è§£æé‡å¤–çš„å¥å­ã€‚å®ƒä¹Ÿå¯ä»¥åº”ç”¨äºåŸºå› ï¼Œä»£ç ï¼Œå–œæ¬¢ï¼Œæ’­æ”¾åˆ—è¡¨ï¼Œç¤¾äº¤åª’ä½“å›¾å’Œå…¶ä»–å¯ä»¥è¾¨åˆ«æ¨¡å¼çš„è¯­è¨€æˆ–ç¬¦å·ç³»åˆ—ã€‚ ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºå•è¯å°±åƒä¸Šé¢æåˆ°çš„å…¶ä»–æ•°æ®ä¸€æ ·åªæ˜¯ç¦»æ•£çŠ¶æ€ï¼Œæˆ‘ä»¬åªæ˜¯åœ¨å¯»æ‰¾è¿™äº›çŠ¶æ€ä¹‹é—´çš„è¿‡æ¸¡æ¦‚ç‡ï¼šå®ƒä»¬å…±åŒå‘ç”Ÿçš„å¯èƒ½æ€§ã€‚æ‰€ä»¥gene2vecï¼Œlike2vecå’Œfollower2vecéƒ½æ˜¯å¯èƒ½çš„ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œä¸‹é¢çš„æ•™ç¨‹å°†å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¸ºä»»ä½•ç¦»æ•£å’Œå…±ç°çŠ¶æ€ç»„åˆ›å»ºç¥ç»åµŒå…¥ã€‚ Word2Vecçš„ç›®çš„å’Œç”¨å¤„æ˜¯å°†ç›¸ä¼¼å•è¯çš„å‘é‡ç»„åˆåœ¨å‘é‡ç©ºé—´ä¸­ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¥æ•°å­¦æ–¹å¼æ£€æµ‹ç›¸ä¼¼æ€§ã€‚ Word2Vecåˆ›å»ºçš„å‘é‡æ˜¯å•è¯ç‰¹å¾çš„åˆ†å¸ƒå¼æ•°å­—è¡¨ç¤ºï¼Œè¯¸å¦‚å•ä¸ªå•è¯çš„ä¸Šä¸‹æ–‡ä¹‹ç±»çš„ç‰¹å¾ã€‚å®ƒæ²¡æœ‰äººä¸ºå¹²é¢„å°±è¿™æ ·åšäº†ã€‚ æœ‰äº†è¶³å¤Ÿçš„æ•°æ®ï¼Œç”¨æ³•å’Œä¸Šä¸‹æ–‡ï¼ŒWord2Vecå¯ä»¥æ ¹æ®è¿‡å»çš„å¤–è§‚å¯¹å•è¯çš„å«ä¹‰è¿›è¡Œé«˜åº¦å‡†ç¡®çš„çŒœæµ‹ã€‚è¿™äº›çŒœæµ‹å¯ç”¨äºå»ºç«‹å•è¯ä¸å…¶ä»–å•è¯çš„å…³è”ï¼ˆä¾‹å¦‚â€œç”·äººâ€æ˜¯â€œç”·å­©â€ï¼Œâ€œå¥³äººâ€æ˜¯â€œå¥³å­©â€ï¼‰ï¼Œæˆ–é›†ç¾¤æ–‡æ¡£å¹¶æŒ‰ä¸»é¢˜å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚è¿™äº›é›†ç¾¤å¯ä»¥æ„æˆæœç´¢ï¼Œæƒ…æ„Ÿåˆ†æå’Œç§‘å­¦ç ”ç©¶ï¼Œæ³•å¾‹å‘ç°ï¼Œç”µå­å•†åŠ¡å’Œå®¢æˆ·å…³ç³»ç®¡ç†ç­‰å¤šä¸ªé¢†åŸŸçš„å»ºè®®çš„åŸºç¡€ã€‚ Word2Vecç¥ç»ç½‘ç»œçš„è¾“å‡ºæ˜¯ä¸€ä¸ªè¯æ±‡è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®éƒ½é™„æœ‰ä¸€ä¸ªå‘é‡ï¼Œå¯ä»¥å°†å…¶è¾“å…¥æ·±åº¦å­¦ä¹ ç½‘ç»œæˆ–ç®€å•åœ°æŸ¥è¯¢ä»¥æ£€æµ‹å•è¯ä¹‹é—´çš„å…³ç³»ã€‚ æµ‹é‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ²¡æœ‰ç›¸ä¼¼æ€§è¡¨ç¤ºä¸º90åº¦è§’ï¼Œè€Œ1çš„æ€»ç›¸ä¼¼åº¦æ˜¯0åº¦è§’ï¼Œå®Œå…¨é‡å ;ç‘å…¸ç­‰äºç‘å…¸ï¼Œè€ŒæŒªå¨ä¸ç‘å…¸çš„ä½™å¼¦è·ç¦»ä¸º0.760124ï¼Œæ˜¯å…¶ä»–ä»»ä½•å›½å®¶ä¸­æœ€é«˜çš„ã€‚ ä»¥ä¸‹æ˜¯ä½¿ç”¨Word2vecä¸â€œç‘å…¸â€ç›¸å…³è”çš„å•è¯åˆ—è¡¨ï¼ŒæŒ‰ç…§æ¥è¿‘é¡ºåºæ’åˆ—ï¼š æ–¯å ªçš„çº³ç»´äºšå›½å®¶å’Œå‡ ä¸ªå¯Œè£•ï¼ŒåŒ—æ¬§ï¼Œæ—¥è€³æ›¼å›½å®¶éƒ½ä½åˆ—å‰ä¹ã€‚ Neural Word Embeddings The vectors we use to represent words are called neural word embeddings, and representations are strange. One thing describes another, even though those two things are radically different. As Elvis Costello said: â€œWriting about music is like dancing about architecture.â€ Word2vec â€œvectorizesâ€ about words, and by doing so it makes natural language computer-readable â€“ we can start to perform powerful mathematical operations on words to detect their similarities. So a neural word embedding represents a word with numbers. Itâ€™s a simple, yet unlikely, translation. Word2vec is similar to an autoencoder, encoding each word in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, word2vec trains words against other words that neighbor them in the input corpus. It does so in one of two ways, either using context to predict a target word (a method known as continuous bag of words, or CBOW), or using a word to predict a target context, which is called skip-gram. We use the latter method because it produces more accurate results on large datasets. When the feature vector assigned to a word cannot be used to accurately predict that wordâ€™s context, the components of the vector are adjusted. Each wordâ€™s context in the corpus is the teacher sending error signals back to adjust the feature vector. The vectors of words judged similar by their context are nudged closer together by adjusting the numbers in the vector. Just as Van Goghâ€™s painting of sunflowers is a two-dimensional mixture of oil on canvas that represents vegetable matter in a three-dimensional space in Paris in the late 1880s, so 500 numbers arranged in a vector can represent a word or group of words. Those numbers locate each word as a point in 500-dimensional vectorspace. Spaces of more than three dimensions are difficult to visualize. (Geoff Hinton, teaching people to imagine 13-dimensional space, suggests that students first picture 3-dimensional space and then say to themselves: â€œThirteen, thirteen, thirteen.â€ ğŸ˜ƒ A well trained set of word vectors will place similar words close to each other in that space. The words oak, elm and birch might cluster in one corner, while war, conflict and strife huddle together in another. Similar things and ideas are shown to be â€œcloseâ€. Their relative meanings have been translated to measurable distances. Qualities become quantities, and algorithms can do their work. But similarity is just the basis of many associations that Word2vec can learn. For example, it can gauge relations between words of one language, and map them to another. These vectors are the basis of a more comprehensive geometry of words. Not only will Rome, Paris, Berlin and Beijing cluster near each other, but they will each have similar distances in vectorspace to the countries whose capitals they are; i.e. Rome - Italy = Beijing - China. And if you only knew that Rome was the capital of Italy, and were wondering about the capital of China, then the equation Rome -Italy + China would return Beijing. No kidding. Amusing Word2Vec Results Letâ€™s look at some other associations Word2vec can produce. Instead of the pluses, minus and equals signs, weâ€™ll give you the results in the notation of logical analogies, where : means â€œis toâ€ and :: means â€œasâ€; e.g. â€œRome is to Italy as Beijing is to Chinaâ€ = Rome:Italy::Beijing:China. In the last spot, rather than supplying the â€œanswerâ€, weâ€™ll give you the list of words that a Word2vec model proposes, when given the first three elements: king:queen:ğŸ‘¨[woman, Attempted abduction, teenager, girl] //Weird, but you can kind of see it China:Taiwan::Russia:[Ukraine, Moscow, Moldova, Armenia] //Two large countries and their small, estranged neighbors house:roof::castle:[dome, bell_tower, spire, crenellations, turrets] knee:leg::elbow:[forearm, arm, ulna_bone] New York Times:Sulzberger::Fox:[Murdoch, Chernin, Bancroft, Ailes] //The Sulzberger-Ochs family owns and runs the NYT. //The Murdoch family owns News Corp., which owns Fox News. //Peter Chernin was News Corp.&#39;s COO for 13 yrs. //Roger Ailes is president of Fox News. //The Bancroft family sold the Wall St. Journal to News Corp. love:indifference::fear:[apathy, callousness, timidity, helplessness, inaction] //the poetry of this single array is simply amazingâ€¦ Donald Trump:Republican::Barack Obama:[Democratic, GOP, Democrats, McCain] //Itâ€™s interesting to note that, just as Obama and McCain were rivals, //so too, Word2vec thinks Trump has a rivalry with the idea Republican. monkey:human::dinosaur:[fossil, fossilized, Ice_Age_mammals, fossilization] //Humans are fossilized monkeys? Humans are whatâ€™s left //over from monkeys? Humans are the species that beat monkeys //just as Ice Age mammals beat dinosaurs? Plausible. building:architect::software:[programmer, SecurityCenter, WinPcap] This model was trained on the Google News vocab, which you can import and play with. Contemplate, for a moment, that the Word2vec algorithm has never been taught a single rule of English syntax. It knows nothing about the world, and is unassociated with any rules-based symbolic logic or knowledge graph. And yet it learns more, in a flexible and automated fashion, than most knowledge graphs will learn after many years of human labor. It comes to the Google News documents as a blank slate, and by the end of training, it can compute complex analogies that mean something to humans. You can also query a Word2vec model for other assocations. Not everything has to be two analogies that mirror each other. Geopolitics: Iraq - Violence = Jordan Distinction: Human - Animal = Ethics President - Power = Prime Minister Library - Books = Hall Analogy: Stock Market â‰ˆ Thermometer By building a sense of one wordâ€™s proximity to other similar words, which do not necessarily contain the same letters, we have moved beyond hard tokens to a smoother and more general sense of meaning. N-grams &amp; Skip-grams Words are read into the vector one at a time, and scanned back and forth within a certain range. Those ranges are n-grams, and an n-gram is a contiguous sequence of n items from a given linguistic sequence; it is the nth version of unigram, bigram, trigram, four-gram or five-gram. A skip-gram simply drops items from the n-gram. The skip-gram representation popularized by Mikolov and used in the DL4J implementation has proven to be more accurate than other models, such as continuous bag of words, due to the more generalizable contexts generated. This n-gram is then fed into a neural network to learn the significance of a given word vector; i.e. significance is defined as its usefulness as an indicator of certain larger meanings, or labels. Advances in NLP: ElMO, BERT and GPT-2 Word vectors form the basis of most recent advances in natural-language processing, including language models such as ElMO, ULMFit and BERT. But those language models change how they represent words; that is, that which the vectors represent changes. Word2vec is an algorithm used to produce distributed representations of words, and by that we mean word types; i.e. any given word in a vocabulary, such as get or grab or go has its own word vector, and those vectors are effectively stored in a lookup table or dictionary. Unfortunately, this approach to word representation does not addres polysemy, or the co-existence of many possible meanings for a given word or phrase. For example, go is a verb and it is also a board game; get is a verb and it is also an animalâ€™s offspring. The meaning of a given word type such as go or get varies according to its context; i.e. the words that surround it. One thing that ElMO and BERT demonstrate is that by encoding the context of a given word, by including information about preceding and succeeding words in the vector that represents a given instance of a word, we can obtain much better results in natural language processing tasks. BERT owes its performance to the attention mechanism. Tested on the SWAG benchmark, which measures commonsense reasoning, ELMo was found to produce a 5% error reduction relaitve to non-contextual word vectors, while BERT showed an additional 66% error reduction past ELMo. More recently, OpenAIâ€™s work with GPT-2 showed surprisingly good results in generating natural language in response to a prompt." />
<meta property="og:site_name" content="æœ‰ç»„ç»‡åœ¨ï¼" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-22T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"A Beginnerâ€™s Guide to Word2Vec and Neural Word Embeddings Introduction to Word2Vec Word2vecæ˜¯ä¸€ä¸ªå¤„ç†æ–‡æœ¬çš„åŒå±‚ç¥ç»ç½‘ç»œã€‚å®ƒçš„è¾“å…¥æ˜¯ä¸€ä¸ªæ–‡æœ¬è¯­æ–™åº“ï¼Œå®ƒçš„è¾“å‡ºæ˜¯ä¸€ç»„å‘é‡ï¼šè¯¥è¯­æ–™åº“ä¸­å•è¯çš„ç‰¹å¾å‘é‡ã€‚è™½ç„¶Word2vecä¸æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œï¼Œä½†å®ƒå°†æ–‡æœ¬è½¬æ¢ä¸ºæ·±ç½‘å¯ä»¥ç†è§£çš„æ•°å­—å½¢å¼ã€‚ Deeplearning4jå®ç°äº†ä¸€ä¸ªåˆ†å¸ƒå¼çš„Word2vec for Javaå’ŒScalaï¼Œå®ƒå¯ä»¥åœ¨Sparkä¸Šè¿è¡ŒGPUã€‚ Word2vecçš„åº”ç”¨ç¨‹åºä¸ä»…ä»…æ˜¯è§£æé‡å¤–çš„å¥å­ã€‚å®ƒä¹Ÿå¯ä»¥åº”ç”¨äºåŸºå› ï¼Œä»£ç ï¼Œå–œæ¬¢ï¼Œæ’­æ”¾åˆ—è¡¨ï¼Œç¤¾äº¤åª’ä½“å›¾å’Œå…¶ä»–å¯ä»¥è¾¨åˆ«æ¨¡å¼çš„è¯­è¨€æˆ–ç¬¦å·ç³»åˆ—ã€‚ ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºå•è¯å°±åƒä¸Šé¢æåˆ°çš„å…¶ä»–æ•°æ®ä¸€æ ·åªæ˜¯ç¦»æ•£çŠ¶æ€ï¼Œæˆ‘ä»¬åªæ˜¯åœ¨å¯»æ‰¾è¿™äº›çŠ¶æ€ä¹‹é—´çš„è¿‡æ¸¡æ¦‚ç‡ï¼šå®ƒä»¬å…±åŒå‘ç”Ÿçš„å¯èƒ½æ€§ã€‚æ‰€ä»¥gene2vecï¼Œlike2vecå’Œfollower2vecéƒ½æ˜¯å¯èƒ½çš„ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œä¸‹é¢çš„æ•™ç¨‹å°†å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¸ºä»»ä½•ç¦»æ•£å’Œå…±ç°çŠ¶æ€ç»„åˆ›å»ºç¥ç»åµŒå…¥ã€‚ Word2Vecçš„ç›®çš„å’Œç”¨å¤„æ˜¯å°†ç›¸ä¼¼å•è¯çš„å‘é‡ç»„åˆåœ¨å‘é‡ç©ºé—´ä¸­ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¥æ•°å­¦æ–¹å¼æ£€æµ‹ç›¸ä¼¼æ€§ã€‚ Word2Vecåˆ›å»ºçš„å‘é‡æ˜¯å•è¯ç‰¹å¾çš„åˆ†å¸ƒå¼æ•°å­—è¡¨ç¤ºï¼Œè¯¸å¦‚å•ä¸ªå•è¯çš„ä¸Šä¸‹æ–‡ä¹‹ç±»çš„ç‰¹å¾ã€‚å®ƒæ²¡æœ‰äººä¸ºå¹²é¢„å°±è¿™æ ·åšäº†ã€‚ æœ‰äº†è¶³å¤Ÿçš„æ•°æ®ï¼Œç”¨æ³•å’Œä¸Šä¸‹æ–‡ï¼ŒWord2Vecå¯ä»¥æ ¹æ®è¿‡å»çš„å¤–è§‚å¯¹å•è¯çš„å«ä¹‰è¿›è¡Œé«˜åº¦å‡†ç¡®çš„çŒœæµ‹ã€‚è¿™äº›çŒœæµ‹å¯ç”¨äºå»ºç«‹å•è¯ä¸å…¶ä»–å•è¯çš„å…³è”ï¼ˆä¾‹å¦‚â€œç”·äººâ€æ˜¯â€œç”·å­©â€ï¼Œâ€œå¥³äººâ€æ˜¯â€œå¥³å­©â€ï¼‰ï¼Œæˆ–é›†ç¾¤æ–‡æ¡£å¹¶æŒ‰ä¸»é¢˜å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚è¿™äº›é›†ç¾¤å¯ä»¥æ„æˆæœç´¢ï¼Œæƒ…æ„Ÿåˆ†æå’Œç§‘å­¦ç ”ç©¶ï¼Œæ³•å¾‹å‘ç°ï¼Œç”µå­å•†åŠ¡å’Œå®¢æˆ·å…³ç³»ç®¡ç†ç­‰å¤šä¸ªé¢†åŸŸçš„å»ºè®®çš„åŸºç¡€ã€‚ Word2Vecç¥ç»ç½‘ç»œçš„è¾“å‡ºæ˜¯ä¸€ä¸ªè¯æ±‡è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®éƒ½é™„æœ‰ä¸€ä¸ªå‘é‡ï¼Œå¯ä»¥å°†å…¶è¾“å…¥æ·±åº¦å­¦ä¹ ç½‘ç»œæˆ–ç®€å•åœ°æŸ¥è¯¢ä»¥æ£€æµ‹å•è¯ä¹‹é—´çš„å…³ç³»ã€‚ æµ‹é‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ²¡æœ‰ç›¸ä¼¼æ€§è¡¨ç¤ºä¸º90åº¦è§’ï¼Œè€Œ1çš„æ€»ç›¸ä¼¼åº¦æ˜¯0åº¦è§’ï¼Œå®Œå…¨é‡å ;ç‘å…¸ç­‰äºç‘å…¸ï¼Œè€ŒæŒªå¨ä¸ç‘å…¸çš„ä½™å¼¦è·ç¦»ä¸º0.760124ï¼Œæ˜¯å…¶ä»–ä»»ä½•å›½å®¶ä¸­æœ€é«˜çš„ã€‚ ä»¥ä¸‹æ˜¯ä½¿ç”¨Word2vecä¸â€œç‘å…¸â€ç›¸å…³è”çš„å•è¯åˆ—è¡¨ï¼ŒæŒ‰ç…§æ¥è¿‘é¡ºåºæ’åˆ—ï¼š æ–¯å ªçš„çº³ç»´äºšå›½å®¶å’Œå‡ ä¸ªå¯Œè£•ï¼ŒåŒ—æ¬§ï¼Œæ—¥è€³æ›¼å›½å®¶éƒ½ä½åˆ—å‰ä¹ã€‚ Neural Word Embeddings The vectors we use to represent words are called neural word embeddings, and representations are strange. One thing describes another, even though those two things are radically different. As Elvis Costello said: â€œWriting about music is like dancing about architecture.â€ Word2vec â€œvectorizesâ€ about words, and by doing so it makes natural language computer-readable â€“ we can start to perform powerful mathematical operations on words to detect their similarities. So a neural word embedding represents a word with numbers. Itâ€™s a simple, yet unlikely, translation. Word2vec is similar to an autoencoder, encoding each word in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, word2vec trains words against other words that neighbor them in the input corpus. It does so in one of two ways, either using context to predict a target word (a method known as continuous bag of words, or CBOW), or using a word to predict a target context, which is called skip-gram. We use the latter method because it produces more accurate results on large datasets. When the feature vector assigned to a word cannot be used to accurately predict that wordâ€™s context, the components of the vector are adjusted. Each wordâ€™s context in the corpus is the teacher sending error signals back to adjust the feature vector. The vectors of words judged similar by their context are nudged closer together by adjusting the numbers in the vector. Just as Van Goghâ€™s painting of sunflowers is a two-dimensional mixture of oil on canvas that represents vegetable matter in a three-dimensional space in Paris in the late 1880s, so 500 numbers arranged in a vector can represent a word or group of words. Those numbers locate each word as a point in 500-dimensional vectorspace. Spaces of more than three dimensions are difficult to visualize. (Geoff Hinton, teaching people to imagine 13-dimensional space, suggests that students first picture 3-dimensional space and then say to themselves: â€œThirteen, thirteen, thirteen.â€ ğŸ˜ƒ A well trained set of word vectors will place similar words close to each other in that space. The words oak, elm and birch might cluster in one corner, while war, conflict and strife huddle together in another. Similar things and ideas are shown to be â€œcloseâ€. Their relative meanings have been translated to measurable distances. Qualities become quantities, and algorithms can do their work. But similarity is just the basis of many associations that Word2vec can learn. For example, it can gauge relations between words of one language, and map them to another. These vectors are the basis of a more comprehensive geometry of words. Not only will Rome, Paris, Berlin and Beijing cluster near each other, but they will each have similar distances in vectorspace to the countries whose capitals they are; i.e. Rome - Italy = Beijing - China. And if you only knew that Rome was the capital of Italy, and were wondering about the capital of China, then the equation Rome -Italy + China would return Beijing. No kidding. Amusing Word2Vec Results Letâ€™s look at some other associations Word2vec can produce. Instead of the pluses, minus and equals signs, weâ€™ll give you the results in the notation of logical analogies, where : means â€œis toâ€ and :: means â€œasâ€; e.g. â€œRome is to Italy as Beijing is to Chinaâ€ = Rome:Italy::Beijing:China. In the last spot, rather than supplying the â€œanswerâ€, weâ€™ll give you the list of words that a Word2vec model proposes, when given the first three elements: king:queen:ğŸ‘¨[woman, Attempted abduction, teenager, girl] //Weird, but you can kind of see it China:Taiwan::Russia:[Ukraine, Moscow, Moldova, Armenia] //Two large countries and their small, estranged neighbors house:roof::castle:[dome, bell_tower, spire, crenellations, turrets] knee:leg::elbow:[forearm, arm, ulna_bone] New York Times:Sulzberger::Fox:[Murdoch, Chernin, Bancroft, Ailes] //The Sulzberger-Ochs family owns and runs the NYT. //The Murdoch family owns News Corp., which owns Fox News. //Peter Chernin was News Corp.&#39;s COO for 13 yrs. //Roger Ailes is president of Fox News. //The Bancroft family sold the Wall St. Journal to News Corp. love:indifference::fear:[apathy, callousness, timidity, helplessness, inaction] //the poetry of this single array is simply amazingâ€¦ Donald Trump:Republican::Barack Obama:[Democratic, GOP, Democrats, McCain] //Itâ€™s interesting to note that, just as Obama and McCain were rivals, //so too, Word2vec thinks Trump has a rivalry with the idea Republican. monkey:human::dinosaur:[fossil, fossilized, Ice_Age_mammals, fossilization] //Humans are fossilized monkeys? Humans are whatâ€™s left //over from monkeys? Humans are the species that beat monkeys //just as Ice Age mammals beat dinosaurs? Plausible. building:architect::software:[programmer, SecurityCenter, WinPcap] This model was trained on the Google News vocab, which you can import and play with. Contemplate, for a moment, that the Word2vec algorithm has never been taught a single rule of English syntax. It knows nothing about the world, and is unassociated with any rules-based symbolic logic or knowledge graph. And yet it learns more, in a flexible and automated fashion, than most knowledge graphs will learn after many years of human labor. It comes to the Google News documents as a blank slate, and by the end of training, it can compute complex analogies that mean something to humans. You can also query a Word2vec model for other assocations. Not everything has to be two analogies that mirror each other. Geopolitics: Iraq - Violence = Jordan Distinction: Human - Animal = Ethics President - Power = Prime Minister Library - Books = Hall Analogy: Stock Market â‰ˆ Thermometer By building a sense of one wordâ€™s proximity to other similar words, which do not necessarily contain the same letters, we have moved beyond hard tokens to a smoother and more general sense of meaning. N-grams &amp; Skip-grams Words are read into the vector one at a time, and scanned back and forth within a certain range. Those ranges are n-grams, and an n-gram is a contiguous sequence of n items from a given linguistic sequence; it is the nth version of unigram, bigram, trigram, four-gram or five-gram. A skip-gram simply drops items from the n-gram. The skip-gram representation popularized by Mikolov and used in the DL4J implementation has proven to be more accurate than other models, such as continuous bag of words, due to the more generalizable contexts generated. This n-gram is then fed into a neural network to learn the significance of a given word vector; i.e. significance is defined as its usefulness as an indicator of certain larger meanings, or labels. Advances in NLP: ElMO, BERT and GPT-2 Word vectors form the basis of most recent advances in natural-language processing, including language models such as ElMO, ULMFit and BERT. But those language models change how they represent words; that is, that which the vectors represent changes. Word2vec is an algorithm used to produce distributed representations of words, and by that we mean word types; i.e. any given word in a vocabulary, such as get or grab or go has its own word vector, and those vectors are effectively stored in a lookup table or dictionary. Unfortunately, this approach to word representation does not addres polysemy, or the co-existence of many possible meanings for a given word or phrase. For example, go is a verb and it is also a board game; get is a verb and it is also an animalâ€™s offspring. The meaning of a given word type such as go or get varies according to its context; i.e. the words that surround it. One thing that ElMO and BERT demonstrate is that by encoding the context of a given word, by including information about preceding and succeeding words in the vector that represents a given instance of a word, we can obtain much better results in natural language processing tasks. BERT owes its performance to the attention mechanism. Tested on the SWAG benchmark, which measures commonsense reasoning, ELMo was found to produce a 5% error reduction relaitve to non-contextual word vectors, while BERT showed an additional 66% error reduction past ELMo. More recently, OpenAIâ€™s work with GPT-2 showed surprisingly good results in generating natural language in response to a prompt.","@type":"BlogPosting","url":"/2019/02/22/c9539c15df3162b923dd8967fb977bdb.html","headline":"è‡ªç„¶è¯­è¨€å¤„ç†ä¹‹â€”â€“Word2Vec","dateModified":"2019-02-22T00:00:00+08:00","datePublished":"2019-02-22T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/02/22/c9539c15df3162b923dd8967fb977bdb.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>è‡ªç„¶è¯­è¨€å¤„ç†ä¹‹-----Word2Vec</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>æŸšå­ç¤¾åŒº</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart ç®­å¤´å›¾æ ‡ å‹¿åˆ  --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <h1><a id="A_Beginners_Guide_to_Word2Vec_and_Neural_Word_Embeddings_0"></a>A Beginnerâ€™s Guide to Word2Vec and Neural Word Embeddings</h1> 
  <ul> 
   <li><strong>Introduction to Word2Vec</strong></li> 
  </ul> 
  <p>Word2vecæ˜¯ä¸€ä¸ªå¤„ç†æ–‡æœ¬çš„åŒå±‚ç¥ç»ç½‘ç»œã€‚å®ƒçš„è¾“å…¥æ˜¯ä¸€ä¸ªæ–‡æœ¬è¯­æ–™åº“ï¼Œå®ƒçš„è¾“å‡ºæ˜¯ä¸€ç»„å‘é‡ï¼šè¯¥è¯­æ–™åº“ä¸­å•è¯çš„ç‰¹å¾å‘é‡ã€‚è™½ç„¶Word2vecä¸æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œï¼Œä½†å®ƒå°†æ–‡æœ¬è½¬æ¢ä¸ºæ·±ç½‘å¯ä»¥ç†è§£çš„æ•°å­—å½¢å¼ã€‚ Deeplearning4jå®ç°äº†ä¸€ä¸ªåˆ†å¸ƒå¼çš„Word2vec for Javaå’ŒScalaï¼Œå®ƒå¯ä»¥åœ¨Sparkä¸Šè¿è¡ŒGPUã€‚</p> 
  <p>Word2vecçš„åº”ç”¨ç¨‹åºä¸ä»…ä»…æ˜¯è§£æé‡å¤–çš„å¥å­ã€‚å®ƒä¹Ÿå¯ä»¥åº”ç”¨äºåŸºå› ï¼Œä»£ç ï¼Œå–œæ¬¢ï¼Œæ’­æ”¾åˆ—è¡¨ï¼Œç¤¾äº¤åª’ä½“å›¾å’Œå…¶ä»–å¯ä»¥è¾¨åˆ«æ¨¡å¼çš„è¯­è¨€æˆ–ç¬¦å·ç³»åˆ—ã€‚</p> 
  <p>ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºå•è¯å°±åƒä¸Šé¢æåˆ°çš„å…¶ä»–æ•°æ®ä¸€æ ·åªæ˜¯ç¦»æ•£çŠ¶æ€ï¼Œæˆ‘ä»¬åªæ˜¯åœ¨å¯»æ‰¾è¿™äº›çŠ¶æ€ä¹‹é—´çš„è¿‡æ¸¡æ¦‚ç‡ï¼šå®ƒä»¬å…±åŒå‘ç”Ÿçš„å¯èƒ½æ€§ã€‚æ‰€ä»¥gene2vecï¼Œlike2vecå’Œfollower2vecéƒ½æ˜¯å¯èƒ½çš„ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œä¸‹é¢çš„æ•™ç¨‹å°†å¸®åŠ©æ‚¨äº†è§£å¦‚ä½•ä¸ºä»»ä½•ç¦»æ•£å’Œå…±ç°çŠ¶æ€ç»„åˆ›å»ºç¥ç»åµŒå…¥ã€‚</p> 
  <p>Word2Vecçš„ç›®çš„å’Œç”¨å¤„æ˜¯å°†ç›¸ä¼¼å•è¯çš„å‘é‡ç»„åˆåœ¨å‘é‡ç©ºé—´ä¸­ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¥æ•°å­¦æ–¹å¼æ£€æµ‹ç›¸ä¼¼æ€§ã€‚ Word2Vecåˆ›å»ºçš„å‘é‡æ˜¯å•è¯ç‰¹å¾çš„åˆ†å¸ƒå¼æ•°å­—è¡¨ç¤ºï¼Œè¯¸å¦‚å•ä¸ªå•è¯çš„ä¸Šä¸‹æ–‡ä¹‹ç±»çš„ç‰¹å¾ã€‚å®ƒæ²¡æœ‰äººä¸ºå¹²é¢„å°±è¿™æ ·åšäº†ã€‚</p> 
  <p>æœ‰äº†è¶³å¤Ÿçš„æ•°æ®ï¼Œç”¨æ³•å’Œä¸Šä¸‹æ–‡ï¼ŒWord2Vecå¯ä»¥æ ¹æ®è¿‡å»çš„å¤–è§‚å¯¹å•è¯çš„å«ä¹‰è¿›è¡Œé«˜åº¦å‡†ç¡®çš„çŒœæµ‹ã€‚è¿™äº›çŒœæµ‹å¯ç”¨äºå»ºç«‹å•è¯ä¸å…¶ä»–å•è¯çš„å…³è”ï¼ˆä¾‹å¦‚â€œç”·äººâ€æ˜¯â€œç”·å­©â€ï¼Œâ€œå¥³äººâ€æ˜¯â€œå¥³å­©â€ï¼‰ï¼Œæˆ–é›†ç¾¤æ–‡æ¡£å¹¶æŒ‰ä¸»é¢˜å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚è¿™äº›é›†ç¾¤å¯ä»¥æ„æˆæœç´¢ï¼Œæƒ…æ„Ÿåˆ†æå’Œç§‘å­¦ç ”ç©¶ï¼Œæ³•å¾‹å‘ç°ï¼Œç”µå­å•†åŠ¡å’Œå®¢æˆ·å…³ç³»ç®¡ç†ç­‰å¤šä¸ªé¢†åŸŸçš„å»ºè®®çš„åŸºç¡€ã€‚</p> 
  <p>Word2Vecç¥ç»ç½‘ç»œçš„è¾“å‡ºæ˜¯ä¸€ä¸ªè¯æ±‡è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªé¡¹ç›®éƒ½é™„æœ‰ä¸€ä¸ªå‘é‡ï¼Œå¯ä»¥å°†å…¶è¾“å…¥æ·±åº¦å­¦ä¹ ç½‘ç»œæˆ–ç®€å•åœ°æŸ¥è¯¢ä»¥æ£€æµ‹å•è¯ä¹‹é—´çš„å…³ç³»ã€‚</p> 
  <p>æµ‹é‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ²¡æœ‰ç›¸ä¼¼æ€§è¡¨ç¤ºä¸º90åº¦è§’ï¼Œè€Œ1çš„æ€»ç›¸ä¼¼åº¦æ˜¯0åº¦è§’ï¼Œå®Œå…¨é‡å ;ç‘å…¸ç­‰äºç‘å…¸ï¼Œè€ŒæŒªå¨ä¸ç‘å…¸çš„ä½™å¼¦è·ç¦»ä¸º0.760124ï¼Œæ˜¯å…¶ä»–ä»»ä½•å›½å®¶ä¸­æœ€é«˜çš„ã€‚</p> 
  <p>ä»¥ä¸‹æ˜¯ä½¿ç”¨Word2vecä¸â€œç‘å…¸â€ç›¸å…³è”çš„å•è¯åˆ—è¡¨ï¼ŒæŒ‰ç…§æ¥è¿‘é¡ºåºæ’åˆ—ï¼š<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222222650546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> æ–¯å ªçš„çº³ç»´äºšå›½å®¶å’Œå‡ ä¸ªå¯Œè£•ï¼ŒåŒ—æ¬§ï¼Œæ—¥è€³æ›¼å›½å®¶éƒ½ä½åˆ—å‰ä¹ã€‚</p> 
  <ul> 
   <li><strong>Neural Word Embeddings</strong><br> The vectors we use to represent words are called neural word embeddings, and representations are strange. One thing describes another, even though those two things are radically different. As Elvis Costello said: â€œWriting about music is like dancing about architecture.â€ Word2vec â€œvectorizesâ€ about words, and by doing so it makes natural language computer-readable â€“ we can start to perform powerful mathematical operations on words to detect their similarities.</li> 
  </ul> 
  <p>So a neural word embedding represents a word with numbers. Itâ€™s a simple, yet unlikely, translation.</p> 
  <p>Word2vec is similar to an autoencoder, encoding each word in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, word2vec trains words against other words that neighbor them in the input corpus.</p> 
  <p>It does so in one of two ways, either using context to predict a target word (a method known as continuous bag of words, or CBOW), or using a word to predict a target context, which is called skip-gram. We use the latter method because it produces more accurate results on large datasets.<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222222826506.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> 
  <p>When the feature vector assigned to a word cannot be used to accurately predict that wordâ€™s context, the components of the vector are adjusted. Each wordâ€™s context in the corpus is the teacher sending error signals back to adjust the feature vector. The vectors of words judged similar by their context are nudged closer together by adjusting the numbers in the vector.</p> 
  <p>Just as Van Goghâ€™s painting of sunflowers is a two-dimensional mixture of oil on canvas that represents vegetable matter in a three-dimensional space in Paris in the late 1880s, so 500 numbers arranged in a vector can represent a word or group of words.</p> 
  <p>Those numbers locate each word as a point in 500-dimensional vectorspace. Spaces of more than three dimensions are difficult to visualize. (Geoff Hinton, teaching people to imagine 13-dimensional space, suggests that students first picture 3-dimensional space and then say to themselves: â€œThirteen, thirteen, thirteen.â€ ğŸ˜ƒ</p> 
  <p>A well trained set of word vectors will place similar words close to each other in that space. The words oak, elm and birch might cluster in one corner, while war, conflict and strife huddle together in another.</p> 
  <p>Similar things and ideas are shown to be â€œcloseâ€. Their relative meanings have been translated to measurable distances. Qualities become quantities, and algorithms can do their work. But similarity is just the basis of many associations that Word2vec can learn. For example, it can gauge relations between words of one language, and map them to another.</p> 
  <p><img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190222222907809.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br> These vectors are the basis of a more comprehensive geometry of words. Not only will Rome, Paris, Berlin and Beijing cluster near each other, but they will each have similar distances in vectorspace to the countries whose capitals they are; i.e. Rome - Italy = Beijing - China. And if you only knew that Rome was the capital of Italy, and were wondering about the capital of China, then the equation Rome -Italy + China would return Beijing. No kidding.</p> 
  <p><img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/201902222229372.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p> 
  <ul> 
   <li><strong>Amusing Word2Vec Results</strong><br> Letâ€™s look at some other associations Word2vec can produce.</li> 
  </ul> 
  <p>Instead of the pluses, minus and equals signs, weâ€™ll give you the results in the notation of logical analogies, where : means â€œis toâ€ and :: means â€œasâ€; e.g. â€œRome is to Italy as Beijing is to Chinaâ€ = Rome:Italy::Beijing:China. In the last spot, rather than supplying the â€œanswerâ€, weâ€™ll give you the list of words that a Word2vec model proposes, when given the first three elements:</p> 
  <p>king:queen:ğŸ‘¨[woman, Attempted abduction, teenager, girl]<br> //Weird, but you can kind of see it</p> 
  <p>China:Taiwan::Russia:[Ukraine, Moscow, Moldova, Armenia]<br> //Two large countries and their small, estranged neighbors</p> 
  <p>house:roof::castle:[dome, bell_tower, spire, crenellations, turrets]</p> 
  <p>knee:leg::elbow:[forearm, arm, ulna_bone]</p> 
  <p>New York Times:Sulzberger::Fox:[Murdoch, Chernin, Bancroft, Ailes]<br> //The Sulzberger-Ochs family owns and runs the NYT.<br> //The Murdoch family owns News Corp., which owns Fox News.<br> //Peter Chernin was News Corp.'s COO for 13 yrs.<br> //Roger Ailes is president of Fox News.<br> //The Bancroft family sold the Wall St. Journal to News Corp.</p> 
  <p>love:indifference::fear:[apathy, callousness, timidity, helplessness, inaction]<br> //the poetry of this single array is simply amazingâ€¦</p> 
  <p>Donald Trump:Republican::Barack Obama:[Democratic, GOP, Democrats, McCain]<br> //Itâ€™s interesting to note that, just as Obama and McCain were rivals,<br> //so too, Word2vec thinks Trump has a rivalry with the idea Republican.</p> 
  <p>monkey:human::dinosaur:[fossil, fossilized, Ice_Age_mammals, fossilization]<br> //Humans are fossilized monkeys? Humans are whatâ€™s left<br> //over from monkeys? Humans are the species that beat monkeys<br> //just as Ice Age mammals beat dinosaurs? Plausible.</p> 
  <p>building:architect::software:[programmer, SecurityCenter, WinPcap]<br> This model was trained on the Google News vocab, which you can import and play with. Contemplate, for a moment, that the Word2vec algorithm has never been taught a single rule of English syntax. It knows nothing about the world, and is unassociated with any rules-based symbolic logic or knowledge graph. And yet it learns more, in a flexible and automated fashion, than most knowledge graphs will learn after many years of human labor. It comes to the Google News documents as a blank slate, and by the end of training, it can compute complex analogies that mean something to humans.</p> 
  <p>You can also query a Word2vec model for other assocations. Not everything has to be two analogies that mirror each other.</p> 
  <p>Geopolitics: Iraq - Violence = Jordan<br> Distinction: Human - Animal = Ethics<br> President - Power = Prime Minister<br> Library - Books = Hall<br> Analogy: Stock Market â‰ˆ Thermometer<br> By building a sense of one wordâ€™s proximity to other similar words, which do not necessarily contain the same letters, we have moved beyond hard tokens to a smoother and more general sense of meaning.</p> 
  <ul> 
   <li><strong>N-grams &amp; Skip-grams</strong></li> 
  </ul> 
  <p>Words are read into the vector one at a time, and scanned back and forth within a certain range. Those ranges are n-grams, and an n-gram is a contiguous sequence of n items from a given linguistic sequence; it is the nth version of unigram, bigram, trigram, four-gram or five-gram. A skip-gram simply drops items from the n-gram.</p> 
  <p>The skip-gram representation popularized by Mikolov and used in the DL4J implementation has proven to be more accurate than other models, such as continuous bag of words, due to the more generalizable contexts generated.</p> 
  <p>This n-gram is then fed into a neural network to learn the significance of a given word vector; i.e. significance is defined as its usefulness as an indicator of certain larger meanings, or labels.</p> 
  <ul> 
   <li><strong>Advances in NLP: ElMO, BERT and GPT-2</strong><br> Word vectors form the basis of most recent advances in natural-language processing, including language models such as ElMO, ULMFit and BERT. But those language models change how they represent words; that is, that which the vectors represent changes.</li> 
  </ul> 
  <p>Word2vec is an algorithm used to produce distributed representations of words, and by that we mean word types; i.e. any given word in a vocabulary, such as get or grab or go has its own word vector, and those vectors are effectively stored in a lookup table or dictionary. Unfortunately, this approach to word representation does not addres polysemy, or the co-existence of many possible meanings for a given word or phrase. For example, go is a verb and it is also a board game; get is a verb and it is also an animalâ€™s offspring. The meaning of a given word type such as go or get varies according to its context; i.e. the words that surround it.</p> 
  <p>One thing that ElMO and BERT demonstrate is that by encoding the context of a given word, by including information about preceding and succeeding words in the vector that represents a given instance of a word, we can obtain much better results in natural language processing tasks. BERT owes its performance to the attention mechanism.</p> 
  <p>Tested on the SWAG benchmark, which measures commonsense reasoning, ELMo was found to produce a 5% error reduction relaitve to non-contextual word vectors, while BERT showed an additional 66% error reduction past ELMo. More recently, OpenAIâ€™s work with GPT-2 showed surprisingly good results in generating natural language in response to a prompt.</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- è‡ªå®šä¹‰å¹¿å‘Š -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">æ›´å¤šç²¾å½©å†…å®¹</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">å›é¦–é¡µ</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
