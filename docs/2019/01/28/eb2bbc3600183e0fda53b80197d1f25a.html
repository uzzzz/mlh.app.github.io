<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>深度学习 训练网络trick——label smoothing | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="深度学习 训练网络trick——label smoothing" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="#### 1.背景介绍： 在多分类训练任务中，输入图片经过神级网络的计算，会得到当前输入图片对应于各个类别的置信度分数，这些分数会被softmax进行归一化处理，最终得到当前输入图片属于每个类别的概率。 &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/2019012823351056.png&quot;&gt; 之后在使用交叉熵函数来计算损失值： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/2019012822592855.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70&quot;&gt; 最终在训练网络时，最小化预测概率和标签真实概率的交叉熵，从而得到最优的预测概率分布。在此过程中，为了达到最好的拟合效果，最优的预测概率分布为： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128230110723.png&quot;&gt; 也就是说，网络会驱使自身往正确标签和错误标签差值大的方向学习，在训练数据不足以表征所以的样本特征的情况下，这就会导致网络过拟合。 #### 2.label smoothing原理 label smoothing的提出就是为了解决上述问题。最早是在Inception v2中被提出，是一种正则化的策略。其通过&quot;软化&quot;传统的one-hot类型标签，使得在计算损失值时能够有效抑制过拟合现象。如下图所示，label smoothing相当于减少真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。 &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128230614285.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70&quot;&gt; 1.label smoothing将真实概率分布作如下改变： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128232922339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70&quot;&gt; 其实更新后的分布就相当于往真实分布中加入了噪声，为了便于计算，该噪声服从简单的均匀分布。 2.与之对应，label smoothing将交叉熵损失函数作如下改变： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128232406554.png&quot;&gt; 3.与之对应，label smoothing将最优的预测概率分布作如下改变： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128232827323.png&quot;&gt; 阿尔法可以是任意实数，最终通过抑制正负样本输出差值，使得网络能有更好的泛化能力。 #### 3.代码实现 1.修改caffe.proto添加字段 因为所有层的参数定义都存放在caffe.proto文件中，因此修改参数后需要将新的参数添加到该文件对应的层参数中，本例中就将参数添加到LossParameter中。 ``` message LossParameter { &nbsp; optional int32 ignore_label = 1; &nbsp; enum NormalizationMode { &nbsp; FULL = 0; &nbsp; &nbsp; VALID = 1; &nbsp; &nbsp; BATCH_SIZE = 2; &nbsp; &nbsp; NONE = 3; &nbsp; } &nbsp; optional NormalizationMode normalization = 3 [default = VALID]; &nbsp; optional bool normalize = 2; &nbsp; optional float label_smooth = 4; } ``` 2.修改hpp文件 修改softmax_loss_layer.hpp，添加&nbsp; bool has_label_smooth_;&nbsp; float label_smooth_;这两个参数的定义。 ``` #ifndef CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_ #define CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_ #include &lt;vector&gt; #include &quot;caffe/blob.hpp&quot; #include &quot;caffe/layer.hpp&quot; #include &quot;caffe/proto/caffe.pb.h&quot; #include &quot;caffe/layers/loss_layer.hpp&quot; #include &quot;caffe/layers/softmax_layer.hpp&quot; namespace caffe { template &lt;typename Dtype&gt; class SoftmaxWithLossLayer : public LossLayer&lt;Dtype&gt; { public: &nbsp; explicit SoftmaxWithLossLayer(const LayerParameter&amp; param) &nbsp; &nbsp; &nbsp; : LossLayer&lt;Dtype&gt;(param) {} &nbsp; virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual inline const char* type() const { return &quot;SoftmaxWithLoss&quot;; } &nbsp; virtual inline int ExactNumTopBlobs() const { return -1; } &nbsp; virtual inline int MinTopBlobs() const { return 1; } &nbsp; virtual inline int MaxTopBlobs() const { return 2; } protected: &nbsp; virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, &nbsp; &nbsp; &nbsp; const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom); &nbsp; virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, &nbsp; &nbsp; &nbsp; const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom); &nbsp; virtual Dtype get_normalizer( &nbsp; &nbsp; &nbsp; LossParameter_NormalizationMode normalization_mode, int valid_count); &nbsp; /// The internal SoftmaxLayer used to map predictions to a distribution. &nbsp; shared_ptr&lt;Layer&lt;Dtype&gt; &gt; softmax_layer_; &nbsp; /// prob stores the output probability predictions from the SoftmaxLayer. &nbsp; Blob&lt;Dtype&gt; prob_; &nbsp; /// bottom vector holder used in call to the underlying SoftmaxLayer::Forward &nbsp; vector&lt;Blob&lt;Dtype&gt;*&gt; softmax_bottom_vec_; &nbsp; /// top vector holder used in call to the underlying SoftmaxLayer::Forward &nbsp; vector&lt;Blob&lt;Dtype&gt;*&gt; softmax_top_vec_; &nbsp; /// Whether to ignore instances with a certain label. &nbsp; bool has_ignore_label_; &nbsp; /// The label indicating that an instance should be ignored. &nbsp; int ignore_label_; &nbsp; /// How to normalize the output loss. &nbsp; LossParameter_NormalizationMode normalization_; &nbsp; int softmax_axis_, outer_num_, inner_num_; &nbsp; bool has_label_smooth_; &nbsp; float label_smooth_; }; }&nbsp; // namespace caffe #endif&nbsp; // CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_ ``` 3.修改cpp，cu文件 文件的主要修改点如下，完整代码：[GitHub传送门](https://github.com/qiu931110/Caffe_label_smooth) cpp文件： ``` has_label_smooth_ = this-&gt;layer_param_.loss_param().has_label_smooth(); if (has_label_smooth_ ){ &nbsp; label_smooth_ = this-&gt;layer_param_.loss_param().label_smooth(); } ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; loss -= coeff * log(std::max(prob_data[i*dim + c*inner_num_ + j],Dtype(FLT_MIN)));; &nbsp; } } ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; bottom_diff[i*dim + c*inner_num_ + j] -= coeff; &nbsp; } } ``` cu文件： ``` __global__ void SoftmaxLossForwardGPU() ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; SoftmaxLossForwardGPU&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(nthreads), &nbsp; &nbsp; &nbsp; CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(nthreads, prob_data, label, loss_data, &nbsp; &nbsp; &nbsp; outer_num_, dim, inner_num_, has_ignore_label_, ignore_label_, counts, has_label_smooth_,label_smooth_,num_class); if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; loss[index] -= coeff * log(max(prob_data[n*dim + c*spatial_dim + s],Dtype(FLT_MIN)));; &nbsp; } } ``` ``` __global__ void SoftmaxLossBackwardGPU() ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; SoftmaxLossBackwardGPU&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(nthreads), &nbsp; &nbsp; &nbsp; &nbsp; CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(nthreads, top_data, label, bottom_diff, &nbsp; &nbsp; &nbsp; &nbsp; outer_num_, dim, inner_num_, has_ignore_label_, ignore_label_, counts, has_label_smooth_,label_smooth_,num_class); if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; bottom_diff[n*dim + c*spatial_dim + s] -= coeff; &nbsp; } } ``` 4.编译 返回到caffe的根目录，使用make指令(下面几个都可以，任选一个)，即可。 ``` make make -j make -j16 make -j32&nbsp; &nbsp; // 这里j后面的数字与电脑配置有关系，可以加速编译 ``` 5.使用 ``` layer{ name:&quot;loss&quot; type:&quot;SoftmaxwithLoss&quot; bottom:&quot;fc&quot; bottom:&quot;label&quot; top:&quot;loss&quot; loss_param{ label_smooth:0.1 } } ```" />
<meta property="og:description" content="#### 1.背景介绍： 在多分类训练任务中，输入图片经过神级网络的计算，会得到当前输入图片对应于各个类别的置信度分数，这些分数会被softmax进行归一化处理，最终得到当前输入图片属于每个类别的概率。 &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/2019012823351056.png&quot;&gt; 之后在使用交叉熵函数来计算损失值： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/2019012822592855.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70&quot;&gt; 最终在训练网络时，最小化预测概率和标签真实概率的交叉熵，从而得到最优的预测概率分布。在此过程中，为了达到最好的拟合效果，最优的预测概率分布为： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128230110723.png&quot;&gt; 也就是说，网络会驱使自身往正确标签和错误标签差值大的方向学习，在训练数据不足以表征所以的样本特征的情况下，这就会导致网络过拟合。 #### 2.label smoothing原理 label smoothing的提出就是为了解决上述问题。最早是在Inception v2中被提出，是一种正则化的策略。其通过&quot;软化&quot;传统的one-hot类型标签，使得在计算损失值时能够有效抑制过拟合现象。如下图所示，label smoothing相当于减少真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。 &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128230614285.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70&quot;&gt; 1.label smoothing将真实概率分布作如下改变： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128232922339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70&quot;&gt; 其实更新后的分布就相当于往真实分布中加入了噪声，为了便于计算，该噪声服从简单的均匀分布。 2.与之对应，label smoothing将交叉熵损失函数作如下改变： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128232406554.png&quot;&gt; 3.与之对应，label smoothing将最优的预测概率分布作如下改变： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128232827323.png&quot;&gt; 阿尔法可以是任意实数，最终通过抑制正负样本输出差值，使得网络能有更好的泛化能力。 #### 3.代码实现 1.修改caffe.proto添加字段 因为所有层的参数定义都存放在caffe.proto文件中，因此修改参数后需要将新的参数添加到该文件对应的层参数中，本例中就将参数添加到LossParameter中。 ``` message LossParameter { &nbsp; optional int32 ignore_label = 1; &nbsp; enum NormalizationMode { &nbsp; FULL = 0; &nbsp; &nbsp; VALID = 1; &nbsp; &nbsp; BATCH_SIZE = 2; &nbsp; &nbsp; NONE = 3; &nbsp; } &nbsp; optional NormalizationMode normalization = 3 [default = VALID]; &nbsp; optional bool normalize = 2; &nbsp; optional float label_smooth = 4; } ``` 2.修改hpp文件 修改softmax_loss_layer.hpp，添加&nbsp; bool has_label_smooth_;&nbsp; float label_smooth_;这两个参数的定义。 ``` #ifndef CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_ #define CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_ #include &lt;vector&gt; #include &quot;caffe/blob.hpp&quot; #include &quot;caffe/layer.hpp&quot; #include &quot;caffe/proto/caffe.pb.h&quot; #include &quot;caffe/layers/loss_layer.hpp&quot; #include &quot;caffe/layers/softmax_layer.hpp&quot; namespace caffe { template &lt;typename Dtype&gt; class SoftmaxWithLossLayer : public LossLayer&lt;Dtype&gt; { public: &nbsp; explicit SoftmaxWithLossLayer(const LayerParameter&amp; param) &nbsp; &nbsp; &nbsp; : LossLayer&lt;Dtype&gt;(param) {} &nbsp; virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual inline const char* type() const { return &quot;SoftmaxWithLoss&quot;; } &nbsp; virtual inline int ExactNumTopBlobs() const { return -1; } &nbsp; virtual inline int MinTopBlobs() const { return 1; } &nbsp; virtual inline int MaxTopBlobs() const { return 2; } protected: &nbsp; virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, &nbsp; &nbsp; &nbsp; const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom); &nbsp; virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, &nbsp; &nbsp; &nbsp; const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom); &nbsp; virtual Dtype get_normalizer( &nbsp; &nbsp; &nbsp; LossParameter_NormalizationMode normalization_mode, int valid_count); &nbsp; /// The internal SoftmaxLayer used to map predictions to a distribution. &nbsp; shared_ptr&lt;Layer&lt;Dtype&gt; &gt; softmax_layer_; &nbsp; /// prob stores the output probability predictions from the SoftmaxLayer. &nbsp; Blob&lt;Dtype&gt; prob_; &nbsp; /// bottom vector holder used in call to the underlying SoftmaxLayer::Forward &nbsp; vector&lt;Blob&lt;Dtype&gt;*&gt; softmax_bottom_vec_; &nbsp; /// top vector holder used in call to the underlying SoftmaxLayer::Forward &nbsp; vector&lt;Blob&lt;Dtype&gt;*&gt; softmax_top_vec_; &nbsp; /// Whether to ignore instances with a certain label. &nbsp; bool has_ignore_label_; &nbsp; /// The label indicating that an instance should be ignored. &nbsp; int ignore_label_; &nbsp; /// How to normalize the output loss. &nbsp; LossParameter_NormalizationMode normalization_; &nbsp; int softmax_axis_, outer_num_, inner_num_; &nbsp; bool has_label_smooth_; &nbsp; float label_smooth_; }; }&nbsp; // namespace caffe #endif&nbsp; // CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_ ``` 3.修改cpp，cu文件 文件的主要修改点如下，完整代码：[GitHub传送门](https://github.com/qiu931110/Caffe_label_smooth) cpp文件： ``` has_label_smooth_ = this-&gt;layer_param_.loss_param().has_label_smooth(); if (has_label_smooth_ ){ &nbsp; label_smooth_ = this-&gt;layer_param_.loss_param().label_smooth(); } ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; loss -= coeff * log(std::max(prob_data[i*dim + c*inner_num_ + j],Dtype(FLT_MIN)));; &nbsp; } } ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; bottom_diff[i*dim + c*inner_num_ + j] -= coeff; &nbsp; } } ``` cu文件： ``` __global__ void SoftmaxLossForwardGPU() ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; SoftmaxLossForwardGPU&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(nthreads), &nbsp; &nbsp; &nbsp; CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(nthreads, prob_data, label, loss_data, &nbsp; &nbsp; &nbsp; outer_num_, dim, inner_num_, has_ignore_label_, ignore_label_, counts, has_label_smooth_,label_smooth_,num_class); if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; loss[index] -= coeff * log(max(prob_data[n*dim + c*spatial_dim + s],Dtype(FLT_MIN)));; &nbsp; } } ``` ``` __global__ void SoftmaxLossBackwardGPU() ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; SoftmaxLossBackwardGPU&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(nthreads), &nbsp; &nbsp; &nbsp; &nbsp; CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(nthreads, top_data, label, bottom_diff, &nbsp; &nbsp; &nbsp; &nbsp; outer_num_, dim, inner_num_, has_ignore_label_, ignore_label_, counts, has_label_smooth_,label_smooth_,num_class); if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; bottom_diff[n*dim + c*spatial_dim + s] -= coeff; &nbsp; } } ``` 4.编译 返回到caffe的根目录，使用make指令(下面几个都可以，任选一个)，即可。 ``` make make -j make -j16 make -j32&nbsp; &nbsp; // 这里j后面的数字与电脑配置有关系，可以加速编译 ``` 5.使用 ``` layer{ name:&quot;loss&quot; type:&quot;SoftmaxwithLoss&quot; bottom:&quot;fc&quot; bottom:&quot;label&quot; top:&quot;loss&quot; loss_param{ label_smooth:0.1 } } ```" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-28T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"#### 1.背景介绍： 在多分类训练任务中，输入图片经过神级网络的计算，会得到当前输入图片对应于各个类别的置信度分数，这些分数会被softmax进行归一化处理，最终得到当前输入图片属于每个类别的概率。 &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/2019012823351056.png&quot;&gt; 之后在使用交叉熵函数来计算损失值： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/2019012822592855.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70&quot;&gt; 最终在训练网络时，最小化预测概率和标签真实概率的交叉熵，从而得到最优的预测概率分布。在此过程中，为了达到最好的拟合效果，最优的预测概率分布为： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128230110723.png&quot;&gt; 也就是说，网络会驱使自身往正确标签和错误标签差值大的方向学习，在训练数据不足以表征所以的样本特征的情况下，这就会导致网络过拟合。 #### 2.label smoothing原理 label smoothing的提出就是为了解决上述问题。最早是在Inception v2中被提出，是一种正则化的策略。其通过&quot;软化&quot;传统的one-hot类型标签，使得在计算损失值时能够有效抑制过拟合现象。如下图所示，label smoothing相当于减少真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。 &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128230614285.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70&quot;&gt; 1.label smoothing将真实概率分布作如下改变： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128232922339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70&quot;&gt; 其实更新后的分布就相当于往真实分布中加入了噪声，为了便于计算，该噪声服从简单的均匀分布。 2.与之对应，label smoothing将交叉熵损失函数作如下改变： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128232406554.png&quot;&gt; 3.与之对应，label smoothing将最优的预测概率分布作如下改变： &lt;center&gt; &lt;img src=&quot;https://img-blog.csdnimg.cn/20190128232827323.png&quot;&gt; 阿尔法可以是任意实数，最终通过抑制正负样本输出差值，使得网络能有更好的泛化能力。 #### 3.代码实现 1.修改caffe.proto添加字段 因为所有层的参数定义都存放在caffe.proto文件中，因此修改参数后需要将新的参数添加到该文件对应的层参数中，本例中就将参数添加到LossParameter中。 ``` message LossParameter { &nbsp; optional int32 ignore_label = 1; &nbsp; enum NormalizationMode { &nbsp; FULL = 0; &nbsp; &nbsp; VALID = 1; &nbsp; &nbsp; BATCH_SIZE = 2; &nbsp; &nbsp; NONE = 3; &nbsp; } &nbsp; optional NormalizationMode normalization = 3 [default = VALID]; &nbsp; optional bool normalize = 2; &nbsp; optional float label_smooth = 4; } ``` 2.修改hpp文件 修改softmax_loss_layer.hpp，添加&nbsp; bool has_label_smooth_;&nbsp; float label_smooth_;这两个参数的定义。 ``` #ifndef CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_ #define CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_ #include &lt;vector&gt; #include &quot;caffe/blob.hpp&quot; #include &quot;caffe/layer.hpp&quot; #include &quot;caffe/proto/caffe.pb.h&quot; #include &quot;caffe/layers/loss_layer.hpp&quot; #include &quot;caffe/layers/softmax_layer.hpp&quot; namespace caffe { template &lt;typename Dtype&gt; class SoftmaxWithLossLayer : public LossLayer&lt;Dtype&gt; { public: &nbsp; explicit SoftmaxWithLossLayer(const LayerParameter&amp; param) &nbsp; &nbsp; &nbsp; : LossLayer&lt;Dtype&gt;(param) {} &nbsp; virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual inline const char* type() const { return &quot;SoftmaxWithLoss&quot;; } &nbsp; virtual inline int ExactNumTopBlobs() const { return -1; } &nbsp; virtual inline int MinTopBlobs() const { return 1; } &nbsp; virtual inline int MaxTopBlobs() const { return 2; } protected: &nbsp; virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, &nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top); &nbsp; virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, &nbsp; &nbsp; &nbsp; const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom); &nbsp; virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, &nbsp; &nbsp; &nbsp; const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom); &nbsp; virtual Dtype get_normalizer( &nbsp; &nbsp; &nbsp; LossParameter_NormalizationMode normalization_mode, int valid_count); &nbsp; /// The internal SoftmaxLayer used to map predictions to a distribution. &nbsp; shared_ptr&lt;Layer&lt;Dtype&gt; &gt; softmax_layer_; &nbsp; /// prob stores the output probability predictions from the SoftmaxLayer. &nbsp; Blob&lt;Dtype&gt; prob_; &nbsp; /// bottom vector holder used in call to the underlying SoftmaxLayer::Forward &nbsp; vector&lt;Blob&lt;Dtype&gt;*&gt; softmax_bottom_vec_; &nbsp; /// top vector holder used in call to the underlying SoftmaxLayer::Forward &nbsp; vector&lt;Blob&lt;Dtype&gt;*&gt; softmax_top_vec_; &nbsp; /// Whether to ignore instances with a certain label. &nbsp; bool has_ignore_label_; &nbsp; /// The label indicating that an instance should be ignored. &nbsp; int ignore_label_; &nbsp; /// How to normalize the output loss. &nbsp; LossParameter_NormalizationMode normalization_; &nbsp; int softmax_axis_, outer_num_, inner_num_; &nbsp; bool has_label_smooth_; &nbsp; float label_smooth_; }; }&nbsp; // namespace caffe #endif&nbsp; // CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_ ``` 3.修改cpp，cu文件 文件的主要修改点如下，完整代码：[GitHub传送门](https://github.com/qiu931110/Caffe_label_smooth) cpp文件： ``` has_label_smooth_ = this-&gt;layer_param_.loss_param().has_label_smooth(); if (has_label_smooth_ ){ &nbsp; label_smooth_ = this-&gt;layer_param_.loss_param().label_smooth(); } ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; loss -= coeff * log(std::max(prob_data[i*dim + c*inner_num_ + j],Dtype(FLT_MIN)));; &nbsp; } } ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; bottom_diff[i*dim + c*inner_num_ + j] -= coeff; &nbsp; } } ``` cu文件： ``` __global__ void SoftmaxLossForwardGPU() ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; SoftmaxLossForwardGPU&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(nthreads), &nbsp; &nbsp; &nbsp; CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(nthreads, prob_data, label, loss_data, &nbsp; &nbsp; &nbsp; outer_num_, dim, inner_num_, has_ignore_label_, ignore_label_, counts, has_label_smooth_,label_smooth_,num_class); if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; loss[index] -= coeff * log(max(prob_data[n*dim + c*spatial_dim + s],Dtype(FLT_MIN)));; &nbsp; } } ``` ``` __global__ void SoftmaxLossBackwardGPU() ``` ``` int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1; SoftmaxLossBackwardGPU&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(nthreads), &nbsp; &nbsp; &nbsp; &nbsp; CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(nthreads, top_data, label, bottom_diff, &nbsp; &nbsp; &nbsp; &nbsp; outer_num_, dim, inner_num_, has_ignore_label_, ignore_label_, counts, has_label_smooth_,label_smooth_,num_class); if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){ &nbsp; for (int c=0;c&lt;=num_class;c++){ &nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class)); &nbsp; &nbsp; bottom_diff[n*dim + c*spatial_dim + s] -= coeff; &nbsp; } } ``` 4.编译 返回到caffe的根目录，使用make指令(下面几个都可以，任选一个)，即可。 ``` make make -j make -j16 make -j32&nbsp; &nbsp; // 这里j后面的数字与电脑配置有关系，可以加速编译 ``` 5.使用 ``` layer{ name:&quot;loss&quot; type:&quot;SoftmaxwithLoss&quot; bottom:&quot;fc&quot; bottom:&quot;label&quot; top:&quot;loss&quot; loss_param{ label_smooth:0.1 } } ```","@type":"BlogPosting","url":"/2019/01/28/eb2bbc3600183e0fda53b80197d1f25a.html","headline":"深度学习 训练网络trick——label smoothing","dateModified":"2019-01-28T00:00:00+08:00","datePublished":"2019-01-28T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/01/28/eb2bbc3600183e0fda53b80197d1f25a.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>深度学习 | 训练网络trick——label smoothing</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="show-content-free"> 
   <p>#### 1.背景介绍：</p>
   <p>在多分类训练任务中，输入图片经过神级网络的计算，会得到当前输入图片对应于各个类别的置信度分数，这些分数会被softmax进行归一化处理，最终得到当前输入图片属于每个类别的概率。</p>
   <p>&lt;center&gt;</p>
   <p>&lt;img src="https://img-blog.csdnimg.cn/2019012823351056.png"&gt;</p>
   <p>之后在使用交叉熵函数来计算损失值：</p>
   <p>&lt;center&gt;</p>
   <p>&lt;img src="https://img-blog.csdnimg.cn/2019012822592855.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70"&gt;</p>
   <p>最终在训练网络时，最小化预测概率和标签真实概率的交叉熵，从而得到最优的预测概率分布。在此过程中，为了达到最好的拟合效果，最优的预测概率分布为：</p>
   <p>&lt;center&gt;</p>
   <p>&lt;img src="https://img-blog.csdnimg.cn/20190128230110723.png"&gt;</p>
   <p>也就是说，网络会驱使自身往正确标签和错误标签差值大的方向学习，在训练数据不足以表征所以的样本特征的情况下，这就会导致网络过拟合。</p>
   <p>#### 2.label smoothing原理</p>
   <p>label smoothing的提出就是为了解决上述问题。最早是在Inception v2中被提出，是一种正则化的策略。其通过"软化"传统的one-hot类型标签，使得在计算损失值时能够有效抑制过拟合现象。如下图所示，label smoothing相当于减少真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。</p>
   <p>&lt;center&gt;</p>
   <p>&lt;img src="https://img-blog.csdnimg.cn/20190128230614285.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70"&gt;</p>
   <p>1.label smoothing将真实概率分布作如下改变：</p>
   <p>&lt;center&gt;</p>
   <p>&lt;img src="https://img-blog.csdnimg.cn/20190128232922339.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpdTkzMTExMA==,size_16,color_FFFFFF,t_70"&gt;</p>
   <p>其实更新后的分布就相当于往真实分布中加入了噪声，为了便于计算，该噪声服从简单的均匀分布。</p>
   <p>2.与之对应，label smoothing将交叉熵损失函数作如下改变：</p>
   <p>&lt;center&gt;</p>
   <p>&lt;img src="https://img-blog.csdnimg.cn/20190128232406554.png"&gt;</p>
   <p>3.与之对应，label smoothing将最优的预测概率分布作如下改变：</p>
   <p>&lt;center&gt;</p>
   <p>&lt;img src="https://img-blog.csdnimg.cn/20190128232827323.png"&gt;</p>
   <p>阿尔法可以是任意实数，最终通过抑制正负样本输出差值，使得网络能有更好的泛化能力。</p>
   <p>#### 3.代码实现</p>
   <p>1.修改caffe.proto添加字段</p>
   <p>因为所有层的参数定义都存放在caffe.proto文件中，因此修改参数后需要将新的参数添加到该文件对应的层参数中，本例中就将参数添加到LossParameter中。</p>
   <p>```</p>
   <p>message LossParameter {</p>
   <p>&nbsp; optional int32 ignore_label = 1;</p>
   <p>&nbsp; enum NormalizationMode {</p>
   <p>&nbsp; FULL = 0;</p>
   <p>&nbsp; &nbsp; VALID = 1;</p>
   <p>&nbsp; &nbsp; BATCH_SIZE = 2;</p>
   <p>&nbsp; &nbsp; NONE = 3;</p>
   <p>&nbsp; }</p>
   <p>&nbsp; optional NormalizationMode normalization = 3 [default = VALID];</p>
   <p>&nbsp; optional bool normalize = 2;</p>
   <p>&nbsp; optional float label_smooth = 4;</p>
   <p>}</p>
   <p>```</p>
   <p>2.修改hpp文件</p>
   <p>修改softmax_loss_layer.hpp，添加&nbsp; bool has_label_smooth_;&nbsp; float label_smooth_;这两个参数的定义。</p>
   <p>```</p>
   <p>#ifndef CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_</p>
   <p>#define CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_</p>
   <p>#include &lt;vector&gt;</p>
   <p>#include "caffe/blob.hpp"</p>
   <p>#include "caffe/layer.hpp"</p>
   <p>#include "caffe/proto/caffe.pb.h"</p>
   <p>#include "caffe/layers/loss_layer.hpp"</p>
   <p>#include "caffe/layers/softmax_layer.hpp"</p>
   <p>namespace caffe {</p>
   <p>template &lt;typename Dtype&gt;</p>
   <p>class SoftmaxWithLossLayer : public LossLayer&lt;Dtype&gt; {</p>
   <p> public:</p>
   <p>&nbsp; explicit SoftmaxWithLossLayer(const LayerParameter&amp; param)</p>
   <p>&nbsp; &nbsp; &nbsp; : LossLayer&lt;Dtype&gt;(param) {}</p>
   <p>&nbsp; virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</p>
   <p>&nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);</p>
   <p>&nbsp; virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</p>
   <p>&nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);</p>
   <p>&nbsp; virtual inline const char* type() const { return "SoftmaxWithLoss"; }</p>
   <p>&nbsp; virtual inline int ExactNumTopBlobs() const { return -1; }</p>
   <p>&nbsp; virtual inline int MinTopBlobs() const { return 1; }</p>
   <p>&nbsp; virtual inline int MaxTopBlobs() const { return 2; }</p>
   <p> protected:</p>
   <p>&nbsp; virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</p>
   <p>&nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);</p>
   <p>&nbsp; virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</p>
   <p>&nbsp; &nbsp; &nbsp; const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top);</p>
   <p>&nbsp; virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</p>
   <p>&nbsp; &nbsp; &nbsp; const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);</p>
   <p>&nbsp; virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</p>
   <p>&nbsp; &nbsp; &nbsp; const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom);</p>
   <p>&nbsp; virtual Dtype get_normalizer(</p>
   <p>&nbsp; &nbsp; &nbsp; LossParameter_NormalizationMode normalization_mode, int valid_count);</p>
   <p>&nbsp; /// The internal SoftmaxLayer used to map predictions to a distribution.</p>
   <p>&nbsp; shared_ptr&lt;Layer&lt;Dtype&gt; &gt; softmax_layer_;</p>
   <p>&nbsp; /// prob stores the output probability predictions from the SoftmaxLayer.</p>
   <p>&nbsp; Blob&lt;Dtype&gt; prob_;</p>
   <p>&nbsp; /// bottom vector holder used in call to the underlying SoftmaxLayer::Forward</p>
   <p>&nbsp; vector&lt;Blob&lt;Dtype&gt;*&gt; softmax_bottom_vec_;</p>
   <p>&nbsp; /// top vector holder used in call to the underlying SoftmaxLayer::Forward</p>
   <p>&nbsp; vector&lt;Blob&lt;Dtype&gt;*&gt; softmax_top_vec_;</p>
   <p>&nbsp; /// Whether to ignore instances with a certain label.</p>
   <p>&nbsp; bool has_ignore_label_;</p>
   <p>&nbsp; /// The label indicating that an instance should be ignored.</p>
   <p>&nbsp; int ignore_label_;</p>
   <p>&nbsp; /// How to normalize the output loss.</p>
   <p>&nbsp; LossParameter_NormalizationMode normalization_;</p>
   <p>&nbsp; int softmax_axis_, outer_num_, inner_num_;</p>
   <p>&nbsp; bool has_label_smooth_;</p>
   <p>&nbsp; float label_smooth_;</p>
   <p>};</p>
   <p>}&nbsp; // namespace caffe</p>
   <p>#endif&nbsp; // CAFFE_SOFTMAX_WITH_LOSS_LAYER_HPP_</p>
   <p>```</p>
   <p>3.修改cpp，cu文件</p>
   <p>文件的主要修改点如下，完整代码：[GitHub传送门](https://github.com/qiu931110/Caffe_label_smooth)</p>
   <p>cpp文件：</p>
   <p>```</p>
   <p>has_label_smooth_ = this-&gt;layer_param_.loss_param().has_label_smooth();</p>
   <p>if (has_label_smooth_ ){</p>
   <p>&nbsp; label_smooth_ = this-&gt;layer_param_.loss_param().label_smooth();</p>
   <p>}</p>
   <p>```</p>
   <p>```</p>
   <p>int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1;</p>
   <p>if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){</p>
   <p>&nbsp; for (int c=0;c&lt;=num_class;c++){</p>
   <p>&nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class));</p>
   <p>&nbsp; &nbsp; loss -= coeff * log(std::max(prob_data[i*dim + c*inner_num_ + j],Dtype(FLT_MIN)));;</p>
   <p>&nbsp; }</p>
   <p>}</p>
   <p>```</p>
   <p>```</p>
   <p>int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1;</p>
   <p>if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){</p>
   <p>&nbsp; for (int c=0;c&lt;=num_class;c++){</p>
   <p>&nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class));</p>
   <p>&nbsp; &nbsp; bottom_diff[i*dim + c*inner_num_ + j] -= coeff;</p>
   <p>&nbsp; }</p>
   <p>}</p>
   <p>```</p>
   <p>cu文件：</p>
   <p>```</p>
   <p>__global__ void SoftmaxLossForwardGPU()</p>
   <p>```</p>
   <p>```</p>
   <p>int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1;</p>
   <p>SoftmaxLossForwardGPU&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(nthreads),</p>
   <p>&nbsp; &nbsp; &nbsp; CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(nthreads, prob_data, label, loss_data,</p>
   <p>&nbsp; &nbsp; &nbsp; outer_num_, dim, inner_num_, has_ignore_label_, ignore_label_, counts, has_label_smooth_,label_smooth_,num_class);</p>
   <p><br></p>
   <p>if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){</p>
   <p>&nbsp; for (int c=0;c&lt;=num_class;c++){</p>
   <p>&nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class));</p>
   <p>&nbsp; &nbsp; loss[index] -= coeff * log(max(prob_data[n*dim + c*spatial_dim + s],Dtype(FLT_MIN)));;</p>
   <p>&nbsp; }</p>
   <p>}</p>
   <p>```</p>
   <p>```</p>
   <p>__global__ void SoftmaxLossBackwardGPU()</p>
   <p>```</p>
   <p>```</p>
   <p>int num_class = bottom[0]-&gt;shape(softmax_axis_) - 1;</p>
   <p>SoftmaxLossBackwardGPU&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(nthreads),</p>
   <p>&nbsp; &nbsp; &nbsp; &nbsp; CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;(nthreads, top_data, label, bottom_diff,</p>
   <p>&nbsp; &nbsp; &nbsp; &nbsp; outer_num_, dim, inner_num_, has_ignore_label_, ignore_label_, counts, has_label_smooth_,label_smooth_,num_class);</p>
   <p><br></p>
   <p>if (has_label_smooth_ &amp;&amp; (label_smooth_&gt;0.F)){</p>
   <p>&nbsp; for (int c=0;c&lt;=num_class;c++){</p>
   <p>&nbsp; &nbsp; float coeff = (c == label_value)?(1.F - label_smooth_):(label_smooth_ / float(num_class));</p>
   <p>&nbsp; &nbsp; bottom_diff[n*dim + c*spatial_dim + s] -= coeff;</p>
   <p>&nbsp; }</p>
   <p>}</p>
   <p>```</p>
   <p>4.编译</p>
   <p>返回到caffe的根目录，使用make指令(下面几个都可以，任选一个)，即可。</p>
   <p>```</p>
   <p> make</p>
   <p> make -j</p>
   <p> make -j16</p>
   <p> make -j32&nbsp; &nbsp; // 这里j后面的数字与电脑配置有关系，可以加速编译</p>
   <p> ```</p>
   <p>5.使用</p>
   <p>```</p>
   <p>layer{</p>
   <p> name:"loss"</p>
   <p> type:"SoftmaxwithLoss"</p>
   <p> bottom:"fc"</p>
   <p> bottom:"label"</p>
   <p> top:"loss"</p>
   <p> loss_param{</p>
   <p> label_smooth:0.1</p>
   <p> }</p>
   <p>}</p>
   <p>```</p> 
  </div> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
