<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>【IOS】基于VGG16模型的机器学习CoreML demo体验 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="【IOS】基于VGG16模型的机器学习CoreML demo体验" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="IOS下的机器学习 人工智能愈发火热，可以看到各大公司给AI工程师的薪资也是越涨越烈，而机器学习是人工智能的核心，也是实现人工智能的根本途径。机器学习专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。 而苹果在 iOS 5 里引入了 NSLinguisticTagger 来分析自然语言。iOS 8 出了 Metal，提供了对设备 GPU 的底层访问。 去年，苹果在 Accelerate 框架添加了 Basic Neural Network Subroutines (BNNS)，使开发者可以构建用于推理（不是训练）的神经网络。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 在2017年，苹果带来了&nbsp;Core ML 和 Vision。 Core ML 让我们更容易在 App 中使用训练过的模型。 Vision 让我们轻松访问苹果的模型，用于面部检测、面部特征点、文字、矩形、条形码和物体。 当然，模型的训练过程是根本不可能在单机上完成的，这需要通过特定算法，使用大量数据来进行模型生成的这个过程，是非常地消耗计算和存储资源的，特别一些大型的神经网络，普通的IOS设备或许根本跑都跑不起来，更多的情况我们都是放在后端的HBase、Cassandra这种数据库中，然后再通过诸如Caffe, Scikit-Learn, Tensorflow这些来进行相应的训练，最后生成我们想要的模型。拿到模型后，对于数据的判断，移动端的设备似乎就能够做一些力所能及之事了。 Core ML 便是这样的存在，开发者不需要理会模型的生成过程，而只需要关注输入和输出的参数，便能够轻松的将机器学习集合到自己的应用中并高效运行起来， 我们分几个步骤来完成这个demo 完成壳app，能够照相或者从相册选取照片 下载机器学习模型 将Core ML模型整合到项目中 根据不同的卷积神经网络模型，苹果提供了几个已经训练好的CoreML模型我们可以直接使用。 Places205-GoogleNet ResNet50 Inception v3 VGG16 关于不同模型的详细内容可以阅读苹果官网的文档。 在这里我选择了VGG16，其简洁性和实用性是目前较流行的卷积神经网络模型。 完成demo 壳APP我们要完成照相或者从相册选取照片的功能，肯定想到使用UIImagePickerController，这个很简单。不过要注意的是要在info.plist中添加Privacy - Photo Library Usage Description 、Privacy - Photo Library Additions Usage Description和 Privacy - Camera Usage Description。否则程序会崩溃。 UIBarButtonItem *photoBtniItem = [[UIBarButtonItem alloc]initWithBarButtonSystemItem:UIBarButtonSystemItemCamera target:self action:@selector(alertcController)]; self.navigationItem.leftBarButtonItem = photoBtniItem; self.imagePickController = [[UIImagePickerController alloc]init]; self.imagePickController.delegate = (id)self; self.imagePickController.modalTransitionStyle = UIModalTransitionStyleFlipHorizontal; self.imagePickController.allowsEditing = YES; 通过AlertController来让用户选择照相还是从相册选择照片。 -(void)alertcController{ UIAlertController *alert = [UIAlertController alertControllerWithTitle:@&quot;提示&quot; message:nil preferredStyle:UIAlertControllerStyleActionSheet]; UIAlertAction *cancel = [UIAlertAction actionWithTitle:@&quot;取消&quot; style:UIAlertActionStyleCancel handler:^(UIAlertAction * _Nonnull action) { }]; UIAlertAction *takePhoto = [UIAlertAction actionWithTitle:@&quot;打开相机&quot; style:UIAlertActionStyleDefault handler:^(UIAlertAction * _Nonnull action) { self.imagePickController.sourceType = UIImagePickerControllerSourceTypeCamera; self.imagePickController.mediaTypes = @[(NSString *)kUTTypeImage]; self.imagePickController.cameraCaptureMode = UIImagePickerControllerCameraCaptureModePhoto; [self.navigationController presentViewController:self.imagePickController animated:YES completion:nil]; }]; UIAlertAction *cameraLibrary = [UIAlertAction actionWithTitle:@&quot;打开相册&quot; style:UIAlertActionStyleDefault handler:^(UIAlertAction * _Nonnull action) { self.imagePickController.sourceType = UIImagePickerControllerSourceTypePhotoLibrary; self.imagePickController.mediaTypes = @[(NSString *)kUTTypeImage]; [self.navigationController presentViewController:self.imagePickController animated:YES completion:nil]; }]; [alert addAction:cancel]; [alert addAction:takePhoto]; [alert addAction:cameraLibrary]; [self presentViewController:alert animated:YES completion:nil]; } 通过MainStroyborard给界面添加上一个ImageView和一个label,ImageView展示输入图片，label展示模型识别后的输出结果。 接下来，我们只需要在苹果官网下载好VGG16模型，拖进项目文件后便可以直接使用。 VGG16模型的详细介绍，要注意的是，VGG16模型要求输入参数是224*224大小的图片，因此我们必须对选择好的照片进行一定的修改。CVPixelBufferRef这种图像格式的处理与UIImage, CGImageRef的处理需小心，容易造成内存泄漏。 - (CVPixelBufferRef) pixelBufferFromCGImage: (CGImageRef) image { NSDictionary *options = @{ (NSString*)kCVPixelBufferCGImageCompatibilityKey : @YES, (NSString*)kCVPixelBufferCGBitmapContextCompatibilityKey : @YES, }; CVPixelBufferRef pxbuffer = NULL; CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, CGImageGetWidth(image), CGImageGetHeight(image), kCVPixelFormatType_32ARGB, (__bridge CFDictionaryRef) options, &amp;pxbuffer); if (status!=kCVReturnSuccess) { NSLog(@&quot;Operation failed&quot;); } NSParameterAssert(status == kCVReturnSuccess &amp;&amp; pxbuffer != NULL); CVPixelBufferLockBaseAddress(pxbuffer, 0); void *pxdata = CVPixelBufferGetBaseAddress(pxbuffer); CGColorSpaceRef rgbColorSpace = CGColorSpaceCreateDeviceRGB(); CGContextRef context = CGBitmapContextCreate(pxdata, CGImageGetWidth(image), CGImageGetHeight(image), 8, 4*CGImageGetWidth(image), rgbColorSpace, kCGImageAlphaNoneSkipFirst); NSParameterAssert(context); CGContextConcatCTM(context, CGAffineTransformMakeRotation(0)); CGAffineTransform flipVertical = CGAffineTransformMake( 1, 0, 0, -1, 0, CGImageGetHeight(image) ); CGContextConcatCTM(context, flipVertical); CGAffineTransform flipHorizontal = CGAffineTransformMake( -1.0, 0.0, 0.0, 1.0, CGImageGetWidth(image), 0.0 ); CGContextConcatCTM(context, flipHorizontal); CGContextDrawImage(context, CGRectMake(0, 0, CGImageGetWidth(image), CGImageGetHeight(image)), image); CGColorSpaceRelease(rgbColorSpace); CGContextRelease(context); CVPixelBufferUnlockBaseAddress(pxbuffer, 0); return pxbuffer; } - (UIImage*)image:(UIImage *)image scaleToSize:(CGSize)size{ UIGraphicsBeginImageContext(size); [image drawInRect:CGRectMake(0, 0, size.width, size.height)]; UIImage* scaledImage = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); return scaledImage; } 最后就像使用函数一样，便可以轻松的使用VGG16模型 - (void)imagePickerController:(UIImagePickerController *)picker didFinishPickingMediaWithInfo:(NSDictionary *)info { NSString *mediaType=[info objectForKey:UIImagePickerControllerMediaType]; if ([mediaType isEqualToString:(NSString *)kUTTypeImage]){ CGSize thesize = CGSizeMake(224, 224); UIImage *theimage = [self image:info[UIImagePickerControllerEditedImage] scaleToSize:thesize]; self.imageView.image = theimage; CVPixelBufferRef imageRef = [self pixelBufferFromCGImage:theimage.CGImage]; VGG16 *VGGModel = [[VGG16 alloc] init]; NSError *error = nil; VGG16Output *output = [VGGModel predictionFromImage:imageRef error:&amp;error]; if (error == nil) { NSLog(@&quot;%@&quot;,output.classLabel); self.photoLable.text = output.classLabel; } else { NSLog(@&quot;Error is %@&quot;, error.localizedDescription); } } UIImagePickerController *imagePickerVC = picker; [imagePickerVC dismissViewControllerAnimated:YES completion:^{ 结果展示" />
<meta property="og:description" content="IOS下的机器学习 人工智能愈发火热，可以看到各大公司给AI工程师的薪资也是越涨越烈，而机器学习是人工智能的核心，也是实现人工智能的根本途径。机器学习专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。 而苹果在 iOS 5 里引入了 NSLinguisticTagger 来分析自然语言。iOS 8 出了 Metal，提供了对设备 GPU 的底层访问。 去年，苹果在 Accelerate 框架添加了 Basic Neural Network Subroutines (BNNS)，使开发者可以构建用于推理（不是训练）的神经网络。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 在2017年，苹果带来了&nbsp;Core ML 和 Vision。 Core ML 让我们更容易在 App 中使用训练过的模型。 Vision 让我们轻松访问苹果的模型，用于面部检测、面部特征点、文字、矩形、条形码和物体。 当然，模型的训练过程是根本不可能在单机上完成的，这需要通过特定算法，使用大量数据来进行模型生成的这个过程，是非常地消耗计算和存储资源的，特别一些大型的神经网络，普通的IOS设备或许根本跑都跑不起来，更多的情况我们都是放在后端的HBase、Cassandra这种数据库中，然后再通过诸如Caffe, Scikit-Learn, Tensorflow这些来进行相应的训练，最后生成我们想要的模型。拿到模型后，对于数据的判断，移动端的设备似乎就能够做一些力所能及之事了。 Core ML 便是这样的存在，开发者不需要理会模型的生成过程，而只需要关注输入和输出的参数，便能够轻松的将机器学习集合到自己的应用中并高效运行起来， 我们分几个步骤来完成这个demo 完成壳app，能够照相或者从相册选取照片 下载机器学习模型 将Core ML模型整合到项目中 根据不同的卷积神经网络模型，苹果提供了几个已经训练好的CoreML模型我们可以直接使用。 Places205-GoogleNet ResNet50 Inception v3 VGG16 关于不同模型的详细内容可以阅读苹果官网的文档。 在这里我选择了VGG16，其简洁性和实用性是目前较流行的卷积神经网络模型。 完成demo 壳APP我们要完成照相或者从相册选取照片的功能，肯定想到使用UIImagePickerController，这个很简单。不过要注意的是要在info.plist中添加Privacy - Photo Library Usage Description 、Privacy - Photo Library Additions Usage Description和 Privacy - Camera Usage Description。否则程序会崩溃。 UIBarButtonItem *photoBtniItem = [[UIBarButtonItem alloc]initWithBarButtonSystemItem:UIBarButtonSystemItemCamera target:self action:@selector(alertcController)]; self.navigationItem.leftBarButtonItem = photoBtniItem; self.imagePickController = [[UIImagePickerController alloc]init]; self.imagePickController.delegate = (id)self; self.imagePickController.modalTransitionStyle = UIModalTransitionStyleFlipHorizontal; self.imagePickController.allowsEditing = YES; 通过AlertController来让用户选择照相还是从相册选择照片。 -(void)alertcController{ UIAlertController *alert = [UIAlertController alertControllerWithTitle:@&quot;提示&quot; message:nil preferredStyle:UIAlertControllerStyleActionSheet]; UIAlertAction *cancel = [UIAlertAction actionWithTitle:@&quot;取消&quot; style:UIAlertActionStyleCancel handler:^(UIAlertAction * _Nonnull action) { }]; UIAlertAction *takePhoto = [UIAlertAction actionWithTitle:@&quot;打开相机&quot; style:UIAlertActionStyleDefault handler:^(UIAlertAction * _Nonnull action) { self.imagePickController.sourceType = UIImagePickerControllerSourceTypeCamera; self.imagePickController.mediaTypes = @[(NSString *)kUTTypeImage]; self.imagePickController.cameraCaptureMode = UIImagePickerControllerCameraCaptureModePhoto; [self.navigationController presentViewController:self.imagePickController animated:YES completion:nil]; }]; UIAlertAction *cameraLibrary = [UIAlertAction actionWithTitle:@&quot;打开相册&quot; style:UIAlertActionStyleDefault handler:^(UIAlertAction * _Nonnull action) { self.imagePickController.sourceType = UIImagePickerControllerSourceTypePhotoLibrary; self.imagePickController.mediaTypes = @[(NSString *)kUTTypeImage]; [self.navigationController presentViewController:self.imagePickController animated:YES completion:nil]; }]; [alert addAction:cancel]; [alert addAction:takePhoto]; [alert addAction:cameraLibrary]; [self presentViewController:alert animated:YES completion:nil]; } 通过MainStroyborard给界面添加上一个ImageView和一个label,ImageView展示输入图片，label展示模型识别后的输出结果。 接下来，我们只需要在苹果官网下载好VGG16模型，拖进项目文件后便可以直接使用。 VGG16模型的详细介绍，要注意的是，VGG16模型要求输入参数是224*224大小的图片，因此我们必须对选择好的照片进行一定的修改。CVPixelBufferRef这种图像格式的处理与UIImage, CGImageRef的处理需小心，容易造成内存泄漏。 - (CVPixelBufferRef) pixelBufferFromCGImage: (CGImageRef) image { NSDictionary *options = @{ (NSString*)kCVPixelBufferCGImageCompatibilityKey : @YES, (NSString*)kCVPixelBufferCGBitmapContextCompatibilityKey : @YES, }; CVPixelBufferRef pxbuffer = NULL; CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, CGImageGetWidth(image), CGImageGetHeight(image), kCVPixelFormatType_32ARGB, (__bridge CFDictionaryRef) options, &amp;pxbuffer); if (status!=kCVReturnSuccess) { NSLog(@&quot;Operation failed&quot;); } NSParameterAssert(status == kCVReturnSuccess &amp;&amp; pxbuffer != NULL); CVPixelBufferLockBaseAddress(pxbuffer, 0); void *pxdata = CVPixelBufferGetBaseAddress(pxbuffer); CGColorSpaceRef rgbColorSpace = CGColorSpaceCreateDeviceRGB(); CGContextRef context = CGBitmapContextCreate(pxdata, CGImageGetWidth(image), CGImageGetHeight(image), 8, 4*CGImageGetWidth(image), rgbColorSpace, kCGImageAlphaNoneSkipFirst); NSParameterAssert(context); CGContextConcatCTM(context, CGAffineTransformMakeRotation(0)); CGAffineTransform flipVertical = CGAffineTransformMake( 1, 0, 0, -1, 0, CGImageGetHeight(image) ); CGContextConcatCTM(context, flipVertical); CGAffineTransform flipHorizontal = CGAffineTransformMake( -1.0, 0.0, 0.0, 1.0, CGImageGetWidth(image), 0.0 ); CGContextConcatCTM(context, flipHorizontal); CGContextDrawImage(context, CGRectMake(0, 0, CGImageGetWidth(image), CGImageGetHeight(image)), image); CGColorSpaceRelease(rgbColorSpace); CGContextRelease(context); CVPixelBufferUnlockBaseAddress(pxbuffer, 0); return pxbuffer; } - (UIImage*)image:(UIImage *)image scaleToSize:(CGSize)size{ UIGraphicsBeginImageContext(size); [image drawInRect:CGRectMake(0, 0, size.width, size.height)]; UIImage* scaledImage = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); return scaledImage; } 最后就像使用函数一样，便可以轻松的使用VGG16模型 - (void)imagePickerController:(UIImagePickerController *)picker didFinishPickingMediaWithInfo:(NSDictionary *)info { NSString *mediaType=[info objectForKey:UIImagePickerControllerMediaType]; if ([mediaType isEqualToString:(NSString *)kUTTypeImage]){ CGSize thesize = CGSizeMake(224, 224); UIImage *theimage = [self image:info[UIImagePickerControllerEditedImage] scaleToSize:thesize]; self.imageView.image = theimage; CVPixelBufferRef imageRef = [self pixelBufferFromCGImage:theimage.CGImage]; VGG16 *VGGModel = [[VGG16 alloc] init]; NSError *error = nil; VGG16Output *output = [VGGModel predictionFromImage:imageRef error:&amp;error]; if (error == nil) { NSLog(@&quot;%@&quot;,output.classLabel); self.photoLable.text = output.classLabel; } else { NSLog(@&quot;Error is %@&quot;, error.localizedDescription); } } UIImagePickerController *imagePickerVC = picker; [imagePickerVC dismissViewControllerAnimated:YES completion:^{ 结果展示" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-14T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"IOS下的机器学习 人工智能愈发火热，可以看到各大公司给AI工程师的薪资也是越涨越烈，而机器学习是人工智能的核心，也是实现人工智能的根本途径。机器学习专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。 而苹果在 iOS 5 里引入了 NSLinguisticTagger 来分析自然语言。iOS 8 出了 Metal，提供了对设备 GPU 的底层访问。 去年，苹果在 Accelerate 框架添加了 Basic Neural Network Subroutines (BNNS)，使开发者可以构建用于推理（不是训练）的神经网络。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 在2017年，苹果带来了&nbsp;Core ML 和 Vision。 Core ML 让我们更容易在 App 中使用训练过的模型。 Vision 让我们轻松访问苹果的模型，用于面部检测、面部特征点、文字、矩形、条形码和物体。 当然，模型的训练过程是根本不可能在单机上完成的，这需要通过特定算法，使用大量数据来进行模型生成的这个过程，是非常地消耗计算和存储资源的，特别一些大型的神经网络，普通的IOS设备或许根本跑都跑不起来，更多的情况我们都是放在后端的HBase、Cassandra这种数据库中，然后再通过诸如Caffe, Scikit-Learn, Tensorflow这些来进行相应的训练，最后生成我们想要的模型。拿到模型后，对于数据的判断，移动端的设备似乎就能够做一些力所能及之事了。 Core ML 便是这样的存在，开发者不需要理会模型的生成过程，而只需要关注输入和输出的参数，便能够轻松的将机器学习集合到自己的应用中并高效运行起来， 我们分几个步骤来完成这个demo 完成壳app，能够照相或者从相册选取照片 下载机器学习模型 将Core ML模型整合到项目中 根据不同的卷积神经网络模型，苹果提供了几个已经训练好的CoreML模型我们可以直接使用。 Places205-GoogleNet ResNet50 Inception v3 VGG16 关于不同模型的详细内容可以阅读苹果官网的文档。 在这里我选择了VGG16，其简洁性和实用性是目前较流行的卷积神经网络模型。 完成demo 壳APP我们要完成照相或者从相册选取照片的功能，肯定想到使用UIImagePickerController，这个很简单。不过要注意的是要在info.plist中添加Privacy - Photo Library Usage Description 、Privacy - Photo Library Additions Usage Description和 Privacy - Camera Usage Description。否则程序会崩溃。 UIBarButtonItem *photoBtniItem = [[UIBarButtonItem alloc]initWithBarButtonSystemItem:UIBarButtonSystemItemCamera target:self action:@selector(alertcController)]; self.navigationItem.leftBarButtonItem = photoBtniItem; self.imagePickController = [[UIImagePickerController alloc]init]; self.imagePickController.delegate = (id)self; self.imagePickController.modalTransitionStyle = UIModalTransitionStyleFlipHorizontal; self.imagePickController.allowsEditing = YES; 通过AlertController来让用户选择照相还是从相册选择照片。 -(void)alertcController{ UIAlertController *alert = [UIAlertController alertControllerWithTitle:@&quot;提示&quot; message:nil preferredStyle:UIAlertControllerStyleActionSheet]; UIAlertAction *cancel = [UIAlertAction actionWithTitle:@&quot;取消&quot; style:UIAlertActionStyleCancel handler:^(UIAlertAction * _Nonnull action) { }]; UIAlertAction *takePhoto = [UIAlertAction actionWithTitle:@&quot;打开相机&quot; style:UIAlertActionStyleDefault handler:^(UIAlertAction * _Nonnull action) { self.imagePickController.sourceType = UIImagePickerControllerSourceTypeCamera; self.imagePickController.mediaTypes = @[(NSString *)kUTTypeImage]; self.imagePickController.cameraCaptureMode = UIImagePickerControllerCameraCaptureModePhoto; [self.navigationController presentViewController:self.imagePickController animated:YES completion:nil]; }]; UIAlertAction *cameraLibrary = [UIAlertAction actionWithTitle:@&quot;打开相册&quot; style:UIAlertActionStyleDefault handler:^(UIAlertAction * _Nonnull action) { self.imagePickController.sourceType = UIImagePickerControllerSourceTypePhotoLibrary; self.imagePickController.mediaTypes = @[(NSString *)kUTTypeImage]; [self.navigationController presentViewController:self.imagePickController animated:YES completion:nil]; }]; [alert addAction:cancel]; [alert addAction:takePhoto]; [alert addAction:cameraLibrary]; [self presentViewController:alert animated:YES completion:nil]; } 通过MainStroyborard给界面添加上一个ImageView和一个label,ImageView展示输入图片，label展示模型识别后的输出结果。 接下来，我们只需要在苹果官网下载好VGG16模型，拖进项目文件后便可以直接使用。 VGG16模型的详细介绍，要注意的是，VGG16模型要求输入参数是224*224大小的图片，因此我们必须对选择好的照片进行一定的修改。CVPixelBufferRef这种图像格式的处理与UIImage, CGImageRef的处理需小心，容易造成内存泄漏。 - (CVPixelBufferRef) pixelBufferFromCGImage: (CGImageRef) image { NSDictionary *options = @{ (NSString*)kCVPixelBufferCGImageCompatibilityKey : @YES, (NSString*)kCVPixelBufferCGBitmapContextCompatibilityKey : @YES, }; CVPixelBufferRef pxbuffer = NULL; CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, CGImageGetWidth(image), CGImageGetHeight(image), kCVPixelFormatType_32ARGB, (__bridge CFDictionaryRef) options, &amp;pxbuffer); if (status!=kCVReturnSuccess) { NSLog(@&quot;Operation failed&quot;); } NSParameterAssert(status == kCVReturnSuccess &amp;&amp; pxbuffer != NULL); CVPixelBufferLockBaseAddress(pxbuffer, 0); void *pxdata = CVPixelBufferGetBaseAddress(pxbuffer); CGColorSpaceRef rgbColorSpace = CGColorSpaceCreateDeviceRGB(); CGContextRef context = CGBitmapContextCreate(pxdata, CGImageGetWidth(image), CGImageGetHeight(image), 8, 4*CGImageGetWidth(image), rgbColorSpace, kCGImageAlphaNoneSkipFirst); NSParameterAssert(context); CGContextConcatCTM(context, CGAffineTransformMakeRotation(0)); CGAffineTransform flipVertical = CGAffineTransformMake( 1, 0, 0, -1, 0, CGImageGetHeight(image) ); CGContextConcatCTM(context, flipVertical); CGAffineTransform flipHorizontal = CGAffineTransformMake( -1.0, 0.0, 0.0, 1.0, CGImageGetWidth(image), 0.0 ); CGContextConcatCTM(context, flipHorizontal); CGContextDrawImage(context, CGRectMake(0, 0, CGImageGetWidth(image), CGImageGetHeight(image)), image); CGColorSpaceRelease(rgbColorSpace); CGContextRelease(context); CVPixelBufferUnlockBaseAddress(pxbuffer, 0); return pxbuffer; } - (UIImage*)image:(UIImage *)image scaleToSize:(CGSize)size{ UIGraphicsBeginImageContext(size); [image drawInRect:CGRectMake(0, 0, size.width, size.height)]; UIImage* scaledImage = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); return scaledImage; } 最后就像使用函数一样，便可以轻松的使用VGG16模型 - (void)imagePickerController:(UIImagePickerController *)picker didFinishPickingMediaWithInfo:(NSDictionary *)info { NSString *mediaType=[info objectForKey:UIImagePickerControllerMediaType]; if ([mediaType isEqualToString:(NSString *)kUTTypeImage]){ CGSize thesize = CGSizeMake(224, 224); UIImage *theimage = [self image:info[UIImagePickerControllerEditedImage] scaleToSize:thesize]; self.imageView.image = theimage; CVPixelBufferRef imageRef = [self pixelBufferFromCGImage:theimage.CGImage]; VGG16 *VGGModel = [[VGG16 alloc] init]; NSError *error = nil; VGG16Output *output = [VGGModel predictionFromImage:imageRef error:&amp;error]; if (error == nil) { NSLog(@&quot;%@&quot;,output.classLabel); self.photoLable.text = output.classLabel; } else { NSLog(@&quot;Error is %@&quot;, error.localizedDescription); } } UIImagePickerController *imagePickerVC = picker; [imagePickerVC dismissViewControllerAnimated:YES completion:^{ 结果展示","@type":"BlogPosting","url":"/2019/01/14/7824de02eb057651a19fb6a4434101a9.html","headline":"【IOS】基于VGG16模型的机器学习CoreML demo体验","dateModified":"2019-01-14T00:00:00+08:00","datePublished":"2019-01-14T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/01/14/7824de02eb057651a19fb6a4434101a9.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>【IOS】基于VGG16模型的机器学习CoreML demo体验</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h2 id="IOS%E4%B8%8B%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">IOS下的机器学习</h2> 
  <hr>
  <p style="text-indent:50px;">人工智能愈发火热，可以看到各大公司给AI工程师的薪资也是越涨越烈，而机器学习是人工智能的核心，也是实现人工智能的根本途径。机器学习专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。</p> 
  <p style="text-indent:50px;">而苹果在 iOS 5 里引入了 <code>NSLinguisticTagger</code> 来分析自然语言。iOS 8 出了 Metal，提供了对设备 GPU 的底层访问。</p> 
  <p>去年，苹果在 Accelerate 框架添加了 Basic Neural Network Subroutines (BNNS)，使开发者可以构建用于推理（不是训练）的神经网络。</p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 在2017年，苹果带来了&nbsp;Core ML 和 Vision。</p> 
  <ul>
   <li><strong>Core ML</strong> 让我们更容易在 App 中使用训练过的模型。</li> 
   <li><strong>Vision</strong> 让我们轻松访问苹果的模型，用于面部检测、面部特征点、文字、矩形、条形码和物体。</li> 
  </ul>
  <p style="text-indent:50px;">当然，模型的训练过程是根本不可能在单机上完成的，这需要通过特定算法，使用大量数据来进行模型生成的这个过程，是非常地消耗计算和存储资源的，特别一些大型的神经网络，普通的IOS设备或许根本跑都跑不起来，更多的情况我们都是放在后端的HBase、Cassandra这种数据库中，然后再通过诸如Caffe, Scikit-Learn, Tensorflow这些来进行相应的训练，最后生成我们想要的模型。拿到模型后，对于数据的判断，移动端的设备似乎就能够做一些力所能及之事了。</p> 
  <p style="text-indent:50px;">Core ML 便是这样的存在，开发者不需要理会模型的生成过程，而只需要关注输入和输出的参数，便能够轻松的将机器学习集合到自己的应用中并高效运行起来，</p> 
  <p style="text-indent:50px;">我们分几个步骤来完成这个demo</p> 
  <ul>
   <li>完成壳app，能够照相或者从相册选取照片</li> 
   <li>下载机器学习模型</li> 
   <li>将Core ML模型整合到项目中</li> 
  </ul>
  <p style="text-indent:50px;">根据不同的卷积神经网络模型，苹果提供了几个已经训练好的CoreML模型我们可以直接使用。</p> 
  <ul>
   <li> <p>Places205-GoogleNet</p> </li> 
   <li> <p>ResNet50</p> </li> 
   <li> <p>Inception v3</p> </li> 
   <li> <p>VGG16</p> </li> 
  </ul>
  <p style="text-indent:50px;">关于不同模型的详细内容可以阅读<a href="https://developer.apple.com/machine-learning/build-run-models/" rel="nofollow">苹果官网</a>的文档。</p> 
  <p style="text-indent:50px;">在这里我选择了VGG16，其简洁性和实用性是目前较流行的卷积神经网络模型。</p> 
  <h2 id="%E5%AE%8C%E6%88%90%E5%A3%B3APP">完成demo</h2> 
  <hr>
  <p style="text-indent:50px;">壳APP我们要完成照相或者从相册选取照片的功能，肯定想到使用UIImagePickerController，这个很简单。不过要注意的是要在info.plist中添加Privacy - Photo Library Usage Description 、Privacy - Photo Library Additions Usage Description和 Privacy - Camera Usage Description。否则程序会崩溃。</p> 
  <pre class="has">
<code> UIBarButtonItem *photoBtniItem = [[UIBarButtonItem alloc]initWithBarButtonSystemItem:UIBarButtonSystemItemCamera target:self action:@selector(alertcController)];
    self.navigationItem.leftBarButtonItem = photoBtniItem;
    self.imagePickController = [[UIImagePickerController alloc]init];
    self.imagePickController.delegate = (id)self;
    self.imagePickController.modalTransitionStyle = UIModalTransitionStyleFlipHorizontal;
    self.imagePickController.allowsEditing = YES;</code></pre> 
  <p>通过AlertController来让用户选择照相还是从相册选择照片。</p> 
  <pre class="has">
<code>-(void)alertcController{
    UIAlertController *alert = [UIAlertController alertControllerWithTitle:@"提示" message:nil preferredStyle:UIAlertControllerStyleActionSheet];
    UIAlertAction *cancel = [UIAlertAction actionWithTitle:@"取消" style:UIAlertActionStyleCancel handler:^(UIAlertAction * _Nonnull action) {
        
    }];
    UIAlertAction *takePhoto = [UIAlertAction actionWithTitle:@"打开相机" style:UIAlertActionStyleDefault handler:^(UIAlertAction * _Nonnull action) {
        self.imagePickController.sourceType = UIImagePickerControllerSourceTypeCamera;
        self.imagePickController.mediaTypes = @[(NSString *)kUTTypeImage];
        self.imagePickController.cameraCaptureMode = UIImagePickerControllerCameraCaptureModePhoto;
        [self.navigationController presentViewController:self.imagePickController
                                                animated:YES
                                              completion:nil];
    }];
    UIAlertAction *cameraLibrary = [UIAlertAction actionWithTitle:@"打开相册" style:UIAlertActionStyleDefault handler:^(UIAlertAction * _Nonnull action) {
        self.imagePickController.sourceType = UIImagePickerControllerSourceTypePhotoLibrary;
        self.imagePickController.mediaTypes = @[(NSString *)kUTTypeImage];
        [self.navigationController presentViewController:self.imagePickController
                                                animated:YES
                                              completion:nil];
    }];
    [alert addAction:cancel];
    [alert addAction:takePhoto];
    [alert addAction:cameraLibrary];
    [self presentViewController:alert animated:YES completion:nil];
    
}</code></pre> 
  <p>通过MainStroyborard给界面添加上一个ImageView和一个label,ImageView展示输入图片，label展示模型识别后的输出结果。</p> 
  <p>接下来，我们只需要在苹果官网下载好VGG16模型，拖进项目文件后便可以直接使用。</p> 
  <p><img alt="" class="has" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190114175320519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzkzMjA2,size_16,color_FFFFFF,t_70"></p> 
  <p>VGG16模型的详细介绍，要注意的是，VGG16模型要求输入参数是224*224大小的图片，因此我们必须对选择好的照片进行一定的修改。CVPixelBufferRef这种图像格式的处理与UIImage, CGImageRef的处理需小心，容易造成内存泄漏。</p> 
  <pre class="has">
<code>- (CVPixelBufferRef) pixelBufferFromCGImage: (CGImageRef) image {
    NSDictionary *options = @{
                              (NSString*)kCVPixelBufferCGImageCompatibilityKey : @YES,
                              (NSString*)kCVPixelBufferCGBitmapContextCompatibilityKey : @YES,
                              };
    CVPixelBufferRef pxbuffer = NULL;
    CVReturn status = CVPixelBufferCreate(kCFAllocatorDefault, CGImageGetWidth(image),
                                          CGImageGetHeight(image), kCVPixelFormatType_32ARGB, (__bridge CFDictionaryRef) options,
                                          &amp;pxbuffer);
    if (status!=kCVReturnSuccess) {
        NSLog(@"Operation failed");
    }
    NSParameterAssert(status == kCVReturnSuccess &amp;&amp; pxbuffer != NULL);
    CVPixelBufferLockBaseAddress(pxbuffer, 0);
    void *pxdata = CVPixelBufferGetBaseAddress(pxbuffer);
    CGColorSpaceRef rgbColorSpace = CGColorSpaceCreateDeviceRGB();
    CGContextRef context = CGBitmapContextCreate(pxdata, CGImageGetWidth(image),
                                                 CGImageGetHeight(image), 8, 4*CGImageGetWidth(image), rgbColorSpace,
                                                 kCGImageAlphaNoneSkipFirst);
    NSParameterAssert(context);
    CGContextConcatCTM(context, CGAffineTransformMakeRotation(0));
    CGAffineTransform flipVertical = CGAffineTransformMake( 1, 0, 0, -1, 0, CGImageGetHeight(image) );
    CGContextConcatCTM(context, flipVertical);
    CGAffineTransform flipHorizontal = CGAffineTransformMake( -1.0, 0.0, 0.0, 1.0, CGImageGetWidth(image), 0.0 );
    CGContextConcatCTM(context, flipHorizontal);
    CGContextDrawImage(context, CGRectMake(0, 0, CGImageGetWidth(image),
                                           CGImageGetHeight(image)), image);
    CGColorSpaceRelease(rgbColorSpace);
    CGContextRelease(context);
    CVPixelBufferUnlockBaseAddress(pxbuffer, 0);
    return pxbuffer;
}
- (UIImage*)image:(UIImage *)image scaleToSize:(CGSize)size{
    
    UIGraphicsBeginImageContext(size);
    [image drawInRect:CGRectMake(0, 0, size.width, size.height)];
    UIImage* scaledImage = UIGraphicsGetImageFromCurrentImageContext();
    UIGraphicsEndImageContext();
    return scaledImage;
}</code></pre> 
  <p>最后就像使用函数一样，便可以轻松的使用VGG16模型</p> 
  <pre class="has">
<code>- (void)imagePickerController:(UIImagePickerController *)picker
didFinishPickingMediaWithInfo:(NSDictionary *)info {
    NSString *mediaType=[info objectForKey:UIImagePickerControllerMediaType];
    if ([mediaType isEqualToString:(NSString *)kUTTypeImage]){
        CGSize thesize = CGSizeMake(224, 224);
        UIImage *theimage = [self image:info[UIImagePickerControllerEditedImage] scaleToSize:thesize];
        self.imageView.image = theimage;
        
        CVPixelBufferRef imageRef = [self pixelBufferFromCGImage:theimage.CGImage];
        VGG16 *VGGModel = [[VGG16 alloc] init];
        NSError *error = nil;
        VGG16Output *output = [VGGModel predictionFromImage:imageRef
                                                              error:&amp;error];
        if (error == nil) {
            NSLog(@"%@",output.classLabel);
            self.photoLable.text = output.classLabel;
        } else {
            NSLog(@"Error is %@", error.localizedDescription);
        }
    }
    
    UIImagePickerController *imagePickerVC = picker;
    [imagePickerVC dismissViewControllerAnimated:YES completion:^{
    </code></pre> 
  <p>结果展示<br><img alt="" class="has" height="406" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190114180414790.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2NzkzMjA2,size_16,color_FFFFFF,t_70" width="228"></p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
