<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Neural network and deep learning阅读笔记（5）梯度消失问题 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Neural network and deep learning阅读笔记（5）梯度消失问题" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="之前我们都是处理一层隐藏层的神经网络，但是在处理实际问题中，一层隐藏层往往不够，所以我们需要多层隐藏层： 比如，当我们在处理视觉模式识别问题时，第一层可能用来识别边缘，第二层可以识别稍微复杂的形状，比如三角形，第三层处理更加复杂的图形，以此类推，到最后我们可以识别非常复杂的物体。所以理论上来说，深度网络比浅层网络更好。这一章，作者主要讲了如何训练深度神经网络，用到的方法是之前讲到的随机梯度下降和后向传播算法。但是也会遇到深度网络并不比浅层网络效果好的情况。 会出现的一种情况是：不同层之间训练的速度不一样，后面的层训练的时候，前面的层会停止训练，这种learning slowdown问题跟我们基于梯度的训练方法有很大的关系。相反的情况也有可能会出现：有时候前面的层训练的很好，而后面的层却不再训练了。 The vanishing gradient problem 之前的MNIST项目作者放到GitHub了，用下面的命令行可以看到： git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git 然后我们下载数据包，像之前一样训练： import mnist_loader training_data, validation_data, test_data = mnist_loader.load_data_wrapper() import network2 net = network2.Network([784, 30, 10]) net.SGD(training_data, 30, 10, 0.1, lmbda=5.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True) 我们得到的accuracy是96.48%。现在我们加上一层隐层，也是30个神经元，其他参数相同： net = network2.Network([784, 30, 30, 10]) net.SGD(training_data, 30, 10, 0.1, lmbda=5.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True) 这次的accuracy稍微提高了一点：96.90%。如果我们再加一层隐层：[784, 30, 30, 30, 10]，这次的准确率却变到了96.57%。如果再加一层，正确率96.53%。 假设问题并不是出在我们的算法上，也就是说网络找到了正确的权重和偏置。 上图是我们[784, 30, 30, 10]网络中间的两层隐藏层的一部分（只取了30个神经元之中的前六个），每一个神经元上面都有一个小条，代表了这个神经元变化的快慢。小条长表示权重和偏置改变的很快，短表示改变的慢。可以看到，第二层的bar比第一层的bar普遍要高，说明第二层比第一层学习速度要快。 记第l层第j个神经元梯度为 δ \delta δjl= ∂ \partial ∂C/ ∂ \partial ∂bjl，我们可以把 δ \delta δl作为决定第l层神经元学习速度快慢的向量，将其模长作为这一层学习速度。比如，|| δ \delta δ1||是第一层神经元学习的速度。我们可以计算出上面网络的每一层学习速度：|| δ \delta δ1||=0.07，|| δ \delta δ2||=0.31。如果我们训练了[784, 30, 30, 30, 10]的网络，那三层学习速度分别是0.012，0.060，0.283。再增加隐藏层还是同样的情况：前面的层比后面的层学习速度要慢。速度变化如下图： 四层隐层的时候，第一层比第四层慢了100倍，所以肯定会出现问题。一个重要的发现：在某些深度网络中，当我们反向传播的时候，梯度变小了。这个现象被称作梯度消失问题vanishing gradient problem。【梯度变大的问题叫exploding gradient problem 梯度消失问题产生的原因 考虑每层只有一个神经元的神经网络： 根据后向传播算法，我们可以得到第一个神经元的偏置的偏导： 除了最后一项，这个式子都是形如wj σ \sigma σ’(zj)的乘积，回忆 σ \sigma σ函数的导数图像： 导数最大在 σ \sigma σ’(0)处，值为1/4。我们初始化权重和偏置时常用的方法是均值为0，标准差为1的正态分布，所以权重满足|wj|&lt;1，所以|wj σ \sigma σ’(zj)|&lt;1/4。所以乘积项越多，导数会以指数减少。越往前，偏导就越小，这是梯度消失最主要的原因。 如果权重在学习过程中变大，或者是刚开始我们不按照正常方法初始化，而是直接设定权重为很大的数，那么就不满足|wj σ \sigma σ’(zj)|&lt;1/4了，就会出现exploding gradient problem。 其实我们使用sigmoid函数作为激活函数的时候，大多数情况下出现的是vanish，因为即使w变大了，但是 σ \sigma σ’(z)还是基于w的： σ \sigma σ’(z)= σ \sigma σ’(wa+b)，当w非常大时， σ \sigma σ’(z)会趋近于0。不过最主要的问题不是vanishing还是exploding，重要的是前面的层权重是非常不稳定的，基于后面的许多层的权重、偏导，如果网络太深，我们很难把控前面的偏导。 当神经元变多之后，梯度变成： Σ \Sigma Σ‘(zl)是一个对角矩阵，元素为第l层的 σ \sigma σ’(z)。其他的分析与上面相同，都是由于sigmoid函数的导数性质导致的梯度消失。作者并没有在这一章展开描述如何处理这个问题，但是他给出的解决方法是选取其他的有效区间更大的激活函数或许可以解决这个问题。" />
<meta property="og:description" content="之前我们都是处理一层隐藏层的神经网络，但是在处理实际问题中，一层隐藏层往往不够，所以我们需要多层隐藏层： 比如，当我们在处理视觉模式识别问题时，第一层可能用来识别边缘，第二层可以识别稍微复杂的形状，比如三角形，第三层处理更加复杂的图形，以此类推，到最后我们可以识别非常复杂的物体。所以理论上来说，深度网络比浅层网络更好。这一章，作者主要讲了如何训练深度神经网络，用到的方法是之前讲到的随机梯度下降和后向传播算法。但是也会遇到深度网络并不比浅层网络效果好的情况。 会出现的一种情况是：不同层之间训练的速度不一样，后面的层训练的时候，前面的层会停止训练，这种learning slowdown问题跟我们基于梯度的训练方法有很大的关系。相反的情况也有可能会出现：有时候前面的层训练的很好，而后面的层却不再训练了。 The vanishing gradient problem 之前的MNIST项目作者放到GitHub了，用下面的命令行可以看到： git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git 然后我们下载数据包，像之前一样训练： import mnist_loader training_data, validation_data, test_data = mnist_loader.load_data_wrapper() import network2 net = network2.Network([784, 30, 10]) net.SGD(training_data, 30, 10, 0.1, lmbda=5.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True) 我们得到的accuracy是96.48%。现在我们加上一层隐层，也是30个神经元，其他参数相同： net = network2.Network([784, 30, 30, 10]) net.SGD(training_data, 30, 10, 0.1, lmbda=5.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True) 这次的accuracy稍微提高了一点：96.90%。如果我们再加一层隐层：[784, 30, 30, 30, 10]，这次的准确率却变到了96.57%。如果再加一层，正确率96.53%。 假设问题并不是出在我们的算法上，也就是说网络找到了正确的权重和偏置。 上图是我们[784, 30, 30, 10]网络中间的两层隐藏层的一部分（只取了30个神经元之中的前六个），每一个神经元上面都有一个小条，代表了这个神经元变化的快慢。小条长表示权重和偏置改变的很快，短表示改变的慢。可以看到，第二层的bar比第一层的bar普遍要高，说明第二层比第一层学习速度要快。 记第l层第j个神经元梯度为 δ \delta δjl= ∂ \partial ∂C/ ∂ \partial ∂bjl，我们可以把 δ \delta δl作为决定第l层神经元学习速度快慢的向量，将其模长作为这一层学习速度。比如，|| δ \delta δ1||是第一层神经元学习的速度。我们可以计算出上面网络的每一层学习速度：|| δ \delta δ1||=0.07，|| δ \delta δ2||=0.31。如果我们训练了[784, 30, 30, 30, 10]的网络，那三层学习速度分别是0.012，0.060，0.283。再增加隐藏层还是同样的情况：前面的层比后面的层学习速度要慢。速度变化如下图： 四层隐层的时候，第一层比第四层慢了100倍，所以肯定会出现问题。一个重要的发现：在某些深度网络中，当我们反向传播的时候，梯度变小了。这个现象被称作梯度消失问题vanishing gradient problem。【梯度变大的问题叫exploding gradient problem 梯度消失问题产生的原因 考虑每层只有一个神经元的神经网络： 根据后向传播算法，我们可以得到第一个神经元的偏置的偏导： 除了最后一项，这个式子都是形如wj σ \sigma σ’(zj)的乘积，回忆 σ \sigma σ函数的导数图像： 导数最大在 σ \sigma σ’(0)处，值为1/4。我们初始化权重和偏置时常用的方法是均值为0，标准差为1的正态分布，所以权重满足|wj|&lt;1，所以|wj σ \sigma σ’(zj)|&lt;1/4。所以乘积项越多，导数会以指数减少。越往前，偏导就越小，这是梯度消失最主要的原因。 如果权重在学习过程中变大，或者是刚开始我们不按照正常方法初始化，而是直接设定权重为很大的数，那么就不满足|wj σ \sigma σ’(zj)|&lt;1/4了，就会出现exploding gradient problem。 其实我们使用sigmoid函数作为激活函数的时候，大多数情况下出现的是vanish，因为即使w变大了，但是 σ \sigma σ’(z)还是基于w的： σ \sigma σ’(z)= σ \sigma σ’(wa+b)，当w非常大时， σ \sigma σ’(z)会趋近于0。不过最主要的问题不是vanishing还是exploding，重要的是前面的层权重是非常不稳定的，基于后面的许多层的权重、偏导，如果网络太深，我们很难把控前面的偏导。 当神经元变多之后，梯度变成： Σ \Sigma Σ‘(zl)是一个对角矩阵，元素为第l层的 σ \sigma σ’(z)。其他的分析与上面相同，都是由于sigmoid函数的导数性质导致的梯度消失。作者并没有在这一章展开描述如何处理这个问题，但是他给出的解决方法是选取其他的有效区间更大的激活函数或许可以解决这个问题。" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-14T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"之前我们都是处理一层隐藏层的神经网络，但是在处理实际问题中，一层隐藏层往往不够，所以我们需要多层隐藏层： 比如，当我们在处理视觉模式识别问题时，第一层可能用来识别边缘，第二层可以识别稍微复杂的形状，比如三角形，第三层处理更加复杂的图形，以此类推，到最后我们可以识别非常复杂的物体。所以理论上来说，深度网络比浅层网络更好。这一章，作者主要讲了如何训练深度神经网络，用到的方法是之前讲到的随机梯度下降和后向传播算法。但是也会遇到深度网络并不比浅层网络效果好的情况。 会出现的一种情况是：不同层之间训练的速度不一样，后面的层训练的时候，前面的层会停止训练，这种learning slowdown问题跟我们基于梯度的训练方法有很大的关系。相反的情况也有可能会出现：有时候前面的层训练的很好，而后面的层却不再训练了。 The vanishing gradient problem 之前的MNIST项目作者放到GitHub了，用下面的命令行可以看到： git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git 然后我们下载数据包，像之前一样训练： import mnist_loader training_data, validation_data, test_data = mnist_loader.load_data_wrapper() import network2 net = network2.Network([784, 30, 10]) net.SGD(training_data, 30, 10, 0.1, lmbda=5.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True) 我们得到的accuracy是96.48%。现在我们加上一层隐层，也是30个神经元，其他参数相同： net = network2.Network([784, 30, 30, 10]) net.SGD(training_data, 30, 10, 0.1, lmbda=5.0, evaluation_data=validation_data, monitor_evaluation_accuracy=True) 这次的accuracy稍微提高了一点：96.90%。如果我们再加一层隐层：[784, 30, 30, 30, 10]，这次的准确率却变到了96.57%。如果再加一层，正确率96.53%。 假设问题并不是出在我们的算法上，也就是说网络找到了正确的权重和偏置。 上图是我们[784, 30, 30, 10]网络中间的两层隐藏层的一部分（只取了30个神经元之中的前六个），每一个神经元上面都有一个小条，代表了这个神经元变化的快慢。小条长表示权重和偏置改变的很快，短表示改变的慢。可以看到，第二层的bar比第一层的bar普遍要高，说明第二层比第一层学习速度要快。 记第l层第j个神经元梯度为 δ \\delta δjl= ∂ \\partial ∂C/ ∂ \\partial ∂bjl，我们可以把 δ \\delta δl作为决定第l层神经元学习速度快慢的向量，将其模长作为这一层学习速度。比如，|| δ \\delta δ1||是第一层神经元学习的速度。我们可以计算出上面网络的每一层学习速度：|| δ \\delta δ1||=0.07，|| δ \\delta δ2||=0.31。如果我们训练了[784, 30, 30, 30, 10]的网络，那三层学习速度分别是0.012，0.060，0.283。再增加隐藏层还是同样的情况：前面的层比后面的层学习速度要慢。速度变化如下图： 四层隐层的时候，第一层比第四层慢了100倍，所以肯定会出现问题。一个重要的发现：在某些深度网络中，当我们反向传播的时候，梯度变小了。这个现象被称作梯度消失问题vanishing gradient problem。【梯度变大的问题叫exploding gradient problem 梯度消失问题产生的原因 考虑每层只有一个神经元的神经网络： 根据后向传播算法，我们可以得到第一个神经元的偏置的偏导： 除了最后一项，这个式子都是形如wj σ \\sigma σ’(zj)的乘积，回忆 σ \\sigma σ函数的导数图像： 导数最大在 σ \\sigma σ’(0)处，值为1/4。我们初始化权重和偏置时常用的方法是均值为0，标准差为1的正态分布，所以权重满足|wj|&lt;1，所以|wj σ \\sigma σ’(zj)|&lt;1/4。所以乘积项越多，导数会以指数减少。越往前，偏导就越小，这是梯度消失最主要的原因。 如果权重在学习过程中变大，或者是刚开始我们不按照正常方法初始化，而是直接设定权重为很大的数，那么就不满足|wj σ \\sigma σ’(zj)|&lt;1/4了，就会出现exploding gradient problem。 其实我们使用sigmoid函数作为激活函数的时候，大多数情况下出现的是vanish，因为即使w变大了，但是 σ \\sigma σ’(z)还是基于w的： σ \\sigma σ’(z)= σ \\sigma σ’(wa+b)，当w非常大时， σ \\sigma σ’(z)会趋近于0。不过最主要的问题不是vanishing还是exploding，重要的是前面的层权重是非常不稳定的，基于后面的许多层的权重、偏导，如果网络太深，我们很难把控前面的偏导。 当神经元变多之后，梯度变成： Σ \\Sigma Σ‘(zl)是一个对角矩阵，元素为第l层的 σ \\sigma σ’(z)。其他的分析与上面相同，都是由于sigmoid函数的导数性质导致的梯度消失。作者并没有在这一章展开描述如何处理这个问题，但是他给出的解决方法是选取其他的有效区间更大的激活函数或许可以解决这个问题。","@type":"BlogPosting","url":"/2019/01/14/4c71f616f7b01fcac1ce4d8147cb7344.html","headline":"Neural network and deep learning阅读笔记（5）梯度消失问题","dateModified":"2019-01-14T00:00:00+08:00","datePublished":"2019-01-14T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/01/14/4c71f616f7b01fcac1ce4d8147cb7344.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Neural network and deep learning阅读笔记（5）梯度消失问题</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p>之前我们都是处理一层隐藏层的神经网络，但是在处理实际问题中，一层隐藏层往往不够，所以我们需要多层隐藏层：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190114192029509.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25peWlkYW4wNTI3,size_16,color_FFFFFF,t_70" alt=""><br> 比如，当我们在处理视觉模式识别问题时，第一层可能用来识别边缘，第二层可以识别稍微复杂的形状，比如三角形，第三层处理更加复杂的图形，以此类推，到最后我们可以识别非常复杂的物体。所以理论上来说，深度网络比浅层网络更好。这一章，作者主要讲了如何训练深度神经网络，用到的方法是之前讲到的随机梯度下降和后向传播算法。但是也会遇到深度网络并不比浅层网络效果好的情况。<br> 会出现的一种情况是：不同层之间训练的速度不一样，后面的层训练的时候，前面的层会停止训练，这种learning slowdown问题跟我们基于梯度的训练方法有很大的关系。相反的情况也有可能会出现：有时候前面的层训练的很好，而后面的层却不再训练了。</p> 
  <h2><a id="_The_vanishing_gradient_problem__5"></a><mark><em>The vanishing gradient problem</em></mark></h2> 
  <p>之前的MNIST项目作者放到GitHub了，用下面的命令行可以看到：</p> 
  <pre><code class="prism language-python">git clone https<span class="token punctuation">:</span><span class="token operator">//</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>mnielsen<span class="token operator">/</span>neural<span class="token operator">-</span>networks<span class="token operator">-</span><span class="token operator">and</span><span class="token operator">-</span>deep<span class="token operator">-</span>learning<span class="token punctuation">.</span>git
</code></pre> 
  <p>然后我们下载数据包，像之前一样训练：</p> 
  <pre><code class="prism language-python"><span class="token keyword">import</span> mnist_loader
training_data<span class="token punctuation">,</span> validation_data<span class="token punctuation">,</span> test_data <span class="token operator">=</span> mnist_loader<span class="token punctuation">.</span>load_data_wrapper<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">import</span> network2
net <span class="token operator">=</span> network2<span class="token punctuation">.</span>Network<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

net<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>training_data<span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> lmbda<span class="token operator">=</span><span class="token number">5.0</span><span class="token punctuation">,</span> evaluation_data<span class="token operator">=</span>validation_data<span class="token punctuation">,</span> monitor_evaluation_accuracy<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
  <p>我们得到的accuracy是96.48%。现在我们加上一层隐层，也是30个神经元，其他参数相同：</p> 
  <pre><code class="prism language-python">net <span class="token operator">=</span> network2<span class="token punctuation">.</span>Network<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>training_data<span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> lmbda<span class="token operator">=</span><span class="token number">5.0</span><span class="token punctuation">,</span> evaluation_data<span class="token operator">=</span>validation_data<span class="token punctuation">,</span> monitor_evaluation_accuracy<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
  <p>这次的accuracy稍微提高了一点：96.90%。如果我们再加一层隐层：[784, 30, 30, 30, 10]，这次的准确率却变到了96.57%。如果再加一层，正确率96.53%。<br> 假设问题并不是出在我们的算法上，也就是说网络找到了正确的权重和偏置。<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190114204545887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25peWlkYW4wNTI3,size_16,color_FFFFFF,t_70" alt=""><br> 上图是我们[784, 30, 30, 10]网络中间的两层隐藏层的一部分（只取了30个神经元之中的前六个），每一个神经元上面都有一个小条，代表了这个神经元变化的快慢。小条长表示权重和偏置改变的很快，短表示改变的慢。可以看到，第二层的bar比第一层的bar普遍要高，说明第二层比第一层学习速度要快。<br> 记第l层第j个神经元梯度为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          δ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \delta
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03785em;">δ</span></span></span></span></span><sub>j</sub><sup>l</sup>=<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi mathvariant="normal">
          ∂
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \partial
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord" style="margin-right: 0.05556em;">∂</span></span></span></span></span>C/<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi mathvariant="normal">
          ∂
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \partial
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord" style="margin-right: 0.05556em;">∂</span></span></span></span></span>b<sub>j</sub><sup>l</sup>，我们可以把<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          δ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \delta
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03785em;">δ</span></span></span></span></span><sup>l</sup>作为决定第l层神经元学习速度快慢的向量，将其模长作为这一层学习速度。比如，||<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          δ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \delta
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03785em;">δ</span></span></span></span></span><sup>1</sup>||是第一层神经元学习的速度。我们可以计算出上面网络的每一层学习速度：||<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          δ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \delta
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03785em;">δ</span></span></span></span></span><sup>1</sup>||=0.07，||<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          δ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \delta
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03785em;">δ</span></span></span></span></span><sup>2</sup>||=0.31。如果我们训练了[784, 30, 30, 30, 10]的网络，那三层学习速度分别是0.012，0.060，0.283。再增加隐藏层还是同样的情况：前面的层比后面的层学习速度要慢。速度变化如下图：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190114210745574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25peWlkYW4wNTI3,size_16,color_FFFFFF,t_70" alt=""><br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190114210957464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25peWlkYW4wNTI3,size_16,color_FFFFFF,t_70" alt=""><br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/2019011421100992.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25peWlkYW4wNTI3,size_16,color_FFFFFF,t_70" alt=""><br> 四层隐层的时候，第一层比第四层慢了100倍，所以肯定会出现问题。一个重要的发现：在某些深度网络中，当我们反向传播的时候，梯度变小了。这个现象被称作梯度消失问题vanishing gradient problem。【梯度变大的问题叫exploding gradient problem</p> 
  <h2><a id="___35"></a><mark><em>梯度消失问题产生的原因</em></mark></h2> 
  <p>考虑每层只有一个神经元的神经网络：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190115111735555.png" alt=""><br> 根据后向传播算法，我们可以得到第一个神经元的偏置的偏导：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190115112112331.png" alt=""><br> 除了最后一项，这个式子都是形如w<sub>j</sub><span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          σ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \sigma
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">σ</span></span></span></span></span>’(z<sub>j</sub>)的乘积，回忆<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          σ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \sigma
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">σ</span></span></span></span></span>函数的导数图像：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/201901151124293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25peWlkYW4wNTI3,size_16,color_FFFFFF,t_70" alt=""><br> 导数最大在<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          σ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \sigma
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">σ</span></span></span></span></span>’(0)处，值为1/4。我们初始化权重和偏置时常用的方法是均值为0，标准差为1的正态分布，所以权重满足|w<sub>j</sub>|&lt;1，所以|w<sub>j</sub><span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          σ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \sigma
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">σ</span></span></span></span></span>’(z<sub>j</sub>)|&lt;1/4。所以乘积项越多，导数会以指数减少。越往前，偏导就越小，这是梯度消失最主要的原因。<br> 如果权重在学习过程中变大，或者是刚开始我们不按照正常方法初始化，而是直接设定权重为很大的数，那么就不满足|w<sub>j</sub><span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          σ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \sigma
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">σ</span></span></span></span></span>’(z<sub>j</sub>)|&lt;1/4了，就会出现exploding gradient problem。<br> 其实我们使用sigmoid函数作为激活函数的时候，大多数情况下出现的是vanish，因为即使w变大了，但是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          σ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \sigma
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">σ</span></span></span></span></span>’(z)还是基于w的：<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          σ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \sigma
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">σ</span></span></span></span></span>’(z)=<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          σ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \sigma
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">σ</span></span></span></span></span>’(wa+b)，当w非常大时，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          σ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \sigma
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">σ</span></span></span></span></span>’(z)会趋近于0。不过最主要的问题不是vanishing还是exploding，重要的是前面的层权重是非常不稳定的，基于后面的许多层的权重、偏导，如果网络太深，我们很难把控前面的偏导。</p> 
  <p>当神经元变多之后，梯度变成：<br> <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190115130020107.png" alt=""><br> <span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi mathvariant="normal">
          Σ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \Sigma
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord">Σ</span></span></span></span></span>‘(z<sup>l</sup>)是一个对角矩阵，元素为第l层的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      <math>
       <semantics>
        <mrow>
         <mi>
          σ
         </mi>
        </mrow>
        <annotation encoding="application/x-tex">
         \sigma
        </annotation>
       </semantics>
      </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">σ</span></span></span></span></span>’(z)。其他的分析与上面相同，都是由于sigmoid函数的导数性质导致的梯度消失。作者并没有在这一章展开描述如何处理这个问题，但是他给出的解决方法是选取其他的有效区间更大的激活函数或许可以解决这个问题。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
