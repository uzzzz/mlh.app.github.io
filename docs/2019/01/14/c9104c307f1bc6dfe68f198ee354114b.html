<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>斯坦福机器学习公开课笔记 一 –单变量线性回归 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="斯坦福机器学习公开课笔记 一 –单变量线性回归" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 公开课地址：https://class.coursera.org/ml-003/class/index&nbsp; 授课老师：Andrew&nbsp;Ng 1、model&nbsp;representation(建立模型) 考虑一个问题，如果给定一些房屋售价和房屋面积的数据，现在要预测给定其他面积时的房屋售价，那该怎么办？其实这是一个线性回归问题，给定的数据作为训练样本，用其训练得到一个表示售价和面积关系的模型（其实是一个函数），然后用这个函数进行预测。基本流程如下： 2、cost&nbsp;function(代价函数) ps:其实这一节中代价函数并未出现代价函数 既然我们已经明确了只需要训练得到一个函数，那我们首先要做的就是对函数的形式进行假设。这里我们可以假设是最简单的线性函数： 下面问题就转变为如何求出theta值。由于我们已经有一些训练数据，虽然不知道数据之间是不是像假设那样成线性关系，但存在一点偏差我们并不介意，只需要让函数值尽可能接近真实值即可。这里我们让x表示面积，y表示售价，在二维平面上描出一系列的点。 3、cost&nbsp;function&nbsp;intuition&nbsp;1(代价函数初步1) 前面已经说明了需要让h函数值和真实值之间误差尽可能小，这里给出一个更清晰的说明： 代价函数J就代表了上面提到的误差，这里为了后面求导方便函数被写成了这种形式，我们的目标就是让代价函数J值最小，注意在J中theta变成了变量，m代表训练样本的数目（也就是坐标系中点的个数）。 4、cost&nbsp;function&nbsp;intuition&nbsp;2(代价函数初步2) 在给定一个theta0和theta1时，我们能找到相应的h函数（一条线）和代价J（一个值），当把这两者放在一起对比观察时，就能清楚的看到代价值在h函数和样本点拟合的很好时是最小的，而且存在全局唯一的最小值。代价函数J用三维表示大概就如下图所示： 从上图可以看出，此时h函数与样本点拟合的情况最好，而代价函数J也取到了最小值，theta0和theta1此时的取值可以从J的横纵坐标上看出。 5、gradient&nbsp;descent(梯度下降) 为了让J最小，我们的想法是通过改变theta的取值来改变J的取值。这里对theta的初始值不做要求，只考虑改变时的变化。梯度下降在这里的意思也就是沿着J函数的梯度方向改变theta让J取值减小，形象化的表示如下： 如果懂一点微积分知识，我们就可以把这个下降过程用数学式子像下面这样表达出来： 其中alpha是学习率（大于0），可以理解为每一次下降的步长，需要人为设定。 6、gradient&nbsp;descent&nbsp;intuition(梯度下降初步) 我们可以对上面梯度下降的公式进行一下简单的验证，如下图，当theta值过大时，梯度为正值，每一次迭代theta减小，J值也减小，同理theta值过小时，迭代让theta增大，J值也减小。所以梯度下降的思想是正确的。 相比之下，alpha的选择就不那么简单了，alpha选择要求适中，过大过小都不好： 如上图所说的，alpha取值太小步长太小，要走很多步才能下降到最小值，处理速度太慢。当alpha取值较大时，步长太大，会在最小值左右发生震荡，而永远也达不到最小值。不过，即使alpha取值适中，我们也可能会陷入到局部最小值，达不到全局最小。 补充一点，即使alpha值固定了，在梯度下降的过程中步长也会自动逐渐减小，所以我们不需要在函数逼近最小值的时候减小alpha的取值，以防止步长过大可能会跳过最低点。 7、gradient&nbsp;descent&nbsp;for&nbsp;linear&nbsp;regression(针对线性回归的梯度下降) 针对线性回归，由于我们已经有了代价函数J的形式，只需要代入即可： 写的更好看一点就是这样： 通过迭代，我们就能到达一个J的最小值，这里并不保证是全局最小。 --------------------------------------------弱弱的分割线-------------------------------------------------------------- 以上就是第一讲单变量线性回归的内容，思想还是很明确的。首先根据训练数据定义出模型函数形式，通过与真实值求误差得到代价函数，然后通过对代价函数梯度下降确定出模型函数的参数。确定好参数，下面就可以拿这个模型进行预测了，不过由于这里是线性的，变量只有一个，不是很准。自然下面就该引入多变量，非线性的情况了，那个肯定要复杂一些。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 再分享一下我老师大神的人工智能教程吧。零基础！通俗易懂！风趣幽默！还带黄段子！希望你也加入到我们人工智能的队伍中来！https://blog.csdn.net/jiangjunshow" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 公开课地址：https://class.coursera.org/ml-003/class/index&nbsp; 授课老师：Andrew&nbsp;Ng 1、model&nbsp;representation(建立模型) 考虑一个问题，如果给定一些房屋售价和房屋面积的数据，现在要预测给定其他面积时的房屋售价，那该怎么办？其实这是一个线性回归问题，给定的数据作为训练样本，用其训练得到一个表示售价和面积关系的模型（其实是一个函数），然后用这个函数进行预测。基本流程如下： 2、cost&nbsp;function(代价函数) ps:其实这一节中代价函数并未出现代价函数 既然我们已经明确了只需要训练得到一个函数，那我们首先要做的就是对函数的形式进行假设。这里我们可以假设是最简单的线性函数： 下面问题就转变为如何求出theta值。由于我们已经有一些训练数据，虽然不知道数据之间是不是像假设那样成线性关系，但存在一点偏差我们并不介意，只需要让函数值尽可能接近真实值即可。这里我们让x表示面积，y表示售价，在二维平面上描出一系列的点。 3、cost&nbsp;function&nbsp;intuition&nbsp;1(代价函数初步1) 前面已经说明了需要让h函数值和真实值之间误差尽可能小，这里给出一个更清晰的说明： 代价函数J就代表了上面提到的误差，这里为了后面求导方便函数被写成了这种形式，我们的目标就是让代价函数J值最小，注意在J中theta变成了变量，m代表训练样本的数目（也就是坐标系中点的个数）。 4、cost&nbsp;function&nbsp;intuition&nbsp;2(代价函数初步2) 在给定一个theta0和theta1时，我们能找到相应的h函数（一条线）和代价J（一个值），当把这两者放在一起对比观察时，就能清楚的看到代价值在h函数和样本点拟合的很好时是最小的，而且存在全局唯一的最小值。代价函数J用三维表示大概就如下图所示： 从上图可以看出，此时h函数与样本点拟合的情况最好，而代价函数J也取到了最小值，theta0和theta1此时的取值可以从J的横纵坐标上看出。 5、gradient&nbsp;descent(梯度下降) 为了让J最小，我们的想法是通过改变theta的取值来改变J的取值。这里对theta的初始值不做要求，只考虑改变时的变化。梯度下降在这里的意思也就是沿着J函数的梯度方向改变theta让J取值减小，形象化的表示如下： 如果懂一点微积分知识，我们就可以把这个下降过程用数学式子像下面这样表达出来： 其中alpha是学习率（大于0），可以理解为每一次下降的步长，需要人为设定。 6、gradient&nbsp;descent&nbsp;intuition(梯度下降初步) 我们可以对上面梯度下降的公式进行一下简单的验证，如下图，当theta值过大时，梯度为正值，每一次迭代theta减小，J值也减小，同理theta值过小时，迭代让theta增大，J值也减小。所以梯度下降的思想是正确的。 相比之下，alpha的选择就不那么简单了，alpha选择要求适中，过大过小都不好： 如上图所说的，alpha取值太小步长太小，要走很多步才能下降到最小值，处理速度太慢。当alpha取值较大时，步长太大，会在最小值左右发生震荡，而永远也达不到最小值。不过，即使alpha取值适中，我们也可能会陷入到局部最小值，达不到全局最小。 补充一点，即使alpha值固定了，在梯度下降的过程中步长也会自动逐渐减小，所以我们不需要在函数逼近最小值的时候减小alpha的取值，以防止步长过大可能会跳过最低点。 7、gradient&nbsp;descent&nbsp;for&nbsp;linear&nbsp;regression(针对线性回归的梯度下降) 针对线性回归，由于我们已经有了代价函数J的形式，只需要代入即可： 写的更好看一点就是这样： 通过迭代，我们就能到达一个J的最小值，这里并不保证是全局最小。 --------------------------------------------弱弱的分割线-------------------------------------------------------------- 以上就是第一讲单变量线性回归的内容，思想还是很明确的。首先根据训练数据定义出模型函数形式，通过与真实值求误差得到代价函数，然后通过对代价函数梯度下降确定出模型函数的参数。确定好参数，下面就可以拿这个模型进行预测了，不过由于这里是线性的，变量只有一个，不是很准。自然下面就该引入多变量，非线性的情况了，那个肯定要复杂一些。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 再分享一下我老师大神的人工智能教程吧。零基础！通俗易懂！风趣幽默！还带黄段子！希望你也加入到我们人工智能的队伍中来！https://blog.csdn.net/jiangjunshow" />
<link rel="canonical" href="https://mlh.app/2019/01/14/c9104c307f1bc6dfe68f198ee354114b.html" />
<meta property="og:url" content="https://mlh.app/2019/01/14/c9104c307f1bc6dfe68f198ee354114b.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-14T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 公开课地址：https://class.coursera.org/ml-003/class/index&nbsp; 授课老师：Andrew&nbsp;Ng 1、model&nbsp;representation(建立模型) 考虑一个问题，如果给定一些房屋售价和房屋面积的数据，现在要预测给定其他面积时的房屋售价，那该怎么办？其实这是一个线性回归问题，给定的数据作为训练样本，用其训练得到一个表示售价和面积关系的模型（其实是一个函数），然后用这个函数进行预测。基本流程如下： 2、cost&nbsp;function(代价函数) ps:其实这一节中代价函数并未出现代价函数 既然我们已经明确了只需要训练得到一个函数，那我们首先要做的就是对函数的形式进行假设。这里我们可以假设是最简单的线性函数： 下面问题就转变为如何求出theta值。由于我们已经有一些训练数据，虽然不知道数据之间是不是像假设那样成线性关系，但存在一点偏差我们并不介意，只需要让函数值尽可能接近真实值即可。这里我们让x表示面积，y表示售价，在二维平面上描出一系列的点。 3、cost&nbsp;function&nbsp;intuition&nbsp;1(代价函数初步1) 前面已经说明了需要让h函数值和真实值之间误差尽可能小，这里给出一个更清晰的说明： 代价函数J就代表了上面提到的误差，这里为了后面求导方便函数被写成了这种形式，我们的目标就是让代价函数J值最小，注意在J中theta变成了变量，m代表训练样本的数目（也就是坐标系中点的个数）。 4、cost&nbsp;function&nbsp;intuition&nbsp;2(代价函数初步2) 在给定一个theta0和theta1时，我们能找到相应的h函数（一条线）和代价J（一个值），当把这两者放在一起对比观察时，就能清楚的看到代价值在h函数和样本点拟合的很好时是最小的，而且存在全局唯一的最小值。代价函数J用三维表示大概就如下图所示： 从上图可以看出，此时h函数与样本点拟合的情况最好，而代价函数J也取到了最小值，theta0和theta1此时的取值可以从J的横纵坐标上看出。 5、gradient&nbsp;descent(梯度下降) 为了让J最小，我们的想法是通过改变theta的取值来改变J的取值。这里对theta的初始值不做要求，只考虑改变时的变化。梯度下降在这里的意思也就是沿着J函数的梯度方向改变theta让J取值减小，形象化的表示如下： 如果懂一点微积分知识，我们就可以把这个下降过程用数学式子像下面这样表达出来： 其中alpha是学习率（大于0），可以理解为每一次下降的步长，需要人为设定。 6、gradient&nbsp;descent&nbsp;intuition(梯度下降初步) 我们可以对上面梯度下降的公式进行一下简单的验证，如下图，当theta值过大时，梯度为正值，每一次迭代theta减小，J值也减小，同理theta值过小时，迭代让theta增大，J值也减小。所以梯度下降的思想是正确的。 相比之下，alpha的选择就不那么简单了，alpha选择要求适中，过大过小都不好： 如上图所说的，alpha取值太小步长太小，要走很多步才能下降到最小值，处理速度太慢。当alpha取值较大时，步长太大，会在最小值左右发生震荡，而永远也达不到最小值。不过，即使alpha取值适中，我们也可能会陷入到局部最小值，达不到全局最小。 补充一点，即使alpha值固定了，在梯度下降的过程中步长也会自动逐渐减小，所以我们不需要在函数逼近最小值的时候减小alpha的取值，以防止步长过大可能会跳过最低点。 7、gradient&nbsp;descent&nbsp;for&nbsp;linear&nbsp;regression(针对线性回归的梯度下降) 针对线性回归，由于我们已经有了代价函数J的形式，只需要代入即可： 写的更好看一点就是这样： 通过迭代，我们就能到达一个J的最小值，这里并不保证是全局最小。 --------------------------------------------弱弱的分割线-------------------------------------------------------------- 以上就是第一讲单变量线性回归的内容，思想还是很明确的。首先根据训练数据定义出模型函数形式，通过与真实值求误差得到代价函数，然后通过对代价函数梯度下降确定出模型函数的参数。确定好参数，下面就可以拿这个模型进行预测了，不过由于这里是线性的，变量只有一个，不是很准。自然下面就该引入多变量，非线性的情况了，那个肯定要复杂一些。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 再分享一下我老师大神的人工智能教程吧。零基础！通俗易懂！风趣幽默！还带黄段子！希望你也加入到我们人工智能的队伍中来！https://blog.csdn.net/jiangjunshow","@type":"BlogPosting","url":"https://mlh.app/2019/01/14/c9104c307f1bc6dfe68f198ee354114b.html","headline":"斯坦福机器学习公开课笔记 一 –单变量线性回归","dateModified":"2019-01-14T00:00:00+08:00","datePublished":"2019-01-14T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/01/14/c9104c307f1bc6dfe68f198ee354114b.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>斯坦福机器学习公开课笔记 一 --单变量线性回归</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div class="htmledit_views" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
   <p><br></p>
   <p><span>公开课地址：<a href="https://class.coursera.org/ml-003/class/index" rel="nofollow" target="_blank"><span>https://class.coursera.org/ml-003/class/index</span></a>&nbsp;</span></p>
   <p><span>授课老师：<span>Andrew&nbsp;Ng</span></span></p>
   <h2>1、model&nbsp;representation(<span>建立模型</span><span>)</span></h2>
   <p>考虑一个问题，如果给定一些房屋售价和房屋面积的数据，现在要预测给定其他面积时的房屋售价，那该怎么办？其实这是一个线性回归问题，给定的数据作为训练样本，用其训练得到一个表示售价和面积关系的模型（其实是一个函数），然后用这个函数进行预测。基本流程如下：</p>
   <p><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122510310"><br></p>
   <h2>2、cost&nbsp;function(<span>代价函数</span><span>)</span></h2>
   <p><span><span>ps:</span><span>其实这一节中代价函数并未出现代价函数</span></span></p>
   <p>既然我们已经明确了只需要训练得到一个函数，那我们首先要做的就是对函数的形式进行假设。这里我们可以假设是最简单的线性函数：</p>
   <p><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122553715"><br></p>
   <p></p>
   <p>下面问题就转变为如何求出<span>theta</span><span>值。由于我们已经有一些训练数据，虽然不知道数据之间是不是像假设那样成线性关系，但存在一点偏差我们并不介意，只需要让函数值尽可能接近真实值即可。这里我们让</span><span>x</span><span>表示面积，</span><span>y</span><span>表示售价，在二维平面上描出一系列的点。</span></p>
   <p><span><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122608558"><br></span></p>
   <p></p>
   <h2>3、cost&nbsp;function&nbsp;intuition&nbsp;1(<span>代价函数初步</span><span>1)</span></h2>
   <p>前面已经说明了需要让<span>h</span><span>函数值和真实值之间误差尽可能小，这里给出一个更清晰的说明：</span></p>
   <p><span><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122638651"><br></span></p>
   <p></p>
   <p></p>
   <p>代价函数<span>J</span><span>就代表了上面提到的误差，这里为了后面求导方便函数被写成了这种形式，我们的目标就是让代价函数</span><span>J</span><span>值最小，注意在</span><span>J</span><span>中</span><span>theta</span><span>变成了变量，</span><span>m</span><span>代表训练样本的数目（也就是坐标系中点的个数）。</span></p>
   <h2>4、cost&nbsp;function&nbsp;intuition&nbsp;2(<span>代价函数初步</span><span>2)</span></h2>
   <p>在给定一个<span>theta0</span><span>和</span><span>theta1</span><span>时，我们能找到相应的</span><span>h</span><span>函数（一条线）和代价</span><span>J</span><span>（一个值），当把这两者放在一起对比观察时，就能清楚的看到代价值在</span><span>h</span><span>函数和样本点拟合的很好时是最小的，而且存在全局唯一的最小值。代价函数</span><span>J</span><span>用三维表示大概就如下图所示：</span></p>
   <p><span><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122704010"><br></span></p>
   <p><span><br></span></p>
   <p><span><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122721900"><br></span></p>
   <p></p>
   <p></p>
   <p>从上图可以看出，此时<span>h</span><span>函数与样本点拟合的情况最好，而代价函数</span><span>J</span><span>也取到了最小值，</span><span>theta0</span><span>和</span><span>theta1</span><span>此时的取值可以从</span><span>J</span><span>的横纵坐标上看出。</span></p>
   <h2>5、gradient&nbsp;descent(<span>梯度下降</span><span>)</span></h2>
   <p>为了让<span>J</span><span>最小，我们的想法是通过改变</span><span>theta</span><span>的取值来改变</span><span>J</span><span>的取值。这里对</span><span>theta</span><span>的初始值不做要求，只考虑改变时的变化。梯度下降在这里的意思也就是沿着</span><span>J</span><span>函数的梯度方向改变</span><span>theta</span><span>让</span><span>J</span><span>取值减小，形象化的表示如下：</span></p>
   <p><span><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122735290"><br></span></p>
   <p></p>
   <p>如果懂一点微积分知识，我们就可以把这个下降过程用数学式子像下面这样表达出来：</p>
   <p><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122744118"><br></p>
   <p></p>
   <p>其中<span>alpha</span><span>是学习率（大于</span><span>0</span><span>），可以理解为每一次下降的步长，需要人为设定。</span></p>
   <h2>6、gradient&nbsp;descent&nbsp;intuition(<span>梯度下降初步</span><span>)</span></h2>
   <p>我们可以对上面梯度下降的公式进行一下简单的验证，如下图，当<span>theta</span><span>值过大时，梯度为正值，每一次迭代</span><span>theta</span><span>减小，</span><span>J</span><span>值也减小，同理</span><span>theta</span><span>值过小时，迭代让</span><span>theta</span><span>增大，</span><span>J</span><span>值也减小。所以梯度下降的思想是正确的。</span></p>
   <p><span><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122808633"><br></span></p>
   <p></p>
   <p>相比之下，<span>alpha</span><span>的选择就不那么简单了，</span><span>alpha</span><span>选择要求适中，过大过小都不好：</span></p>
   <p><span><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122829085"><br></span></p>
   <p></p>
   <p>如上图所说的，<span>alpha</span><span>取值太小步长太小，要走很多步才能下降到最小值，处理速度太慢。当</span><span>alpha</span><span>取值较大时，步长太大，会在最小值左右发生震荡，而永远也达不到最小值。不过，即使</span><span>alpha</span><span>取值适中，我们也可能会陷入到局部最小值，达不到全局最小。</span></p>
   <p><span><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122901381"><br></span></p>
   <p></p>
   <p>补充一点，即使<span>alpha</span><span>值固定了，在梯度下降的过程中步长也会自动逐渐减小，所以我们不需要在函数逼近最小值的时候减小</span><span>alpha</span><span>的取值，以防止步长过大可能会跳过最低点。</span></p>
   <h2>7、gradient&nbsp;descent&nbsp;for&nbsp;linear&nbsp;regression(<span>针对线性回归的梯度下降</span><span>)</span></h2>
   <p>针对线性回归，由于我们已经有了代价函数<span>J</span><span>的形式，只需要代入即可：</span></p>
   <p><span><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122919006"><br></span></p>
   <p></p>
   <p>写的更好看一点就是这样：</p>
   <p><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122927052"><br></p>
   <p></p>
   <p>通过迭代，我们就能到达一个<span>J</span><span>的最小值，这里并不保证是全局最小。</span></p>
   <p><span><img alt="" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20130524122955146"><br></span></p>
   <p></p>
   <p></p>
   <p><br></p>
   <p><span>--------------------------------------------弱弱的分割线--------------------------------------------------------------</span></p>
   <p><span>以上就是第一讲单变量线性回归的内容，思想还是很明确的。首先根据训练数据定义出模型函数形式，通过与真实值求误差得到代价函数，然后通过对代价函数梯度下降确定出模型函数的参数。确定好参数，下面就可以拿这个模型进行预测了，不过由于这里是线性的，变量只有一个，不是很准。自然下面就该引入多变量，非线性的情况了，那个肯定要复杂一些。</span></p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div>
  <p>再分享一下我老师大神的人工智能教程吧。零基础！通俗易懂！风趣幽默！还带黄段子！希望你也加入到我们人工智能的队伍中来！<a href="https://blog.csdn.net/jiangjunshow/article/details/77338485" rel="nofollow">https://blog.csdn.net/jiangjunshow</a></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
