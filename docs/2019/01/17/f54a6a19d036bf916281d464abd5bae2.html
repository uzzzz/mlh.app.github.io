<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>工具篇Flair之训练模型教程 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="工具篇Flair之训练模型教程" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="版权声明：转载请注明出处，谢谢~~ https://blog.csdn.net/m0_37306360/article/details/86527728 更多实时更新的个人学习笔记分享，请关注： 知乎：https://www.zhihu.com/people/yuquanle/columns 微信订阅号：AI小白入门 ID: StudyForAI Flair工具使用教程之如何训练自己的模型 教程地址：https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md 由于我本地电脑网速太差，预训词向量没下载下来，所以本文为翻译学习，基本上按照这个过程跑起来是没问题的~ 本教程的这一部分展示了如何使用最先进的单词嵌入来训练您自己的序列标签和文本分类模型。 训练序列标记模型 下面是使用简单的GloVe嵌入对CoNLL-03数据训练的小型NER模型的示例代码。 要运行此代码，首先需要获取CoNLL-03英语数据集（或者，使用NLPTaskDataFetcher.load_corpus（NLPTask.WNUT）代替具有免费可用数据的任务）。 作者提供复现他论文效果的代码：zalandoresearch/flair 在此示例中，我们将数据下采样到原始数据的10％： from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings from typing import List # 1. get the corpus corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03).downsample(0.1) print(corpus) # 2. what tag do we want to predict? tag_type = &#39;ner&#39; # 3. make the tag dictionary from the corpus tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type) print(tag_dictionary.idx2item) # 4. initialize embeddings embedding_types: List[TokenEmbeddings] = [ WordEmbeddings(&#39;glove&#39;), # comment in this line to use character embeddings # CharacterEmbeddings(), # comment in these lines to use flair embeddings # FlairEmbeddings(&#39;news-forward&#39;), # FlairEmbeddings(&#39;news-backward&#39;), ] embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types) # 5. initialize sequence tagger from flair.models import SequenceTagger tagger: SequenceTagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type, use_crf=True) # 6. initialize trainer from flair.trainers import ModelTrainer trainer: ModelTrainer = ModelTrainer(tagger, corpus) # 7. start training trainer.train(&#39;resources/taggers/example-ner&#39;, learning_rate=0.1, mini_batch_size=32, max_epochs=150) # 8. plot training curves (optional) from flair.visual.training_curves import Plotter plotter = Plotter() plotter.plot_training_curves(&#39;resources/taggers/example-ner/loss.tsv&#39;) plotter.plot_weights(&#39;resources/taggers/example-ner/weights.txt&#39;) 训练文本分类模型 下面是使用简单的GloVe嵌入和Flair嵌入的组合，在AGNews语料库上训练文本分类器的示例代码。 您需要先下载AGNews才能运行此代码。 AGNews语料库下载地址AG’s corpus of news articles： 在此示例中，我们将数据下采样到原始数据的10％。 from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings from flair.models import TextClassifier from flair.trainers import ModelTrainer # 1. get the corpus corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.AG_NEWS, &#39;path/to/data/folder&#39;).downsample(0.1) # 2. create the label dictionary label_dict = corpus.make_label_dictionary() # 3. make a list of word embeddings word_embeddings = [WordEmbeddings(&#39;glove&#39;), # comment in flair embeddings for state-of-the-art results # FlairEmbeddings(&#39;news-forward&#39;), # FlairEmbeddings(&#39;news-backward&#39;), ] # 4. init document embedding by passing list of word embeddings document_embeddings: DocumentLSTMEmbeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256, ) # 5. create the text classifier classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, multi_label=False) # 6. initialize the text classifier trainer trainer = ModelTrainer(classifier, corpus) # 7. start the training trainer.train(&#39;resources/taggers/ag_news&#39;, learning_rate=0.1, mini_batch_size=32, anneal_factor=0.5, patience=5, max_epochs=150) # 8. plot training curves (optional) from flair.visual.training_curves import Plotter plotter = Plotter() plotter.plot_training_curves(&#39;resources/taggers/ag_news/loss.tsv&#39;) plotter.plot_weights(&#39;resources/taggers/ag_news/weights.txt&#39;) 训练模型后，您可以加载它来预测新句子的类别。 只需调用模型的预测(predict)方法即可。 classifier = TextClassifier.load_from_file(&#39;resources/taggers/ag_news/final-model.pt&#39;) # create example sentence sentence = Sentence(&#39;France is the current world cup winner.&#39;) # predict tags and print classifier.predict(sentence) print(sentence.labels) 多数据集训练 现在，让我们训练一个可以用英语和德语标记PoS标签文本的模型。 为此，我们加载英语和德语UD语料库并创建MultiCorpus对象。 我们还使用新的多语言Flair嵌入来完成此任务。 请注意，这里我们使用MICRO_ACCURACY评估指标而不是默认的MICRO_F1_SCORE。 所有其余的都和以前一样，例如： from typing import List from flair.data import MultiCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import FlairEmbeddings, TokenEmbeddings, StackedEmbeddings from flair.training_utils import EvaluationMetric # 1. get the corpora - English and German UD corpus: MultiCorpus = NLPTaskDataFetcher.load_corpora([NLPTask.UD_ENGLISH, NLPTask.UD_GERMAN]).downsample(0.1) # 2. what tag do we want to predict? tag_type = &#39;upos&#39; # 3. make the tag dictionary from the corpus tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type) print(tag_dictionary.idx2item) # 4. initialize embeddings embedding_types: List[TokenEmbeddings] = [ # we use multilingual Flair embeddings in this task FlairEmbeddings(&#39;multi-forward&#39;), FlairEmbeddings(&#39;multi-backward&#39;), ] embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types) # 5. initialize sequence tagger from flair.models import SequenceTagger tagger: SequenceTagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type, use_crf=True) # 6. initialize trainer from flair.trainers import ModelTrainer trainer: ModelTrainer = ModelTrainer(tagger, corpus) # 7. start training trainer.train(&#39;resources/taggers/example-universal-pos&#39;, learning_rate=0.1, mini_batch_size=32, max_epochs=150, evaluation_metric=EvaluationMetric.MICRO_ACCURACY) 绘制训练曲线和权重 Flair包括一个辅助方法，用于绘制神经网络中的训练曲线和权重。 ModelTrainer自动在结果文件夹中生成loss.tsv和weights.txt文件。 训练之后，简单地将绘图仪指向这些文件。 这会在结果文件夹中生成PNG图。 from flair.visual.training_curves import Plotter plotter = Plotter() plotter.plot_training_curves(&#39;loss.tsv&#39;) plotter.plot_weights(&#39;weights.txt&#39;) 恢复训练 如果要在某个时刻停止训练并稍后恢复训练，则应将参数检查点设置为True进行训练。 这将在每个epoch后保存模型和训练参数。 因此，您可以在以后任何时候加载模型和trainer，并在您离开的地方继续训练。 下面的示例代码显示了如何训练，停止和继续训练SequenceTagger。 TextClassifier也可以这样做。 from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings from typing import List # 1. get the corpus corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03).downsample(0.1) # 2. what tag do we want to predict? tag_type = &#39;ner&#39; # 3. make the tag dictionary from the corpus tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type) # 4. initialize embeddings embedding_types: List[TokenEmbeddings] = [ WordEmbeddings(&#39;glove&#39;) ] embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types) # 5. initialize sequence tagger from flair.models import SequenceTagger tagger: SequenceTagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type, use_crf=True) # 6. initialize trainer from flair.trainers import ModelTrainer from flair.training_utils import EvaluationMetric trainer: ModelTrainer = ModelTrainer(tagger, corpus) # 7. start training trainer.train(&#39;resources/taggers/example-ner&#39;, EvaluationMetric.MICRO_F1_SCORE, learning_rate=0.1, mini_batch_size=32, max_epochs=150, checkpoint=True) # 8. stop training at any point # 9. continue trainer at later point from pathlib import Path trainer = ModelTrainer.load_from_checkpoint(Path(&#39;resources/taggers/example-ner/checkpoint.pt&#39;), &#39;SequenceTagger&#39;, corpus) trainer.train(&#39;resources/taggers/example-ner&#39;, EvaluationMetric.MICRO_F1_SCORE, learning_rate=0.1, mini_batch_size=32, max_epochs=150, checkpoint=True) 可扩展性：大数据集训练 使用FlairEmbeddings（你应该）时要考虑的主要事情是，为大型训练数据集生成它们的成本有些高。根据您的设置，您可以设置选项以优化培训时间。有三个问题要问： 1.你有GPU吗？ CharLMEmbeddings使用Pytorch RNN生成，因此针对GPU进行了优化。如果您有一个，您可以设置大型小批量大小以使用批处理。如果没有，您可能想要使用较小的语言模型。对于英语，我们打包嵌入式的“快速”变体，可加载如下：FlairEmbeddings（‘news-forward-fast’）。 2.整个数据集的嵌入是否适合内存？ 在最佳情况下，数据集的所有嵌入都适合您的常规内存，这极大地提高了训练速度。如果不是这种情况，则必须在相应的培训师（即ModelTrainer）中设置标志embeddings_in_memory = False以避免内存问题。使用该标志，嵌入是（a）在每个纪元重新计算或（b）从磁盘检索，如果您选择实现磁盘。 3.你有快速硬盘吗？ 如果您有快速硬盘驱动器，请考虑将嵌入物实现到磁盘。您可以通过以下方式实现我的实例化FlairEmbedding：FlairEmbeddings（‘news-forward-fast’，use_cache = True）。如果嵌入不适合内存，这可能会有所帮助。此外，如果您没有GPU并且想要对同一数据集进行重复实验，这会有所帮助，因为嵌入只需要计算一次，然后始终从磁盘中检索。" />
<meta property="og:description" content="版权声明：转载请注明出处，谢谢~~ https://blog.csdn.net/m0_37306360/article/details/86527728 更多实时更新的个人学习笔记分享，请关注： 知乎：https://www.zhihu.com/people/yuquanle/columns 微信订阅号：AI小白入门 ID: StudyForAI Flair工具使用教程之如何训练自己的模型 教程地址：https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md 由于我本地电脑网速太差，预训词向量没下载下来，所以本文为翻译学习，基本上按照这个过程跑起来是没问题的~ 本教程的这一部分展示了如何使用最先进的单词嵌入来训练您自己的序列标签和文本分类模型。 训练序列标记模型 下面是使用简单的GloVe嵌入对CoNLL-03数据训练的小型NER模型的示例代码。 要运行此代码，首先需要获取CoNLL-03英语数据集（或者，使用NLPTaskDataFetcher.load_corpus（NLPTask.WNUT）代替具有免费可用数据的任务）。 作者提供复现他论文效果的代码：zalandoresearch/flair 在此示例中，我们将数据下采样到原始数据的10％： from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings from typing import List # 1. get the corpus corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03).downsample(0.1) print(corpus) # 2. what tag do we want to predict? tag_type = &#39;ner&#39; # 3. make the tag dictionary from the corpus tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type) print(tag_dictionary.idx2item) # 4. initialize embeddings embedding_types: List[TokenEmbeddings] = [ WordEmbeddings(&#39;glove&#39;), # comment in this line to use character embeddings # CharacterEmbeddings(), # comment in these lines to use flair embeddings # FlairEmbeddings(&#39;news-forward&#39;), # FlairEmbeddings(&#39;news-backward&#39;), ] embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types) # 5. initialize sequence tagger from flair.models import SequenceTagger tagger: SequenceTagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type, use_crf=True) # 6. initialize trainer from flair.trainers import ModelTrainer trainer: ModelTrainer = ModelTrainer(tagger, corpus) # 7. start training trainer.train(&#39;resources/taggers/example-ner&#39;, learning_rate=0.1, mini_batch_size=32, max_epochs=150) # 8. plot training curves (optional) from flair.visual.training_curves import Plotter plotter = Plotter() plotter.plot_training_curves(&#39;resources/taggers/example-ner/loss.tsv&#39;) plotter.plot_weights(&#39;resources/taggers/example-ner/weights.txt&#39;) 训练文本分类模型 下面是使用简单的GloVe嵌入和Flair嵌入的组合，在AGNews语料库上训练文本分类器的示例代码。 您需要先下载AGNews才能运行此代码。 AGNews语料库下载地址AG’s corpus of news articles： 在此示例中，我们将数据下采样到原始数据的10％。 from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings from flair.models import TextClassifier from flair.trainers import ModelTrainer # 1. get the corpus corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.AG_NEWS, &#39;path/to/data/folder&#39;).downsample(0.1) # 2. create the label dictionary label_dict = corpus.make_label_dictionary() # 3. make a list of word embeddings word_embeddings = [WordEmbeddings(&#39;glove&#39;), # comment in flair embeddings for state-of-the-art results # FlairEmbeddings(&#39;news-forward&#39;), # FlairEmbeddings(&#39;news-backward&#39;), ] # 4. init document embedding by passing list of word embeddings document_embeddings: DocumentLSTMEmbeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256, ) # 5. create the text classifier classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, multi_label=False) # 6. initialize the text classifier trainer trainer = ModelTrainer(classifier, corpus) # 7. start the training trainer.train(&#39;resources/taggers/ag_news&#39;, learning_rate=0.1, mini_batch_size=32, anneal_factor=0.5, patience=5, max_epochs=150) # 8. plot training curves (optional) from flair.visual.training_curves import Plotter plotter = Plotter() plotter.plot_training_curves(&#39;resources/taggers/ag_news/loss.tsv&#39;) plotter.plot_weights(&#39;resources/taggers/ag_news/weights.txt&#39;) 训练模型后，您可以加载它来预测新句子的类别。 只需调用模型的预测(predict)方法即可。 classifier = TextClassifier.load_from_file(&#39;resources/taggers/ag_news/final-model.pt&#39;) # create example sentence sentence = Sentence(&#39;France is the current world cup winner.&#39;) # predict tags and print classifier.predict(sentence) print(sentence.labels) 多数据集训练 现在，让我们训练一个可以用英语和德语标记PoS标签文本的模型。 为此，我们加载英语和德语UD语料库并创建MultiCorpus对象。 我们还使用新的多语言Flair嵌入来完成此任务。 请注意，这里我们使用MICRO_ACCURACY评估指标而不是默认的MICRO_F1_SCORE。 所有其余的都和以前一样，例如： from typing import List from flair.data import MultiCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import FlairEmbeddings, TokenEmbeddings, StackedEmbeddings from flair.training_utils import EvaluationMetric # 1. get the corpora - English and German UD corpus: MultiCorpus = NLPTaskDataFetcher.load_corpora([NLPTask.UD_ENGLISH, NLPTask.UD_GERMAN]).downsample(0.1) # 2. what tag do we want to predict? tag_type = &#39;upos&#39; # 3. make the tag dictionary from the corpus tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type) print(tag_dictionary.idx2item) # 4. initialize embeddings embedding_types: List[TokenEmbeddings] = [ # we use multilingual Flair embeddings in this task FlairEmbeddings(&#39;multi-forward&#39;), FlairEmbeddings(&#39;multi-backward&#39;), ] embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types) # 5. initialize sequence tagger from flair.models import SequenceTagger tagger: SequenceTagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type, use_crf=True) # 6. initialize trainer from flair.trainers import ModelTrainer trainer: ModelTrainer = ModelTrainer(tagger, corpus) # 7. start training trainer.train(&#39;resources/taggers/example-universal-pos&#39;, learning_rate=0.1, mini_batch_size=32, max_epochs=150, evaluation_metric=EvaluationMetric.MICRO_ACCURACY) 绘制训练曲线和权重 Flair包括一个辅助方法，用于绘制神经网络中的训练曲线和权重。 ModelTrainer自动在结果文件夹中生成loss.tsv和weights.txt文件。 训练之后，简单地将绘图仪指向这些文件。 这会在结果文件夹中生成PNG图。 from flair.visual.training_curves import Plotter plotter = Plotter() plotter.plot_training_curves(&#39;loss.tsv&#39;) plotter.plot_weights(&#39;weights.txt&#39;) 恢复训练 如果要在某个时刻停止训练并稍后恢复训练，则应将参数检查点设置为True进行训练。 这将在每个epoch后保存模型和训练参数。 因此，您可以在以后任何时候加载模型和trainer，并在您离开的地方继续训练。 下面的示例代码显示了如何训练，停止和继续训练SequenceTagger。 TextClassifier也可以这样做。 from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings from typing import List # 1. get the corpus corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03).downsample(0.1) # 2. what tag do we want to predict? tag_type = &#39;ner&#39; # 3. make the tag dictionary from the corpus tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type) # 4. initialize embeddings embedding_types: List[TokenEmbeddings] = [ WordEmbeddings(&#39;glove&#39;) ] embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types) # 5. initialize sequence tagger from flair.models import SequenceTagger tagger: SequenceTagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type, use_crf=True) # 6. initialize trainer from flair.trainers import ModelTrainer from flair.training_utils import EvaluationMetric trainer: ModelTrainer = ModelTrainer(tagger, corpus) # 7. start training trainer.train(&#39;resources/taggers/example-ner&#39;, EvaluationMetric.MICRO_F1_SCORE, learning_rate=0.1, mini_batch_size=32, max_epochs=150, checkpoint=True) # 8. stop training at any point # 9. continue trainer at later point from pathlib import Path trainer = ModelTrainer.load_from_checkpoint(Path(&#39;resources/taggers/example-ner/checkpoint.pt&#39;), &#39;SequenceTagger&#39;, corpus) trainer.train(&#39;resources/taggers/example-ner&#39;, EvaluationMetric.MICRO_F1_SCORE, learning_rate=0.1, mini_batch_size=32, max_epochs=150, checkpoint=True) 可扩展性：大数据集训练 使用FlairEmbeddings（你应该）时要考虑的主要事情是，为大型训练数据集生成它们的成本有些高。根据您的设置，您可以设置选项以优化培训时间。有三个问题要问： 1.你有GPU吗？ CharLMEmbeddings使用Pytorch RNN生成，因此针对GPU进行了优化。如果您有一个，您可以设置大型小批量大小以使用批处理。如果没有，您可能想要使用较小的语言模型。对于英语，我们打包嵌入式的“快速”变体，可加载如下：FlairEmbeddings（‘news-forward-fast’）。 2.整个数据集的嵌入是否适合内存？ 在最佳情况下，数据集的所有嵌入都适合您的常规内存，这极大地提高了训练速度。如果不是这种情况，则必须在相应的培训师（即ModelTrainer）中设置标志embeddings_in_memory = False以避免内存问题。使用该标志，嵌入是（a）在每个纪元重新计算或（b）从磁盘检索，如果您选择实现磁盘。 3.你有快速硬盘吗？ 如果您有快速硬盘驱动器，请考虑将嵌入物实现到磁盘。您可以通过以下方式实现我的实例化FlairEmbedding：FlairEmbeddings（‘news-forward-fast’，use_cache = True）。如果嵌入不适合内存，这可能会有所帮助。此外，如果您没有GPU并且想要对同一数据集进行重复实验，这会有所帮助，因为嵌入只需要计算一次，然后始终从磁盘中检索。" />
<link rel="canonical" href="https://mlh.app/2019/01/17/f54a6a19d036bf916281d464abd5bae2.html" />
<meta property="og:url" content="https://mlh.app/2019/01/17/f54a6a19d036bf916281d464abd5bae2.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-17T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"版权声明：转载请注明出处，谢谢~~ https://blog.csdn.net/m0_37306360/article/details/86527728 更多实时更新的个人学习笔记分享，请关注： 知乎：https://www.zhihu.com/people/yuquanle/columns 微信订阅号：AI小白入门 ID: StudyForAI Flair工具使用教程之如何训练自己的模型 教程地址：https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md 由于我本地电脑网速太差，预训词向量没下载下来，所以本文为翻译学习，基本上按照这个过程跑起来是没问题的~ 本教程的这一部分展示了如何使用最先进的单词嵌入来训练您自己的序列标签和文本分类模型。 训练序列标记模型 下面是使用简单的GloVe嵌入对CoNLL-03数据训练的小型NER模型的示例代码。 要运行此代码，首先需要获取CoNLL-03英语数据集（或者，使用NLPTaskDataFetcher.load_corpus（NLPTask.WNUT）代替具有免费可用数据的任务）。 作者提供复现他论文效果的代码：zalandoresearch/flair 在此示例中，我们将数据下采样到原始数据的10％： from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings from typing import List # 1. get the corpus corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03).downsample(0.1) print(corpus) # 2. what tag do we want to predict? tag_type = &#39;ner&#39; # 3. make the tag dictionary from the corpus tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type) print(tag_dictionary.idx2item) # 4. initialize embeddings embedding_types: List[TokenEmbeddings] = [ WordEmbeddings(&#39;glove&#39;), # comment in this line to use character embeddings # CharacterEmbeddings(), # comment in these lines to use flair embeddings # FlairEmbeddings(&#39;news-forward&#39;), # FlairEmbeddings(&#39;news-backward&#39;), ] embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types) # 5. initialize sequence tagger from flair.models import SequenceTagger tagger: SequenceTagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type, use_crf=True) # 6. initialize trainer from flair.trainers import ModelTrainer trainer: ModelTrainer = ModelTrainer(tagger, corpus) # 7. start training trainer.train(&#39;resources/taggers/example-ner&#39;, learning_rate=0.1, mini_batch_size=32, max_epochs=150) # 8. plot training curves (optional) from flair.visual.training_curves import Plotter plotter = Plotter() plotter.plot_training_curves(&#39;resources/taggers/example-ner/loss.tsv&#39;) plotter.plot_weights(&#39;resources/taggers/example-ner/weights.txt&#39;) 训练文本分类模型 下面是使用简单的GloVe嵌入和Flair嵌入的组合，在AGNews语料库上训练文本分类器的示例代码。 您需要先下载AGNews才能运行此代码。 AGNews语料库下载地址AG’s corpus of news articles： 在此示例中，我们将数据下采样到原始数据的10％。 from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings from flair.models import TextClassifier from flair.trainers import ModelTrainer # 1. get the corpus corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.AG_NEWS, &#39;path/to/data/folder&#39;).downsample(0.1) # 2. create the label dictionary label_dict = corpus.make_label_dictionary() # 3. make a list of word embeddings word_embeddings = [WordEmbeddings(&#39;glove&#39;), # comment in flair embeddings for state-of-the-art results # FlairEmbeddings(&#39;news-forward&#39;), # FlairEmbeddings(&#39;news-backward&#39;), ] # 4. init document embedding by passing list of word embeddings document_embeddings: DocumentLSTMEmbeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256, ) # 5. create the text classifier classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, multi_label=False) # 6. initialize the text classifier trainer trainer = ModelTrainer(classifier, corpus) # 7. start the training trainer.train(&#39;resources/taggers/ag_news&#39;, learning_rate=0.1, mini_batch_size=32, anneal_factor=0.5, patience=5, max_epochs=150) # 8. plot training curves (optional) from flair.visual.training_curves import Plotter plotter = Plotter() plotter.plot_training_curves(&#39;resources/taggers/ag_news/loss.tsv&#39;) plotter.plot_weights(&#39;resources/taggers/ag_news/weights.txt&#39;) 训练模型后，您可以加载它来预测新句子的类别。 只需调用模型的预测(predict)方法即可。 classifier = TextClassifier.load_from_file(&#39;resources/taggers/ag_news/final-model.pt&#39;) # create example sentence sentence = Sentence(&#39;France is the current world cup winner.&#39;) # predict tags and print classifier.predict(sentence) print(sentence.labels) 多数据集训练 现在，让我们训练一个可以用英语和德语标记PoS标签文本的模型。 为此，我们加载英语和德语UD语料库并创建MultiCorpus对象。 我们还使用新的多语言Flair嵌入来完成此任务。 请注意，这里我们使用MICRO_ACCURACY评估指标而不是默认的MICRO_F1_SCORE。 所有其余的都和以前一样，例如： from typing import List from flair.data import MultiCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import FlairEmbeddings, TokenEmbeddings, StackedEmbeddings from flair.training_utils import EvaluationMetric # 1. get the corpora - English and German UD corpus: MultiCorpus = NLPTaskDataFetcher.load_corpora([NLPTask.UD_ENGLISH, NLPTask.UD_GERMAN]).downsample(0.1) # 2. what tag do we want to predict? tag_type = &#39;upos&#39; # 3. make the tag dictionary from the corpus tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type) print(tag_dictionary.idx2item) # 4. initialize embeddings embedding_types: List[TokenEmbeddings] = [ # we use multilingual Flair embeddings in this task FlairEmbeddings(&#39;multi-forward&#39;), FlairEmbeddings(&#39;multi-backward&#39;), ] embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types) # 5. initialize sequence tagger from flair.models import SequenceTagger tagger: SequenceTagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type, use_crf=True) # 6. initialize trainer from flair.trainers import ModelTrainer trainer: ModelTrainer = ModelTrainer(tagger, corpus) # 7. start training trainer.train(&#39;resources/taggers/example-universal-pos&#39;, learning_rate=0.1, mini_batch_size=32, max_epochs=150, evaluation_metric=EvaluationMetric.MICRO_ACCURACY) 绘制训练曲线和权重 Flair包括一个辅助方法，用于绘制神经网络中的训练曲线和权重。 ModelTrainer自动在结果文件夹中生成loss.tsv和weights.txt文件。 训练之后，简单地将绘图仪指向这些文件。 这会在结果文件夹中生成PNG图。 from flair.visual.training_curves import Plotter plotter = Plotter() plotter.plot_training_curves(&#39;loss.tsv&#39;) plotter.plot_weights(&#39;weights.txt&#39;) 恢复训练 如果要在某个时刻停止训练并稍后恢复训练，则应将参数检查点设置为True进行训练。 这将在每个epoch后保存模型和训练参数。 因此，您可以在以后任何时候加载模型和trainer，并在您离开的地方继续训练。 下面的示例代码显示了如何训练，停止和继续训练SequenceTagger。 TextClassifier也可以这样做。 from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher, NLPTask from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings from typing import List # 1. get the corpus corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03).downsample(0.1) # 2. what tag do we want to predict? tag_type = &#39;ner&#39; # 3. make the tag dictionary from the corpus tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type) # 4. initialize embeddings embedding_types: List[TokenEmbeddings] = [ WordEmbeddings(&#39;glove&#39;) ] embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types) # 5. initialize sequence tagger from flair.models import SequenceTagger tagger: SequenceTagger = SequenceTagger(hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, tag_type=tag_type, use_crf=True) # 6. initialize trainer from flair.trainers import ModelTrainer from flair.training_utils import EvaluationMetric trainer: ModelTrainer = ModelTrainer(tagger, corpus) # 7. start training trainer.train(&#39;resources/taggers/example-ner&#39;, EvaluationMetric.MICRO_F1_SCORE, learning_rate=0.1, mini_batch_size=32, max_epochs=150, checkpoint=True) # 8. stop training at any point # 9. continue trainer at later point from pathlib import Path trainer = ModelTrainer.load_from_checkpoint(Path(&#39;resources/taggers/example-ner/checkpoint.pt&#39;), &#39;SequenceTagger&#39;, corpus) trainer.train(&#39;resources/taggers/example-ner&#39;, EvaluationMetric.MICRO_F1_SCORE, learning_rate=0.1, mini_batch_size=32, max_epochs=150, checkpoint=True) 可扩展性：大数据集训练 使用FlairEmbeddings（你应该）时要考虑的主要事情是，为大型训练数据集生成它们的成本有些高。根据您的设置，您可以设置选项以优化培训时间。有三个问题要问： 1.你有GPU吗？ CharLMEmbeddings使用Pytorch RNN生成，因此针对GPU进行了优化。如果您有一个，您可以设置大型小批量大小以使用批处理。如果没有，您可能想要使用较小的语言模型。对于英语，我们打包嵌入式的“快速”变体，可加载如下：FlairEmbeddings（‘news-forward-fast’）。 2.整个数据集的嵌入是否适合内存？ 在最佳情况下，数据集的所有嵌入都适合您的常规内存，这极大地提高了训练速度。如果不是这种情况，则必须在相应的培训师（即ModelTrainer）中设置标志embeddings_in_memory = False以避免内存问题。使用该标志，嵌入是（a）在每个纪元重新计算或（b）从磁盘检索，如果您选择实现磁盘。 3.你有快速硬盘吗？ 如果您有快速硬盘驱动器，请考虑将嵌入物实现到磁盘。您可以通过以下方式实现我的实例化FlairEmbedding：FlairEmbeddings（‘news-forward-fast’，use_cache = True）。如果嵌入不适合内存，这可能会有所帮助。此外，如果您没有GPU并且想要对同一数据集进行重复实验，这会有所帮助，因为嵌入只需要计算一次，然后始终从磁盘中检索。","@type":"BlogPosting","url":"https://mlh.app/2019/01/17/f54a6a19d036bf916281d464abd5bae2.html","headline":"工具篇Flair之训练模型教程","dateModified":"2019-01-17T00:00:00+08:00","datePublished":"2019-01-17T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/01/17/f54a6a19d036bf916281d464abd5bae2.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>工具篇Flair之训练模型教程</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div class="article-copyright">
   版权声明：转载请注明出处，谢谢~~ https://blog.csdn.net/m0_37306360/article/details/86527728 
 </div> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p><strong>更多实时更新的个人学习笔记分享，请关注：<br> 知乎：<a href="https://www.zhihu.com/people/yuquanle/columns" rel="nofollow">https://www.zhihu.com/people/yuquanle/columns</a><br> 微信订阅号：AI小白入门<br> ID: StudyForAI</strong></p> 
  <center> 
   <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190104163636941.jpg" width="66%"> 
  </center>
  <hr> 
  <hr> 
  <h2><a id="Flair_10"></a>Flair工具使用教程之如何训练自己的模型</h2> 
  <ul> 
   <li>教程地址：<a href="https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md" rel="nofollow">https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md</a></li> 
   <li>由于我本地电脑网速太差，预训词向量没下载下来，所以本文为翻译学习，基本上按照这个过程跑起来是没问题的~</li> 
   <li>本教程的这一部分展示了如何使用最先进的单词嵌入来训练您自己的序列标签和文本分类模型。</li> 
  </ul> 
  <hr> 
  <h2><a id="_18"></a>训练序列标记模型</h2> 
  <ul> 
   <li>下面是使用简单的GloVe嵌入对CoNLL-03数据训练的小型NER模型的示例代码。 要运行此代码，首先需要获取CoNLL-03英语数据集（或者，使用NLPTaskDataFetcher.load_corpus（NLPTask.WNUT）代替具有免费可用数据的任务）。</li> 
   <li>作者提供复现他论文效果的代码：zalandoresearch/flair</li> 
   <li>在此示例中，我们将数据下采样到原始数据的10％：</li> 
  </ul> 
  <pre><code>from flair.data import TaggedCorpus
from flair.data_fetcher import NLPTaskDataFetcher, NLPTask
from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings
from typing import List

# 1. get the corpus
corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03).downsample(0.1)
print(corpus)

# 2. what tag do we want to predict?
tag_type = 'ner'

# 3. make the tag dictionary from the corpus
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
print(tag_dictionary.idx2item)

# 4. initialize embeddings
embedding_types: List[TokenEmbeddings] = [

    WordEmbeddings('glove'),

    # comment in this line to use character embeddings
    # CharacterEmbeddings(),

    # comment in these lines to use flair embeddings
    # FlairEmbeddings('news-forward'),
    # FlairEmbeddings('news-backward'),
]

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

# 5. initialize sequence tagger
from flair.models import SequenceTagger

tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        use_crf=True)

# 6. initialize trainer
from flair.trainers import ModelTrainer

trainer: ModelTrainer = ModelTrainer(tagger, corpus)

# 7. start training
trainer.train('resources/taggers/example-ner',
              learning_rate=0.1,
              mini_batch_size=32,
              max_epochs=150)

# 8. plot training curves (optional)
from flair.visual.training_curves import Plotter
plotter = Plotter()
plotter.plot_training_curves('resources/taggers/example-ner/loss.tsv')
plotter.plot_weights('resources/taggers/example-ner/weights.txt')
</code></pre> 
  <h2><a id="_83"></a>训练文本分类模型</h2> 
  <ul> 
   <li>下面是使用简单的GloVe嵌入和Flair嵌入的组合，在AGNews语料库上训练文本分类器的示例代码。</li> 
   <li>您需要先下载AGNews才能运行此代码。 AGNews语料库下载地址AG’s corpus of news articles：</li> 
   <li>在此示例中，我们将数据下采样到原始数据的10％。</li> 
  </ul> 
  <pre><code>from flair.data import TaggedCorpus
from flair.data_fetcher import NLPTaskDataFetcher, NLPTask
from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings
from flair.models import TextClassifier
from flair.trainers import ModelTrainer


# 1. get the corpus
corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.AG_NEWS, 'path/to/data/folder').downsample(0.1)

# 2. create the label dictionary
label_dict = corpus.make_label_dictionary()

# 3. make a list of word embeddings
word_embeddings = [WordEmbeddings('glove'),

                   # comment in flair embeddings for state-of-the-art results 
                   # FlairEmbeddings('news-forward'),
                   # FlairEmbeddings('news-backward'),
                   ]

# 4. init document embedding by passing list of word embeddings
document_embeddings: DocumentLSTMEmbeddings = DocumentLSTMEmbeddings(word_embeddings,
                                                                     hidden_size=512,
                                                                     reproject_words=True,
                                                                     reproject_words_dimension=256,
                                                                     )

# 5. create the text classifier
classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, multi_label=False)

# 6. initialize the text classifier trainer
trainer = ModelTrainer(classifier, corpus)

# 7. start the training
trainer.train('resources/taggers/ag_news',
              learning_rate=0.1,
              mini_batch_size=32,
              anneal_factor=0.5,
              patience=5,
              max_epochs=150)

# 8. plot training curves (optional)
from flair.visual.training_curves import Plotter
plotter = Plotter()
plotter.plot_training_curves('resources/taggers/ag_news/loss.tsv')
plotter.plot_weights('resources/taggers/ag_news/weights.txt')
</code></pre> 
  <p>训练模型后，您可以加载它来预测新句子的类别。 只需调用模型的预测(predict)方法即可。</p> 
  <pre><code>classifier = TextClassifier.load_from_file('resources/taggers/ag_news/final-model.pt')

# create example sentence
sentence = Sentence('France is the current world cup winner.')

# predict tags and print
classifier.predict(sentence)

print(sentence.labels)
</code></pre> 
  <h2><a id="_153"></a>多数据集训练</h2> 
  <ul> 
   <li>现在，让我们训练一个可以用英语和德语标记PoS标签文本的模型。 为此，我们加载英语和德语UD语料库并创建MultiCorpus对象。 我们还使用新的多语言Flair嵌入来完成此任务。</li> 
   <li>请注意，这里我们使用MICRO_ACCURACY评估指标而不是默认的MICRO_F1_SCORE。</li> 
   <li>所有其余的都和以前一样，例如：</li> 
  </ul> 
  <pre><code>from typing import List
from flair.data import MultiCorpus
from flair.data_fetcher import NLPTaskDataFetcher, NLPTask
from flair.embeddings import FlairEmbeddings, TokenEmbeddings, StackedEmbeddings
from flair.training_utils import EvaluationMetric


# 1. get the corpora - English and German UD
corpus: MultiCorpus = NLPTaskDataFetcher.load_corpora([NLPTask.UD_ENGLISH, NLPTask.UD_GERMAN]).downsample(0.1)

# 2. what tag do we want to predict?
tag_type = 'upos'

# 3. make the tag dictionary from the corpus
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
print(tag_dictionary.idx2item)

# 4. initialize embeddings
embedding_types: List[TokenEmbeddings] = [

    # we use multilingual Flair embeddings in this task
    FlairEmbeddings('multi-forward'),
    FlairEmbeddings('multi-backward'),
]

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

# 5. initialize sequence tagger
from flair.models import SequenceTagger

tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        use_crf=True)

# 6. initialize trainer
from flair.trainers import ModelTrainer

trainer: ModelTrainer = ModelTrainer(tagger, corpus)

# 7. start training
trainer.train('resources/taggers/example-universal-pos',
              learning_rate=0.1,
              mini_batch_size=32,
              max_epochs=150,
              evaluation_metric=EvaluationMetric.MICRO_ACCURACY)
</code></pre> 
  <h2><a id="_210"></a>绘制训练曲线和权重</h2> 
  <ul> 
   <li>Flair包括一个辅助方法，用于绘制神经网络中的训练曲线和权重。 ModelTrainer自动在结果文件夹中生成loss.tsv和weights.txt文件。</li> 
   <li>训练之后，简单地将绘图仪指向这些文件。</li> 
   <li>这会在结果文件夹中生成PNG图。</li> 
  </ul> 
  <pre><code>from flair.visual.training_curves import Plotter
plotter = Plotter()
plotter.plot_training_curves('loss.tsv')
plotter.plot_weights('weights.txt')
</code></pre> 
  <h2><a id="_223"></a>恢复训练</h2> 
  <ul> 
   <li>如果要在某个时刻停止训练并稍后恢复训练，则应将参数检查点设置为True进行训练。 这将在每个epoch后保存模型和训练参数。 因此，您可以在以后任何时候加载模型和trainer，并在您离开的地方继续训练。</li> 
   <li>下面的示例代码显示了如何训练，停止和继续训练SequenceTagger。 TextClassifier也可以这样做。</li> 
  </ul> 
  <pre><code>from flair.data import TaggedCorpus
from flair.data_fetcher import NLPTaskDataFetcher, NLPTask
from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings
from typing import List

# 1. get the corpus
corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03).downsample(0.1)

# 2. what tag do we want to predict?
tag_type = 'ner'

# 3. make the tag dictionary from the corpus
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)

# 4. initialize embeddings
embedding_types: List[TokenEmbeddings] = [
    WordEmbeddings('glove')
]

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)

# 5. initialize sequence tagger
from flair.models import SequenceTagger

tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        use_crf=True)

# 6. initialize trainer
from flair.trainers import ModelTrainer
from flair.training_utils import EvaluationMetric

trainer: ModelTrainer = ModelTrainer(tagger, corpus)

# 7. start training
trainer.train('resources/taggers/example-ner',
              EvaluationMetric.MICRO_F1_SCORE,
              learning_rate=0.1,
              mini_batch_size=32,
              max_epochs=150,
              checkpoint=True)

# 8. stop training at any point

# 9. continue trainer at later point
from pathlib import Path

trainer = ModelTrainer.load_from_checkpoint(Path('resources/taggers/example-ner/checkpoint.pt'), 'SequenceTagger', corpus)
trainer.train('resources/taggers/example-ner',
              EvaluationMetric.MICRO_F1_SCORE,
              learning_rate=0.1,
              mini_batch_size=32,
              max_epochs=150,
              checkpoint=True)
</code></pre> 
  <h2><a id="_288"></a>可扩展性：大数据集训练</h2> 
  <p>使用FlairEmbeddings（你应该）时要考虑的主要事情是，为大型训练数据集生成它们的成本有些高。根据您的设置，您可以设置选项以优化培训时间。有三个问题要问：</p> 
  <p>1.你有GPU吗？</p> 
  <p>CharLMEmbeddings使用Pytorch RNN生成，因此针对GPU进行了优化。如果您有一个，您可以设置大型小批量大小以使用批处理。如果没有，您可能想要使用较小的语言模型。对于英语，我们打包嵌入式的“快速”变体，可加载如下：FlairEmbeddings（‘news-forward-fast’）。</p> 
  <p>2.整个数据集的嵌入是否适合内存？</p> 
  <p>在最佳情况下，数据集的所有嵌入都适合您的常规内存，这极大地提高了训练速度。如果不是这种情况，则必须在相应的培训师（即ModelTrainer）中设置标志embeddings_in_memory = False以避免内存问题。使用该标志，嵌入是（a）在每个纪元重新计算或（b）从磁盘检索，如果您选择实现磁盘。</p> 
  <p>3.你有快速硬盘吗？</p> 
  <p>如果您有快速硬盘驱动器，请考虑将嵌入物实现到磁盘。您可以通过以下方式实现我的实例化FlairEmbedding：FlairEmbeddings（‘news-forward-fast’，use_cache = True）。如果嵌入不适合内存，这可能会有所帮助。此外，如果您没有GPU并且想要对同一数据集进行重复实验，这会有所帮助，因为嵌入只需要计算一次，然后始终从磁盘中检索。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
