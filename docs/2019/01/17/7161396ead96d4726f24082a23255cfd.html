<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>工具篇Flair之使用加载语料库教程 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="工具篇Flair之使用加载语料库教程" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="版权声明：转载请注明出处，谢谢~~ https://blog.csdn.net/m0_37306360/article/details/86527516 更多实时更新的个人学习笔记分享，请关注： 知乎：https://www.zhihu.com/people/yuquanle/columns 微信订阅号：AI小白入门 ID: StudyForAI Flair工具使用教程之如何加载自己的语料库 本教程的展示了如何加载自己的语料库，以便训练自己的模型。 教程地址：https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md 读取序列标记数据集 NLP中的大多数序列标记数据集使用某种列格式，其中每一行都是一个单词，每列都是一级语言标注。 比如说这句话： George N B-PER Washington N I-PER went V O to P O Washington N B-LOC 第一列是单词本身； 第二列是词性(POS)标签； 第三列是用BIO注释的命名实体(NER)标签，B表示实体的开始，O表示其他； 要读取此类数据集，需要将列结构定义为字典并使用辅助方法。 from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher # define columns columns = {0: &#39;text&#39;, 1: &#39;pos&#39;, 2: &#39;np&#39;} # this is the folder in which train, test and dev files reside data_folder = &#39;/path/to/data/folder&#39; # retrieve corpus using column format, data folder and the names of the train, dev and test files corpus: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns, train_file=&#39;train.txt&#39;, test_file=&#39;test.txt&#39;, dev_file=&#39;dev.txt&#39;) 这将为您提供TaggedCorpus对象，其中包含train，dev和test 集，每个对象都有一个Sentence列表。 因此，要检查训练分组中有多少句子，请执行： len(corpus.train) 您还可以访问句子并查看注释。 让我们假设训练集中的第一个句子是上面的例句，然后使用下面代码： print(corpus.train[0].to_tagged_string(&#39;pos&#39;)) print(corpus.train[0].to_tagged_string(&#39;ner&#39;)) 输出具有不同注释层的句子： George &lt;N&gt; Washington &lt;N&gt; went &lt;V&gt; to &lt;P&gt; Washington &lt;N&gt; George &lt;B-PER&gt; Washington &lt;I-PER&gt; went to Washington &lt;B-LOC&gt; . 读取文本分类数据集 Flair使用的文本分类数据格式基于FastText格式，其中文件中的每一行代表一个文本文档。 一个文档可以有一个或多个标签，这些标签在以#_label__前缀开头的行的开头定义。 这看起来像这样： __label__&lt;label_1&gt; &lt;text&gt; __label__&lt;label_1&gt; __label__&lt;label_2&gt; &lt;text&gt; 要为文本分类任务创建TaggedCorpus，您需要以上述格式将三个文件（train，dev和test）放在一个文件夹中。 例如，对于IMDB任务，此数据文件夹结构可能如下所示： /resources/tasks/imdb/train.txt /resources/tasks/imdb/dev.txt /resources/tasks/imdb/test.txt 如果您现在将NLPTaskDataFetcher指向此文件夹（/ resources / tasks / imdb），它将从三个不同的文件中创建TaggedCorpus。 因此，文件中的每一行都被转换为用标签注释的Sentence对象。代码如下： from flair.data_fetcher import NLPTaskDataFetcher from pathlib import Path # use your own data path data_folder = Path(&#39;/resources/tasks/imdb&#39;) # load corpus containing training, test and dev data corpus: TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(data_folder, test_file=&#39;test.txt&#39;, dev_file=&#39;dev.txt&#39;, train_file=&#39;train.txt&#39;) 注意：一行中的文本可以有多个句子。 因此，Sentence对象实际上可以包含多个句子。 如果您只想读取单个文件，可以使用NLPTaskDataFetcher.read_text_classification_file（&#39;path / to / file.txt），它返回Sentence对象的列表。 下载数据集 Flair还支持开箱即用的几个数据集。 例如，您可以通过调用加载这些数据集： corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) 这行代码将下载UD_ENGLISH数据集并将其放入〜/ .flair / datasets / ud_english。 该方法返回TaggedCorpus，可直接用于训练您的模型。 目前Flair支持以下数据集： TaggedCorpus对象 TaggedCorpus代表您的整个数据集。 TaggedCorpus由测试集句子列表，开发集句子列表和测试集句子列表组成。 TaggedCorpus包含许多有用的辅助函数。 例如，您可以通过调用downsample（）并传递比率来对数据进行下采样。 所以，如果你通常得到这样的语料库： original_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) 然后你可以对语料库进行下采样，就像这样： downsampled_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH).downsample(0.1) 如果您打印两个语料库，您会看到第二个语料库已被下采样到10％的数据。 print(&quot;--- 1 Original ---&quot;) print(original_corpus) print(&quot;--- 2 Downsampled ---&quot;) print(downsampled_corpus) 输出： --- 1 Original --- TaggedCorpus: 12543 train + 2002 dev + 2077 test sentences --- 2 Downsampled --- TaggedCorpus: 1255 train + 201 dev + 208 test sentences 对于许多学习任务，您需要创建目标字典。 因此，TaggedCorpus使您可以创建标签或标签字典，具体取决于您要学习的任务。例如： # create tag dictionary for a PoS task corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) print(corpus.make_tag_dictionary(&#39;upos&#39;)) # create tag dictionary for an NER task corpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03_DUTCH) print(corpus.make_tag_dictionary(&#39;ner&#39;)) # create label dictionary for a text classification task corpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB, base_path=&#39;path/to/data/folder&#39;) print(corpus.make_label_dictionary()) 另一个有用的函数是obtain_statistics（），它返回一个python字典，其中包含有关数据集的有用统计信息。 例如，在像这样的IMDB数据集上使用它： from flair.data_fetcher import NLPTaskDataFetcher, NLPTask corpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB, base_path=&#39;path/to/data/folder&#39;) stats = corpus.obtain_statistics() print(stats) 输出： { &#39;TRAIN&#39;: { &#39;dataset&#39;: &#39;TRAIN&#39;, &#39;total_number_of_documents&#39;: 25000, &#39;number_of_documents_per_class&#39;: {&#39;POSITIVE&#39;: 12500, &#39;NEGATIVE&#39;: 12500}, &#39;number_of_tokens&#39;: {&#39;total&#39;: 6868314, &#39;min&#39;: 10, &#39;max&#39;: 2786, &#39;avg&#39;: 274.73256} }, &#39;TEST&#39;: { &#39;dataset&#39;: &#39;TEST&#39;, &#39;total_number_of_documents&#39;: 12500, &#39;number_of_documents_per_class&#39;: {&#39;NEGATIVE&#39;: 6245, &#39;POSITIVE&#39;: 6255}, &#39;number_of_tokens&#39;: {&#39;total&#39;: 3379510, &#39;min&#39;: 8, &#39;max&#39;: 2768, &#39;avg&#39;: 270.3608} }, &#39;DEV&#39;: { &#39;dataset&#39;: &#39;DEV&#39;, &#39;total_number_of_documents&#39;: 12500, &#39;number_of_documents_per_class&#39;: {&#39;POSITIVE&#39;: 6245, &#39;NEGATIVE&#39;: 6255}, &#39;number_of_tokens&#39;: {&#39;total&#39;: 3334898, &#39;min&#39;: 7, &#39;max&#39;: 2574, &#39;avg&#39;: 266.79184} } } MultiCorpus对象 如果需要一次训练多个任务，可以使用MultiCorpus对象。 要启动MultiCorpus，首先需要创建任意数量的TaggedCorpus对象。 之后，您可以将TaggedCorpus列表传递给MultiCorpus对象。 english_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) german_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_GERMAN) dutch_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_DUTCH) multi_corpus = MultiCorpus([english_corpus, german_corpus, dutch_corpus]) MultiCorpus对象具有与TaggedCorpus相同的接口。 您可以简单地将MultiCorpus传递给训练函数而不是TaggedCorpus，训练函数不会知道差异，训练将照常运作。" />
<meta property="og:description" content="版权声明：转载请注明出处，谢谢~~ https://blog.csdn.net/m0_37306360/article/details/86527516 更多实时更新的个人学习笔记分享，请关注： 知乎：https://www.zhihu.com/people/yuquanle/columns 微信订阅号：AI小白入门 ID: StudyForAI Flair工具使用教程之如何加载自己的语料库 本教程的展示了如何加载自己的语料库，以便训练自己的模型。 教程地址：https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md 读取序列标记数据集 NLP中的大多数序列标记数据集使用某种列格式，其中每一行都是一个单词，每列都是一级语言标注。 比如说这句话： George N B-PER Washington N I-PER went V O to P O Washington N B-LOC 第一列是单词本身； 第二列是词性(POS)标签； 第三列是用BIO注释的命名实体(NER)标签，B表示实体的开始，O表示其他； 要读取此类数据集，需要将列结构定义为字典并使用辅助方法。 from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher # define columns columns = {0: &#39;text&#39;, 1: &#39;pos&#39;, 2: &#39;np&#39;} # this is the folder in which train, test and dev files reside data_folder = &#39;/path/to/data/folder&#39; # retrieve corpus using column format, data folder and the names of the train, dev and test files corpus: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns, train_file=&#39;train.txt&#39;, test_file=&#39;test.txt&#39;, dev_file=&#39;dev.txt&#39;) 这将为您提供TaggedCorpus对象，其中包含train，dev和test 集，每个对象都有一个Sentence列表。 因此，要检查训练分组中有多少句子，请执行： len(corpus.train) 您还可以访问句子并查看注释。 让我们假设训练集中的第一个句子是上面的例句，然后使用下面代码： print(corpus.train[0].to_tagged_string(&#39;pos&#39;)) print(corpus.train[0].to_tagged_string(&#39;ner&#39;)) 输出具有不同注释层的句子： George &lt;N&gt; Washington &lt;N&gt; went &lt;V&gt; to &lt;P&gt; Washington &lt;N&gt; George &lt;B-PER&gt; Washington &lt;I-PER&gt; went to Washington &lt;B-LOC&gt; . 读取文本分类数据集 Flair使用的文本分类数据格式基于FastText格式，其中文件中的每一行代表一个文本文档。 一个文档可以有一个或多个标签，这些标签在以#_label__前缀开头的行的开头定义。 这看起来像这样： __label__&lt;label_1&gt; &lt;text&gt; __label__&lt;label_1&gt; __label__&lt;label_2&gt; &lt;text&gt; 要为文本分类任务创建TaggedCorpus，您需要以上述格式将三个文件（train，dev和test）放在一个文件夹中。 例如，对于IMDB任务，此数据文件夹结构可能如下所示： /resources/tasks/imdb/train.txt /resources/tasks/imdb/dev.txt /resources/tasks/imdb/test.txt 如果您现在将NLPTaskDataFetcher指向此文件夹（/ resources / tasks / imdb），它将从三个不同的文件中创建TaggedCorpus。 因此，文件中的每一行都被转换为用标签注释的Sentence对象。代码如下： from flair.data_fetcher import NLPTaskDataFetcher from pathlib import Path # use your own data path data_folder = Path(&#39;/resources/tasks/imdb&#39;) # load corpus containing training, test and dev data corpus: TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(data_folder, test_file=&#39;test.txt&#39;, dev_file=&#39;dev.txt&#39;, train_file=&#39;train.txt&#39;) 注意：一行中的文本可以有多个句子。 因此，Sentence对象实际上可以包含多个句子。 如果您只想读取单个文件，可以使用NLPTaskDataFetcher.read_text_classification_file（&#39;path / to / file.txt），它返回Sentence对象的列表。 下载数据集 Flair还支持开箱即用的几个数据集。 例如，您可以通过调用加载这些数据集： corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) 这行代码将下载UD_ENGLISH数据集并将其放入〜/ .flair / datasets / ud_english。 该方法返回TaggedCorpus，可直接用于训练您的模型。 目前Flair支持以下数据集： TaggedCorpus对象 TaggedCorpus代表您的整个数据集。 TaggedCorpus由测试集句子列表，开发集句子列表和测试集句子列表组成。 TaggedCorpus包含许多有用的辅助函数。 例如，您可以通过调用downsample（）并传递比率来对数据进行下采样。 所以，如果你通常得到这样的语料库： original_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) 然后你可以对语料库进行下采样，就像这样： downsampled_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH).downsample(0.1) 如果您打印两个语料库，您会看到第二个语料库已被下采样到10％的数据。 print(&quot;--- 1 Original ---&quot;) print(original_corpus) print(&quot;--- 2 Downsampled ---&quot;) print(downsampled_corpus) 输出： --- 1 Original --- TaggedCorpus: 12543 train + 2002 dev + 2077 test sentences --- 2 Downsampled --- TaggedCorpus: 1255 train + 201 dev + 208 test sentences 对于许多学习任务，您需要创建目标字典。 因此，TaggedCorpus使您可以创建标签或标签字典，具体取决于您要学习的任务。例如： # create tag dictionary for a PoS task corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) print(corpus.make_tag_dictionary(&#39;upos&#39;)) # create tag dictionary for an NER task corpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03_DUTCH) print(corpus.make_tag_dictionary(&#39;ner&#39;)) # create label dictionary for a text classification task corpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB, base_path=&#39;path/to/data/folder&#39;) print(corpus.make_label_dictionary()) 另一个有用的函数是obtain_statistics（），它返回一个python字典，其中包含有关数据集的有用统计信息。 例如，在像这样的IMDB数据集上使用它： from flair.data_fetcher import NLPTaskDataFetcher, NLPTask corpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB, base_path=&#39;path/to/data/folder&#39;) stats = corpus.obtain_statistics() print(stats) 输出： { &#39;TRAIN&#39;: { &#39;dataset&#39;: &#39;TRAIN&#39;, &#39;total_number_of_documents&#39;: 25000, &#39;number_of_documents_per_class&#39;: {&#39;POSITIVE&#39;: 12500, &#39;NEGATIVE&#39;: 12500}, &#39;number_of_tokens&#39;: {&#39;total&#39;: 6868314, &#39;min&#39;: 10, &#39;max&#39;: 2786, &#39;avg&#39;: 274.73256} }, &#39;TEST&#39;: { &#39;dataset&#39;: &#39;TEST&#39;, &#39;total_number_of_documents&#39;: 12500, &#39;number_of_documents_per_class&#39;: {&#39;NEGATIVE&#39;: 6245, &#39;POSITIVE&#39;: 6255}, &#39;number_of_tokens&#39;: {&#39;total&#39;: 3379510, &#39;min&#39;: 8, &#39;max&#39;: 2768, &#39;avg&#39;: 270.3608} }, &#39;DEV&#39;: { &#39;dataset&#39;: &#39;DEV&#39;, &#39;total_number_of_documents&#39;: 12500, &#39;number_of_documents_per_class&#39;: {&#39;POSITIVE&#39;: 6245, &#39;NEGATIVE&#39;: 6255}, &#39;number_of_tokens&#39;: {&#39;total&#39;: 3334898, &#39;min&#39;: 7, &#39;max&#39;: 2574, &#39;avg&#39;: 266.79184} } } MultiCorpus对象 如果需要一次训练多个任务，可以使用MultiCorpus对象。 要启动MultiCorpus，首先需要创建任意数量的TaggedCorpus对象。 之后，您可以将TaggedCorpus列表传递给MultiCorpus对象。 english_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) german_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_GERMAN) dutch_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_DUTCH) multi_corpus = MultiCorpus([english_corpus, german_corpus, dutch_corpus]) MultiCorpus对象具有与TaggedCorpus相同的接口。 您可以简单地将MultiCorpus传递给训练函数而不是TaggedCorpus，训练函数不会知道差异，训练将照常运作。" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-17T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"版权声明：转载请注明出处，谢谢~~ https://blog.csdn.net/m0_37306360/article/details/86527516 更多实时更新的个人学习笔记分享，请关注： 知乎：https://www.zhihu.com/people/yuquanle/columns 微信订阅号：AI小白入门 ID: StudyForAI Flair工具使用教程之如何加载自己的语料库 本教程的展示了如何加载自己的语料库，以便训练自己的模型。 教程地址：https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md 读取序列标记数据集 NLP中的大多数序列标记数据集使用某种列格式，其中每一行都是一个单词，每列都是一级语言标注。 比如说这句话： George N B-PER Washington N I-PER went V O to P O Washington N B-LOC 第一列是单词本身； 第二列是词性(POS)标签； 第三列是用BIO注释的命名实体(NER)标签，B表示实体的开始，O表示其他； 要读取此类数据集，需要将列结构定义为字典并使用辅助方法。 from flair.data import TaggedCorpus from flair.data_fetcher import NLPTaskDataFetcher # define columns columns = {0: &#39;text&#39;, 1: &#39;pos&#39;, 2: &#39;np&#39;} # this is the folder in which train, test and dev files reside data_folder = &#39;/path/to/data/folder&#39; # retrieve corpus using column format, data folder and the names of the train, dev and test files corpus: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns, train_file=&#39;train.txt&#39;, test_file=&#39;test.txt&#39;, dev_file=&#39;dev.txt&#39;) 这将为您提供TaggedCorpus对象，其中包含train，dev和test 集，每个对象都有一个Sentence列表。 因此，要检查训练分组中有多少句子，请执行： len(corpus.train) 您还可以访问句子并查看注释。 让我们假设训练集中的第一个句子是上面的例句，然后使用下面代码： print(corpus.train[0].to_tagged_string(&#39;pos&#39;)) print(corpus.train[0].to_tagged_string(&#39;ner&#39;)) 输出具有不同注释层的句子： George &lt;N&gt; Washington &lt;N&gt; went &lt;V&gt; to &lt;P&gt; Washington &lt;N&gt; George &lt;B-PER&gt; Washington &lt;I-PER&gt; went to Washington &lt;B-LOC&gt; . 读取文本分类数据集 Flair使用的文本分类数据格式基于FastText格式，其中文件中的每一行代表一个文本文档。 一个文档可以有一个或多个标签，这些标签在以#_label__前缀开头的行的开头定义。 这看起来像这样： __label__&lt;label_1&gt; &lt;text&gt; __label__&lt;label_1&gt; __label__&lt;label_2&gt; &lt;text&gt; 要为文本分类任务创建TaggedCorpus，您需要以上述格式将三个文件（train，dev和test）放在一个文件夹中。 例如，对于IMDB任务，此数据文件夹结构可能如下所示： /resources/tasks/imdb/train.txt /resources/tasks/imdb/dev.txt /resources/tasks/imdb/test.txt 如果您现在将NLPTaskDataFetcher指向此文件夹（/ resources / tasks / imdb），它将从三个不同的文件中创建TaggedCorpus。 因此，文件中的每一行都被转换为用标签注释的Sentence对象。代码如下： from flair.data_fetcher import NLPTaskDataFetcher from pathlib import Path # use your own data path data_folder = Path(&#39;/resources/tasks/imdb&#39;) # load corpus containing training, test and dev data corpus: TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(data_folder, test_file=&#39;test.txt&#39;, dev_file=&#39;dev.txt&#39;, train_file=&#39;train.txt&#39;) 注意：一行中的文本可以有多个句子。 因此，Sentence对象实际上可以包含多个句子。 如果您只想读取单个文件，可以使用NLPTaskDataFetcher.read_text_classification_file（&#39;path / to / file.txt），它返回Sentence对象的列表。 下载数据集 Flair还支持开箱即用的几个数据集。 例如，您可以通过调用加载这些数据集： corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) 这行代码将下载UD_ENGLISH数据集并将其放入〜/ .flair / datasets / ud_english。 该方法返回TaggedCorpus，可直接用于训练您的模型。 目前Flair支持以下数据集： TaggedCorpus对象 TaggedCorpus代表您的整个数据集。 TaggedCorpus由测试集句子列表，开发集句子列表和测试集句子列表组成。 TaggedCorpus包含许多有用的辅助函数。 例如，您可以通过调用downsample（）并传递比率来对数据进行下采样。 所以，如果你通常得到这样的语料库： original_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) 然后你可以对语料库进行下采样，就像这样： downsampled_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH).downsample(0.1) 如果您打印两个语料库，您会看到第二个语料库已被下采样到10％的数据。 print(&quot;--- 1 Original ---&quot;) print(original_corpus) print(&quot;--- 2 Downsampled ---&quot;) print(downsampled_corpus) 输出： --- 1 Original --- TaggedCorpus: 12543 train + 2002 dev + 2077 test sentences --- 2 Downsampled --- TaggedCorpus: 1255 train + 201 dev + 208 test sentences 对于许多学习任务，您需要创建目标字典。 因此，TaggedCorpus使您可以创建标签或标签字典，具体取决于您要学习的任务。例如： # create tag dictionary for a PoS task corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) print(corpus.make_tag_dictionary(&#39;upos&#39;)) # create tag dictionary for an NER task corpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03_DUTCH) print(corpus.make_tag_dictionary(&#39;ner&#39;)) # create label dictionary for a text classification task corpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB, base_path=&#39;path/to/data/folder&#39;) print(corpus.make_label_dictionary()) 另一个有用的函数是obtain_statistics（），它返回一个python字典，其中包含有关数据集的有用统计信息。 例如，在像这样的IMDB数据集上使用它： from flair.data_fetcher import NLPTaskDataFetcher, NLPTask corpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB, base_path=&#39;path/to/data/folder&#39;) stats = corpus.obtain_statistics() print(stats) 输出： { &#39;TRAIN&#39;: { &#39;dataset&#39;: &#39;TRAIN&#39;, &#39;total_number_of_documents&#39;: 25000, &#39;number_of_documents_per_class&#39;: {&#39;POSITIVE&#39;: 12500, &#39;NEGATIVE&#39;: 12500}, &#39;number_of_tokens&#39;: {&#39;total&#39;: 6868314, &#39;min&#39;: 10, &#39;max&#39;: 2786, &#39;avg&#39;: 274.73256} }, &#39;TEST&#39;: { &#39;dataset&#39;: &#39;TEST&#39;, &#39;total_number_of_documents&#39;: 12500, &#39;number_of_documents_per_class&#39;: {&#39;NEGATIVE&#39;: 6245, &#39;POSITIVE&#39;: 6255}, &#39;number_of_tokens&#39;: {&#39;total&#39;: 3379510, &#39;min&#39;: 8, &#39;max&#39;: 2768, &#39;avg&#39;: 270.3608} }, &#39;DEV&#39;: { &#39;dataset&#39;: &#39;DEV&#39;, &#39;total_number_of_documents&#39;: 12500, &#39;number_of_documents_per_class&#39;: {&#39;POSITIVE&#39;: 6245, &#39;NEGATIVE&#39;: 6255}, &#39;number_of_tokens&#39;: {&#39;total&#39;: 3334898, &#39;min&#39;: 7, &#39;max&#39;: 2574, &#39;avg&#39;: 266.79184} } } MultiCorpus对象 如果需要一次训练多个任务，可以使用MultiCorpus对象。 要启动MultiCorpus，首先需要创建任意数量的TaggedCorpus对象。 之后，您可以将TaggedCorpus列表传递给MultiCorpus对象。 english_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH) german_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_GERMAN) dutch_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_DUTCH) multi_corpus = MultiCorpus([english_corpus, german_corpus, dutch_corpus]) MultiCorpus对象具有与TaggedCorpus相同的接口。 您可以简单地将MultiCorpus传递给训练函数而不是TaggedCorpus，训练函数不会知道差异，训练将照常运作。","@type":"BlogPosting","url":"/2019/01/17/7161396ead96d4726f24082a23255cfd.html","headline":"工具篇Flair之使用加载语料库教程","dateModified":"2019-01-17T00:00:00+08:00","datePublished":"2019-01-17T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/01/17/7161396ead96d4726f24082a23255cfd.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>工具篇Flair之使用加载语料库教程</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div class="article-copyright">
   版权声明：转载请注明出处，谢谢~~ https://blog.csdn.net/m0_37306360/article/details/86527516 
 </div> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <p><strong>更多实时更新的个人学习笔记分享，请关注：<br> 知乎：<a href="https://www.zhihu.com/people/yuquanle/columns" rel="nofollow">https://www.zhihu.com/people/yuquanle/columns</a><br> 微信订阅号：AI小白入门<br> ID: StudyForAI</strong></p> 
  <center> 
   <img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190104163636941.jpg" width="66%"> 
  </center>
  <hr> 
  <hr> 
  <h2><a id="Flair_10"></a>Flair工具使用教程之如何加载自己的语料库</h2> 
  <ul> 
   <li>本教程的展示了如何加载自己的语料库，以便训练自己的模型。</li> 
   <li>教程地址：<a href="https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md" rel="nofollow">https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md</a></li> 
  </ul> 
  <h2><a id="_16"></a>读取序列标记数据集</h2> 
  <p>NLP中的大多数序列标记数据集使用某种列格式，其中每一行都是一个单词，每列都是一级语言标注。 比如说这句话：</p> 
  <pre><code>George N B-PER
Washington N I-PER
went V O
to P O
Washington N B-LOC
</code></pre> 
  <ul> 
   <li>第一列是单词本身；</li> 
   <li>第二列是词性(POS)标签；</li> 
   <li>第三列是用BIO注释的命名实体(NER)标签，B表示实体的开始，O表示其他；</li> 
  </ul> 
  <p>要读取此类数据集，需要将列结构定义为字典并使用辅助方法。</p> 
  <pre><code>from flair.data import TaggedCorpus
from flair.data_fetcher import NLPTaskDataFetcher

# define columns
columns = {0: 'text', 1: 'pos', 2: 'np'}

# this is the folder in which train, test and dev files reside
data_folder = '/path/to/data/folder'

# retrieve corpus using column format, data folder and the names of the train, dev and test files
corpus: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,
                                                              train_file='train.txt',
                                                              test_file='test.txt',
                                                              dev_file='dev.txt')
</code></pre> 
  <p>这将为您提供TaggedCorpus对象，其中包含train，dev和test 集，每个对象都有一个Sentence列表。 因此，要检查训练分组中有多少句子，请执行：</p> 
  <pre><code>len(corpus.train)
</code></pre> 
  <p>您还可以访问句子并查看注释。 让我们假设训练集中的第一个句子是上面的例句，然后使用下面代码：</p> 
  <pre><code>print(corpus.train[0].to_tagged_string('pos'))
print(corpus.train[0].to_tagged_string('ner'))
</code></pre> 
  <p>输出具有不同注释层的句子：</p> 
  <pre><code>George &lt;N&gt; Washington &lt;N&gt; went &lt;V&gt; to &lt;P&gt; Washington &lt;N&gt;

George &lt;B-PER&gt; Washington &lt;I-PER&gt; went to Washington &lt;B-LOC&gt; .
</code></pre> 
  <hr> 
  <h2><a id="_76"></a>读取文本分类数据集</h2> 
  <p>Flair使用的文本分类数据格式基于FastText格式，其中文件中的每一行代表一个文本文档。 一个文档可以有一个或多个标签，这些标签在以#_label__前缀开头的行的开头定义。 这看起来像这样：</p> 
  <pre><code>__label__&lt;label_1&gt; &lt;text&gt;
__label__&lt;label_1&gt; __label__&lt;label_2&gt; &lt;text&gt;
</code></pre> 
  <p>要为文本分类任务创建TaggedCorpus，您需要以上述格式将三个文件（train，dev和test）放在一个文件夹中。 例如，对于IMDB任务，此数据文件夹结构可能如下所示：</p> 
  <pre><code>/resources/tasks/imdb/train.txt
/resources/tasks/imdb/dev.txt
/resources/tasks/imdb/test.txt
</code></pre> 
  <p>如果您现在将NLPTaskDataFetcher指向此文件夹（/ resources / tasks / imdb），它将从三个不同的文件中创建TaggedCorpus。 因此，文件中的每一行都被转换为用标签注释的Sentence对象。代码如下：</p> 
  <pre><code>from flair.data_fetcher import NLPTaskDataFetcher
from pathlib import Path

# use your own data path
data_folder = Path('/resources/tasks/imdb')

# load corpus containing training, test and dev data
corpus: TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(data_folder,
                                                                     test_file='test.txt',
                                                                     dev_file='dev.txt',
                                                                     train_file='train.txt')
</code></pre> 
  <p>注意：一行中的文本可以有多个句子。 因此，Sentence对象实际上可以包含多个句子。</p> 
  <p>如果您只想读取单个文件，可以使用NLPTaskDataFetcher.read_text_classification_file（'path / to / file.txt），它返回Sentence对象的列表。</p> 
  <h2><a id="_115"></a>下载数据集</h2> 
  <p>Flair还支持开箱即用的几个数据集。 例如，您可以通过调用加载这些数据集：</p> 
  <pre><code>corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)
</code></pre> 
  <p>这行代码将下载UD_ENGLISH数据集并将其放入〜/ .flair / datasets / ud_english。 该方法返回TaggedCorpus，可直接用于训练您的模型。</p> 
  <p>目前Flair支持以下数据集：</p> 
  <p><img src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190117165239119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzM3MzA2MzYw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="TaggedCorpus_129"></a>TaggedCorpus对象</h2> 
  <ul> 
   <li> <p>TaggedCorpus代表您的整个数据集。 TaggedCorpus由测试集句子列表，开发集句子列表和测试集句子列表组成。</p> </li> 
   <li> <p>TaggedCorpus包含许多有用的辅助函数。 例如，您可以通过调用downsample（）并传递比率来对数据进行下采样。 所以，如果你通常得到这样的语料库：</p> </li> 
  </ul> 
  <pre><code>original_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)
</code></pre> 
  <p>然后你可以对语料库进行下采样，就像这样：</p> 
  <pre><code>downsampled_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH).downsample(0.1)
</code></pre> 
  <p>如果您打印两个语料库，您会看到第二个语料库已被下采样到10％的数据。</p> 
  <pre><code>print("--- 1 Original ---")
print(original_corpus)

print("--- 2 Downsampled ---")
print(downsampled_corpus)
</code></pre> 
  <p>输出：</p> 
  <pre><code>--- 1 Original ---
TaggedCorpus: 12543 train + 2002 dev + 2077 test sentences

--- 2 Downsampled ---
TaggedCorpus: 1255 train + 201 dev + 208 test sentences
</code></pre> 
  <p>对于许多学习任务，您需要创建目标字典。 因此，TaggedCorpus使您可以创建标签或标签字典，具体取决于您要学习的任务。例如：</p> 
  <pre><code># create tag dictionary for a PoS task
corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)
print(corpus.make_tag_dictionary('upos'))

# create tag dictionary for an NER task
corpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03_DUTCH)
print(corpus.make_tag_dictionary('ner'))

# create label dictionary for a text classification task
corpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB, base_path='path/to/data/folder')
print(corpus.make_label_dictionary())
</code></pre> 
  <p>另一个有用的函数是obtain_statistics（），它返回一个python字典，其中包含有关数据集的有用统计信息。 例如，在像这样的IMDB数据集上使用它：</p> 
  <pre><code>from flair.data_fetcher import NLPTaskDataFetcher, NLPTask
 
corpus = NLPTaskDataFetcher.load_corpus(NLPTask.IMDB, base_path='path/to/data/folder')
stats = corpus.obtain_statistics()
print(stats)
</code></pre> 
  <p>输出：</p> 
  <pre><code>{
  'TRAIN': {
    'dataset': 'TRAIN', 
    'total_number_of_documents': 25000, 
    'number_of_documents_per_class': {'POSITIVE': 12500, 'NEGATIVE': 12500}, 
    'number_of_tokens': {'total': 6868314, 'min': 10, 'max': 2786, 'avg': 274.73256}
  }, 
  'TEST': {
    'dataset': 'TEST', 
    'total_number_of_documents': 12500, 
    'number_of_documents_per_class': {'NEGATIVE': 6245, 'POSITIVE': 6255}, 
    'number_of_tokens': {'total': 3379510, 'min': 8, 'max': 2768, 'avg': 270.3608}
  }, 'DEV': {
    'dataset': 'DEV', 
    'total_number_of_documents': 12500, 
    'number_of_documents_per_class': {'POSITIVE': 6245, 'NEGATIVE': 6255}, 
    'number_of_tokens': {'total': 3334898, 'min': 7, 'max': 2574, 'avg': 266.79184}
  }
}
</code></pre> 
  <h2><a id="MultiCorpus_216"></a>MultiCorpus对象</h2> 
  <p>如果需要一次训练多个任务，可以使用MultiCorpus对象。 要启动MultiCorpus，首先需要创建任意数量的TaggedCorpus对象。 之后，您可以将TaggedCorpus列表传递给MultiCorpus对象。</p> 
  <pre><code>    english_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_ENGLISH)
    german_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_GERMAN)
    dutch_corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_DUTCH)
    
    multi_corpus = MultiCorpus([english_corpus, german_corpus, dutch_corpus])
</code></pre> 
  <p>MultiCorpus对象具有与TaggedCorpus相同的接口。 您可以简单地将MultiCorpus传递给训练函数而不是TaggedCorpus，训练函数不会知道差异，训练将照常运作。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
