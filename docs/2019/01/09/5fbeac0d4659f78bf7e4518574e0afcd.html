<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>DCGAN——菜鸟系列 model.py | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="DCGAN——菜鸟系列 model.py" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="参考 [1] DGAN代码简读 https://www.colabug.com/2958322.html [2]基于DCGAN的动漫头像生成神经网络实现 https://blog.csdn.net/sinat_33741547/article/details/77871170 [3] batch norm原理及代码详解 博客： https://blog.csdn.net/qq_25737169/article/details/79048516 视频：https://www.bilibili.com/video/av15405597?from=search&amp;seid=8881273700429864348 博客：https://www.cnblogs.com/eilearn/p/9780696.html 说明：2重点在代码，3重点在理论（但是一点都不枯燥），1有点像两个的结合。推荐2的系列视频以及资料，很精品，且免费。推荐看完视频再看3,而且3中有很多联立起来的理论，奉为神作不过分。 [3] tensorflow,tensorboard可视化网络结构 https://blog.csdn.net/helei001/article/details/51842531 [4] tensorflow变量作用域 https://www.cnblogs.com/MY0213/p/9208503.html [5] 此接[4],同时解决了当初jupyter notebook的困惑 https://blog.csdn.net/Jerr__y/article/details/70809528#commentBox [6] 激活函数 ReLU、Leaky ReLU、PReLU 和 RReLU http://www.cnblogs.com/chamie/p/8665251.html [7] conv2d函数简介 http://www.cnblogs.com/qggg/p/6832342.html [8] DCGAN 源码分析 https://blog.csdn.net/nongfu_spring/article/details/54342861/ [9] DCGAN代码简单读，里面有对应训练效果，这里看中了某个实现效果…so，做个记录先 http://www.cnblogs.com/lyrichu/p/9093411.html [10] 一个写的还算详细，但是排版和我彼此彼此的博客——题目：DCGAN https://blog.csdn.net/Candy_GL/article/details/81138297 [11] tensorflow保存下载模型 https://blog.csdn.net/xiezongsheng1990/article/details/81011115 说明：作者正在逐渐熟悉markdown…勿喷 [12] mmp的视频，早看到可以少走N多个坑（注：博主自己理了一遍代码以后，找论文时找到的视频，讲的对我来说正正好）看完视频，完善了部分代码注释 https://www.bilibili.com/video/av20033914/?p=13 [13] B站找的视频 其实到最后，疑惑的也就部分tensorflow框架以及代码了，所以此处学习一波回来再去完善一下唉 https://www.bilibili.com/video/av19360545/?p=1 代码解释 引入模块 from __future__ import division import os import time import math from glob import glob import tensorflow as tf import numpy as np from six.moves import xrange from ops import * from utils import * from _ future_ import division 这句话当python的版本为2.x时生效，可以让两个整数数字相除的结果返回一个浮点数(在python2中默认是整数,python3默认为浮点数)。 glob可以以简单的正则表达式筛选的方式返回某个文件夹下符合要求的文件名列表。[1] conv_out_size_same 函数 def conv_out_size_same(size, stride): return int(math.ceil(float(size) / float(stride))) math.ceil(): ceil() 函数返回数字的上入整数,即向上取整 stride : 步幅 整个函数返回了一个整形值， class DCGAN init def __init__(self, sess, input_height=108, input_width=108, crop=True, batch_size=64, sample_num = 64, output_height=64,output_width=64, y_dim=None, z_dim=100, gf_dim=64, df_dim=64, gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name=&#39;default&#39;, input_fname_pattern=&#39;*.jpg&#39;, checkpoint_dir=None, sample_dir=None, data_dir=&#39;./data&#39;): 具体参数用到再细谈，此处注意特殊的那个: y_dim就好 以下给出对应注释 &quot;&quot;&quot; Args: sess: TensorFlow session batch_size: The size of batch. Should be specified before training. y_dim: (optional) Dimension of dim for y. [None] z_dim: (optional) Dimension of dim for Z. [100] gf_dim: (optional) Dimension of gen filters in first conv layer.[64] df_dim: (optional) Dimension of discrim filters in first conv layer[64] gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024] dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024] c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3] &quot;&quot;&quot; self.sess = sess self.crop = crop self.batch_size = batch_size # 测试要用到，由G生成，看效果 self.sample_num = sample_num self.input_height = input_height self.input_width = input_width self.output_height = output_height self.output_width = output_width self.y_dim = y_dim # Gnerator最开始输入的噪音数据点的维度 self.z_dim = z_dim # generator的中卷积网络的filter的维度，以下同理 self.gf_dim = gf_dim self.df_dim = df_dim # generator全连接的维度 self.gfc_dim = gfc_dim self.dfc_dim = dfc_dim # batch normalization : deals with poor initialization helps gradient flow self.d_bn1 = batch_norm(name=&#39;d_bn1&#39;) self.d_bn2 = batch_norm(name=&#39;d_bn2&#39;) if not self.y_dim: self.d_bn3 = batch_norm(name=&#39;d_bn3&#39;) self.g_bn0 = batch_norm(name=&#39;g_bn0&#39;) self.g_bn1 = batch_norm(name=&#39;g_bn1&#39;) self.g_bn2 = batch_norm(name=&#39;g_bn2&#39;) if not self.y_dim: self.g_bn3 = batch_norm(name=&#39;g_bn3&#39;) self.dataset_name = dataset_name self.input_fname_pattern = input_fname_pattern self.checkpoint_dir = checkpoint_dir self.data_dir = data_dir 这段就是赋值而已 batch_norm 的水很深，需要一定功夫研究，具体参考[3], 此处贴出作者的一份回答：如果在每一层之后都归一化成0-1的高斯分布（减均值除方差）那么数据的分布一直都是高斯分布，数据分布都是固定的了，这样即使加更多层就没有意义了，深度网络就是想学习数据的分布发现规律性，BN就是不让学习的数据分布偏离太远，详细细节你可以去看论文。beta gama都是学习的，代码里他们定义的是variable， trainable是True。——个人感觉很有道理 具体学习详看[3] if self.dataset_name == &#39;mnist&#39;: self.data_X, self.data_y = self.load_mnist() self.c_dim = self.data_X[0].shape[-1] else: data_path = os.path.join(self.data_dir, self.dataset_name, self.input_fname_pattern) #返回的数据类型是list self.data = glob(data_path) #用它可以查找符合自己目的的文件 if len(self.data) == 0: raise Exception(&quot;[!] No data found in &#39;&quot; + data_path + &quot;&#39;&quot;) np.random.shuffle(self.data) #打乱数据 #读取一张图片 imreadImg = imread(self.data[0]) #check if image is a non-grayscale image by checking channel number if len(imreadImg.shape) &gt;= 3: self.c_dim = imread(self.data[0]).shape[-1] else: self.c_dim = 1 if len(self.data) &lt; self.batch_size: raise Exception(&quot;[!] Entire dataset size is less than the configured batch_size&quot;) DCGAN的构造方法除了设置一大堆的属性之外，还要注意区分dataset是否是mnist,因为mnist是灰度图像,所以应该设置channel = 1( self.c_dim = 1 ),如果是彩色图像，则 self.c_dim = 3 or self.c_dim = 4[1] load_mnist为自定义方法，方法实现如下 其余代码均有注释，整个代码块只是在读入数据集，并进行对应的可能报错处理。 def load_mnist(self): data_dir = os.path.join(self.data_dir, self.dataset_name) #用于路径拼接文件路径 &#39;&#39;&#39; train-images-idx3-ubyte: 参考网址：https://www.jianshu.com/p/84f72791806f 简介:这是IDX文件格式，是一种用来存储向量与多维度矩阵的文件格式 &#39;&#39;&#39; fd = open(os.path.join(data_dir,&#39;train-images-idx3-ubyte&#39;)) #创建文件对象 loaded = np.fromfile(file=fd,dtype=np.uint8) #numpy 读取文件 trX = loaded[16:].reshape((60000,28,28,1)).astype(np.float) #对应的数据处理 fd = open(os.path.join(data_dir,&#39;train-labels-idx1-ubyte&#39;)) loaded = np.fromfile(file=fd,dtype=np.uint8) trY = loaded[8:].reshape((60000)).astype(np.float) fd = open(os.path.join(data_dir,&#39;t10k-images-idx3-ubyte&#39;)) loaded = np.fromfile(file=fd,dtype=np.uint8) teX = loaded[16:].reshape((10000,28,28,1)).astype(np.float) fd = open(os.path.join(data_dir,&#39;t10k-labels-idx1-ubyte&#39;)) loaded = np.fromfile(file=fd,dtype=np.uint8) teY = loaded[8:].reshape((10000)).astype(np.float) # 将结构数据转化为ndarray， 例子详见： https://www.jb51.net/article/138281.htm # 可以理解为：两个变量用法同一份内存，只是数据格式不同 trY = np.asarray(trY) teY = np.asarray(teY) # 数组拼接 #concatenate([a, b]) #连接，连接后ndim不变，a和b可以有一维size不同，但size不同的维度必须是要连接的维度 X = np.concatenate((trX, teX), axis=0) y = np.concatenate((trY, teY), axis=0).astype(np.int) # seed # http://www.cnblogs.com/subic/p/8454025.html # 简单介绍： seed = 547 np.random.seed(seed) np.random.shuffle(X) #np.random.shuffle np.random.seed(seed) np.random.shuffle(y) y_vec = np.zeros((len(y), self.y_dim), dtype=np.float) for i, label in enumerate(y): y_vec[i,y[i]] = 1.0 return X/255.,y_vec 具体参考注释内容 #是否为灰度 self.grayscale = (self.c_dim == 1) #自定义的方法 self.build_model() 无特殊说明 build_model(self) def build_model(self): #可以理解为申明了一个（batch_size,y_dim）大小的地方，用于mini_batc时往里面喂对应的数据。 if self.y_dim: self.y = tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name=&#39;y&#39;) else: self.y = None #is_crop为要不要裁剪图像 [True or False] #所以此处是判断图像是否裁剪过 # https://www.jianshu.com/p/3e46ce8e7ddd if self.crop: # 把输出维度定义好 image_dims = [self.output_height, self.output_width, self.c_dim] else: image_dims = [self.input_height, self.input_width, self.c_dim] #道理同上面解释，真实图片的输入[batchsize,height,width,c_dim] self.inputs = tf.placeholder( tf.float32, [self.batch_size] + image_dims, name=&#39;real_images&#39;) inputs = self.inputs # 噪音数据（None表示多少都可以，实际用到再填充batch） self.z = tf.placeholder( tf.float32, [None, self.z_dim], name=&#39;z&#39;) # 位于 opt.py里 # histogram_summary = tf.histogram_summary # 将z在tensorboard里可视化 self.z_sum = histogram_summary(&quot;z&quot;, self.z) # 以下为网络结构 self.G = self.generator(self.z, self.y) #真实数据 self.D, self.D_logits = self.discriminator(inputs, self.y, reuse=False) # 测试网络 self.sampler = self.sampler(self.z, self.y) # G产生的数据 self.D_, self.D_logits_ = self.discriminator(self.G, self.y, reuse=True) # 同上可视化 self.d_sum = histogram_summary(&quot;d&quot;, self.D) self.d__sum = histogram_summary(&quot;d_&quot;, self.D_) # opt.py # image_summary = tf.image_summary # tensorboar内图像可视化 ： https://blog.csdn.net/dxmkkk/article/details/54925728 self.G_sum = image_summary(&quot;G&quot;, self.G) 显而易见，网络结构才是重点，其他都是陪衬。大致介绍参考[1]: self.generator 用于构造生成器; self.discriminator 用于构造鉴别器; self.sampler 用于随机采样(用于生成样本)。这里需要注意的是, self.y 只有当dataset是mnist的时候才不为None,不是mnist的情况下,只需要 self.z 即可生成samples sigmoid_cross_entropy_with_logits(x, y) （内置于build_model） def sigmoid_cross_entropy_with_logits(x, y): try: return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y) except: return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, targets=y) [1] sigmoid_cross_entropy_with_logits 函数被重新定义了，是为了兼容不同版本的tensorflow。该函数首先使用sigmoid activation，然后计算cross-entropy loss。 这个函数有自己的定义，可以参考： https://blog.csdn.net/QW_sunny/article/details/72885403 作者为对此函数进行过深了解，仅限于清楚大致处理过程。 ​ 承接上面代码 # 真实图片的鉴别器损失 # D_logits,鉴别网络对真实数据的评分， tf.ones_like(self.D)，与D大小一样的1. # tf.nn.sigmoid(h3), h3 # 可以理解为两参数分别为x,y。 x是网络产生，y是实际希望，然后通过这个函数计算出一个损失值。不同于一般的平方求和，此函数解决了sigmoid梯度缓慢的问题，并且另外考虑了溢出的问题。 self.d_loss_real = tf.reduce_mean( sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D))) # 虚假图片损失 self.d_loss_fake = tf.reduce_mean( sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_))) # 生成器损失 self.g_loss = tf.reduce_mean( sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_))) # 可视化 self.d_loss_real_sum = scalar_summary(&quot;d_loss_real&quot;, self.d_loss_real) self.d_loss_fake_sum = scalar_summary(&quot;d_loss_fake&quot;, self.d_loss_fake) # 总的鉴别器损失，越小越好 self.d_loss = self.d_loss_real + self.d_loss_fake sigmoid_cross_entropy_with_logits的output不是一个数，而是一个batch中每个样本的loss,所以一般配合tf.reduce_mean(loss)使用 self.g_loss 是生成器损失; self.d_loss_real 是真实图片的鉴别器损失; self.d_loss_fake 是虚假图片(由生成器生成的fake images)的损失; self.d_loss 是总的鉴别器损失。 # 可视化 self.g_loss_sum = scalar_summary(&quot;g_loss&quot;, self.g_loss) self.d_loss_sum = scalar_summary(&quot;d_loss&quot;, self.d_loss) # 所有的变量 t_vars = tf.trainable_variables() # 寻找所有以d_开头的变量，优化时的参数（例如：如果是梯度下降，我们需要将权重和loss传进去，这里d_vars代表generator里要优化的变量） self.d_vars = [var for var in t_vars if &#39;d_&#39; in var.name] self.g_vars = [var for var in t_vars if &#39;g_&#39; in var.name] # 保存节点 self.saver = tf.train.Saver() [1] tf.trainable_variables() 可以获取model的全部可训练参数,由于我们在定义生成器和鉴别器变量的时候使用了不同的name,因此我们可以通过variable的name来获取得到 self.d_vars (鉴别器相关变量), self.g_vars (生成器相关变量)。 self.saver = tf.train.Saver() 用于保存训练好的模型参数到checkpoint。 discriminator def discriminator(self, image, y=None, reuse=False): # 变量共享，我理解为： 即使函数被重复调用，其共享的节点不会重复定义、声明 [5] with tf.variable_scope(&quot;discriminator&quot;) as scope: if reuse: scope.reuse_variables() # 如果没有这一维 if not self.y_dim: # conv2d简介具体看 [7]，但第二个参数的介绍是不对的，补上[8]. &#39;&#39;&#39; 参数简介： 第一个参数input：具有[batch, in_height, in_width, in_channels]这样的shape 第二个参数filter：conv2d中输出的特征图个数，是个1维的参数，即output_dim， output_dim是 conv2d函数的第二个入参，由外部传入。比如，下面的这句话，表示h1 是输入，通过卷积之后，输出的特征图个数为gf_dim* *4，这里gf_dim = 128，则输出 特征图为128*4=512个。即这里一共有512个卷积核。 第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4。[1, d_h, d_w, 1]，其中第一个对应一次跳过batch中的多少图片，第二个d_h对应一次跳过图片中 多少行，第三个d_w对应一次跳过图片中多少列，第四个对应一次跳过图像的多少个通道。这 里直接设置为[1，2，2，1]。即每次卷积后，图像的滑动步长为2，特征图会缩小为原来的 1/4。 第四个参数padding：string类型的量，只能是&quot;SAME&quot;,&quot;VALID&quot;其中之一，这个值决定了不 同的卷积方式 第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true 第六个参数：name=None &#39;&#39;&#39; # df_dim 作为类的参数传进来 # df_dim: (optional) Dimension of discrim filters in first conv layer[64] # lrelu 在opt.py 中 # 输入是一个 image h0 = lrelu(conv2d(image, self.df_dim, name=&#39;d_h0_conv&#39;)) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name=&#39;d_h1_conv&#39;))) h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name=&#39;d_h2_conv&#39;))) h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name=&#39;d_h3_conv&#39;))) #全连接层，-1表示不确定大小。 linear 在opt.py中,自己构建的。 h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, &#39;d_h4_lin&#39;) return tf.nn.sigmoid(h4), h4 else: # y_dim = 10 yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) # image被当作参数传进来的参数，Builder里是inputs. # 原型在opt里，基于concat函数基础上修改。这里并没有理解为什么要这样操作.... # 这里代码以及构建不难，但是这里所有的concat尚且不了解原因。 x = conv_cond_concat(image, yb) h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name=&#39;d_h0_conv&#39;)) h0 = conv_cond_concat(h0, yb) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name=&#39;d_h1_conv&#39;))) h1 = tf.reshape(h1, [self.batch_size, -1]) h1 = concat([h1, y], 1) h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, &#39;d_h2_lin&#39;))) h2 = concat([h2, y], 1) h3 = linear(h2, 1, &#39;d_h3_lin&#39;) return tf.nn.sigmoid(h3), h3 [1] 下面是discriminator(鉴别器)的具体实现。 首先鉴别器使用 conv (卷积)操作，激活函数使用 leaky-relu ,每一个layer需要使用batch normalization。tensorflow的batch normalization使用 tf.contrib.layers.batch_norm 实现。 如果不是mnist,则第一层使用 leaky-relu+conv2d ,后面三层都使用 conv2d+BN+leaky-relu ,最后加上一个one hidden unit的linear layer,再送入sigmoid函数即可； 如果是mnist,则 yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) 首先给y增加两维，以便可以和image连接起来，这里实际上是使用了conditional GAN(条件GAN)的思想。 x = conv_cond_concat(image, yb) 得到condition和image合并之后的结果，然后 h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name=‘d_h0_conv’)) 进行卷积操作。第二次进行 conv2d+leaky-relu+concat 操作。第三次进行 conv2d+BN+leaky-relu+reshape+concat 操作。第四次进行 linear+BN+leaky-relu+concat 操作。最后同样是 linear+sigmoid 操作。 补充：卷积网络 作者了解卷积神经网络的基础知识，但未真正自己建立过卷积神经网络，此处不甘心于函数，所以此处去重新学习了下整个卷积网络的构建,详见 卷积网络学习系列1.md 作者在温习以后才去标注的discriminator的卷积注释，所以写的不够小白。具体原理详见上。 generator def generator(self, z, y=None): with tf.variable_scope(&quot;generator&quot;) as scope: if not self.y_dim: s_h, s_w = self.output_height, self.output_width # return int(math.ceil(float(size) / float(stride))) s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape #self.z = tf.placeholder(tf.float32, [None, self.z_dim], name=&#39;z&#39;) # gf_dim是generator的filter大小的基，*8 *4 *2 *1 c_dim这是他的变化规律，下面代码内找找仔细观察看看 self.z_, self.h0_w, self.h0_b = linear( z, self.gf_dim*8*s_h16*s_w16, &#39;g_h0_lin&#39;, with_w=True) # 确定三个维度，可以计算出最后一个维度，那么这个维度可以用-1表示。 将全连接层reshape成特征图形状 self.h0 = tf.reshape( self.z_, [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(self.h0)) # 产生batch_size个 设置好大小的特征图 self.h1, self.h1_w, self.h1_b = deconv2d( h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name=&#39;g_h1&#39;, with_w=True) h1 = tf.nn.relu(self.g_bn1(self.h1)) h2, self.h2_w, self.h2_b = deconv2d( h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name=&#39;g_h2&#39;, with_w=True) h2 = tf.nn.relu(self.g_bn2(h2)) h3, self.h3_w, self.h3_b = deconv2d( h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name=&#39;g_h3&#39;, with_w=True) h3 = tf.nn.relu(self.g_bn3(h3)) # c_dim,通道数 3 h4, self.h4_w, self.h4_b = deconv2d( h3, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h4&#39;, with_w=True) return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.expand_dims(tf.expand_dims(y, 1),2) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu( self.g_bn0(linear(z, self.gfc_dim, &#39;g_h0_lin&#39;))) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, &#39;g_h1_lin&#39;))) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name=&#39;g_h2&#39;))) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid( deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h3&#39;)) [1]下面是generator(生成器)的具体实现。和discriminator不同的是,generator需要使用deconv(反卷积)以及relu 激活函数。 generator的结构是: 1.如果不是mnist: linear+reshape+BN+relu----&gt;(deconv+BN+relu)x3 ----&gt;deconv+tanh ; 2.如果是mnist,则除了需要考虑输入z之外，还需要考虑label y,即需要将z和y连接起来(Conditional GAN),具体的结构是:reshape+concat----&gt;linear+BN+relu+concat----&gt;linear+BN+relu+reshape+concat----&gt;deconv+BN+relu+concat----&gt;deconv+sigmoid。 注意的最后的激活函数没有采用通常的tanh,而是采用了sigmoid(其输出会直接映射到0-1之间)。 大佬们都不解释为什么要这样构建模型（补充：其实看里很久才知道，大佬们不说是因为原论文就是这样写构建的…不必纠结这个结构，因为还没到能设计结构的水平），所以，我只能硬着头皮自己解释。个人感觉，generator是从一个小输入，不断变大，变大，变成图片大小的过程，反卷积就用在这里。 def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False): shape = input_.get_shape().as_list() with tf.variable_scope(scope or &quot;Linear&quot;): try: matrix = tf.get_variable(&quot;Matrix&quot;, [shape[1], output_size], tf.float32, tf.random_normal_initializer(stddev=stddev)) except ValueError as err: msg = &quot;NOTE: Usually, this is due to an issue with the image dimensions. Did you correctly set &#39;--crop&#39; or &#39;--input_height&#39; or &#39;--output_height&#39;?&quot; err.args = err.args + (msg,) raise bias = tf.get_variable(&quot;bias&quot;, [output_size], initializer=tf.constant_initializer(bias_start)) # 是否输出权重 if with_w: return tf.matmul(input_, matrix) + bias, matrix, bias else: return tf.matmul(input_, matrix) + bias 反卷积 恭喜，又是一个没听过的知识点… 此处另外学习反卷积，详见反卷积系列。 def deconv2d(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=&quot;deconv2d&quot;, with_w=False): with tf.variable_scope(name): # filter : [height, width, output_channels, in_channels] w = tf.get_variable(&#39;w&#39;, [k_h, k_w, output_shape[-1], input_.get_shape()[-1]], initializer=tf.random_normal_initializer(stddev=stddev)) try: deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1]) # Support for verisons of TensorFlow before 0.7.0 except AttributeError: # d_h,d_w参考原论文 deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1]) # b的大小跟输出的特征图个数一样 biases = tf.get_variable(&#39;biases&#39;, [output_shape[-1]], initializer=tf.constant_initializer(0.0)) # 先反卷积，后加偏置 deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape()) # 是否输出权重 if with_w: return deconv, w, biases else: return deconv sampler def sampler(self, z, y=None): with tf.variable_scope(&quot;generator&quot;) as scope: scope.reuse_variables() if not self.y_dim: s_h, s_w = self.output_height, self.output_width s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape h0 = tf.reshape( linear(z, self.gf_dim*8*s_h16*s_w16, &#39;g_h0_lin&#39;), [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(h0, train=False)) h1 = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name=&#39;g_h1&#39;) h1 = tf.nn.relu(self.g_bn1(h1, train=False)) h2 = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name=&#39;g_h2&#39;) h2 = tf.nn.relu(self.g_bn2(h2, train=False)) h3 = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name=&#39;g_h3&#39;) h3 = tf.nn.relu(self.g_bn3(h3, train=False)) h4 = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h4&#39;) return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.reshape(y, [-1, 1, 1, self.y_dim]) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, &#39;g_h0_lin&#39;), train=False)) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, &#39;g_h1_lin&#39;), train=False)) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2( deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name=&#39;g_h2&#39;), train=False)) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h3&#39;)) sampler函数是采样函数，用于生成样本送入当前训练的生成器，查看训练效果。其逻辑和generator函数基本类似,也是需要区分是否是mnist,二者需要采用不同的结构。不是mnist时,y=None即可；否则mnist还需要考虑y。 如果单纯去看代码，前面那么代码的铺垫已经够了，但是，不理解为什么要用这个sampler,有什么用，怎么感觉他在做和generator一样的事儿。 具体为什么用它，得等到最后我去啃DGCAN原论文时候再了解吧。 DCGAN原文解读 此处为看完train代码后补的，单单看懂代码的大致逻辑很容易，但是细节完全把握不住，由于前面没有取细读论文，这里集中爆发了，感觉确实需要读完论文再往下做，所以，开始论文时间。 train 函数 补充，终于到这儿了，突然感觉真的好多啊，以前没有这样做过，但这样来一遍，发现自己啥都不是，学到了很多，但也知道很多自己都没写出来，水平有限。当然，万分感谢那些我一头水雾时找到的大佬博客，动力之源啊。 这应该是最难啃的一块骨头了，目测代码量瑟瑟发抖… def train(self, config): # 用adam优化 d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.d_loss, var_list=self.d_vars) g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.g_loss, var_list=self.g_vars) #避免tensorflow版本问题报错 try: tf.global_variables_initializer().run() except: tf.initialize_all_variables().run() # merge_summary 函数和 SummaryWriter 用于构建summary,在tensorboard中显示。 self.g_sum = merge_summary([self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum]) self.d_sum = merge_summary( [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum]) self.writer = SummaryWriter(&quot;./logs&quot;, self.sess.graph) # 噪音向量 [个数，维度] sample_z = np.random.uniform(-1, 1, size=(self.sample_num , self.z_dim)) if config.dataset == &#39;mnist&#39;: sample_inputs = self.data_X[0:self.sample_num] sample_labels = self.data_y[0:self.sample_num] else: #读取图片并且改变大小 sample_files = self.data[0:self.sample_num] sample = [ get_image(sample_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, crop=self.crop, grayscale=self.grayscale) for sample_file in sample_files] #如果处理的图像是灰度图像,则需要再增加一个dim,表示图像的 channel=1 if (self.grayscale): sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None] else: sample_inputs = np.array(sample).astype(np.float32) counter = 1 start_time = time.time() # 下载一波模型 could_load, checkpoint_counter = self.load(self.checkpoint_dir) # 下载成功 if could_load: counter = checkpoint_counter print(&quot; [*] Load SUCCESS&quot;) else: print(&quot; [!] Load failed...&quot;) # 循环训练开始 for epoch in xrange(config.epoch): if config.dataset == &#39;mnist&#39;: batch_idxs = min(len(self.data_X), config.train_size) // config.batch_size else: # 数据集所有数据文件名称 self.data = glob(os.path.join( config.data_dir, config.dataset, self.input_fname_pattern)) # 打乱 np.random.shuffle(self.data) # bath_size 表示一次指定几张图片 batch_idxs = min(len(self.data), config.train_size) // config.batch_size for idx in xrange(0, int(batch_idxs)): #每次循环的数据集抽取 if config.dataset == &#39;mnist&#39;: batch_images = self.data_X[idx*config.batch_size:(idx+1)*config.batch_size] batch_labels = self.data_y[idx*config.batch_size:(idx+1)*config.batch_size] else: batch_files = self.data[idx*config.batch_size:(idx+1)*config.batch_size] batch = [ get_image(batch_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, crop=self.crop, grayscale=self.grayscale) for batch_file in batch_files] if self.grayscale: batch_images = np.array(batch).astype(np.float32)[:, :, :, None] else: batch_images = np.array(batch).astype(np.float32) batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \ .astype(np.float32) # run if config.dataset == &#39;mnist&#39;: # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z, self.y:batch_labels }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images, self.y:batch_labels }) errG = self.g_loss.eval({ self.z: batch_z, self.y: batch_labels }) else: # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images }) errG = self.g_loss.eval({self.z: batch_z}) counter += 1 print(&quot;Epoch: [%2d/%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f&quot; \ % (epoch, config.epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG)) &#39;&#39;&#39; np.mod(counter, config.print_every) == 1 表示每 print_every 次生成一次samples; np.mod(counter, config.checkpoint_every) == 2 表示每 checkpoint_every 次保存一下 checkpoint file。 &#39;&#39;&#39; if np.mod(counter, 100) == 1: if config.dataset == &#39;mnist&#39;: samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, self.y:sample_labels, } ) save_images(samples, image_manifold_size(samples.shape[0]), &#39;./{}/train_{:02d}_{:04d}.png&#39;.format(config.sample_dir, epoch, idx)) print(&quot;[Sample] d_loss: %.8f, g_loss: %.8f&quot; % (d_loss, g_loss)) else: try: samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, }, ) save_images(samples, image_manifold_size(samples.shape[0]), &#39;./{}/train_{:02d}_{:04d}.png&#39;.format(config.sample_dir, epoch, idx)) print(&quot;[Sample] d_loss: %.8f, g_loss: %.8f&quot; % (d_loss, g_loss)) except: print(&quot;one pic error!...&quot;) if np.mod(counter, 500) == 2: self.save(config.checkpoint_dir, counter) 下载模型的代码 [11] def load(self, checkpoint_dir): import re print(&quot; [*] Reading checkpoints...&quot;) checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) ckpt = tf.train.get_checkpoint_state(checkpoint_dir) if ckpt and ckpt.model_checkpoint_path: ckpt_name = os.path.basename(ckpt.model_checkpoint_path) self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name)) # 不太清楚这句话有什么用 counter = int(next(re.finditer(&quot;(\d+)(?!.*\d)&quot;,ckpt_name)).group(0)) print(&quot; [*] Success to read {}&quot;.format(ckpt_name)) return True, counter else: print(&quot; [*] Failed to find a checkpoint&quot;) return False, 0" />
<meta property="og:description" content="参考 [1] DGAN代码简读 https://www.colabug.com/2958322.html [2]基于DCGAN的动漫头像生成神经网络实现 https://blog.csdn.net/sinat_33741547/article/details/77871170 [3] batch norm原理及代码详解 博客： https://blog.csdn.net/qq_25737169/article/details/79048516 视频：https://www.bilibili.com/video/av15405597?from=search&amp;seid=8881273700429864348 博客：https://www.cnblogs.com/eilearn/p/9780696.html 说明：2重点在代码，3重点在理论（但是一点都不枯燥），1有点像两个的结合。推荐2的系列视频以及资料，很精品，且免费。推荐看完视频再看3,而且3中有很多联立起来的理论，奉为神作不过分。 [3] tensorflow,tensorboard可视化网络结构 https://blog.csdn.net/helei001/article/details/51842531 [4] tensorflow变量作用域 https://www.cnblogs.com/MY0213/p/9208503.html [5] 此接[4],同时解决了当初jupyter notebook的困惑 https://blog.csdn.net/Jerr__y/article/details/70809528#commentBox [6] 激活函数 ReLU、Leaky ReLU、PReLU 和 RReLU http://www.cnblogs.com/chamie/p/8665251.html [7] conv2d函数简介 http://www.cnblogs.com/qggg/p/6832342.html [8] DCGAN 源码分析 https://blog.csdn.net/nongfu_spring/article/details/54342861/ [9] DCGAN代码简单读，里面有对应训练效果，这里看中了某个实现效果…so，做个记录先 http://www.cnblogs.com/lyrichu/p/9093411.html [10] 一个写的还算详细，但是排版和我彼此彼此的博客——题目：DCGAN https://blog.csdn.net/Candy_GL/article/details/81138297 [11] tensorflow保存下载模型 https://blog.csdn.net/xiezongsheng1990/article/details/81011115 说明：作者正在逐渐熟悉markdown…勿喷 [12] mmp的视频，早看到可以少走N多个坑（注：博主自己理了一遍代码以后，找论文时找到的视频，讲的对我来说正正好）看完视频，完善了部分代码注释 https://www.bilibili.com/video/av20033914/?p=13 [13] B站找的视频 其实到最后，疑惑的也就部分tensorflow框架以及代码了，所以此处学习一波回来再去完善一下唉 https://www.bilibili.com/video/av19360545/?p=1 代码解释 引入模块 from __future__ import division import os import time import math from glob import glob import tensorflow as tf import numpy as np from six.moves import xrange from ops import * from utils import * from _ future_ import division 这句话当python的版本为2.x时生效，可以让两个整数数字相除的结果返回一个浮点数(在python2中默认是整数,python3默认为浮点数)。 glob可以以简单的正则表达式筛选的方式返回某个文件夹下符合要求的文件名列表。[1] conv_out_size_same 函数 def conv_out_size_same(size, stride): return int(math.ceil(float(size) / float(stride))) math.ceil(): ceil() 函数返回数字的上入整数,即向上取整 stride : 步幅 整个函数返回了一个整形值， class DCGAN init def __init__(self, sess, input_height=108, input_width=108, crop=True, batch_size=64, sample_num = 64, output_height=64,output_width=64, y_dim=None, z_dim=100, gf_dim=64, df_dim=64, gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name=&#39;default&#39;, input_fname_pattern=&#39;*.jpg&#39;, checkpoint_dir=None, sample_dir=None, data_dir=&#39;./data&#39;): 具体参数用到再细谈，此处注意特殊的那个: y_dim就好 以下给出对应注释 &quot;&quot;&quot; Args: sess: TensorFlow session batch_size: The size of batch. Should be specified before training. y_dim: (optional) Dimension of dim for y. [None] z_dim: (optional) Dimension of dim for Z. [100] gf_dim: (optional) Dimension of gen filters in first conv layer.[64] df_dim: (optional) Dimension of discrim filters in first conv layer[64] gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024] dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024] c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3] &quot;&quot;&quot; self.sess = sess self.crop = crop self.batch_size = batch_size # 测试要用到，由G生成，看效果 self.sample_num = sample_num self.input_height = input_height self.input_width = input_width self.output_height = output_height self.output_width = output_width self.y_dim = y_dim # Gnerator最开始输入的噪音数据点的维度 self.z_dim = z_dim # generator的中卷积网络的filter的维度，以下同理 self.gf_dim = gf_dim self.df_dim = df_dim # generator全连接的维度 self.gfc_dim = gfc_dim self.dfc_dim = dfc_dim # batch normalization : deals with poor initialization helps gradient flow self.d_bn1 = batch_norm(name=&#39;d_bn1&#39;) self.d_bn2 = batch_norm(name=&#39;d_bn2&#39;) if not self.y_dim: self.d_bn3 = batch_norm(name=&#39;d_bn3&#39;) self.g_bn0 = batch_norm(name=&#39;g_bn0&#39;) self.g_bn1 = batch_norm(name=&#39;g_bn1&#39;) self.g_bn2 = batch_norm(name=&#39;g_bn2&#39;) if not self.y_dim: self.g_bn3 = batch_norm(name=&#39;g_bn3&#39;) self.dataset_name = dataset_name self.input_fname_pattern = input_fname_pattern self.checkpoint_dir = checkpoint_dir self.data_dir = data_dir 这段就是赋值而已 batch_norm 的水很深，需要一定功夫研究，具体参考[3], 此处贴出作者的一份回答：如果在每一层之后都归一化成0-1的高斯分布（减均值除方差）那么数据的分布一直都是高斯分布，数据分布都是固定的了，这样即使加更多层就没有意义了，深度网络就是想学习数据的分布发现规律性，BN就是不让学习的数据分布偏离太远，详细细节你可以去看论文。beta gama都是学习的，代码里他们定义的是variable， trainable是True。——个人感觉很有道理 具体学习详看[3] if self.dataset_name == &#39;mnist&#39;: self.data_X, self.data_y = self.load_mnist() self.c_dim = self.data_X[0].shape[-1] else: data_path = os.path.join(self.data_dir, self.dataset_name, self.input_fname_pattern) #返回的数据类型是list self.data = glob(data_path) #用它可以查找符合自己目的的文件 if len(self.data) == 0: raise Exception(&quot;[!] No data found in &#39;&quot; + data_path + &quot;&#39;&quot;) np.random.shuffle(self.data) #打乱数据 #读取一张图片 imreadImg = imread(self.data[0]) #check if image is a non-grayscale image by checking channel number if len(imreadImg.shape) &gt;= 3: self.c_dim = imread(self.data[0]).shape[-1] else: self.c_dim = 1 if len(self.data) &lt; self.batch_size: raise Exception(&quot;[!] Entire dataset size is less than the configured batch_size&quot;) DCGAN的构造方法除了设置一大堆的属性之外，还要注意区分dataset是否是mnist,因为mnist是灰度图像,所以应该设置channel = 1( self.c_dim = 1 ),如果是彩色图像，则 self.c_dim = 3 or self.c_dim = 4[1] load_mnist为自定义方法，方法实现如下 其余代码均有注释，整个代码块只是在读入数据集，并进行对应的可能报错处理。 def load_mnist(self): data_dir = os.path.join(self.data_dir, self.dataset_name) #用于路径拼接文件路径 &#39;&#39;&#39; train-images-idx3-ubyte: 参考网址：https://www.jianshu.com/p/84f72791806f 简介:这是IDX文件格式，是一种用来存储向量与多维度矩阵的文件格式 &#39;&#39;&#39; fd = open(os.path.join(data_dir,&#39;train-images-idx3-ubyte&#39;)) #创建文件对象 loaded = np.fromfile(file=fd,dtype=np.uint8) #numpy 读取文件 trX = loaded[16:].reshape((60000,28,28,1)).astype(np.float) #对应的数据处理 fd = open(os.path.join(data_dir,&#39;train-labels-idx1-ubyte&#39;)) loaded = np.fromfile(file=fd,dtype=np.uint8) trY = loaded[8:].reshape((60000)).astype(np.float) fd = open(os.path.join(data_dir,&#39;t10k-images-idx3-ubyte&#39;)) loaded = np.fromfile(file=fd,dtype=np.uint8) teX = loaded[16:].reshape((10000,28,28,1)).astype(np.float) fd = open(os.path.join(data_dir,&#39;t10k-labels-idx1-ubyte&#39;)) loaded = np.fromfile(file=fd,dtype=np.uint8) teY = loaded[8:].reshape((10000)).astype(np.float) # 将结构数据转化为ndarray， 例子详见： https://www.jb51.net/article/138281.htm # 可以理解为：两个变量用法同一份内存，只是数据格式不同 trY = np.asarray(trY) teY = np.asarray(teY) # 数组拼接 #concatenate([a, b]) #连接，连接后ndim不变，a和b可以有一维size不同，但size不同的维度必须是要连接的维度 X = np.concatenate((trX, teX), axis=0) y = np.concatenate((trY, teY), axis=0).astype(np.int) # seed # http://www.cnblogs.com/subic/p/8454025.html # 简单介绍： seed = 547 np.random.seed(seed) np.random.shuffle(X) #np.random.shuffle np.random.seed(seed) np.random.shuffle(y) y_vec = np.zeros((len(y), self.y_dim), dtype=np.float) for i, label in enumerate(y): y_vec[i,y[i]] = 1.0 return X/255.,y_vec 具体参考注释内容 #是否为灰度 self.grayscale = (self.c_dim == 1) #自定义的方法 self.build_model() 无特殊说明 build_model(self) def build_model(self): #可以理解为申明了一个（batch_size,y_dim）大小的地方，用于mini_batc时往里面喂对应的数据。 if self.y_dim: self.y = tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name=&#39;y&#39;) else: self.y = None #is_crop为要不要裁剪图像 [True or False] #所以此处是判断图像是否裁剪过 # https://www.jianshu.com/p/3e46ce8e7ddd if self.crop: # 把输出维度定义好 image_dims = [self.output_height, self.output_width, self.c_dim] else: image_dims = [self.input_height, self.input_width, self.c_dim] #道理同上面解释，真实图片的输入[batchsize,height,width,c_dim] self.inputs = tf.placeholder( tf.float32, [self.batch_size] + image_dims, name=&#39;real_images&#39;) inputs = self.inputs # 噪音数据（None表示多少都可以，实际用到再填充batch） self.z = tf.placeholder( tf.float32, [None, self.z_dim], name=&#39;z&#39;) # 位于 opt.py里 # histogram_summary = tf.histogram_summary # 将z在tensorboard里可视化 self.z_sum = histogram_summary(&quot;z&quot;, self.z) # 以下为网络结构 self.G = self.generator(self.z, self.y) #真实数据 self.D, self.D_logits = self.discriminator(inputs, self.y, reuse=False) # 测试网络 self.sampler = self.sampler(self.z, self.y) # G产生的数据 self.D_, self.D_logits_ = self.discriminator(self.G, self.y, reuse=True) # 同上可视化 self.d_sum = histogram_summary(&quot;d&quot;, self.D) self.d__sum = histogram_summary(&quot;d_&quot;, self.D_) # opt.py # image_summary = tf.image_summary # tensorboar内图像可视化 ： https://blog.csdn.net/dxmkkk/article/details/54925728 self.G_sum = image_summary(&quot;G&quot;, self.G) 显而易见，网络结构才是重点，其他都是陪衬。大致介绍参考[1]: self.generator 用于构造生成器; self.discriminator 用于构造鉴别器; self.sampler 用于随机采样(用于生成样本)。这里需要注意的是, self.y 只有当dataset是mnist的时候才不为None,不是mnist的情况下,只需要 self.z 即可生成samples sigmoid_cross_entropy_with_logits(x, y) （内置于build_model） def sigmoid_cross_entropy_with_logits(x, y): try: return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y) except: return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, targets=y) [1] sigmoid_cross_entropy_with_logits 函数被重新定义了，是为了兼容不同版本的tensorflow。该函数首先使用sigmoid activation，然后计算cross-entropy loss。 这个函数有自己的定义，可以参考： https://blog.csdn.net/QW_sunny/article/details/72885403 作者为对此函数进行过深了解，仅限于清楚大致处理过程。 ​ 承接上面代码 # 真实图片的鉴别器损失 # D_logits,鉴别网络对真实数据的评分， tf.ones_like(self.D)，与D大小一样的1. # tf.nn.sigmoid(h3), h3 # 可以理解为两参数分别为x,y。 x是网络产生，y是实际希望，然后通过这个函数计算出一个损失值。不同于一般的平方求和，此函数解决了sigmoid梯度缓慢的问题，并且另外考虑了溢出的问题。 self.d_loss_real = tf.reduce_mean( sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D))) # 虚假图片损失 self.d_loss_fake = tf.reduce_mean( sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_))) # 生成器损失 self.g_loss = tf.reduce_mean( sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_))) # 可视化 self.d_loss_real_sum = scalar_summary(&quot;d_loss_real&quot;, self.d_loss_real) self.d_loss_fake_sum = scalar_summary(&quot;d_loss_fake&quot;, self.d_loss_fake) # 总的鉴别器损失，越小越好 self.d_loss = self.d_loss_real + self.d_loss_fake sigmoid_cross_entropy_with_logits的output不是一个数，而是一个batch中每个样本的loss,所以一般配合tf.reduce_mean(loss)使用 self.g_loss 是生成器损失; self.d_loss_real 是真实图片的鉴别器损失; self.d_loss_fake 是虚假图片(由生成器生成的fake images)的损失; self.d_loss 是总的鉴别器损失。 # 可视化 self.g_loss_sum = scalar_summary(&quot;g_loss&quot;, self.g_loss) self.d_loss_sum = scalar_summary(&quot;d_loss&quot;, self.d_loss) # 所有的变量 t_vars = tf.trainable_variables() # 寻找所有以d_开头的变量，优化时的参数（例如：如果是梯度下降，我们需要将权重和loss传进去，这里d_vars代表generator里要优化的变量） self.d_vars = [var for var in t_vars if &#39;d_&#39; in var.name] self.g_vars = [var for var in t_vars if &#39;g_&#39; in var.name] # 保存节点 self.saver = tf.train.Saver() [1] tf.trainable_variables() 可以获取model的全部可训练参数,由于我们在定义生成器和鉴别器变量的时候使用了不同的name,因此我们可以通过variable的name来获取得到 self.d_vars (鉴别器相关变量), self.g_vars (生成器相关变量)。 self.saver = tf.train.Saver() 用于保存训练好的模型参数到checkpoint。 discriminator def discriminator(self, image, y=None, reuse=False): # 变量共享，我理解为： 即使函数被重复调用，其共享的节点不会重复定义、声明 [5] with tf.variable_scope(&quot;discriminator&quot;) as scope: if reuse: scope.reuse_variables() # 如果没有这一维 if not self.y_dim: # conv2d简介具体看 [7]，但第二个参数的介绍是不对的，补上[8]. &#39;&#39;&#39; 参数简介： 第一个参数input：具有[batch, in_height, in_width, in_channels]这样的shape 第二个参数filter：conv2d中输出的特征图个数，是个1维的参数，即output_dim， output_dim是 conv2d函数的第二个入参，由外部传入。比如，下面的这句话，表示h1 是输入，通过卷积之后，输出的特征图个数为gf_dim* *4，这里gf_dim = 128，则输出 特征图为128*4=512个。即这里一共有512个卷积核。 第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4。[1, d_h, d_w, 1]，其中第一个对应一次跳过batch中的多少图片，第二个d_h对应一次跳过图片中 多少行，第三个d_w对应一次跳过图片中多少列，第四个对应一次跳过图像的多少个通道。这 里直接设置为[1，2，2，1]。即每次卷积后，图像的滑动步长为2，特征图会缩小为原来的 1/4。 第四个参数padding：string类型的量，只能是&quot;SAME&quot;,&quot;VALID&quot;其中之一，这个值决定了不 同的卷积方式 第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true 第六个参数：name=None &#39;&#39;&#39; # df_dim 作为类的参数传进来 # df_dim: (optional) Dimension of discrim filters in first conv layer[64] # lrelu 在opt.py 中 # 输入是一个 image h0 = lrelu(conv2d(image, self.df_dim, name=&#39;d_h0_conv&#39;)) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name=&#39;d_h1_conv&#39;))) h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name=&#39;d_h2_conv&#39;))) h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name=&#39;d_h3_conv&#39;))) #全连接层，-1表示不确定大小。 linear 在opt.py中,自己构建的。 h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, &#39;d_h4_lin&#39;) return tf.nn.sigmoid(h4), h4 else: # y_dim = 10 yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) # image被当作参数传进来的参数，Builder里是inputs. # 原型在opt里，基于concat函数基础上修改。这里并没有理解为什么要这样操作.... # 这里代码以及构建不难，但是这里所有的concat尚且不了解原因。 x = conv_cond_concat(image, yb) h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name=&#39;d_h0_conv&#39;)) h0 = conv_cond_concat(h0, yb) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name=&#39;d_h1_conv&#39;))) h1 = tf.reshape(h1, [self.batch_size, -1]) h1 = concat([h1, y], 1) h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, &#39;d_h2_lin&#39;))) h2 = concat([h2, y], 1) h3 = linear(h2, 1, &#39;d_h3_lin&#39;) return tf.nn.sigmoid(h3), h3 [1] 下面是discriminator(鉴别器)的具体实现。 首先鉴别器使用 conv (卷积)操作，激活函数使用 leaky-relu ,每一个layer需要使用batch normalization。tensorflow的batch normalization使用 tf.contrib.layers.batch_norm 实现。 如果不是mnist,则第一层使用 leaky-relu+conv2d ,后面三层都使用 conv2d+BN+leaky-relu ,最后加上一个one hidden unit的linear layer,再送入sigmoid函数即可； 如果是mnist,则 yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) 首先给y增加两维，以便可以和image连接起来，这里实际上是使用了conditional GAN(条件GAN)的思想。 x = conv_cond_concat(image, yb) 得到condition和image合并之后的结果，然后 h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name=‘d_h0_conv’)) 进行卷积操作。第二次进行 conv2d+leaky-relu+concat 操作。第三次进行 conv2d+BN+leaky-relu+reshape+concat 操作。第四次进行 linear+BN+leaky-relu+concat 操作。最后同样是 linear+sigmoid 操作。 补充：卷积网络 作者了解卷积神经网络的基础知识，但未真正自己建立过卷积神经网络，此处不甘心于函数，所以此处去重新学习了下整个卷积网络的构建,详见 卷积网络学习系列1.md 作者在温习以后才去标注的discriminator的卷积注释，所以写的不够小白。具体原理详见上。 generator def generator(self, z, y=None): with tf.variable_scope(&quot;generator&quot;) as scope: if not self.y_dim: s_h, s_w = self.output_height, self.output_width # return int(math.ceil(float(size) / float(stride))) s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape #self.z = tf.placeholder(tf.float32, [None, self.z_dim], name=&#39;z&#39;) # gf_dim是generator的filter大小的基，*8 *4 *2 *1 c_dim这是他的变化规律，下面代码内找找仔细观察看看 self.z_, self.h0_w, self.h0_b = linear( z, self.gf_dim*8*s_h16*s_w16, &#39;g_h0_lin&#39;, with_w=True) # 确定三个维度，可以计算出最后一个维度，那么这个维度可以用-1表示。 将全连接层reshape成特征图形状 self.h0 = tf.reshape( self.z_, [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(self.h0)) # 产生batch_size个 设置好大小的特征图 self.h1, self.h1_w, self.h1_b = deconv2d( h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name=&#39;g_h1&#39;, with_w=True) h1 = tf.nn.relu(self.g_bn1(self.h1)) h2, self.h2_w, self.h2_b = deconv2d( h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name=&#39;g_h2&#39;, with_w=True) h2 = tf.nn.relu(self.g_bn2(h2)) h3, self.h3_w, self.h3_b = deconv2d( h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name=&#39;g_h3&#39;, with_w=True) h3 = tf.nn.relu(self.g_bn3(h3)) # c_dim,通道数 3 h4, self.h4_w, self.h4_b = deconv2d( h3, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h4&#39;, with_w=True) return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.expand_dims(tf.expand_dims(y, 1),2) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu( self.g_bn0(linear(z, self.gfc_dim, &#39;g_h0_lin&#39;))) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, &#39;g_h1_lin&#39;))) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name=&#39;g_h2&#39;))) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid( deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h3&#39;)) [1]下面是generator(生成器)的具体实现。和discriminator不同的是,generator需要使用deconv(反卷积)以及relu 激活函数。 generator的结构是: 1.如果不是mnist: linear+reshape+BN+relu----&gt;(deconv+BN+relu)x3 ----&gt;deconv+tanh ; 2.如果是mnist,则除了需要考虑输入z之外，还需要考虑label y,即需要将z和y连接起来(Conditional GAN),具体的结构是:reshape+concat----&gt;linear+BN+relu+concat----&gt;linear+BN+relu+reshape+concat----&gt;deconv+BN+relu+concat----&gt;deconv+sigmoid。 注意的最后的激活函数没有采用通常的tanh,而是采用了sigmoid(其输出会直接映射到0-1之间)。 大佬们都不解释为什么要这样构建模型（补充：其实看里很久才知道，大佬们不说是因为原论文就是这样写构建的…不必纠结这个结构，因为还没到能设计结构的水平），所以，我只能硬着头皮自己解释。个人感觉，generator是从一个小输入，不断变大，变大，变成图片大小的过程，反卷积就用在这里。 def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False): shape = input_.get_shape().as_list() with tf.variable_scope(scope or &quot;Linear&quot;): try: matrix = tf.get_variable(&quot;Matrix&quot;, [shape[1], output_size], tf.float32, tf.random_normal_initializer(stddev=stddev)) except ValueError as err: msg = &quot;NOTE: Usually, this is due to an issue with the image dimensions. Did you correctly set &#39;--crop&#39; or &#39;--input_height&#39; or &#39;--output_height&#39;?&quot; err.args = err.args + (msg,) raise bias = tf.get_variable(&quot;bias&quot;, [output_size], initializer=tf.constant_initializer(bias_start)) # 是否输出权重 if with_w: return tf.matmul(input_, matrix) + bias, matrix, bias else: return tf.matmul(input_, matrix) + bias 反卷积 恭喜，又是一个没听过的知识点… 此处另外学习反卷积，详见反卷积系列。 def deconv2d(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=&quot;deconv2d&quot;, with_w=False): with tf.variable_scope(name): # filter : [height, width, output_channels, in_channels] w = tf.get_variable(&#39;w&#39;, [k_h, k_w, output_shape[-1], input_.get_shape()[-1]], initializer=tf.random_normal_initializer(stddev=stddev)) try: deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1]) # Support for verisons of TensorFlow before 0.7.0 except AttributeError: # d_h,d_w参考原论文 deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1]) # b的大小跟输出的特征图个数一样 biases = tf.get_variable(&#39;biases&#39;, [output_shape[-1]], initializer=tf.constant_initializer(0.0)) # 先反卷积，后加偏置 deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape()) # 是否输出权重 if with_w: return deconv, w, biases else: return deconv sampler def sampler(self, z, y=None): with tf.variable_scope(&quot;generator&quot;) as scope: scope.reuse_variables() if not self.y_dim: s_h, s_w = self.output_height, self.output_width s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape h0 = tf.reshape( linear(z, self.gf_dim*8*s_h16*s_w16, &#39;g_h0_lin&#39;), [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(h0, train=False)) h1 = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name=&#39;g_h1&#39;) h1 = tf.nn.relu(self.g_bn1(h1, train=False)) h2 = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name=&#39;g_h2&#39;) h2 = tf.nn.relu(self.g_bn2(h2, train=False)) h3 = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name=&#39;g_h3&#39;) h3 = tf.nn.relu(self.g_bn3(h3, train=False)) h4 = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h4&#39;) return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.reshape(y, [-1, 1, 1, self.y_dim]) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, &#39;g_h0_lin&#39;), train=False)) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, &#39;g_h1_lin&#39;), train=False)) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2( deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name=&#39;g_h2&#39;), train=False)) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h3&#39;)) sampler函数是采样函数，用于生成样本送入当前训练的生成器，查看训练效果。其逻辑和generator函数基本类似,也是需要区分是否是mnist,二者需要采用不同的结构。不是mnist时,y=None即可；否则mnist还需要考虑y。 如果单纯去看代码，前面那么代码的铺垫已经够了，但是，不理解为什么要用这个sampler,有什么用，怎么感觉他在做和generator一样的事儿。 具体为什么用它，得等到最后我去啃DGCAN原论文时候再了解吧。 DCGAN原文解读 此处为看完train代码后补的，单单看懂代码的大致逻辑很容易，但是细节完全把握不住，由于前面没有取细读论文，这里集中爆发了，感觉确实需要读完论文再往下做，所以，开始论文时间。 train 函数 补充，终于到这儿了，突然感觉真的好多啊，以前没有这样做过，但这样来一遍，发现自己啥都不是，学到了很多，但也知道很多自己都没写出来，水平有限。当然，万分感谢那些我一头水雾时找到的大佬博客，动力之源啊。 这应该是最难啃的一块骨头了，目测代码量瑟瑟发抖… def train(self, config): # 用adam优化 d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.d_loss, var_list=self.d_vars) g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.g_loss, var_list=self.g_vars) #避免tensorflow版本问题报错 try: tf.global_variables_initializer().run() except: tf.initialize_all_variables().run() # merge_summary 函数和 SummaryWriter 用于构建summary,在tensorboard中显示。 self.g_sum = merge_summary([self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum]) self.d_sum = merge_summary( [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum]) self.writer = SummaryWriter(&quot;./logs&quot;, self.sess.graph) # 噪音向量 [个数，维度] sample_z = np.random.uniform(-1, 1, size=(self.sample_num , self.z_dim)) if config.dataset == &#39;mnist&#39;: sample_inputs = self.data_X[0:self.sample_num] sample_labels = self.data_y[0:self.sample_num] else: #读取图片并且改变大小 sample_files = self.data[0:self.sample_num] sample = [ get_image(sample_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, crop=self.crop, grayscale=self.grayscale) for sample_file in sample_files] #如果处理的图像是灰度图像,则需要再增加一个dim,表示图像的 channel=1 if (self.grayscale): sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None] else: sample_inputs = np.array(sample).astype(np.float32) counter = 1 start_time = time.time() # 下载一波模型 could_load, checkpoint_counter = self.load(self.checkpoint_dir) # 下载成功 if could_load: counter = checkpoint_counter print(&quot; [*] Load SUCCESS&quot;) else: print(&quot; [!] Load failed...&quot;) # 循环训练开始 for epoch in xrange(config.epoch): if config.dataset == &#39;mnist&#39;: batch_idxs = min(len(self.data_X), config.train_size) // config.batch_size else: # 数据集所有数据文件名称 self.data = glob(os.path.join( config.data_dir, config.dataset, self.input_fname_pattern)) # 打乱 np.random.shuffle(self.data) # bath_size 表示一次指定几张图片 batch_idxs = min(len(self.data), config.train_size) // config.batch_size for idx in xrange(0, int(batch_idxs)): #每次循环的数据集抽取 if config.dataset == &#39;mnist&#39;: batch_images = self.data_X[idx*config.batch_size:(idx+1)*config.batch_size] batch_labels = self.data_y[idx*config.batch_size:(idx+1)*config.batch_size] else: batch_files = self.data[idx*config.batch_size:(idx+1)*config.batch_size] batch = [ get_image(batch_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, crop=self.crop, grayscale=self.grayscale) for batch_file in batch_files] if self.grayscale: batch_images = np.array(batch).astype(np.float32)[:, :, :, None] else: batch_images = np.array(batch).astype(np.float32) batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \ .astype(np.float32) # run if config.dataset == &#39;mnist&#39;: # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z, self.y:batch_labels }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images, self.y:batch_labels }) errG = self.g_loss.eval({ self.z: batch_z, self.y: batch_labels }) else: # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images }) errG = self.g_loss.eval({self.z: batch_z}) counter += 1 print(&quot;Epoch: [%2d/%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f&quot; \ % (epoch, config.epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG)) &#39;&#39;&#39; np.mod(counter, config.print_every) == 1 表示每 print_every 次生成一次samples; np.mod(counter, config.checkpoint_every) == 2 表示每 checkpoint_every 次保存一下 checkpoint file。 &#39;&#39;&#39; if np.mod(counter, 100) == 1: if config.dataset == &#39;mnist&#39;: samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, self.y:sample_labels, } ) save_images(samples, image_manifold_size(samples.shape[0]), &#39;./{}/train_{:02d}_{:04d}.png&#39;.format(config.sample_dir, epoch, idx)) print(&quot;[Sample] d_loss: %.8f, g_loss: %.8f&quot; % (d_loss, g_loss)) else: try: samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, }, ) save_images(samples, image_manifold_size(samples.shape[0]), &#39;./{}/train_{:02d}_{:04d}.png&#39;.format(config.sample_dir, epoch, idx)) print(&quot;[Sample] d_loss: %.8f, g_loss: %.8f&quot; % (d_loss, g_loss)) except: print(&quot;one pic error!...&quot;) if np.mod(counter, 500) == 2: self.save(config.checkpoint_dir, counter) 下载模型的代码 [11] def load(self, checkpoint_dir): import re print(&quot; [*] Reading checkpoints...&quot;) checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) ckpt = tf.train.get_checkpoint_state(checkpoint_dir) if ckpt and ckpt.model_checkpoint_path: ckpt_name = os.path.basename(ckpt.model_checkpoint_path) self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name)) # 不太清楚这句话有什么用 counter = int(next(re.finditer(&quot;(\d+)(?!.*\d)&quot;,ckpt_name)).group(0)) print(&quot; [*] Success to read {}&quot;.format(ckpt_name)) return True, counter else: print(&quot; [*] Failed to find a checkpoint&quot;) return False, 0" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-09T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"参考 [1] DGAN代码简读 https://www.colabug.com/2958322.html [2]基于DCGAN的动漫头像生成神经网络实现 https://blog.csdn.net/sinat_33741547/article/details/77871170 [3] batch norm原理及代码详解 博客： https://blog.csdn.net/qq_25737169/article/details/79048516 视频：https://www.bilibili.com/video/av15405597?from=search&amp;seid=8881273700429864348 博客：https://www.cnblogs.com/eilearn/p/9780696.html 说明：2重点在代码，3重点在理论（但是一点都不枯燥），1有点像两个的结合。推荐2的系列视频以及资料，很精品，且免费。推荐看完视频再看3,而且3中有很多联立起来的理论，奉为神作不过分。 [3] tensorflow,tensorboard可视化网络结构 https://blog.csdn.net/helei001/article/details/51842531 [4] tensorflow变量作用域 https://www.cnblogs.com/MY0213/p/9208503.html [5] 此接[4],同时解决了当初jupyter notebook的困惑 https://blog.csdn.net/Jerr__y/article/details/70809528#commentBox [6] 激活函数 ReLU、Leaky ReLU、PReLU 和 RReLU http://www.cnblogs.com/chamie/p/8665251.html [7] conv2d函数简介 http://www.cnblogs.com/qggg/p/6832342.html [8] DCGAN 源码分析 https://blog.csdn.net/nongfu_spring/article/details/54342861/ [9] DCGAN代码简单读，里面有对应训练效果，这里看中了某个实现效果…so，做个记录先 http://www.cnblogs.com/lyrichu/p/9093411.html [10] 一个写的还算详细，但是排版和我彼此彼此的博客——题目：DCGAN https://blog.csdn.net/Candy_GL/article/details/81138297 [11] tensorflow保存下载模型 https://blog.csdn.net/xiezongsheng1990/article/details/81011115 说明：作者正在逐渐熟悉markdown…勿喷 [12] mmp的视频，早看到可以少走N多个坑（注：博主自己理了一遍代码以后，找论文时找到的视频，讲的对我来说正正好）看完视频，完善了部分代码注释 https://www.bilibili.com/video/av20033914/?p=13 [13] B站找的视频 其实到最后，疑惑的也就部分tensorflow框架以及代码了，所以此处学习一波回来再去完善一下唉 https://www.bilibili.com/video/av19360545/?p=1 代码解释 引入模块 from __future__ import division import os import time import math from glob import glob import tensorflow as tf import numpy as np from six.moves import xrange from ops import * from utils import * from _ future_ import division 这句话当python的版本为2.x时生效，可以让两个整数数字相除的结果返回一个浮点数(在python2中默认是整数,python3默认为浮点数)。 glob可以以简单的正则表达式筛选的方式返回某个文件夹下符合要求的文件名列表。[1] conv_out_size_same 函数 def conv_out_size_same(size, stride): return int(math.ceil(float(size) / float(stride))) math.ceil(): ceil() 函数返回数字的上入整数,即向上取整 stride : 步幅 整个函数返回了一个整形值， class DCGAN init def __init__(self, sess, input_height=108, input_width=108, crop=True, batch_size=64, sample_num = 64, output_height=64,output_width=64, y_dim=None, z_dim=100, gf_dim=64, df_dim=64, gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name=&#39;default&#39;, input_fname_pattern=&#39;*.jpg&#39;, checkpoint_dir=None, sample_dir=None, data_dir=&#39;./data&#39;): 具体参数用到再细谈，此处注意特殊的那个: y_dim就好 以下给出对应注释 &quot;&quot;&quot; Args: sess: TensorFlow session batch_size: The size of batch. Should be specified before training. y_dim: (optional) Dimension of dim for y. [None] z_dim: (optional) Dimension of dim for Z. [100] gf_dim: (optional) Dimension of gen filters in first conv layer.[64] df_dim: (optional) Dimension of discrim filters in first conv layer[64] gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024] dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024] c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3] &quot;&quot;&quot; self.sess = sess self.crop = crop self.batch_size = batch_size # 测试要用到，由G生成，看效果 self.sample_num = sample_num self.input_height = input_height self.input_width = input_width self.output_height = output_height self.output_width = output_width self.y_dim = y_dim # Gnerator最开始输入的噪音数据点的维度 self.z_dim = z_dim # generator的中卷积网络的filter的维度，以下同理 self.gf_dim = gf_dim self.df_dim = df_dim # generator全连接的维度 self.gfc_dim = gfc_dim self.dfc_dim = dfc_dim # batch normalization : deals with poor initialization helps gradient flow self.d_bn1 = batch_norm(name=&#39;d_bn1&#39;) self.d_bn2 = batch_norm(name=&#39;d_bn2&#39;) if not self.y_dim: self.d_bn3 = batch_norm(name=&#39;d_bn3&#39;) self.g_bn0 = batch_norm(name=&#39;g_bn0&#39;) self.g_bn1 = batch_norm(name=&#39;g_bn1&#39;) self.g_bn2 = batch_norm(name=&#39;g_bn2&#39;) if not self.y_dim: self.g_bn3 = batch_norm(name=&#39;g_bn3&#39;) self.dataset_name = dataset_name self.input_fname_pattern = input_fname_pattern self.checkpoint_dir = checkpoint_dir self.data_dir = data_dir 这段就是赋值而已 batch_norm 的水很深，需要一定功夫研究，具体参考[3], 此处贴出作者的一份回答：如果在每一层之后都归一化成0-1的高斯分布（减均值除方差）那么数据的分布一直都是高斯分布，数据分布都是固定的了，这样即使加更多层就没有意义了，深度网络就是想学习数据的分布发现规律性，BN就是不让学习的数据分布偏离太远，详细细节你可以去看论文。beta gama都是学习的，代码里他们定义的是variable， trainable是True。——个人感觉很有道理 具体学习详看[3] if self.dataset_name == &#39;mnist&#39;: self.data_X, self.data_y = self.load_mnist() self.c_dim = self.data_X[0].shape[-1] else: data_path = os.path.join(self.data_dir, self.dataset_name, self.input_fname_pattern) #返回的数据类型是list self.data = glob(data_path) #用它可以查找符合自己目的的文件 if len(self.data) == 0: raise Exception(&quot;[!] No data found in &#39;&quot; + data_path + &quot;&#39;&quot;) np.random.shuffle(self.data) #打乱数据 #读取一张图片 imreadImg = imread(self.data[0]) #check if image is a non-grayscale image by checking channel number if len(imreadImg.shape) &gt;= 3: self.c_dim = imread(self.data[0]).shape[-1] else: self.c_dim = 1 if len(self.data) &lt; self.batch_size: raise Exception(&quot;[!] Entire dataset size is less than the configured batch_size&quot;) DCGAN的构造方法除了设置一大堆的属性之外，还要注意区分dataset是否是mnist,因为mnist是灰度图像,所以应该设置channel = 1( self.c_dim = 1 ),如果是彩色图像，则 self.c_dim = 3 or self.c_dim = 4[1] load_mnist为自定义方法，方法实现如下 其余代码均有注释，整个代码块只是在读入数据集，并进行对应的可能报错处理。 def load_mnist(self): data_dir = os.path.join(self.data_dir, self.dataset_name) #用于路径拼接文件路径 &#39;&#39;&#39; train-images-idx3-ubyte: 参考网址：https://www.jianshu.com/p/84f72791806f 简介:这是IDX文件格式，是一种用来存储向量与多维度矩阵的文件格式 &#39;&#39;&#39; fd = open(os.path.join(data_dir,&#39;train-images-idx3-ubyte&#39;)) #创建文件对象 loaded = np.fromfile(file=fd,dtype=np.uint8) #numpy 读取文件 trX = loaded[16:].reshape((60000,28,28,1)).astype(np.float) #对应的数据处理 fd = open(os.path.join(data_dir,&#39;train-labels-idx1-ubyte&#39;)) loaded = np.fromfile(file=fd,dtype=np.uint8) trY = loaded[8:].reshape((60000)).astype(np.float) fd = open(os.path.join(data_dir,&#39;t10k-images-idx3-ubyte&#39;)) loaded = np.fromfile(file=fd,dtype=np.uint8) teX = loaded[16:].reshape((10000,28,28,1)).astype(np.float) fd = open(os.path.join(data_dir,&#39;t10k-labels-idx1-ubyte&#39;)) loaded = np.fromfile(file=fd,dtype=np.uint8) teY = loaded[8:].reshape((10000)).astype(np.float) # 将结构数据转化为ndarray， 例子详见： https://www.jb51.net/article/138281.htm # 可以理解为：两个变量用法同一份内存，只是数据格式不同 trY = np.asarray(trY) teY = np.asarray(teY) # 数组拼接 #concatenate([a, b]) #连接，连接后ndim不变，a和b可以有一维size不同，但size不同的维度必须是要连接的维度 X = np.concatenate((trX, teX), axis=0) y = np.concatenate((trY, teY), axis=0).astype(np.int) # seed # http://www.cnblogs.com/subic/p/8454025.html # 简单介绍： seed = 547 np.random.seed(seed) np.random.shuffle(X) #np.random.shuffle np.random.seed(seed) np.random.shuffle(y) y_vec = np.zeros((len(y), self.y_dim), dtype=np.float) for i, label in enumerate(y): y_vec[i,y[i]] = 1.0 return X/255.,y_vec 具体参考注释内容 #是否为灰度 self.grayscale = (self.c_dim == 1) #自定义的方法 self.build_model() 无特殊说明 build_model(self) def build_model(self): #可以理解为申明了一个（batch_size,y_dim）大小的地方，用于mini_batc时往里面喂对应的数据。 if self.y_dim: self.y = tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name=&#39;y&#39;) else: self.y = None #is_crop为要不要裁剪图像 [True or False] #所以此处是判断图像是否裁剪过 # https://www.jianshu.com/p/3e46ce8e7ddd if self.crop: # 把输出维度定义好 image_dims = [self.output_height, self.output_width, self.c_dim] else: image_dims = [self.input_height, self.input_width, self.c_dim] #道理同上面解释，真实图片的输入[batchsize,height,width,c_dim] self.inputs = tf.placeholder( tf.float32, [self.batch_size] + image_dims, name=&#39;real_images&#39;) inputs = self.inputs # 噪音数据（None表示多少都可以，实际用到再填充batch） self.z = tf.placeholder( tf.float32, [None, self.z_dim], name=&#39;z&#39;) # 位于 opt.py里 # histogram_summary = tf.histogram_summary # 将z在tensorboard里可视化 self.z_sum = histogram_summary(&quot;z&quot;, self.z) # 以下为网络结构 self.G = self.generator(self.z, self.y) #真实数据 self.D, self.D_logits = self.discriminator(inputs, self.y, reuse=False) # 测试网络 self.sampler = self.sampler(self.z, self.y) # G产生的数据 self.D_, self.D_logits_ = self.discriminator(self.G, self.y, reuse=True) # 同上可视化 self.d_sum = histogram_summary(&quot;d&quot;, self.D) self.d__sum = histogram_summary(&quot;d_&quot;, self.D_) # opt.py # image_summary = tf.image_summary # tensorboar内图像可视化 ： https://blog.csdn.net/dxmkkk/article/details/54925728 self.G_sum = image_summary(&quot;G&quot;, self.G) 显而易见，网络结构才是重点，其他都是陪衬。大致介绍参考[1]: self.generator 用于构造生成器; self.discriminator 用于构造鉴别器; self.sampler 用于随机采样(用于生成样本)。这里需要注意的是, self.y 只有当dataset是mnist的时候才不为None,不是mnist的情况下,只需要 self.z 即可生成samples sigmoid_cross_entropy_with_logits(x, y) （内置于build_model） def sigmoid_cross_entropy_with_logits(x, y): try: return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y) except: return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, targets=y) [1] sigmoid_cross_entropy_with_logits 函数被重新定义了，是为了兼容不同版本的tensorflow。该函数首先使用sigmoid activation，然后计算cross-entropy loss。 这个函数有自己的定义，可以参考： https://blog.csdn.net/QW_sunny/article/details/72885403 作者为对此函数进行过深了解，仅限于清楚大致处理过程。 ​ 承接上面代码 # 真实图片的鉴别器损失 # D_logits,鉴别网络对真实数据的评分， tf.ones_like(self.D)，与D大小一样的1. # tf.nn.sigmoid(h3), h3 # 可以理解为两参数分别为x,y。 x是网络产生，y是实际希望，然后通过这个函数计算出一个损失值。不同于一般的平方求和，此函数解决了sigmoid梯度缓慢的问题，并且另外考虑了溢出的问题。 self.d_loss_real = tf.reduce_mean( sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D))) # 虚假图片损失 self.d_loss_fake = tf.reduce_mean( sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_))) # 生成器损失 self.g_loss = tf.reduce_mean( sigmoid_cross_entropy_with_logits(self.D_logits_, tf.ones_like(self.D_))) # 可视化 self.d_loss_real_sum = scalar_summary(&quot;d_loss_real&quot;, self.d_loss_real) self.d_loss_fake_sum = scalar_summary(&quot;d_loss_fake&quot;, self.d_loss_fake) # 总的鉴别器损失，越小越好 self.d_loss = self.d_loss_real + self.d_loss_fake sigmoid_cross_entropy_with_logits的output不是一个数，而是一个batch中每个样本的loss,所以一般配合tf.reduce_mean(loss)使用 self.g_loss 是生成器损失; self.d_loss_real 是真实图片的鉴别器损失; self.d_loss_fake 是虚假图片(由生成器生成的fake images)的损失; self.d_loss 是总的鉴别器损失。 # 可视化 self.g_loss_sum = scalar_summary(&quot;g_loss&quot;, self.g_loss) self.d_loss_sum = scalar_summary(&quot;d_loss&quot;, self.d_loss) # 所有的变量 t_vars = tf.trainable_variables() # 寻找所有以d_开头的变量，优化时的参数（例如：如果是梯度下降，我们需要将权重和loss传进去，这里d_vars代表generator里要优化的变量） self.d_vars = [var for var in t_vars if &#39;d_&#39; in var.name] self.g_vars = [var for var in t_vars if &#39;g_&#39; in var.name] # 保存节点 self.saver = tf.train.Saver() [1] tf.trainable_variables() 可以获取model的全部可训练参数,由于我们在定义生成器和鉴别器变量的时候使用了不同的name,因此我们可以通过variable的name来获取得到 self.d_vars (鉴别器相关变量), self.g_vars (生成器相关变量)。 self.saver = tf.train.Saver() 用于保存训练好的模型参数到checkpoint。 discriminator def discriminator(self, image, y=None, reuse=False): # 变量共享，我理解为： 即使函数被重复调用，其共享的节点不会重复定义、声明 [5] with tf.variable_scope(&quot;discriminator&quot;) as scope: if reuse: scope.reuse_variables() # 如果没有这一维 if not self.y_dim: # conv2d简介具体看 [7]，但第二个参数的介绍是不对的，补上[8]. &#39;&#39;&#39; 参数简介： 第一个参数input：具有[batch, in_height, in_width, in_channels]这样的shape 第二个参数filter：conv2d中输出的特征图个数，是个1维的参数，即output_dim， output_dim是 conv2d函数的第二个入参，由外部传入。比如，下面的这句话，表示h1 是输入，通过卷积之后，输出的特征图个数为gf_dim* *4，这里gf_dim = 128，则输出 特征图为128*4=512个。即这里一共有512个卷积核。 第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4。[1, d_h, d_w, 1]，其中第一个对应一次跳过batch中的多少图片，第二个d_h对应一次跳过图片中 多少行，第三个d_w对应一次跳过图片中多少列，第四个对应一次跳过图像的多少个通道。这 里直接设置为[1，2，2，1]。即每次卷积后，图像的滑动步长为2，特征图会缩小为原来的 1/4。 第四个参数padding：string类型的量，只能是&quot;SAME&quot;,&quot;VALID&quot;其中之一，这个值决定了不 同的卷积方式 第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true 第六个参数：name=None &#39;&#39;&#39; # df_dim 作为类的参数传进来 # df_dim: (optional) Dimension of discrim filters in first conv layer[64] # lrelu 在opt.py 中 # 输入是一个 image h0 = lrelu(conv2d(image, self.df_dim, name=&#39;d_h0_conv&#39;)) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name=&#39;d_h1_conv&#39;))) h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name=&#39;d_h2_conv&#39;))) h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name=&#39;d_h3_conv&#39;))) #全连接层，-1表示不确定大小。 linear 在opt.py中,自己构建的。 h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, &#39;d_h4_lin&#39;) return tf.nn.sigmoid(h4), h4 else: # y_dim = 10 yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) # image被当作参数传进来的参数，Builder里是inputs. # 原型在opt里，基于concat函数基础上修改。这里并没有理解为什么要这样操作.... # 这里代码以及构建不难，但是这里所有的concat尚且不了解原因。 x = conv_cond_concat(image, yb) h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name=&#39;d_h0_conv&#39;)) h0 = conv_cond_concat(h0, yb) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name=&#39;d_h1_conv&#39;))) h1 = tf.reshape(h1, [self.batch_size, -1]) h1 = concat([h1, y], 1) h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, &#39;d_h2_lin&#39;))) h2 = concat([h2, y], 1) h3 = linear(h2, 1, &#39;d_h3_lin&#39;) return tf.nn.sigmoid(h3), h3 [1] 下面是discriminator(鉴别器)的具体实现。 首先鉴别器使用 conv (卷积)操作，激活函数使用 leaky-relu ,每一个layer需要使用batch normalization。tensorflow的batch normalization使用 tf.contrib.layers.batch_norm 实现。 如果不是mnist,则第一层使用 leaky-relu+conv2d ,后面三层都使用 conv2d+BN+leaky-relu ,最后加上一个one hidden unit的linear layer,再送入sigmoid函数即可； 如果是mnist,则 yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) 首先给y增加两维，以便可以和image连接起来，这里实际上是使用了conditional GAN(条件GAN)的思想。 x = conv_cond_concat(image, yb) 得到condition和image合并之后的结果，然后 h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name=‘d_h0_conv’)) 进行卷积操作。第二次进行 conv2d+leaky-relu+concat 操作。第三次进行 conv2d+BN+leaky-relu+reshape+concat 操作。第四次进行 linear+BN+leaky-relu+concat 操作。最后同样是 linear+sigmoid 操作。 补充：卷积网络 作者了解卷积神经网络的基础知识，但未真正自己建立过卷积神经网络，此处不甘心于函数，所以此处去重新学习了下整个卷积网络的构建,详见 卷积网络学习系列1.md 作者在温习以后才去标注的discriminator的卷积注释，所以写的不够小白。具体原理详见上。 generator def generator(self, z, y=None): with tf.variable_scope(&quot;generator&quot;) as scope: if not self.y_dim: s_h, s_w = self.output_height, self.output_width # return int(math.ceil(float(size) / float(stride))) s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape #self.z = tf.placeholder(tf.float32, [None, self.z_dim], name=&#39;z&#39;) # gf_dim是generator的filter大小的基，*8 *4 *2 *1 c_dim这是他的变化规律，下面代码内找找仔细观察看看 self.z_, self.h0_w, self.h0_b = linear( z, self.gf_dim*8*s_h16*s_w16, &#39;g_h0_lin&#39;, with_w=True) # 确定三个维度，可以计算出最后一个维度，那么这个维度可以用-1表示。 将全连接层reshape成特征图形状 self.h0 = tf.reshape( self.z_, [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(self.h0)) # 产生batch_size个 设置好大小的特征图 self.h1, self.h1_w, self.h1_b = deconv2d( h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name=&#39;g_h1&#39;, with_w=True) h1 = tf.nn.relu(self.g_bn1(self.h1)) h2, self.h2_w, self.h2_b = deconv2d( h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name=&#39;g_h2&#39;, with_w=True) h2 = tf.nn.relu(self.g_bn2(h2)) h3, self.h3_w, self.h3_b = deconv2d( h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name=&#39;g_h3&#39;, with_w=True) h3 = tf.nn.relu(self.g_bn3(h3)) # c_dim,通道数 3 h4, self.h4_w, self.h4_b = deconv2d( h3, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h4&#39;, with_w=True) return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.expand_dims(tf.expand_dims(y, 1),2) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu( self.g_bn0(linear(z, self.gfc_dim, &#39;g_h0_lin&#39;))) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, &#39;g_h1_lin&#39;))) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name=&#39;g_h2&#39;))) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid( deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h3&#39;)) [1]下面是generator(生成器)的具体实现。和discriminator不同的是,generator需要使用deconv(反卷积)以及relu 激活函数。 generator的结构是: 1.如果不是mnist: linear+reshape+BN+relu----&gt;(deconv+BN+relu)x3 ----&gt;deconv+tanh ; 2.如果是mnist,则除了需要考虑输入z之外，还需要考虑label y,即需要将z和y连接起来(Conditional GAN),具体的结构是:reshape+concat----&gt;linear+BN+relu+concat----&gt;linear+BN+relu+reshape+concat----&gt;deconv+BN+relu+concat----&gt;deconv+sigmoid。 注意的最后的激活函数没有采用通常的tanh,而是采用了sigmoid(其输出会直接映射到0-1之间)。 大佬们都不解释为什么要这样构建模型（补充：其实看里很久才知道，大佬们不说是因为原论文就是这样写构建的…不必纠结这个结构，因为还没到能设计结构的水平），所以，我只能硬着头皮自己解释。个人感觉，generator是从一个小输入，不断变大，变大，变成图片大小的过程，反卷积就用在这里。 def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False): shape = input_.get_shape().as_list() with tf.variable_scope(scope or &quot;Linear&quot;): try: matrix = tf.get_variable(&quot;Matrix&quot;, [shape[1], output_size], tf.float32, tf.random_normal_initializer(stddev=stddev)) except ValueError as err: msg = &quot;NOTE: Usually, this is due to an issue with the image dimensions. Did you correctly set &#39;--crop&#39; or &#39;--input_height&#39; or &#39;--output_height&#39;?&quot; err.args = err.args + (msg,) raise bias = tf.get_variable(&quot;bias&quot;, [output_size], initializer=tf.constant_initializer(bias_start)) # 是否输出权重 if with_w: return tf.matmul(input_, matrix) + bias, matrix, bias else: return tf.matmul(input_, matrix) + bias 反卷积 恭喜，又是一个没听过的知识点… 此处另外学习反卷积，详见反卷积系列。 def deconv2d(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=&quot;deconv2d&quot;, with_w=False): with tf.variable_scope(name): # filter : [height, width, output_channels, in_channels] w = tf.get_variable(&#39;w&#39;, [k_h, k_w, output_shape[-1], input_.get_shape()[-1]], initializer=tf.random_normal_initializer(stddev=stddev)) try: deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1]) # Support for verisons of TensorFlow before 0.7.0 except AttributeError: # d_h,d_w参考原论文 deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1]) # b的大小跟输出的特征图个数一样 biases = tf.get_variable(&#39;biases&#39;, [output_shape[-1]], initializer=tf.constant_initializer(0.0)) # 先反卷积，后加偏置 deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape()) # 是否输出权重 if with_w: return deconv, w, biases else: return deconv sampler def sampler(self, z, y=None): with tf.variable_scope(&quot;generator&quot;) as scope: scope.reuse_variables() if not self.y_dim: s_h, s_w = self.output_height, self.output_width s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape h0 = tf.reshape( linear(z, self.gf_dim*8*s_h16*s_w16, &#39;g_h0_lin&#39;), [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(h0, train=False)) h1 = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name=&#39;g_h1&#39;) h1 = tf.nn.relu(self.g_bn1(h1, train=False)) h2 = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name=&#39;g_h2&#39;) h2 = tf.nn.relu(self.g_bn2(h2, train=False)) h3 = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name=&#39;g_h3&#39;) h3 = tf.nn.relu(self.g_bn3(h3, train=False)) h4 = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h4&#39;) return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.reshape(y, [-1, 1, 1, self.y_dim]) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, &#39;g_h0_lin&#39;), train=False)) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, &#39;g_h1_lin&#39;), train=False)) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2( deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name=&#39;g_h2&#39;), train=False)) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name=&#39;g_h3&#39;)) sampler函数是采样函数，用于生成样本送入当前训练的生成器，查看训练效果。其逻辑和generator函数基本类似,也是需要区分是否是mnist,二者需要采用不同的结构。不是mnist时,y=None即可；否则mnist还需要考虑y。 如果单纯去看代码，前面那么代码的铺垫已经够了，但是，不理解为什么要用这个sampler,有什么用，怎么感觉他在做和generator一样的事儿。 具体为什么用它，得等到最后我去啃DGCAN原论文时候再了解吧。 DCGAN原文解读 此处为看完train代码后补的，单单看懂代码的大致逻辑很容易，但是细节完全把握不住，由于前面没有取细读论文，这里集中爆发了，感觉确实需要读完论文再往下做，所以，开始论文时间。 train 函数 补充，终于到这儿了，突然感觉真的好多啊，以前没有这样做过，但这样来一遍，发现自己啥都不是，学到了很多，但也知道很多自己都没写出来，水平有限。当然，万分感谢那些我一头水雾时找到的大佬博客，动力之源啊。 这应该是最难啃的一块骨头了，目测代码量瑟瑟发抖… def train(self, config): # 用adam优化 d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\ .minimize(self.d_loss, var_list=self.d_vars) g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\ .minimize(self.g_loss, var_list=self.g_vars) #避免tensorflow版本问题报错 try: tf.global_variables_initializer().run() except: tf.initialize_all_variables().run() # merge_summary 函数和 SummaryWriter 用于构建summary,在tensorboard中显示。 self.g_sum = merge_summary([self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum]) self.d_sum = merge_summary( [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum]) self.writer = SummaryWriter(&quot;./logs&quot;, self.sess.graph) # 噪音向量 [个数，维度] sample_z = np.random.uniform(-1, 1, size=(self.sample_num , self.z_dim)) if config.dataset == &#39;mnist&#39;: sample_inputs = self.data_X[0:self.sample_num] sample_labels = self.data_y[0:self.sample_num] else: #读取图片并且改变大小 sample_files = self.data[0:self.sample_num] sample = [ get_image(sample_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, crop=self.crop, grayscale=self.grayscale) for sample_file in sample_files] #如果处理的图像是灰度图像,则需要再增加一个dim,表示图像的 channel=1 if (self.grayscale): sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None] else: sample_inputs = np.array(sample).astype(np.float32) counter = 1 start_time = time.time() # 下载一波模型 could_load, checkpoint_counter = self.load(self.checkpoint_dir) # 下载成功 if could_load: counter = checkpoint_counter print(&quot; [*] Load SUCCESS&quot;) else: print(&quot; [!] Load failed...&quot;) # 循环训练开始 for epoch in xrange(config.epoch): if config.dataset == &#39;mnist&#39;: batch_idxs = min(len(self.data_X), config.train_size) // config.batch_size else: # 数据集所有数据文件名称 self.data = glob(os.path.join( config.data_dir, config.dataset, self.input_fname_pattern)) # 打乱 np.random.shuffle(self.data) # bath_size 表示一次指定几张图片 batch_idxs = min(len(self.data), config.train_size) // config.batch_size for idx in xrange(0, int(batch_idxs)): #每次循环的数据集抽取 if config.dataset == &#39;mnist&#39;: batch_images = self.data_X[idx*config.batch_size:(idx+1)*config.batch_size] batch_labels = self.data_y[idx*config.batch_size:(idx+1)*config.batch_size] else: batch_files = self.data[idx*config.batch_size:(idx+1)*config.batch_size] batch = [ get_image(batch_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, crop=self.crop, grayscale=self.grayscale) for batch_file in batch_files] if self.grayscale: batch_images = np.array(batch).astype(np.float32)[:, :, :, None] else: batch_images = np.array(batch).astype(np.float32) batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \\ .astype(np.float32) # run if config.dataset == &#39;mnist&#39;: # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z, self.y:batch_labels }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images, self.y:batch_labels }) errG = self.g_loss.eval({ self.z: batch_z, self.y: batch_labels }) else: # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images }) errG = self.g_loss.eval({self.z: batch_z}) counter += 1 print(&quot;Epoch: [%2d/%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f&quot; \\ % (epoch, config.epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG)) &#39;&#39;&#39; np.mod(counter, config.print_every) == 1 表示每 print_every 次生成一次samples; np.mod(counter, config.checkpoint_every) == 2 表示每 checkpoint_every 次保存一下 checkpoint file。 &#39;&#39;&#39; if np.mod(counter, 100) == 1: if config.dataset == &#39;mnist&#39;: samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, self.y:sample_labels, } ) save_images(samples, image_manifold_size(samples.shape[0]), &#39;./{}/train_{:02d}_{:04d}.png&#39;.format(config.sample_dir, epoch, idx)) print(&quot;[Sample] d_loss: %.8f, g_loss: %.8f&quot; % (d_loss, g_loss)) else: try: samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, }, ) save_images(samples, image_manifold_size(samples.shape[0]), &#39;./{}/train_{:02d}_{:04d}.png&#39;.format(config.sample_dir, epoch, idx)) print(&quot;[Sample] d_loss: %.8f, g_loss: %.8f&quot; % (d_loss, g_loss)) except: print(&quot;one pic error!...&quot;) if np.mod(counter, 500) == 2: self.save(config.checkpoint_dir, counter) 下载模型的代码 [11] def load(self, checkpoint_dir): import re print(&quot; [*] Reading checkpoints...&quot;) checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) ckpt = tf.train.get_checkpoint_state(checkpoint_dir) if ckpt and ckpt.model_checkpoint_path: ckpt_name = os.path.basename(ckpt.model_checkpoint_path) self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name)) # 不太清楚这句话有什么用 counter = int(next(re.finditer(&quot;(\\d+)(?!.*\\d)&quot;,ckpt_name)).group(0)) print(&quot; [*] Success to read {}&quot;.format(ckpt_name)) return True, counter else: print(&quot; [*] Failed to find a checkpoint&quot;) return False, 0","@type":"BlogPosting","url":"/2019/01/09/5fbeac0d4659f78bf7e4518574e0afcd.html","headline":"DCGAN——菜鸟系列 model.py","dateModified":"2019-01-09T00:00:00+08:00","datePublished":"2019-01-09T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/01/09/5fbeac0d4659f78bf7e4518574e0afcd.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>DCGAN——菜鸟系列 model.py</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <h1><a id="_0"></a>参考</h1> 
  <p>[1] DGAN代码简读</p> 
  <blockquote> 
   <p><a href="https://www.colabug.com/2958322.html" rel="nofollow">https://www.colabug.com/2958322.html</a></p> 
  </blockquote> 
  <p>[2]基于DCGAN的动漫头像生成神经网络实现</p> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/sinat_33741547/article/details/77871170" rel="nofollow">https://blog.csdn.net/sinat_33741547/article/details/77871170</a></p> 
  </blockquote> 
  <p>[3] batch norm原理及代码详解</p> 
  <blockquote> 
   <ol> 
    <li>博客： <a href="https://blog.csdn.net/qq_25737169/article/details/79048516" rel="nofollow">https://blog.csdn.net/qq_25737169/article/details/79048516</a></li> 
    <li>视频：<a href="https://www.bilibili.com/video/av15405597?from=search&amp;seid=8881273700429864348" rel="nofollow">https://www.bilibili.com/video/av15405597?from=search&amp;seid=8881273700429864348</a></li> 
    <li>博客：<a href="https://www.cnblogs.com/eilearn/p/9780696.html" rel="nofollow">https://www.cnblogs.com/eilearn/p/9780696.html</a></li> 
   </ol> 
   <p>说明：2重点在代码，3重点在理论（但是一点都不枯燥），1有点像两个的结合。推荐2的系列视频以及资料，很精品，且免费。推荐看完视频再看3,而且3中有很多联立起来的理论，奉为神作不过分。</p> 
  </blockquote> 
  <p>[3] tensorflow,tensorboard可视化网络结构</p> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/helei001/article/details/51842531" rel="nofollow">https://blog.csdn.net/helei001/article/details/51842531</a></p> 
  </blockquote> 
  <p>[4] tensorflow变量作用域</p> 
  <blockquote> 
   <p><a href="https://www.cnblogs.com/MY0213/p/9208503.html" rel="nofollow">https://www.cnblogs.com/MY0213/p/9208503.html</a></p> 
  </blockquote> 
  <p>[5] 此接[4],同时解决了当初jupyter notebook的困惑</p> 
  <blockquote> 
   <p><a href="https://blog.csdn.net/Jerr__y/article/details/70809528#commentBox" rel="nofollow">https://blog.csdn.net/Jerr__y/article/details/70809528#commentBox</a></p> 
  </blockquote> 
  <p>[6] 激活函数 ReLU、Leaky ReLU、PReLU 和 RReLU</p> 
  <blockquote> 
   <p><a href="http://www.cnblogs.com/chamie/p/8665251.html" rel="nofollow">http://www.cnblogs.com/chamie/p/8665251.html</a></p> 
  </blockquote> 
  <p>[7] conv2d函数简介</p> 
  <blockquote> 
   <ul> 
    <li><a href="http://www.cnblogs.com/qggg/p/6832342.html" rel="nofollow">http://www.cnblogs.com/qggg/p/6832342.html</a></li> 
   </ul> 
  </blockquote> 
  <p>[8] DCGAN 源码分析</p> 
  <blockquote> 
   <ul> 
    <li><a href="https://blog.csdn.net/nongfu_spring/article/details/54342861/" rel="nofollow">https://blog.csdn.net/nongfu_spring/article/details/54342861/</a></li> 
   </ul> 
  </blockquote> 
  <p>[9] DCGAN代码简单读，里面有对应训练效果，这里看中了某个实现效果…so，做个记录先</p> 
  <blockquote> 
   <ul> 
    <li><a href="http://www.cnblogs.com/lyrichu/p/9093411.html" rel="nofollow">http://www.cnblogs.com/lyrichu/p/9093411.html</a></li> 
   </ul> 
  </blockquote> 
  <p>[10] 一个写的还算详细，但是排版和我彼此彼此的博客——题目：DCGAN</p> 
  <blockquote> 
   <ul> 
    <li><a href="https://blog.csdn.net/Candy_GL/article/details/81138297" rel="nofollow">https://blog.csdn.net/Candy_GL/article/details/81138297</a></li> 
   </ul> 
  </blockquote> 
  <p>[11] tensorflow保存下载模型</p> 
  <blockquote> 
   <ul> 
    <li><a href="https://blog.csdn.net/xiezongsheng1990/article/details/81011115" rel="nofollow">https://blog.csdn.net/xiezongsheng1990/article/details/81011115</a></li> 
   </ul> 
  </blockquote> 
  <blockquote> 
   <p>说明：作者正在逐渐熟悉markdown…勿喷</p> 
  </blockquote> 
  <p>[12] mmp的视频，早看到可以少走N多个坑（注：博主自己理了一遍代码以后，找论文时找到的视频，讲的对我来说正正好）看完视频，完善了部分代码注释</p> 
  <blockquote> 
   <ul> 
    <li><a href="https://www.bilibili.com/video/av20033914/?p=13" rel="nofollow">https://www.bilibili.com/video/av20033914/?p=13</a></li> 
   </ul> 
  </blockquote> 
  <p>[13] B站找的视频</p> 
  <blockquote> 
   <ul> 
    <li>其实到最后，疑惑的也就部分tensorflow框架以及代码了，所以此处学习一波回来再去完善一下唉</li> 
    <li><a href="https://www.bilibili.com/video/av19360545/?p=1" rel="nofollow">https://www.bilibili.com/video/av19360545/?p=1</a></li> 
   </ul> 
  </blockquote> 
  <h1><a id="_67"></a>代码解释</h1> 
  <h2><a id="_69"></a>引入模块</h2> 
  <pre><code class="prism language-python"><span class="token keyword">from</span> __future__ <span class="token keyword">import</span> division
<span class="token keyword">import</span> os
<span class="token keyword">import</span> time
<span class="token keyword">import</span> math
<span class="token keyword">from</span> glob <span class="token keyword">import</span> glob
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> six<span class="token punctuation">.</span>moves <span class="token keyword">import</span> <span class="token builtin">xrange</span>

<span class="token keyword">from</span> ops <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">from</span> utils <span class="token keyword">import</span> <span class="token operator">*</span>
</code></pre> 
  <blockquote> 
   <ul> 
    <li>from _ <em>future_</em> import division 这句话当python的版本为2.x时生效，可以让两个整数数字相除的结果返回一个浮点数(在python2中默认是整数,python3默认为浮点数)。</li> 
    <li>glob可以以简单的正则表达式筛选的方式返回某个文件夹下符合要求的文件名列表。[1]</li> 
   </ul> 
  </blockquote> 
  <hr> 
  <h2><a id="conv_out_size_same__90"></a>conv_out_size_same 函数</h2> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">conv_out_size_same</span><span class="token punctuation">(</span>size<span class="token punctuation">,</span> stride<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">return</span> <span class="token builtin">int</span><span class="token punctuation">(</span>math<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>size<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">float</span><span class="token punctuation">(</span>stride<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <blockquote> 
   <ul> 
    <li>math.ceil(): ceil() 函数返回数字的上入整数,即向上取整</li> 
    <li>stride : 步幅</li> 
    <li>整个函数返回了一个整形值，</li> 
   </ul> 
  </blockquote> 
  <hr> 
  <h2><a id="class_DCGAN_103"></a>class DCGAN</h2> 
  <h3><a id="__init___105"></a><strong>init</strong></h3> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sess<span class="token punctuation">,</span> input_height<span class="token operator">=</span><span class="token number">108</span><span class="token punctuation">,</span> input_width<span class="token operator">=</span><span class="token number">108</span><span class="token punctuation">,</span> crop<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
         batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> sample_num <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> output_height<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>output_width<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>
         y_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> z_dim<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> gf_dim<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> df_dim<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>
         gfc_dim<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span> dfc_dim<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span> c_dim<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> dataset_name<span class="token operator">=</span><span class="token string">'default'</span><span class="token punctuation">,</span>
         input_fname_pattern<span class="token operator">=</span><span class="token string">'*.jpg'</span><span class="token punctuation">,</span> checkpoint_dir<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> sample_dir<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> data_dir<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</code></pre> 
  <blockquote> 
   <ul> 
    <li>具体参数用到再细谈，此处注意特殊的那个: y_dim就好</li> 
    <li>以下给出对应注释</li> 
   </ul> 
  </blockquote> 
  <pre><code class="prism language-python"><span class="token triple-quoted-string string">""" Args: sess: TensorFlow session batch_size: The size of batch. Should be specified before training. y_dim: (optional) Dimension of dim for y. [None] z_dim: (optional) Dimension of dim for Z. [100] gf_dim: (optional) Dimension of gen filters in first conv layer.[64] df_dim: (optional) Dimension of discrim filters in first conv layer[64] gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024] dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024] c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3] """</span>
</code></pre> 
  <hr> 
  <pre><code class="prism language-python">	self<span class="token punctuation">.</span>sess <span class="token operator">=</span> sess
    self<span class="token punctuation">.</span>crop <span class="token operator">=</span> crop

    self<span class="token punctuation">.</span>batch_size <span class="token operator">=</span> batch_size
    
    <span class="token comment"># 测试要用到，由G生成，看效果</span>
    self<span class="token punctuation">.</span>sample_num <span class="token operator">=</span> sample_num

    self<span class="token punctuation">.</span>input_height <span class="token operator">=</span> input_height
    self<span class="token punctuation">.</span>input_width <span class="token operator">=</span> input_width
    self<span class="token punctuation">.</span>output_height <span class="token operator">=</span> output_height
    self<span class="token punctuation">.</span>output_width <span class="token operator">=</span> output_width

    self<span class="token punctuation">.</span>y_dim <span class="token operator">=</span> y_dim
    
    <span class="token comment"># Gnerator最开始输入的噪音数据点的维度</span>
    self<span class="token punctuation">.</span>z_dim <span class="token operator">=</span> z_dim
	
    <span class="token comment"># generator的中卷积网络的filter的维度，以下同理</span>
    self<span class="token punctuation">.</span>gf_dim <span class="token operator">=</span> gf_dim
    self<span class="token punctuation">.</span>df_dim <span class="token operator">=</span> df_dim
	
    <span class="token comment"># generator全连接的维度</span>
    self<span class="token punctuation">.</span>gfc_dim <span class="token operator">=</span> gfc_dim
    self<span class="token punctuation">.</span>dfc_dim <span class="token operator">=</span> dfc_dim
    
    <span class="token comment"># batch normalization : deals with poor initialization helps gradient flow</span>
    self<span class="token punctuation">.</span>d_bn1 <span class="token operator">=</span> batch_norm<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">'d_bn1'</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>d_bn2 <span class="token operator">=</span> batch_norm<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">'d_bn2'</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">:</span>
      self<span class="token punctuation">.</span>d_bn3 <span class="token operator">=</span> batch_norm<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">'d_bn3'</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>g_bn0 <span class="token operator">=</span> batch_norm<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">'g_bn0'</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>g_bn1 <span class="token operator">=</span> batch_norm<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">'g_bn1'</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>g_bn2 <span class="token operator">=</span> batch_norm<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">'g_bn2'</span><span class="token punctuation">)</span>	
	
    <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">:</span>
      self<span class="token punctuation">.</span>g_bn3 <span class="token operator">=</span> batch_norm<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">'g_bn3'</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>dataset_name <span class="token operator">=</span> dataset_name
    self<span class="token punctuation">.</span>input_fname_pattern <span class="token operator">=</span> input_fname_pattern
    self<span class="token punctuation">.</span>checkpoint_dir <span class="token operator">=</span> checkpoint_dir
    self<span class="token punctuation">.</span>data_dir <span class="token operator">=</span> data_dir
</code></pre> 
  <blockquote> 
   <ul> 
    <li>这段就是赋值而已</li> 
    <li>batch_norm 的水很深，需要一定功夫研究，具体参考[3],</li> 
    <li>此处贴出作者的一份回答：如果在每一层之后都归一化成0-1的高斯分布（减均值除方差）那么数据的分布一直都是高斯分布，数据分布都是固定的了，这样即使加更多层就没有意义了，深度网络就是想学习数据的分布发现规律性，BN就是不让学习的数据分布偏离太远，详细细节你可以去看论文。beta gama都是学习的，代码里他们定义的是variable， trainable是True。——个人感觉很有道理</li> 
    <li>具体学习详看[3]</li> 
   </ul> 
  </blockquote> 
  <hr> 
  <pre><code class="prism language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>dataset_name <span class="token operator">==</span> <span class="token string">'mnist'</span><span class="token punctuation">:</span>
      self<span class="token punctuation">.</span>data_X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>data_y <span class="token operator">=</span> self<span class="token punctuation">.</span>load_mnist<span class="token punctuation">(</span><span class="token punctuation">)</span>
      self<span class="token punctuation">.</span>c_dim <span class="token operator">=</span> self<span class="token punctuation">.</span>data_X<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
      data_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_dir<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dataset_name<span class="token punctuation">,</span> self<span class="token punctuation">.</span>input_fname_pattern<span class="token punctuation">)</span> 
      <span class="token comment">#返回的数据类型是list </span>
      self<span class="token punctuation">.</span>data <span class="token operator">=</span> glob<span class="token punctuation">(</span>data_path<span class="token punctuation">)</span>               <span class="token comment">#用它可以查找符合自己目的的文件</span>
    
      <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">raise</span> Exception<span class="token punctuation">(</span><span class="token string">"[!] No data found in '"</span> <span class="token operator">+</span> data_path <span class="token operator">+</span> <span class="token string">"'"</span><span class="token punctuation">)</span>
        
      np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span>              <span class="token comment">#打乱数据</span>
      
       <span class="token comment">#读取一张图片 </span>
      imreadImg <span class="token operator">=</span> imread<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
      <span class="token comment">#check if image is a non-grayscale image by checking channel number </span>
      <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>imreadImg<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">&gt;=</span> <span class="token number">3</span><span class="token punctuation">:</span> 
        self<span class="token punctuation">.</span>c_dim <span class="token operator">=</span> imread<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>c_dim <span class="token operator">=</span> <span class="token number">1</span>

      <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>batch_size<span class="token punctuation">:</span>
        <span class="token keyword">raise</span> Exception<span class="token punctuation">(</span><span class="token string">"[!] Entire dataset size is less than the configured batch_size"</span><span class="token punctuation">)</span>
</code></pre> 
  <blockquote> 
   <ul> 
    <li>DCGAN的构造方法除了设置一大堆的属性之外，还要注意区分dataset是否是mnist,因为mnist是灰度图像,所以应该设置channel = 1( self.c_dim = 1 ),如果是彩色图像，则 self.c_dim = 3 or self.c_dim = 4[1]</li> 
    <li>load_mnist为自定义方法，方法实现如下</li> 
    <li>其余代码均有注释，整个代码块只是在读入数据集，并进行对应的可能报错处理。</li> 
   </ul> 
  </blockquote> 
  <hr> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">load_mnist</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    data_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_dir<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dataset_name<span class="token punctuation">)</span>    <span class="token comment">#用于路径拼接文件路径</span>
    
    <span class="token triple-quoted-string string">''' train-images-idx3-ubyte: 参考网址：https://www.jianshu.com/p/84f72791806f 简介:这是IDX文件格式，是一种用来存储向量与多维度矩阵的文件格式 '''</span>
    fd <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span><span class="token string">'train-images-idx3-ubyte'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment">#创建文件对象</span>
    loaded <span class="token operator">=</span> np<span class="token punctuation">.</span>fromfile<span class="token punctuation">(</span><span class="token builtin">file</span><span class="token operator">=</span>fd<span class="token punctuation">,</span>dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>                 <span class="token comment">#numpy 读取文件 </span>
    trX <span class="token operator">=</span> loaded<span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">60000</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>  <span class="token comment">#对应的数据处理</span>

    fd <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span><span class="token string">'train-labels-idx1-ubyte'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    loaded <span class="token operator">=</span> np<span class="token punctuation">.</span>fromfile<span class="token punctuation">(</span><span class="token builtin">file</span><span class="token operator">=</span>fd<span class="token punctuation">,</span>dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>
    trY <span class="token operator">=</span> loaded<span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">60000</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>

    fd <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span><span class="token string">'t10k-images-idx3-ubyte'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    loaded <span class="token operator">=</span> np<span class="token punctuation">.</span>fromfile<span class="token punctuation">(</span><span class="token builtin">file</span><span class="token operator">=</span>fd<span class="token punctuation">,</span>dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>
    teX <span class="token operator">=</span> loaded<span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>

    fd <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_dir<span class="token punctuation">,</span><span class="token string">'t10k-labels-idx1-ubyte'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    loaded <span class="token operator">=</span> np<span class="token punctuation">.</span>fromfile<span class="token punctuation">(</span><span class="token builtin">file</span><span class="token operator">=</span>fd<span class="token punctuation">,</span>dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>
    teY <span class="token operator">=</span> loaded<span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>

    <span class="token comment"># 将结构数据转化为ndarray， 例子详见： https://www.jb51.net/article/138281.htm</span>
    <span class="token comment"># 可以理解为：两个变量用法同一份内存，只是数据格式不同</span>
    trY <span class="token operator">=</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>trY<span class="token punctuation">)</span>
    teY <span class="token operator">=</span> np<span class="token punctuation">.</span>asarray<span class="token punctuation">(</span>teY<span class="token punctuation">)</span>
    
    <span class="token comment"># 数组拼接</span>
    <span class="token comment">#concatenate([a, b])</span>
    <span class="token comment">#连接，连接后ndim不变，a和b可以有一维size不同，但size不同的维度必须是要连接的维度</span>
    X <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>trX<span class="token punctuation">,</span> teX<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>trY<span class="token punctuation">,</span> teY<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">)</span>
    
    <span class="token comment"># seed</span>
    <span class="token comment"># http://www.cnblogs.com/subic/p/8454025.html</span>
    <span class="token comment"># 简单介绍：</span>
    seed <span class="token operator">=</span> <span class="token number">547</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>X<span class="token punctuation">)</span>      <span class="token comment">#np.random.shuffle </span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span>seed<span class="token punctuation">)</span>
    np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
    
    y_vec <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> label <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">:</span>
      y_vec<span class="token punctuation">[</span>i<span class="token punctuation">,</span>y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1.0</span>
    
    <span class="token keyword">return</span> X<span class="token operator">/</span><span class="token number">255</span><span class="token punctuation">.</span><span class="token punctuation">,</span>y_vec
</code></pre> 
  <blockquote> 
   <ul> 
    <li>具体参考注释内容</li> 
   </ul> 
  </blockquote> 
  <hr> 
  <pre><code class="prism language-python">	<span class="token comment">#是否为灰度</span>
    self<span class="token punctuation">.</span>grayscale <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>c_dim <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">)</span>

	<span class="token comment">#自定义的方法</span>
    self<span class="token punctuation">.</span>build_model<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <blockquote> 
   <ul> 
    <li>无特殊说明</li> 
   </ul> 
  </blockquote> 
  <hr> 
  <h3><a id="build_modelself_293"></a>build_model(self)</h3> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">build_model</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token comment">#可以理解为申明了一个（batch_size,y_dim）大小的地方，用于mini_batc时往里面喂对应的数据。</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">:</span>
      self<span class="token punctuation">.</span>y <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'y'</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
      self<span class="token punctuation">.</span>y <span class="token operator">=</span> <span class="token boolean">None</span>
	
    <span class="token comment">#is_crop为要不要裁剪图像 [True or False]</span>
    <span class="token comment">#所以此处是判断图像是否裁剪过</span>
    <span class="token comment"># https://www.jianshu.com/p/3e46ce8e7ddd</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>crop<span class="token punctuation">:</span>
      <span class="token comment"># 把输出维度定义好 </span>
      image_dims <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>output_height<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_width<span class="token punctuation">,</span> self<span class="token punctuation">.</span>c_dim<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>  
      image_dims <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>input_height<span class="token punctuation">,</span> self<span class="token punctuation">.</span>input_width<span class="token punctuation">,</span> self<span class="token punctuation">.</span>c_dim<span class="token punctuation">]</span>

    <span class="token comment">#道理同上面解释，真实图片的输入[batchsize,height,width,c_dim]</span>
    self<span class="token punctuation">.</span>inputs <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>
      tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">]</span> <span class="token operator">+</span> image_dims<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'real_images'</span><span class="token punctuation">)</span>

    inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>inputs
	
    <span class="token comment"># 噪音数据（None表示多少都可以，实际用到再填充batch）</span>
    self<span class="token punctuation">.</span>z <span class="token operator">=</span> tf<span class="token punctuation">.</span>placeholder<span class="token punctuation">(</span>
      tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>z_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'z'</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 位于 opt.py里</span>
    <span class="token comment"># histogram_summary = tf.histogram_summary</span>
    <span class="token comment"># 将z在tensorboard里可视化</span>
    self<span class="token punctuation">.</span>z_sum <span class="token operator">=</span> histogram_summary<span class="token punctuation">(</span><span class="token string">"z"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>z<span class="token punctuation">)</span>
	
    <span class="token comment"># 以下为网络结构 </span>
    self<span class="token punctuation">.</span>G                  <span class="token operator">=</span> self<span class="token punctuation">.</span>generator<span class="token punctuation">(</span>self<span class="token punctuation">.</span>z<span class="token punctuation">,</span> self<span class="token punctuation">.</span>y<span class="token punctuation">)</span>
    <span class="token comment">#真实数据</span>
    self<span class="token punctuation">.</span>D<span class="token punctuation">,</span> self<span class="token punctuation">.</span>D_logits   <span class="token operator">=</span> self<span class="token punctuation">.</span>discriminator<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>y<span class="token punctuation">,</span> reuse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 测试网络</span>
    self<span class="token punctuation">.</span>sampler            <span class="token operator">=</span> self<span class="token punctuation">.</span>sampler<span class="token punctuation">(</span>self<span class="token punctuation">.</span>z<span class="token punctuation">,</span> self<span class="token punctuation">.</span>y<span class="token punctuation">)</span>
    <span class="token comment"># G产生的数据</span>
    self<span class="token punctuation">.</span>D_<span class="token punctuation">,</span> self<span class="token punctuation">.</span>D_logits_ <span class="token operator">=</span> self<span class="token punctuation">.</span>discriminator<span class="token punctuation">(</span>self<span class="token punctuation">.</span>G<span class="token punctuation">,</span> self<span class="token punctuation">.</span>y<span class="token punctuation">,</span> reuse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 同上可视化</span>
    self<span class="token punctuation">.</span>d_sum <span class="token operator">=</span> histogram_summary<span class="token punctuation">(</span><span class="token string">"d"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>D<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>d__sum <span class="token operator">=</span> histogram_summary<span class="token punctuation">(</span><span class="token string">"d_"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>D_<span class="token punctuation">)</span>
    
    <span class="token comment"># opt.py</span>
    <span class="token comment"># image_summary = tf.image_summary</span>
    <span class="token comment"># tensorboar内图像可视化 ： https://blog.csdn.net/dxmkkk/article/details/54925728</span>
    self<span class="token punctuation">.</span>G_sum <span class="token operator">=</span> image_summary<span class="token punctuation">(</span><span class="token string">"G"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>G<span class="token punctuation">)</span>
</code></pre> 
  <blockquote> 
   <ul> 
    <li> <p>显而易见，网络结构才是重点，其他都是陪衬。大致介绍参考[1]:</p> <p>self.generator 用于构造生成器; self.discriminator 用于构造鉴别器; self.sampler 用于随机采样(用于生成样本)。这里需要注意的是, self.y 只有当dataset是mnist的时候才不为None,不是mnist的情况下,只需要 self.z 即可生成samples</p> </li> 
   </ul> 
  </blockquote> 
  <hr> 
  <h3><a id="sigmoid_cross_entropy_with_logitsx_y__build_model_354"></a>sigmoid_cross_entropy_with_logits(x, y) （内置于build_model）</h3> 
  <pre><code class="prism language-python"> <span class="token keyword">def</span> <span class="token function">sigmoid_cross_entropy_with_logits</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token keyword">try</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>logits<span class="token operator">=</span>x<span class="token punctuation">,</span> labels<span class="token operator">=</span>y<span class="token punctuation">)</span>
      <span class="token keyword">except</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>logits<span class="token operator">=</span>x<span class="token punctuation">,</span> targets<span class="token operator">=</span>y<span class="token punctuation">)</span>
</code></pre> 
  <blockquote> 
   <ul> 
    <li>[1] sigmoid_cross_entropy_with_logits 函数被重新定义了，是为了兼容不同版本的tensorflow。该函数首先使用sigmoid activation，然后计算cross-entropy loss。</li> 
    <li>这个函数有自己的定义，可以参考： <a href="https://blog.csdn.net/QW_sunny/article/details/72885403" rel="nofollow">https://blog.csdn.net/QW_sunny/article/details/72885403</a></li> 
    <li>作者为对此函数进行过深了解，仅限于清楚大致处理过程。</li> 
   </ul> 
  </blockquote> 
  <hr> 
  <blockquote> 
   <p>​ 承接上面代码</p> 
  </blockquote> 
  <pre><code class="prism language-python">	<span class="token comment"># 真实图片的鉴别器损失</span>
    <span class="token comment"># D_logits,鉴别网络对真实数据的评分， tf.ones_like(self.D)，与D大小一样的1.</span>
    <span class="token comment"># tf.nn.sigmoid(h3), h3</span>
    <span class="token comment"># 可以理解为两参数分别为x,y。 x是网络产生，y是实际希望，然后通过这个函数计算出一个损失值。不同于一般的平方求和，此函数解决了sigmoid梯度缓慢的问题，并且另外考虑了溢出的问题。</span>
    self<span class="token punctuation">.</span>d_loss_real <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>  
      sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>self<span class="token punctuation">.</span>D_logits<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>self<span class="token punctuation">.</span>D<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 虚假图片损失</span>
    self<span class="token punctuation">.</span>d_loss_fake <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>
      sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>self<span class="token punctuation">.</span>D_logits_<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>self<span class="token punctuation">.</span>D_<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 生成器损失</span>
    self<span class="token punctuation">.</span>g_loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>
      sigmoid_cross_entropy_with_logits<span class="token punctuation">(</span>self<span class="token punctuation">.</span>D_logits_<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>self<span class="token punctuation">.</span>D_<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
	
    <span class="token comment"># 可视化</span>
    self<span class="token punctuation">.</span>d_loss_real_sum <span class="token operator">=</span> scalar_summary<span class="token punctuation">(</span><span class="token string">"d_loss_real"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_loss_real<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>d_loss_fake_sum <span class="token operator">=</span> scalar_summary<span class="token punctuation">(</span><span class="token string">"d_loss_fake"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_loss_fake<span class="token punctuation">)</span>
    
    <span class="token comment"># 总的鉴别器损失，越小越好</span>
    self<span class="token punctuation">.</span>d_loss <span class="token operator">=</span> self<span class="token punctuation">.</span>d_loss_real <span class="token operator">+</span> self<span class="token punctuation">.</span>d_loss_fake
</code></pre> 
  <blockquote> 
   <ul> 
    <li>sigmoid_cross_entropy_with_logits的output不是一个数，而是一个batch中每个样本的loss,所以一般配合tf.reduce_mean(loss)使用</li> 
    <li>self.g_loss 是生成器损失; self.d_loss_real 是真实图片的鉴别器损失; self.d_loss_fake 是虚假图片(由生成器生成的fake images)的损失; self.d_loss 是总的鉴别器损失。</li> 
   </ul> 
  </blockquote> 
  <hr> 
  <pre><code class="prism language-python">	<span class="token comment"># 可视化</span>
    self<span class="token punctuation">.</span>g_loss_sum <span class="token operator">=</span> scalar_summary<span class="token punctuation">(</span><span class="token string">"g_loss"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>g_loss<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>d_loss_sum <span class="token operator">=</span> scalar_summary<span class="token punctuation">(</span><span class="token string">"d_loss"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_loss<span class="token punctuation">)</span>

    <span class="token comment"># 所有的变量</span>
    t_vars <span class="token operator">=</span> tf<span class="token punctuation">.</span>trainable_variables<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 寻找所有以d_开头的变量，优化时的参数（例如：如果是梯度下降，我们需要将权重和loss传进去，这里d_vars代表generator里要优化的变量）</span>
    self<span class="token punctuation">.</span>d_vars <span class="token operator">=</span> <span class="token punctuation">[</span>var <span class="token keyword">for</span> var <span class="token keyword">in</span> t_vars <span class="token keyword">if</span> <span class="token string">'d_'</span> <span class="token keyword">in</span> var<span class="token punctuation">.</span>name<span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>g_vars <span class="token operator">=</span> <span class="token punctuation">[</span>var <span class="token keyword">for</span> var <span class="token keyword">in</span> t_vars <span class="token keyword">if</span> <span class="token string">'g_'</span> <span class="token keyword">in</span> var<span class="token punctuation">.</span>name<span class="token punctuation">]</span>

    <span class="token comment"># 保存节点</span>
    self<span class="token punctuation">.</span>saver <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>Saver<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
  <blockquote> 
   <ul> 
    <li>[1] tf.trainable_variables() 可以获取model的全部可训练参数,由于我们在定义生成器和鉴别器变量的时候使用了不同的name,因此我们可以通过variable的name来获取得到 <strong>self.d_vars</strong> (鉴别器相关变量), <strong>self.g_vars</strong> (生成器相关变量)。</li> 
    <li>self.saver = tf.train.Saver() 用于保存训练好的模型参数到checkpoint。</li> 
   </ul> 
  </blockquote> 
  <h3><a id="discriminator_423"></a>discriminator</h3> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">discriminator</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> image<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> reuse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token comment"># 变量共享，我理解为： 即使函数被重复调用，其共享的节点不会重复定义、声明 [5]</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"discriminator"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> scope<span class="token punctuation">:</span>
      <span class="token keyword">if</span> reuse<span class="token punctuation">:</span>
        scope<span class="token punctuation">.</span>reuse_variables<span class="token punctuation">(</span><span class="token punctuation">)</span>
	
      <span class="token comment"># 如果没有这一维 </span>
      <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">:</span>
        <span class="token comment"># conv2d简介具体看 [7]，但第二个参数的介绍是不对的，补上[8].</span>
        <span class="token triple-quoted-string string">''' 参数简介： 第一个参数input：具有[batch, in_height, in_width, in_channels]这样的shape 第二个参数filter：conv2d中输出的特征图个数，是个1维的参数，即output_dim， output_dim是 conv2d函数的第二个入参，由外部传入。比如，下面的这句话，表示h1 是输入，通过卷积之后，输出的特征图个数为gf_dim* *4，这里gf_dim = 128，则输出 特征图为128*4=512个。即这里一共有512个卷积核。 第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4。[1, d_h, d_w, 1]，其中第一个对应一次跳过batch中的多少图片，第二个d_h对应一次跳过图片中 多少行，第三个d_w对应一次跳过图片中多少列，第四个对应一次跳过图像的多少个通道。这 里直接设置为[1，2，2，1]。即每次卷积后，图像的滑动步长为2，特征图会缩小为原来的 1/4。 第四个参数padding：string类型的量，只能是"SAME","VALID"其中之一，这个值决定了不 同的卷积方式 第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true 第六个参数：name=None '''</span>     
        <span class="token comment"># df_dim 作为类的参数传进来</span>
        <span class="token comment"># df_dim: (optional) Dimension of discrim filters in first conv layer[64]</span>
        <span class="token comment"># lrelu 在opt.py 中</span>
        <span class="token comment"># 输入是一个 image</span>
        h0 <span class="token operator">=</span> lrelu<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>image<span class="token punctuation">,</span> self<span class="token punctuation">.</span>df_dim<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'d_h0_conv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h1 <span class="token operator">=</span> lrelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_bn1<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>h0<span class="token punctuation">,</span> self<span class="token punctuation">.</span>df_dim<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'d_h1_conv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h2 <span class="token operator">=</span> lrelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_bn2<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>h1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>df_dim<span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'d_h2_conv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h3 <span class="token operator">=</span> lrelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_bn3<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>h2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>df_dim<span class="token operator">*</span><span class="token number">8</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'d_h3_conv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token comment">#全连接层，-1表示不确定大小。 linear 在opt.py中,自己构建的。</span>
        h4 <span class="token operator">=</span> linear<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>h3<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'d_h4_lin'</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>h4<span class="token punctuation">)</span><span class="token punctuation">,</span> h4
    
      <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token comment"># y_dim = 10</span>
        yb <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        <span class="token comment"># image被当作参数传进来的参数，Builder里是inputs.</span>
        <span class="token comment"># 原型在opt里，基于concat函数基础上修改。这里并没有理解为什么要这样操作....</span>
        <span class="token comment"># 这里代码以及构建不难，但是这里所有的concat尚且不了解原因。 </span>
        x <span class="token operator">=</span> conv_cond_concat<span class="token punctuation">(</span>image<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        h0 <span class="token operator">=</span> lrelu<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>c_dim <span class="token operator">+</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'d_h0_conv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h0 <span class="token operator">=</span> conv_cond_concat<span class="token punctuation">(</span>h0<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        h1 <span class="token operator">=</span> lrelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_bn1<span class="token punctuation">(</span>conv2d<span class="token punctuation">(</span>h0<span class="token punctuation">,</span> self<span class="token punctuation">.</span>df_dim <span class="token operator">+</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">,</span> 																	name<span class="token operator">=</span><span class="token string">'d_h1_conv'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>h1<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>      
        h1 <span class="token operator">=</span> concat<span class="token punctuation">(</span><span class="token punctuation">[</span>h1<span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        
        h2 <span class="token operator">=</span> lrelu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_bn2<span class="token punctuation">(</span>linear<span class="token punctuation">(</span>h1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dfc_dim<span class="token punctuation">,</span> <span class="token string">'d_h2_lin'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h2 <span class="token operator">=</span> concat<span class="token punctuation">(</span><span class="token punctuation">[</span>h2<span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        h3 <span class="token operator">=</span> linear<span class="token punctuation">(</span>h2<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">'d_h3_lin'</span><span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>h3<span class="token punctuation">)</span><span class="token punctuation">,</span> h3
</code></pre> 
  <blockquote> 
   <ul> 
    <li>[1] 下面是discriminator(鉴别器)的具体实现。</li> 
    <li>首先鉴别器使用 <strong>conv</strong> (卷积)操作，激活函数使用 <strong>leaky-relu</strong> ,每一个layer需要使用batch normalization。tensorflow的batch normalization使用 tf.contrib.layers.batch_norm 实现。</li> 
    <li>如果不是mnist,则第一层使用 <strong>leaky-relu+conv2d</strong> ,后面三层都使用 <strong>conv2d+BN+leaky-relu</strong> ,最后加上一个one hidden unit的linear layer,再送入sigmoid函数即可；</li> 
    <li>如果是mnist,则 yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) 首先给y增加两维，以便可以和image连接起来，这里实际上是使用了conditional GAN(条件GAN)的思想。</li> 
    <li>x = conv_cond_concat(image, yb) 得到condition和image合并之后的结果，然后 h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name=‘d_h0_conv’)) 进行卷积操作。第二次进行 <strong>conv2d+leaky-relu+concat</strong> 操作。第三次进行 <strong>conv2d+BN+leaky-relu+reshape+concat</strong> 操作。第四次进行 <strong>linear+BN+leaky-relu+concat</strong> 操作。最后同样是 <strong>linear+sigmoid</strong> 操作。</li> 
   </ul> 
  </blockquote> 
  <hr> 
  <h3><a id="_494"></a>补充：卷积网络</h3> 
  <blockquote> 
   <p>作者了解卷积神经网络的基础知识，但未真正自己建立过卷积神经网络，此处不甘心于函数，所以此处去重新学习了下整个卷积网络的构建,详见 <a href="http://xn--1-ht6av2jgtbfytb99bcqe48dc4a.md" rel="nofollow">卷积网络学习系列1.md</a></p> 
  </blockquote> 
  <blockquote> 
   <p>作者在温习以后才去标注的discriminator的卷积注释，所以写的不够小白。具体原理详见上。</p> 
  </blockquote> 
  <h3><a id="generator_502"></a>generator</h3> 
  <pre><code class="prism language-python"> <span class="token keyword">def</span> <span class="token function">generator</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> z<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"generator"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> scope<span class="token punctuation">:</span>
      
      	
      <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">:</span>
        s_h<span class="token punctuation">,</span> s_w <span class="token operator">=</span> self<span class="token punctuation">.</span>output_height<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_width
        
        <span class="token comment"># return int(math.ceil(float(size) / float(stride)))</span>
        s_h2<span class="token punctuation">,</span> s_w2 <span class="token operator">=</span> conv_out_size_same<span class="token punctuation">(</span>s_h<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> conv_out_size_same<span class="token punctuation">(</span>s_w<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        s_h4<span class="token punctuation">,</span> s_w4 <span class="token operator">=</span> conv_out_size_same<span class="token punctuation">(</span>s_h2<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> conv_out_size_same<span class="token punctuation">(</span>s_w2<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        s_h8<span class="token punctuation">,</span> s_w8 <span class="token operator">=</span> conv_out_size_same<span class="token punctuation">(</span>s_h4<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> conv_out_size_same<span class="token punctuation">(</span>s_w4<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        s_h16<span class="token punctuation">,</span> s_w16 <span class="token operator">=</span> conv_out_size_same<span class="token punctuation">(</span>s_h8<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> conv_out_size_same<span class="token punctuation">(</span>s_w8<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

        <span class="token comment"># project `z` and reshape</span>
        <span class="token comment">#self.z = tf.placeholder(tf.float32, [None, self.z_dim], name='z')</span>
        <span class="token comment"># gf_dim是generator的filter大小的基，*8 *4 *2 *1 c_dim这是他的变化规律，下面代码内找找仔细观察看看</span>
        self<span class="token punctuation">.</span>z_<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h0_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h0_b <span class="token operator">=</span> linear<span class="token punctuation">(</span>
            z<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim<span class="token operator">*</span><span class="token number">8</span><span class="token operator">*</span>s_h16<span class="token operator">*</span>s_w16<span class="token punctuation">,</span> <span class="token string">'g_h0_lin'</span><span class="token punctuation">,</span> with_w<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
		
        <span class="token comment"># 确定三个维度，可以计算出最后一个维度，那么这个维度可以用-1表示。 将全连接层reshape成特征图形状</span>
        self<span class="token punctuation">.</span>h0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>z_<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> s_h16<span class="token punctuation">,</span> s_w16<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim <span class="token operator">*</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        
        h0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn0<span class="token punctuation">(</span>self<span class="token punctuation">.</span>h0<span class="token punctuation">)</span><span class="token punctuation">)</span>
		
        <span class="token comment"># 产生batch_size个 设置好大小的特征图</span>
        self<span class="token punctuation">.</span>h1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h1_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h1_b <span class="token operator">=</span> deconv2d<span class="token punctuation">(</span>
            h0<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h8<span class="token punctuation">,</span> s_w8<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim<span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'g_h1'</span><span class="token punctuation">,</span> 							with_w<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        h1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>h1<span class="token punctuation">)</span><span class="token punctuation">)</span>

        h2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h2_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h2_b <span class="token operator">=</span> deconv2d<span class="token punctuation">(</span>
            h1<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h4<span class="token punctuation">,</span> s_w4<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'g_h2'</span><span class="token punctuation">,</span> 							with_w<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        h2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn2<span class="token punctuation">(</span>h2<span class="token punctuation">)</span><span class="token punctuation">)</span>

        h3<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h3_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h3_b <span class="token operator">=</span> deconv2d<span class="token punctuation">(</span>
            h2<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h2<span class="token punctuation">,</span> s_w2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim<span class="token operator">*</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'g_h3'</span><span class="token punctuation">,</span> 							with_w<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        h3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn3<span class="token punctuation">(</span>h3<span class="token punctuation">)</span><span class="token punctuation">)</span>
	
    	<span class="token comment"># c_dim,通道数 3</span>
        h4<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h4_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>h4_b <span class="token operator">=</span> deconv2d<span class="token punctuation">(</span>
            h3<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h<span class="token punctuation">,</span> s_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>c_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'g_h4'</span><span class="token punctuation">,</span> with_w<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>h4<span class="token punctuation">)</span>
    
      <span class="token keyword">else</span><span class="token punctuation">:</span>
        s_h<span class="token punctuation">,</span> s_w <span class="token operator">=</span> self<span class="token punctuation">.</span>output_height<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_width
        s_h2<span class="token punctuation">,</span> s_h4 <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>s_h<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>s_h<span class="token operator">/</span><span class="token number">4</span><span class="token punctuation">)</span>
        s_w2<span class="token punctuation">,</span> s_w4 <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>s_w<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>s_w<span class="token operator">/</span><span class="token number">4</span><span class="token punctuation">)</span>

        <span class="token comment"># yb = tf.expand_dims(tf.expand_dims(y, 1),2)</span>
        yb <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">]</span><span class="token punctuation">)</span>
        z <span class="token operator">=</span> concat<span class="token punctuation">(</span><span class="token punctuation">[</span>z<span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        h0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>g_bn0<span class="token punctuation">(</span>linear<span class="token punctuation">(</span>z<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gfc_dim<span class="token punctuation">,</span> <span class="token string">'g_h0_lin'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h0 <span class="token operator">=</span> concat<span class="token punctuation">(</span><span class="token punctuation">[</span>h0<span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        h1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn1<span class="token punctuation">(</span>
            linear<span class="token punctuation">(</span>h0<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">*</span>s_h4<span class="token operator">*</span>s_w4<span class="token punctuation">,</span> <span class="token string">'g_h1_lin'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>h1<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h4<span class="token punctuation">,</span> s_w4<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        h1 <span class="token operator">=</span> conv_cond_concat<span class="token punctuation">(</span>h1<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        h2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn2<span class="token punctuation">(</span>deconv2d<span class="token punctuation">(</span>h1<span class="token punctuation">,</span>
            <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h2<span class="token punctuation">,</span> s_w2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'g_h2'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h2 <span class="token operator">=</span> conv_cond_concat<span class="token punctuation">(</span>h2<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>
            deconv2d<span class="token punctuation">(</span>h2<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h<span class="token punctuation">,</span> s_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>c_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'g_h3'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <blockquote> 
   <ul> 
    <li>[1]下面是generator(生成器)的具体实现。和discriminator不同的是,generator需要使用deconv(反卷积)以及relu 激活函数。</li> 
    <li>generator的结构是:</li> 
    <li>1.如果不是mnist: linear+reshape+BN+relu----&gt;(deconv+BN+relu)x3 ----&gt;deconv+tanh ;</li> 
    <li>2.如果是mnist,则除了需要考虑输入z之外，还需要考虑label y,即需要将z和y连接起来(Conditional GAN),具体的结构是:reshape+concat----&gt;linear+BN+relu+concat----&gt;linear+BN+relu+reshape+concat----&gt;deconv+BN+relu+concat----&gt;deconv+sigmoid。</li> 
    <li>注意的最后的激活函数没有采用通常的tanh,而是采用了sigmoid(其输出会直接映射到0-1之间)。</li> 
   </ul> 
  </blockquote> 
  <blockquote> 
   <p>大佬们都不解释为什么要这样构建模型（补充：其实看里很久才知道，大佬们不说是因为原论文就是这样写构建的…不必纠结这个结构，因为还没到能设计结构的水平），所以，我只能硬着头皮自己解释。个人感觉，generator是从一个小输入，不断变大，变大，变成图片大小的过程，反卷积就用在这里。</p> 
  </blockquote> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">linear</span><span class="token punctuation">(</span>input_<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> scope<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">,</span> bias_start<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> with_w<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  
  shape <span class="token operator">=</span> input_<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>as_list<span class="token punctuation">(</span><span class="token punctuation">)</span>

  <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span>scope <span class="token operator">or</span> <span class="token string">"Linear"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
      matrix <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"Matrix"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_size<span class="token punctuation">]</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>
                 tf<span class="token punctuation">.</span>random_normal_initializer<span class="token punctuation">(</span>stddev<span class="token operator">=</span>stddev<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">except</span> ValueError <span class="token keyword">as</span> err<span class="token punctuation">:</span>
        msg <span class="token operator">=</span> <span class="token string">"NOTE: Usually, this is due to an issue with the image dimensions. Did you correctly set '--crop' or '--input_height' or '--output_height'?"</span>
        err<span class="token punctuation">.</span>args <span class="token operator">=</span> err<span class="token punctuation">.</span>args <span class="token operator">+</span> <span class="token punctuation">(</span>msg<span class="token punctuation">,</span><span class="token punctuation">)</span>
        <span class="token keyword">raise</span>
      bias <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">"bias"</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>output_size<span class="token punctuation">]</span><span class="token punctuation">,</span>
      initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>constant_initializer<span class="token punctuation">(</span>bias_start<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 是否输出权重</span>
    <span class="token keyword">if</span> with_w<span class="token punctuation">:</span>
      <span class="token keyword">return</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>input_<span class="token punctuation">,</span> matrix<span class="token punctuation">)</span> <span class="token operator">+</span> bias<span class="token punctuation">,</span> matrix<span class="token punctuation">,</span> bias
    <span class="token keyword">else</span><span class="token punctuation">:</span>
      <span class="token keyword">return</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>input_<span class="token punctuation">,</span> matrix<span class="token punctuation">)</span> <span class="token operator">+</span> bias
</code></pre> 
  <hr> 
  <h3><a id="_617"></a>反卷积</h3> 
  <blockquote> 
   <ul> 
    <li>恭喜，又是一个没听过的知识点…</li> 
    <li>此处另外学习反卷积，详见反卷积系列。</li> 
   </ul> 
  </blockquote> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">deconv2d</span><span class="token punctuation">(</span>input_<span class="token punctuation">,</span> output_shape<span class="token punctuation">,</span>
       k_h<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> k_w<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> d_h<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> d_w<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">,</span>
       name<span class="token operator">=</span><span class="token string">"deconv2d"</span><span class="token punctuation">,</span> with_w<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token comment"># filter : [height, width, output_channels, in_channels]</span>
    w <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'w'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>k_h<span class="token punctuation">,</span> k_w<span class="token punctuation">,</span> output_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> input_<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
              initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>random_normal_initializer<span class="token punctuation">(</span>stddev<span class="token operator">=</span>stddev<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token keyword">try</span><span class="token punctuation">:</span>
      deconv <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>conv2d_transpose<span class="token punctuation">(</span>input_<span class="token punctuation">,</span> w<span class="token punctuation">,</span> output_shape<span class="token operator">=</span>output_shape<span class="token punctuation">,</span>
                strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> d_h<span class="token punctuation">,</span> d_w<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># Support for verisons of TensorFlow before 0.7.0</span>
    <span class="token keyword">except</span> AttributeError<span class="token punctuation">:</span>
      <span class="token comment"># d_h,d_w参考原论文 </span>
      deconv <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>deconv2d<span class="token punctuation">(</span>input_<span class="token punctuation">,</span> w<span class="token punctuation">,</span> output_shape<span class="token operator">=</span>output_shape<span class="token punctuation">,</span>
                strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> d_h<span class="token punctuation">,</span> d_w<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
	
    <span class="token comment"># b的大小跟输出的特征图个数一样</span>
    biases <span class="token operator">=</span> tf<span class="token punctuation">.</span>get_variable<span class="token punctuation">(</span><span class="token string">'biases'</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>output_shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> initializer<span class="token operator">=</span>tf<span class="token punctuation">.</span>constant_initializer<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 先反卷积，后加偏置</span>
    deconv <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>bias_add<span class="token punctuation">(</span>deconv<span class="token punctuation">,</span> biases<span class="token punctuation">)</span><span class="token punctuation">,</span> deconv<span class="token punctuation">.</span>get_shape<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 是否输出权重</span>
    <span class="token keyword">if</span> with_w<span class="token punctuation">:</span>
      <span class="token keyword">return</span> deconv<span class="token punctuation">,</span> w<span class="token punctuation">,</span> biases
    <span class="token keyword">else</span><span class="token punctuation">:</span>
      <span class="token keyword">return</span> deconv
</code></pre> 
  <h3><a id="sampler_658"></a>sampler</h3> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">sampler</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> z<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"generator"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> scope<span class="token punctuation">:</span>
      scope<span class="token punctuation">.</span>reuse_variables<span class="token punctuation">(</span><span class="token punctuation">)</span>

      <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">:</span>
        s_h<span class="token punctuation">,</span> s_w <span class="token operator">=</span> self<span class="token punctuation">.</span>output_height<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_width
        s_h2<span class="token punctuation">,</span> s_w2 <span class="token operator">=</span> conv_out_size_same<span class="token punctuation">(</span>s_h<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> conv_out_size_same<span class="token punctuation">(</span>s_w<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        s_h4<span class="token punctuation">,</span> s_w4 <span class="token operator">=</span> conv_out_size_same<span class="token punctuation">(</span>s_h2<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> conv_out_size_same<span class="token punctuation">(</span>s_w2<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        s_h8<span class="token punctuation">,</span> s_w8 <span class="token operator">=</span> conv_out_size_same<span class="token punctuation">(</span>s_h4<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> conv_out_size_same<span class="token punctuation">(</span>s_w4<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        s_h16<span class="token punctuation">,</span> s_w16 <span class="token operator">=</span> conv_out_size_same<span class="token punctuation">(</span>s_h8<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> conv_out_size_same<span class="token punctuation">(</span>s_w8<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

        <span class="token comment"># project `z` and reshape</span>
        h0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>
            linear<span class="token punctuation">(</span>z<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim<span class="token operator">*</span><span class="token number">8</span><span class="token operator">*</span>s_h16<span class="token operator">*</span>s_w16<span class="token punctuation">,</span> <span class="token string">'g_h0_lin'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> s_h16<span class="token punctuation">,</span> s_w16<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim <span class="token operator">*</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        h0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn0<span class="token punctuation">(</span>h0<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        h1 <span class="token operator">=</span> deconv2d<span class="token punctuation">(</span>h0<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h8<span class="token punctuation">,</span> s_w8<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim<span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'g_h1'</span><span class="token punctuation">)</span>
        h1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn1<span class="token punctuation">(</span>h1<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        h2 <span class="token operator">=</span> deconv2d<span class="token punctuation">(</span>h1<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h4<span class="token punctuation">,</span> s_w4<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'g_h2'</span><span class="token punctuation">)</span>
        h2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn2<span class="token punctuation">(</span>h2<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        h3 <span class="token operator">=</span> deconv2d<span class="token punctuation">(</span>h2<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h2<span class="token punctuation">,</span> s_w2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim<span class="token operator">*</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'g_h3'</span><span class="token punctuation">)</span>
        h3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn3<span class="token punctuation">(</span>h3<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        h4 <span class="token operator">=</span> deconv2d<span class="token punctuation">(</span>h3<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h<span class="token punctuation">,</span> s_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>c_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'g_h4'</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>h4<span class="token punctuation">)</span>
    
      <span class="token keyword">else</span><span class="token punctuation">:</span>
        s_h<span class="token punctuation">,</span> s_w <span class="token operator">=</span> self<span class="token punctuation">.</span>output_height<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_width
        s_h2<span class="token punctuation">,</span> s_h4 <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>s_h<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>s_h<span class="token operator">/</span><span class="token number">4</span><span class="token punctuation">)</span>
        s_w2<span class="token punctuation">,</span> s_w4 <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>s_w<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>s_w<span class="token operator">/</span><span class="token number">4</span><span class="token punctuation">)</span>

        <span class="token comment"># yb = tf.reshape(y, [-1, 1, 1, self.y_dim])</span>
        yb <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>y_dim<span class="token punctuation">]</span><span class="token punctuation">)</span>
        z <span class="token operator">=</span> concat<span class="token punctuation">(</span><span class="token punctuation">[</span>z<span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        h0 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn0<span class="token punctuation">(</span>linear<span class="token punctuation">(</span>z<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gfc_dim<span class="token punctuation">,</span> <span class="token string">'g_h0_lin'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h0 <span class="token operator">=</span> concat<span class="token punctuation">(</span><span class="token punctuation">[</span>h0<span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        h1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn1<span class="token punctuation">(</span>
            linear<span class="token punctuation">(</span>h0<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim<span class="token operator">*</span><span class="token number">2</span><span class="token operator">*</span>s_h4<span class="token operator">*</span>s_w4<span class="token punctuation">,</span> <span class="token string">'g_h1_lin'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>h1<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h4<span class="token punctuation">,</span> s_w4<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        h1 <span class="token operator">=</span> conv_cond_concat<span class="token punctuation">(</span>h1<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        h2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_bn2<span class="token punctuation">(</span>
            deconv2d<span class="token punctuation">(</span>h1<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h2<span class="token punctuation">,</span> s_w2<span class="token punctuation">,</span> self<span class="token punctuation">.</span>gf_dim <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'g_h2'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 							train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        h2 <span class="token operator">=</span> conv_cond_concat<span class="token punctuation">(</span>h2<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        <span class="token keyword">return</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>deconv2d<span class="token punctuation">(</span>h2<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> s_h<span class="token punctuation">,</span> s_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>c_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> 							name<span class="token operator">=</span><span class="token string">'g_h3'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
  <blockquote> 
   <ul> 
    <li>sampler函数是采样函数，用于生成样本送入当前训练的生成器，查看训练效果。其逻辑和generator函数基本类似,也是需要区分是否是mnist,二者需要采用不同的结构。不是mnist时,y=None即可；否则mnist还需要考虑y。</li> 
    <li>如果单纯去看代码，前面那么代码的铺垫已经够了，但是，不理解为什么要用这个sampler,有什么用，怎么感觉他在做和generator一样的事儿。</li> 
    <li>具体为什么用它，得等到最后我去啃DGCAN原论文时候再了解吧。</li> 
   </ul> 
  </blockquote> 
  <hr> 
  <h3><a id="DCGAN_728"></a>DCGAN原文解读</h3> 
  <blockquote> 
   <ul> 
    <li>此处为看完train代码后补的，单单看懂代码的大致逻辑很容易，但是细节完全把握不住，由于前面没有取细读论文，这里集中爆发了，感觉确实需要读完论文再往下做，所以，开始论文时间。</li> 
   </ul> 
  </blockquote> 
  <h3><a id="train__738"></a>train 函数</h3> 
  <blockquote> 
   <ul> 
    <li>补充，终于到这儿了，突然感觉真的好多啊，以前没有这样做过，但这样来一遍，发现自己啥都不是，学到了很多，但也知道很多自己都没写出来，水平有限。当然，万分感谢那些我一头水雾时找到的大佬博客，动力之源啊。</li> 
   </ul> 
  </blockquote> 
  <blockquote> 
   <ul> 
    <li>这应该是最难啃的一块骨头了，目测代码量瑟瑟发抖…</li> 
   </ul> 
  </blockquote> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token comment"># 用adam优化 </span>
    d_optim <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>config<span class="token punctuation">.</span>learning_rate<span class="token punctuation">,</span> beta1<span class="token operator">=</span>config<span class="token punctuation">.</span>beta1<span class="token punctuation">)</span> \
              <span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_loss<span class="token punctuation">,</span> var_list<span class="token operator">=</span>self<span class="token punctuation">.</span>d_vars<span class="token punctuation">)</span>
        
    g_optim <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>AdamOptimizer<span class="token punctuation">(</span>config<span class="token punctuation">.</span>learning_rate<span class="token punctuation">,</span> beta1<span class="token operator">=</span>config<span class="token punctuation">.</span>beta1<span class="token punctuation">)</span> \
              <span class="token punctuation">.</span>minimize<span class="token punctuation">(</span>self<span class="token punctuation">.</span>g_loss<span class="token punctuation">,</span> var_list<span class="token operator">=</span>self<span class="token punctuation">.</span>g_vars<span class="token punctuation">)</span>
    
    <span class="token comment">#避免tensorflow版本问题报错 </span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
      tf<span class="token punctuation">.</span>global_variables_initializer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">except</span><span class="token punctuation">:</span>
      tf<span class="token punctuation">.</span>initialize_all_variables<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">)</span>
	
    <span class="token comment"># merge_summary 函数和 SummaryWriter 用于构建summary,在tensorboard中显示。</span>
    self<span class="token punctuation">.</span>g_sum <span class="token operator">=</span> merge_summary<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>z_sum<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d__sum<span class="token punctuation">,</span>
      self<span class="token punctuation">.</span>G_sum<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_loss_fake_sum<span class="token punctuation">,</span> self<span class="token punctuation">.</span>g_loss_sum<span class="token punctuation">]</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>d_sum <span class="token operator">=</span> merge_summary<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>self<span class="token punctuation">.</span>z_sum<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_sum<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_loss_real_sum<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_loss_sum<span class="token punctuation">]</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span><span class="token string">"./logs"</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>graph<span class="token punctuation">)</span>

    <span class="token comment"># 噪音向量 [个数，维度]</span>
    sample_z <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>sample_num <span class="token punctuation">,</span> self<span class="token punctuation">.</span>z_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token keyword">if</span> config<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">'mnist'</span><span class="token punctuation">:</span>
      sample_inputs <span class="token operator">=</span> self<span class="token punctuation">.</span>data_X<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>sample_num<span class="token punctuation">]</span>
      sample_labels <span class="token operator">=</span> self<span class="token punctuation">.</span>data_y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>sample_num<span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
      <span class="token comment">#读取图片并且改变大小 </span>
      sample_files <span class="token operator">=</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>sample_num<span class="token punctuation">]</span>
      sample <span class="token operator">=</span> <span class="token punctuation">[</span>
          get_image<span class="token punctuation">(</span>sample_file<span class="token punctuation">,</span>
                    input_height<span class="token operator">=</span>self<span class="token punctuation">.</span>input_height<span class="token punctuation">,</span>
                    input_width<span class="token operator">=</span>self<span class="token punctuation">.</span>input_width<span class="token punctuation">,</span>
                    resize_height<span class="token operator">=</span>self<span class="token punctuation">.</span>output_height<span class="token punctuation">,</span>
                    resize_width<span class="token operator">=</span>self<span class="token punctuation">.</span>output_width<span class="token punctuation">,</span>
                    crop<span class="token operator">=</span>self<span class="token punctuation">.</span>crop<span class="token punctuation">,</span>
                    grayscale<span class="token operator">=</span>self<span class="token punctuation">.</span>grayscale<span class="token punctuation">)</span> <span class="token keyword">for</span> sample_file <span class="token keyword">in</span> sample_files<span class="token punctuation">]</span>    
    <span class="token comment">#如果处理的图像是灰度图像,则需要再增加一个dim,表示图像的 channel=1 </span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>grayscale<span class="token punctuation">)</span><span class="token punctuation">:</span>
        sample_inputs <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>sample<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
        sample_inputs <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>sample<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
  
    counter <span class="token operator">=</span> <span class="token number">1</span>
    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 下载一波模型</span>
    could_load<span class="token punctuation">,</span> checkpoint_counter <span class="token operator">=</span> self<span class="token punctuation">.</span>load<span class="token punctuation">(</span>self<span class="token punctuation">.</span>checkpoint_dir<span class="token punctuation">)</span>
    
    <span class="token comment"># 下载成功</span>
    <span class="token keyword">if</span> could_load<span class="token punctuation">:</span>
      counter <span class="token operator">=</span> checkpoint_counter
      <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">" [*] Load SUCCESS"</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
      <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">" [!] Load failed..."</span><span class="token punctuation">)</span>

    <span class="token comment"># 循环训练开始</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">xrange</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>    
      <span class="token keyword">if</span> config<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">'mnist'</span><span class="token punctuation">:</span>
        batch_idxs <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_X<span class="token punctuation">)</span><span class="token punctuation">,</span> config<span class="token punctuation">.</span>train_size<span class="token punctuation">)</span> <span class="token operator">//</span> config<span class="token punctuation">.</span>batch_size
      <span class="token keyword">else</span><span class="token punctuation">:</span> 
        <span class="token comment"># 数据集所有数据文件名称</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> glob<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>
          config<span class="token punctuation">.</span>data_dir<span class="token punctuation">,</span> config<span class="token punctuation">.</span>dataset<span class="token punctuation">,</span> self<span class="token punctuation">.</span>input_fname_pattern<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 打乱</span>
        np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
         
        <span class="token comment"># bath_size 表示一次指定几张图片 </span>
        batch_idxs <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">,</span> config<span class="token punctuation">.</span>train_size<span class="token punctuation">)</span> <span class="token operator">//</span> config<span class="token punctuation">.</span>batch_size

      <span class="token keyword">for</span> idx <span class="token keyword">in</span> <span class="token builtin">xrange</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>batch_idxs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        
        <span class="token comment">#每次循环的数据集抽取</span>
        <span class="token keyword">if</span> config<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">'mnist'</span><span class="token punctuation">:</span>
          batch_images <span class="token operator">=</span> self<span class="token punctuation">.</span>data_X<span class="token punctuation">[</span>idx<span class="token operator">*</span>config<span class="token punctuation">.</span>batch_size<span class="token punctuation">:</span><span class="token punctuation">(</span>idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>config<span class="token punctuation">.</span>batch_size<span class="token punctuation">]</span>
          batch_labels <span class="token operator">=</span> self<span class="token punctuation">.</span>data_y<span class="token punctuation">[</span>idx<span class="token operator">*</span>config<span class="token punctuation">.</span>batch_size<span class="token punctuation">:</span><span class="token punctuation">(</span>idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>config<span class="token punctuation">.</span>batch_size<span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
          batch_files <span class="token operator">=</span> self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>idx<span class="token operator">*</span>config<span class="token punctuation">.</span>batch_size<span class="token punctuation">:</span><span class="token punctuation">(</span>idx<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>config<span class="token punctuation">.</span>batch_size<span class="token punctuation">]</span>
          batch <span class="token operator">=</span> <span class="token punctuation">[</span>
              get_image<span class="token punctuation">(</span>batch_file<span class="token punctuation">,</span>
                        input_height<span class="token operator">=</span>self<span class="token punctuation">.</span>input_height<span class="token punctuation">,</span>
                        input_width<span class="token operator">=</span>self<span class="token punctuation">.</span>input_width<span class="token punctuation">,</span>
                        resize_height<span class="token operator">=</span>self<span class="token punctuation">.</span>output_height<span class="token punctuation">,</span>
                        resize_width<span class="token operator">=</span>self<span class="token punctuation">.</span>output_width<span class="token punctuation">,</span>
                        crop<span class="token operator">=</span>self<span class="token punctuation">.</span>crop<span class="token punctuation">,</span>
                        grayscale<span class="token operator">=</span>self<span class="token punctuation">.</span>grayscale<span class="token punctuation">)</span> <span class="token keyword">for</span> batch_file <span class="token keyword">in</span> batch_files<span class="token punctuation">]</span>
          <span class="token keyword">if</span> self<span class="token punctuation">.</span>grayscale<span class="token punctuation">:</span>
            batch_images <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
          <span class="token keyword">else</span><span class="token punctuation">:</span>
            batch_images <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

            
        batch_z <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>config<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>z_dim<span class="token punctuation">]</span><span class="token punctuation">)</span> \
              <span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>

        <span class="token comment"># run </span>
        <span class="token keyword">if</span> config<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">'mnist'</span><span class="token punctuation">:</span>
          <span class="token comment"># Update D network</span>
          _<span class="token punctuation">,</span> summary_str <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>d_optim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_sum<span class="token punctuation">]</span><span class="token punctuation">,</span>
            feed_dict<span class="token operator">=</span><span class="token punctuation">{</span> 
              self<span class="token punctuation">.</span>inputs<span class="token punctuation">:</span> batch_images<span class="token punctuation">,</span>
              self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> batch_z<span class="token punctuation">,</span>
              self<span class="token punctuation">.</span>y<span class="token punctuation">:</span>batch_labels<span class="token punctuation">,</span>
            <span class="token punctuation">}</span><span class="token punctuation">)</span>
          self<span class="token punctuation">.</span>writer<span class="token punctuation">.</span>add_summary<span class="token punctuation">(</span>summary_str<span class="token punctuation">,</span> counter<span class="token punctuation">)</span>

          <span class="token comment"># Update G network</span>
          _<span class="token punctuation">,</span> summary_str <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>g_optim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>g_sum<span class="token punctuation">]</span><span class="token punctuation">,</span>
            feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>
              self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> batch_z<span class="token punctuation">,</span> 
              self<span class="token punctuation">.</span>y<span class="token punctuation">:</span>batch_labels<span class="token punctuation">,</span>
            <span class="token punctuation">}</span><span class="token punctuation">)</span>
          self<span class="token punctuation">.</span>writer<span class="token punctuation">.</span>add_summary<span class="token punctuation">(</span>summary_str<span class="token punctuation">,</span> counter<span class="token punctuation">)</span>

          <span class="token comment"># Run g_optim twice to make sure that d_loss does not go to zero (different from paper)</span>
          _<span class="token punctuation">,</span> summary_str <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>g_optim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>g_sum<span class="token punctuation">]</span><span class="token punctuation">,</span>
            feed_dict<span class="token operator">=</span><span class="token punctuation">{</span> self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> batch_z<span class="token punctuation">,</span> self<span class="token punctuation">.</span>y<span class="token punctuation">:</span>batch_labels <span class="token punctuation">}</span><span class="token punctuation">)</span>
            
          self<span class="token punctuation">.</span>writer<span class="token punctuation">.</span>add_summary<span class="token punctuation">(</span>summary_str<span class="token punctuation">,</span> counter<span class="token punctuation">)</span>
          
          errD_fake <span class="token operator">=</span> self<span class="token punctuation">.</span>d_loss_fake<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">{</span>
              self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> batch_z<span class="token punctuation">,</span> 
              self<span class="token punctuation">.</span>y<span class="token punctuation">:</span>batch_labels
          <span class="token punctuation">}</span><span class="token punctuation">)</span>
          errD_real <span class="token operator">=</span> self<span class="token punctuation">.</span>d_loss_real<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">{</span>
              self<span class="token punctuation">.</span>inputs<span class="token punctuation">:</span> batch_images<span class="token punctuation">,</span>
              self<span class="token punctuation">.</span>y<span class="token punctuation">:</span>batch_labels
          <span class="token punctuation">}</span><span class="token punctuation">)</span>
          errG <span class="token operator">=</span> self<span class="token punctuation">.</span>g_loss<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">{</span>
              self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> batch_z<span class="token punctuation">,</span>
              self<span class="token punctuation">.</span>y<span class="token punctuation">:</span> batch_labels
          <span class="token punctuation">}</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
          <span class="token comment"># Update D network</span>
          _<span class="token punctuation">,</span> summary_str <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>d_optim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_sum<span class="token punctuation">]</span><span class="token punctuation">,</span>
            feed_dict<span class="token operator">=</span><span class="token punctuation">{</span> self<span class="token punctuation">.</span>inputs<span class="token punctuation">:</span> batch_images<span class="token punctuation">,</span> self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> batch_z <span class="token punctuation">}</span><span class="token punctuation">)</span>
          self<span class="token punctuation">.</span>writer<span class="token punctuation">.</span>add_summary<span class="token punctuation">(</span>summary_str<span class="token punctuation">,</span> counter<span class="token punctuation">)</span>

          <span class="token comment"># Update G network</span>
          _<span class="token punctuation">,</span> summary_str <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>g_optim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>g_sum<span class="token punctuation">]</span><span class="token punctuation">,</span>
            feed_dict<span class="token operator">=</span><span class="token punctuation">{</span> self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> batch_z <span class="token punctuation">}</span><span class="token punctuation">)</span>
          self<span class="token punctuation">.</span>writer<span class="token punctuation">.</span>add_summary<span class="token punctuation">(</span>summary_str<span class="token punctuation">,</span> counter<span class="token punctuation">)</span>

          <span class="token comment"># Run g_optim twice to make sure that d_loss does not go to zero (different from paper)</span>
          _<span class="token punctuation">,</span> summary_str <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token punctuation">[</span>g_optim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>g_sum<span class="token punctuation">]</span><span class="token punctuation">,</span>
            feed_dict<span class="token operator">=</span><span class="token punctuation">{</span> self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> batch_z <span class="token punctuation">}</span><span class="token punctuation">)</span>
          self<span class="token punctuation">.</span>writer<span class="token punctuation">.</span>add_summary<span class="token punctuation">(</span>summary_str<span class="token punctuation">,</span> counter<span class="token punctuation">)</span>
          
          errD_fake <span class="token operator">=</span> self<span class="token punctuation">.</span>d_loss_fake<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">{</span> self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> batch_z <span class="token punctuation">}</span><span class="token punctuation">)</span>
          errD_real <span class="token operator">=</span> self<span class="token punctuation">.</span>d_loss_real<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">{</span> self<span class="token punctuation">.</span>inputs<span class="token punctuation">:</span> batch_images <span class="token punctuation">}</span><span class="token punctuation">)</span>
          errG <span class="token operator">=</span> self<span class="token punctuation">.</span>g_loss<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">{</span>self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> batch_z<span class="token punctuation">}</span><span class="token punctuation">)</span>

        counter <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Epoch: [%2d/%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f"</span> \
          <span class="token operator">%</span> <span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> config<span class="token punctuation">.</span>epoch<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> batch_idxs<span class="token punctuation">,</span>
            time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start_time<span class="token punctuation">,</span> errD_fake<span class="token operator">+</span>errD_real<span class="token punctuation">,</span> errG<span class="token punctuation">)</span><span class="token punctuation">)</span>
		
        <span class="token triple-quoted-string string">''' np.mod(counter, config.print_every) == 1 表示每 print_every 次生成一次samples; np.mod(counter, config.checkpoint_every) == 2 表示每 checkpoint_every 次保存一下 checkpoint file。 '''</span>
        <span class="token keyword">if</span> np<span class="token punctuation">.</span>mod<span class="token punctuation">(</span>counter<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
          <span class="token keyword">if</span> config<span class="token punctuation">.</span>dataset <span class="token operator">==</span> <span class="token string">'mnist'</span><span class="token punctuation">:</span>
            samples<span class="token punctuation">,</span> d_loss<span class="token punctuation">,</span> g_loss <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>
              <span class="token punctuation">[</span>self<span class="token punctuation">.</span>sampler<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_loss<span class="token punctuation">,</span> self<span class="token punctuation">.</span>g_loss<span class="token punctuation">]</span><span class="token punctuation">,</span>
              feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>
                  self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> sample_z<span class="token punctuation">,</span>
                  self<span class="token punctuation">.</span>inputs<span class="token punctuation">:</span> sample_inputs<span class="token punctuation">,</span>
                  self<span class="token punctuation">.</span>y<span class="token punctuation">:</span>sample_labels<span class="token punctuation">,</span>
              <span class="token punctuation">}</span>
            <span class="token punctuation">)</span>
            save_images<span class="token punctuation">(</span>samples<span class="token punctuation">,</span> image_manifold_size<span class="token punctuation">(</span>samples<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                  <span class="token string">'./{}/train_{:02d}_{:04d}.png'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>sample_dir<span class="token punctuation">,</span> epoch<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[Sample] d_loss: %.8f, g_loss: %.8f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>d_loss<span class="token punctuation">,</span> g_loss<span class="token punctuation">)</span><span class="token punctuation">)</span> 
          <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">try</span><span class="token punctuation">:</span>
              samples<span class="token punctuation">,</span> d_loss<span class="token punctuation">,</span> g_loss <span class="token operator">=</span> self<span class="token punctuation">.</span>sess<span class="token punctuation">.</span>run<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>self<span class="token punctuation">.</span>sampler<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_loss<span class="token punctuation">,</span> self<span class="token punctuation">.</span>g_loss<span class="token punctuation">]</span><span class="token punctuation">,</span>
                feed_dict<span class="token operator">=</span><span class="token punctuation">{</span>
                    self<span class="token punctuation">.</span>z<span class="token punctuation">:</span> sample_z<span class="token punctuation">,</span>
                    self<span class="token punctuation">.</span>inputs<span class="token punctuation">:</span> sample_inputs<span class="token punctuation">,</span>
                <span class="token punctuation">}</span><span class="token punctuation">,</span>
              <span class="token punctuation">)</span>
              save_images<span class="token punctuation">(</span>samples<span class="token punctuation">,</span> image_manifold_size<span class="token punctuation">(</span>samples<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token string">'./{}/train_{:02d}_{:04d}.png'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>config<span class="token punctuation">.</span>sample_dir<span class="token punctuation">,</span> epoch<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">)</span>
              <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[Sample] d_loss: %.8f, g_loss: %.8f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>d_loss<span class="token punctuation">,</span> g_loss<span class="token punctuation">)</span><span class="token punctuation">)</span> 
            <span class="token keyword">except</span><span class="token punctuation">:</span>
              <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"one pic error!..."</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> np<span class="token punctuation">.</span>mod<span class="token punctuation">(</span>counter<span class="token punctuation">,</span> <span class="token number">500</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
          self<span class="token punctuation">.</span>save<span class="token punctuation">(</span>config<span class="token punctuation">.</span>checkpoint_dir<span class="token punctuation">,</span> counter<span class="token punctuation">)</span>
</code></pre> 
  <blockquote> 
   <ul> 
    <li>下载模型的代码 [11]</li> 
   </ul> 
  </blockquote> 
  <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">load</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> checkpoint_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">import</span> re
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">" [*] Reading checkpoints..."</span><span class="token punctuation">)</span>
    checkpoint_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>checkpoint_dir<span class="token punctuation">,</span> self<span class="token punctuation">.</span>model_dir<span class="token punctuation">)</span>
	
    ckpt <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>get_checkpoint_state<span class="token punctuation">(</span>checkpoint_dir<span class="token punctuation">)</span>
    <span class="token keyword">if</span> ckpt <span class="token operator">and</span> ckpt<span class="token punctuation">.</span>model_checkpoint_path<span class="token punctuation">:</span>
        
      ckpt_name <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>basename<span class="token punctuation">(</span>ckpt<span class="token punctuation">.</span>model_checkpoint_path<span class="token punctuation">)</span>
    
      self<span class="token punctuation">.</span>saver<span class="token punctuation">.</span>restore<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sess<span class="token punctuation">,</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>checkpoint_dir<span class="token punctuation">,</span> ckpt_name<span class="token punctuation">)</span><span class="token punctuation">)</span>
      
      <span class="token comment"># 不太清楚这句话有什么用</span>
      counter <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">next</span><span class="token punctuation">(</span>re<span class="token punctuation">.</span>finditer<span class="token punctuation">(</span><span class="token string">"(\d+)(?!.*\d)"</span><span class="token punctuation">,</span>ckpt_name<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">" [*] Success to read {}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>ckpt_name<span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token keyword">return</span> <span class="token boolean">True</span><span class="token punctuation">,</span> counter
    <span class="token keyword">else</span><span class="token punctuation">:</span>
      <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">" [*] Failed to find a checkpoint"</span><span class="token punctuation">)</span>
      <span class="token keyword">return</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token number">0</span>
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
