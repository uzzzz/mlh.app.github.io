<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>基于候选区域的目标检测器（两阶段）和单次目标检测器（一阶段） | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="基于候选区域的目标检测器（两阶段）和单次目标检测器（一阶段）" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="https://www.cnblogs.com/cloud-ken/p/9470992.html https://www.cnblogs.com/cloud-ken/tag/DeeplearningAI/default.html?page=3 https://www.cnblogs.com/guoyaohua/p/8994246.html https://blog.csdn.net/jiongnima/article/details/79094159 （这个非常好，对比看看） （&nbsp;联合RPN和Fast R-CNN的训练。没看懂。） 一、基于候选区域的目标检测器（两阶段） &nbsp; &nbsp; 主要是RCNN系列。 1.1 &nbsp;滑动窗口检测器 &nbsp; 伪代码。我们创建很多窗口来检测不同位置的不同目标。要提升性能，一个显而易见的办法就是减少窗口数量。 for window in windows patch = get_patch(image, window) 　　results = detector(patch) 1.2 &nbsp;选择性搜索(SS) 　　我们不使用暴力方法，而是用候选区域方法（region proposal method）创建目标检测的感兴趣区域（ROI）。在选择性搜索（selective search，SS）中，我们首先将每个像素作为一组，计算每一组的纹理，并将两个最接近的组结合起来。但是为了避免单个区域吞噬其他区域，我们首先对较小的组进行分组。我们继续合并区域，直到所有区域都结合在一起。下图第一行展示了如何使区域增长，第二行中的蓝色矩形代表合并过程中所有可能的&nbsp;ROI。 1.3 &nbsp;R-CNN 通过使用更少且更高质量的 ROI，R-CNN 要比滑动窗口方法更快速、更准确。 如果我们有 2000 个候选区域，且每一个都需要独立地馈送到 CNN 中，那么对于不同的 ROI，我们需要重复提取 2000 次特征。（R-CNN很多卷积运算是重复的） ROIs = region_proposal(image) for ROI in ROIs: patch = get_patch(image, ROI) results = detector(pach) 边界框回归器(refinement) 　　候选区域方法有非常高的计算复杂度。为了加速这个过程，我们通常会使用计算量较少的候选区域选择方法构建 ROI，并在后面使用线性回归器（使用全连接层）进一步提炼边界框。 1.4 &nbsp;Fast R-CNN CNN 中的特征图以一种密集的方式表征空间特征，那么我们能直接使用特征图代替原图来检测目标吗？(能) 　Fast R-CNN 使用特征提取器（CNN）先提取整个图像的特征，而不是从头开始对每个图像块提取多次。然后，我们可以将创建候选区域的方法直接应用到提取到的特征图上。例如，Fast R-CNN 选择了 VGG16 中的卷积层 conv5 输出的 Feture Map 来生成 ROI，这些关注区域随后会结合对应的特征图以裁剪为特征图块，并用于目标检测任务中。我们使用 ROI 池化将特征图块转换为固定的大小，并馈送到全连接层进行分类和定位。因为 Fast-RCNN 不会重复提取特征，因此它能显著地减少处理时间。 将候选区域直接应用于特征图，并使用 ROI 池化将其转化为固定大小的特征图块（将 ROI（蓝色区域）与特征图重叠。） 在下面的伪代码中，计算量巨大的特征提取过程从 For 循环中移出来了，因此速度得到显著提升。Fast R-CNN 的训练速度是 R-CNN 的 10 倍，推断速度是后者的 150 倍。 feature_maps = process(image) ROIs = region_proposal(feature_maps) for ROI in ROIs: patch = roi_pooling(feature_maps, ROI) results = detector2(patch) 　　Fast R-CNN 最重要的一点就是包含特征提取器、分类器和边界框回归器在内的整个网络能通过多任务损失函数进行端到端的训练，这种多任务损失即结合了分类损失和定位损失的方法，大大提升了模型准确度。 ROI 池化（？？？？） 　　因为 Fast R-CNN 使用全连接层，所以我们应用 ROI 池化将不同大小的 ROI 转换为固定大小。 1.5 &nbsp;Faster R-CNN &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Faster R-CNN 采用与 Fast R-CNN 相同的设计，只是它用内部深层网络代替了候选区域方法。两个卷积网络，分布用于提取特征和生成ROIs（是否有目标，其位置及目标得分（不是类别得分））。新的候选区域网络（RPN）在生成 ROI 时效率更高，并且以每幅图像 10 毫秒的速度运行。 Faster R-CNN 的流程图与 Fast R-CNN 相同 feature_maps = process(image) ROIs = region_proposal(feature_maps) for ROI in ROIs patch = roi_pooling(feature_maps, ROI) class_scores, box = detector(patch) # Expensive! class_probabilities = softmax(class_scores) 候选区域网络(RPN) 　　候选区域网络（RPN）将第一个卷积网络的输出特征图作为输入。它在特征图上滑动一个 3×3 的卷积核，以使用卷积网络构建与类别无关的候选区域。如ZF 网络最后会输出 256 个值，它们将馈送到两个独立的全连接层，以预测边界框和两个 objectness 分数（边界框是否包含目标）。我们其实可以使用回归器计算单个 objectness 分数，但为简洁起见，Faster R-CNN 使用只有两个类别的分类器：即带有目标的类别和不带有目标的类别。 Faster R-CNN 不会创建随机边界框。相反，它会预测一些与左上角名为「锚点」（这是由经验固定比例的，yolo是聚类得到固定大小的）的参考框相关的偏移量（如x、y）： 首先第一个问题是为什么需要RoI Pooling？答案是在Fast R-CNN中，特征被共享卷积层一次性提取。因此，对于每个RoI而言，需要从共享卷积层上摘取对应的特征，并且送入全连接层进行分类。因此，RoI Pooling主要做了两件事，第一件是为每个RoI选取对应的特征，第二件事是为了满足全连接层的输入需求，将每个RoI对应的特征的维度转化成某个定值（对于每一个RoI，RoI Pooling Layer将其对应的特征从共享卷积层上拿出来，并转化成一样的大小(6×6)，类似对应roi坐标位置的分割？）。RoI Pooling示意图如 1.6 &nbsp;基于区域的全卷积神经网络（R-FCN）（没接触过，多看看） R-FCN 通过减少每个 ROI 所需的工作量实现加速（去掉了全连接层）。上面基于区域的特征图与 ROI 是独立的，可以在每个 ROI 之外单独计算。剩下的工作就比较简单了，因此 R-FCN 的速度比 Faster R-CNN 快。 假设我们只有一个特征图用来检测右眼。那么我们可以使用它定位人脸吗？应该可以。因为右眼应该在人脸图像的左上角，所以我们可以利用这一点定位整个人脸。如果我们还有其他用来检测左眼、鼻子或嘴巴的特征图，那么我们可以将检测结果结合起来，更好地定位人脸。 &nbsp; feature_maps = process(image) ROIs = region_proposal(feature_maps) score_maps = compute_score_map(feature_maps) for ROI in ROIs V = region_roi_pool(score_maps, ROI) class_scores, box = average(V) # Much simpler! class_probabilities = softmax(class_scores) &nbsp; &nbsp; &nbsp; （假设k=3）现在我们来看一下 5 × 5 的特征图 M，内部包含一个蓝色方块。我们将方块平均分成 3 × 3 个区域。现在，我们在 M 中创建了一个新的特征图，来检测方块的左上角（TL）。这个新的特征图如下图（右）所示。只有黄色的网格单元 [2, 2] 处于激活状态。 在左侧创建一个新的特征图，用于检测目标的左上角 　　我们将方块分成 9 个部分，由此创建了 9 个特征图，每个用来检测对应的目标区域。这些特征图叫作位置敏感得分图（position-sensitive score map），因为每个图检测目标的子区域（计算其得分）。 生成 9 个得分图 　　下图中红色虚线矩形是建议的 ROI。我们将其分割成 3 × 3 个区域，并询问每个区域包含目标对应部分的概率是多少。例如，左上角 ROI 区域包含左眼的概率。我们将结果存储成 3 × 3 vote 数组，如下图（右）所示。例如，vote_array[0][0] 包含左上角区域是否包含目标对应部分的得分。 将 ROI 应用到特征图上，输出一个 3 x 3 数组 　　将得分图(Feature Map)和 ROI 映射到 vote 数组的过程叫作位置敏感 ROI 池化（position-sensitive ROI-pool）。该过程与前面讨论过的 ROI 池化非常接近。 将 ROI 的一部分叠加到对应的得分图上，计算 V[i][j] 　　在计算出位置敏感 ROI 池化的所有值后，类别得分是其所有元素得分的平均值。 ROI 池化 　　假如我们有 C 个类别要检测。我们将其扩展为 C + 1 个类别，这样就为背景（非目标）增加了一个新的类别。每个类别有 3 × 3 个得分图，因此一共有 (C+1) × 3 × 3 个得分图。使用每个类别的得分图可以预测出该类别的类别得分。然后我们对这些得分应用 softmax 函数，计算出每个类别的概率。 二、单次目标检测器 　　单次目标检测器（SSD、YOLO、YOLOv2、YOLOv3）。1)分析FPN多尺度特征图如何提高准确率，特别是小目标的检测，其在单次检测器中的检测效果通常很差。2)分析 Focal loss 和 RetinaNet，是如何解决训练过程中的类别不平衡问题的。 &nbsp; &nbsp; &nbsp; 单次检测器通常需要在准确率和实时处理速度之间进行权衡。它们在检测太近距离或太小的目标时容易出现问题。 &nbsp; &nbsp; &nbsp; 作为替代，我们是否需要一个像RCNN系列 分离的候选区域步骤？我们可以直接在一个步骤内得到边界框和类别吗？ feature_maps = process(image) ROIs = region_proposal(feature_maps) for ROI in ROIs patch = roi_align(feature_maps, ROI) results = detector2(patch) # Reduce the amount of work here! 　　转化为： feature_maps = process(image) results = detector3(feature_maps) # No more separate step for ROIs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;让我们再看一下滑动窗口检测器。我们可以通过在特征图上滑动窗口来检测目标。对于不同的目标类型，我们使用不同的窗口类型。以前的滑动窗口方法的致命错误在于使用窗口作为最终的边界框，这就需要非常多的形状来覆盖大部分目标。更有效的方法是将窗口当做初始猜想，这样我们就得到了从当前滑动窗口同时预测类别和边界框的检测器。 基于滑动窗口进行预测 &nbsp; 在特征图每个位置，我们有 k 个anchors（锚（先验框）是固定的初始边界框猜想），一个先验框对应一个特定位置（锚点）。我们使用相同的先验框形状仔细地选择锚点和每个位置。 　　以下是一个锚点的 4 个先验框（绿色）和 4 个对应预测（蓝色），每个预测对应一个特定先验框。（YOLOv3每个尺度特征图的每个锚点3个先验框，3个尺度，应用到这里就是一个锚点有3*3个预测框） 4 个预测，每个预测对应一个先验框 2.2 &nbsp;SSD (Single Shot MultiBox Detector) 　　SSD 是使用 VGG19 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样）的单次检测器。我们在该网络之后添加自定义卷积层（蓝色），并使用卷积核（绿色）执行预测。 同时对类别和位置执行单次预测 　　然而，卷积层降低了空间维度和分辨率。因此上述模型仅可以检测较大的目标。为了解决该问题，我们从多个特征图上执行独立的目标检测。采用多尺度特征图独立检测。 使用多尺度特征图用于检测 　　以下是特征图图示。 　　SSD 使用卷积网络中较深的层来检测目标。如果我们按接近真实的比例重绘上图，我们会发现图像的空间分辨率已经被显著降低，且可能已无法定位在低分辨率中难以检测的小目标。如果出现了这样的问题，我们需要增加输入图像的分辨率。 2.3 &nbsp;YOLO系列 &nbsp; &nbsp; &nbsp; YOLO &nbsp;在卷积层之后使用了 DarkNet 来做特征检测。 　　然而，它并没有使用多尺度特征图来做独立的检测。相反，它将特征图部分平滑化，并将其和另一个较低分辨率的特征图拼接。例如，YOLO 将一个 28 × 28 × 512 的层重塑为 14 × 14 × 2048，然后将它和 14 × 14 ×1024 的特征图拼接。之后，YOLO 在新的 14 × 14 × 3072 层上应用卷积核进行预测。 &nbsp; &nbsp; YOLOv2（YOLO9000）做出了很多实现上的改进，&nbsp;可以检测 9000 种不同类别的目标。 &nbsp; &nbsp; YOLOv3&nbsp;使用了更加复杂的骨干网络来提取特征。DarkNet-53 主要由 3 × 3 和 1× 1 的卷积核以及类似 ResNet 中的跳过连接构成。相比 ResNet-152，DarkNet 有更低的 BFLOP（十亿次浮点数运算），但能以 2 倍的速度得到相同的分类准确率。YOLOv3 还添加了特征金字塔，以更好地检测小目标。 &nbsp; &nbsp; FPN 提供了一条自上而下的路径，从语义丰富的层构建高分辨率的层。 自上而下重建空间分辨率 　　虽然该重建层的语义较强，但在经过所有的上采样和下采样之后，目标的位置不精确。在重建层和相应的特征图之间添加横向连接可以使位置侦测更加准确。 增加跳过连接 &nbsp; FPN 结合 Fast R-CNN 或 Faster R-CNN 　　在 FPN 中，我们生成了一个特征图的金字塔。用 RPN（详见上文）来生成 ROI。基于 ROI 的大小，我们选择最合适尺寸的特征图层来提取特征块。" />
<meta property="og:description" content="https://www.cnblogs.com/cloud-ken/p/9470992.html https://www.cnblogs.com/cloud-ken/tag/DeeplearningAI/default.html?page=3 https://www.cnblogs.com/guoyaohua/p/8994246.html https://blog.csdn.net/jiongnima/article/details/79094159 （这个非常好，对比看看） （&nbsp;联合RPN和Fast R-CNN的训练。没看懂。） 一、基于候选区域的目标检测器（两阶段） &nbsp; &nbsp; 主要是RCNN系列。 1.1 &nbsp;滑动窗口检测器 &nbsp; 伪代码。我们创建很多窗口来检测不同位置的不同目标。要提升性能，一个显而易见的办法就是减少窗口数量。 for window in windows patch = get_patch(image, window) 　　results = detector(patch) 1.2 &nbsp;选择性搜索(SS) 　　我们不使用暴力方法，而是用候选区域方法（region proposal method）创建目标检测的感兴趣区域（ROI）。在选择性搜索（selective search，SS）中，我们首先将每个像素作为一组，计算每一组的纹理，并将两个最接近的组结合起来。但是为了避免单个区域吞噬其他区域，我们首先对较小的组进行分组。我们继续合并区域，直到所有区域都结合在一起。下图第一行展示了如何使区域增长，第二行中的蓝色矩形代表合并过程中所有可能的&nbsp;ROI。 1.3 &nbsp;R-CNN 通过使用更少且更高质量的 ROI，R-CNN 要比滑动窗口方法更快速、更准确。 如果我们有 2000 个候选区域，且每一个都需要独立地馈送到 CNN 中，那么对于不同的 ROI，我们需要重复提取 2000 次特征。（R-CNN很多卷积运算是重复的） ROIs = region_proposal(image) for ROI in ROIs: patch = get_patch(image, ROI) results = detector(pach) 边界框回归器(refinement) 　　候选区域方法有非常高的计算复杂度。为了加速这个过程，我们通常会使用计算量较少的候选区域选择方法构建 ROI，并在后面使用线性回归器（使用全连接层）进一步提炼边界框。 1.4 &nbsp;Fast R-CNN CNN 中的特征图以一种密集的方式表征空间特征，那么我们能直接使用特征图代替原图来检测目标吗？(能) 　Fast R-CNN 使用特征提取器（CNN）先提取整个图像的特征，而不是从头开始对每个图像块提取多次。然后，我们可以将创建候选区域的方法直接应用到提取到的特征图上。例如，Fast R-CNN 选择了 VGG16 中的卷积层 conv5 输出的 Feture Map 来生成 ROI，这些关注区域随后会结合对应的特征图以裁剪为特征图块，并用于目标检测任务中。我们使用 ROI 池化将特征图块转换为固定的大小，并馈送到全连接层进行分类和定位。因为 Fast-RCNN 不会重复提取特征，因此它能显著地减少处理时间。 将候选区域直接应用于特征图，并使用 ROI 池化将其转化为固定大小的特征图块（将 ROI（蓝色区域）与特征图重叠。） 在下面的伪代码中，计算量巨大的特征提取过程从 For 循环中移出来了，因此速度得到显著提升。Fast R-CNN 的训练速度是 R-CNN 的 10 倍，推断速度是后者的 150 倍。 feature_maps = process(image) ROIs = region_proposal(feature_maps) for ROI in ROIs: patch = roi_pooling(feature_maps, ROI) results = detector2(patch) 　　Fast R-CNN 最重要的一点就是包含特征提取器、分类器和边界框回归器在内的整个网络能通过多任务损失函数进行端到端的训练，这种多任务损失即结合了分类损失和定位损失的方法，大大提升了模型准确度。 ROI 池化（？？？？） 　　因为 Fast R-CNN 使用全连接层，所以我们应用 ROI 池化将不同大小的 ROI 转换为固定大小。 1.5 &nbsp;Faster R-CNN &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Faster R-CNN 采用与 Fast R-CNN 相同的设计，只是它用内部深层网络代替了候选区域方法。两个卷积网络，分布用于提取特征和生成ROIs（是否有目标，其位置及目标得分（不是类别得分））。新的候选区域网络（RPN）在生成 ROI 时效率更高，并且以每幅图像 10 毫秒的速度运行。 Faster R-CNN 的流程图与 Fast R-CNN 相同 feature_maps = process(image) ROIs = region_proposal(feature_maps) for ROI in ROIs patch = roi_pooling(feature_maps, ROI) class_scores, box = detector(patch) # Expensive! class_probabilities = softmax(class_scores) 候选区域网络(RPN) 　　候选区域网络（RPN）将第一个卷积网络的输出特征图作为输入。它在特征图上滑动一个 3×3 的卷积核，以使用卷积网络构建与类别无关的候选区域。如ZF 网络最后会输出 256 个值，它们将馈送到两个独立的全连接层，以预测边界框和两个 objectness 分数（边界框是否包含目标）。我们其实可以使用回归器计算单个 objectness 分数，但为简洁起见，Faster R-CNN 使用只有两个类别的分类器：即带有目标的类别和不带有目标的类别。 Faster R-CNN 不会创建随机边界框。相反，它会预测一些与左上角名为「锚点」（这是由经验固定比例的，yolo是聚类得到固定大小的）的参考框相关的偏移量（如x、y）： 首先第一个问题是为什么需要RoI Pooling？答案是在Fast R-CNN中，特征被共享卷积层一次性提取。因此，对于每个RoI而言，需要从共享卷积层上摘取对应的特征，并且送入全连接层进行分类。因此，RoI Pooling主要做了两件事，第一件是为每个RoI选取对应的特征，第二件事是为了满足全连接层的输入需求，将每个RoI对应的特征的维度转化成某个定值（对于每一个RoI，RoI Pooling Layer将其对应的特征从共享卷积层上拿出来，并转化成一样的大小(6×6)，类似对应roi坐标位置的分割？）。RoI Pooling示意图如 1.6 &nbsp;基于区域的全卷积神经网络（R-FCN）（没接触过，多看看） R-FCN 通过减少每个 ROI 所需的工作量实现加速（去掉了全连接层）。上面基于区域的特征图与 ROI 是独立的，可以在每个 ROI 之外单独计算。剩下的工作就比较简单了，因此 R-FCN 的速度比 Faster R-CNN 快。 假设我们只有一个特征图用来检测右眼。那么我们可以使用它定位人脸吗？应该可以。因为右眼应该在人脸图像的左上角，所以我们可以利用这一点定位整个人脸。如果我们还有其他用来检测左眼、鼻子或嘴巴的特征图，那么我们可以将检测结果结合起来，更好地定位人脸。 &nbsp; feature_maps = process(image) ROIs = region_proposal(feature_maps) score_maps = compute_score_map(feature_maps) for ROI in ROIs V = region_roi_pool(score_maps, ROI) class_scores, box = average(V) # Much simpler! class_probabilities = softmax(class_scores) &nbsp; &nbsp; &nbsp; （假设k=3）现在我们来看一下 5 × 5 的特征图 M，内部包含一个蓝色方块。我们将方块平均分成 3 × 3 个区域。现在，我们在 M 中创建了一个新的特征图，来检测方块的左上角（TL）。这个新的特征图如下图（右）所示。只有黄色的网格单元 [2, 2] 处于激活状态。 在左侧创建一个新的特征图，用于检测目标的左上角 　　我们将方块分成 9 个部分，由此创建了 9 个特征图，每个用来检测对应的目标区域。这些特征图叫作位置敏感得分图（position-sensitive score map），因为每个图检测目标的子区域（计算其得分）。 生成 9 个得分图 　　下图中红色虚线矩形是建议的 ROI。我们将其分割成 3 × 3 个区域，并询问每个区域包含目标对应部分的概率是多少。例如，左上角 ROI 区域包含左眼的概率。我们将结果存储成 3 × 3 vote 数组，如下图（右）所示。例如，vote_array[0][0] 包含左上角区域是否包含目标对应部分的得分。 将 ROI 应用到特征图上，输出一个 3 x 3 数组 　　将得分图(Feature Map)和 ROI 映射到 vote 数组的过程叫作位置敏感 ROI 池化（position-sensitive ROI-pool）。该过程与前面讨论过的 ROI 池化非常接近。 将 ROI 的一部分叠加到对应的得分图上，计算 V[i][j] 　　在计算出位置敏感 ROI 池化的所有值后，类别得分是其所有元素得分的平均值。 ROI 池化 　　假如我们有 C 个类别要检测。我们将其扩展为 C + 1 个类别，这样就为背景（非目标）增加了一个新的类别。每个类别有 3 × 3 个得分图，因此一共有 (C+1) × 3 × 3 个得分图。使用每个类别的得分图可以预测出该类别的类别得分。然后我们对这些得分应用 softmax 函数，计算出每个类别的概率。 二、单次目标检测器 　　单次目标检测器（SSD、YOLO、YOLOv2、YOLOv3）。1)分析FPN多尺度特征图如何提高准确率，特别是小目标的检测，其在单次检测器中的检测效果通常很差。2)分析 Focal loss 和 RetinaNet，是如何解决训练过程中的类别不平衡问题的。 &nbsp; &nbsp; &nbsp; 单次检测器通常需要在准确率和实时处理速度之间进行权衡。它们在检测太近距离或太小的目标时容易出现问题。 &nbsp; &nbsp; &nbsp; 作为替代，我们是否需要一个像RCNN系列 分离的候选区域步骤？我们可以直接在一个步骤内得到边界框和类别吗？ feature_maps = process(image) ROIs = region_proposal(feature_maps) for ROI in ROIs patch = roi_align(feature_maps, ROI) results = detector2(patch) # Reduce the amount of work here! 　　转化为： feature_maps = process(image) results = detector3(feature_maps) # No more separate step for ROIs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;让我们再看一下滑动窗口检测器。我们可以通过在特征图上滑动窗口来检测目标。对于不同的目标类型，我们使用不同的窗口类型。以前的滑动窗口方法的致命错误在于使用窗口作为最终的边界框，这就需要非常多的形状来覆盖大部分目标。更有效的方法是将窗口当做初始猜想，这样我们就得到了从当前滑动窗口同时预测类别和边界框的检测器。 基于滑动窗口进行预测 &nbsp; 在特征图每个位置，我们有 k 个anchors（锚（先验框）是固定的初始边界框猜想），一个先验框对应一个特定位置（锚点）。我们使用相同的先验框形状仔细地选择锚点和每个位置。 　　以下是一个锚点的 4 个先验框（绿色）和 4 个对应预测（蓝色），每个预测对应一个特定先验框。（YOLOv3每个尺度特征图的每个锚点3个先验框，3个尺度，应用到这里就是一个锚点有3*3个预测框） 4 个预测，每个预测对应一个先验框 2.2 &nbsp;SSD (Single Shot MultiBox Detector) 　　SSD 是使用 VGG19 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样）的单次检测器。我们在该网络之后添加自定义卷积层（蓝色），并使用卷积核（绿色）执行预测。 同时对类别和位置执行单次预测 　　然而，卷积层降低了空间维度和分辨率。因此上述模型仅可以检测较大的目标。为了解决该问题，我们从多个特征图上执行独立的目标检测。采用多尺度特征图独立检测。 使用多尺度特征图用于检测 　　以下是特征图图示。 　　SSD 使用卷积网络中较深的层来检测目标。如果我们按接近真实的比例重绘上图，我们会发现图像的空间分辨率已经被显著降低，且可能已无法定位在低分辨率中难以检测的小目标。如果出现了这样的问题，我们需要增加输入图像的分辨率。 2.3 &nbsp;YOLO系列 &nbsp; &nbsp; &nbsp; YOLO &nbsp;在卷积层之后使用了 DarkNet 来做特征检测。 　　然而，它并没有使用多尺度特征图来做独立的检测。相反，它将特征图部分平滑化，并将其和另一个较低分辨率的特征图拼接。例如，YOLO 将一个 28 × 28 × 512 的层重塑为 14 × 14 × 2048，然后将它和 14 × 14 ×1024 的特征图拼接。之后，YOLO 在新的 14 × 14 × 3072 层上应用卷积核进行预测。 &nbsp; &nbsp; YOLOv2（YOLO9000）做出了很多实现上的改进，&nbsp;可以检测 9000 种不同类别的目标。 &nbsp; &nbsp; YOLOv3&nbsp;使用了更加复杂的骨干网络来提取特征。DarkNet-53 主要由 3 × 3 和 1× 1 的卷积核以及类似 ResNet 中的跳过连接构成。相比 ResNet-152，DarkNet 有更低的 BFLOP（十亿次浮点数运算），但能以 2 倍的速度得到相同的分类准确率。YOLOv3 还添加了特征金字塔，以更好地检测小目标。 &nbsp; &nbsp; FPN 提供了一条自上而下的路径，从语义丰富的层构建高分辨率的层。 自上而下重建空间分辨率 　　虽然该重建层的语义较强，但在经过所有的上采样和下采样之后，目标的位置不精确。在重建层和相应的特征图之间添加横向连接可以使位置侦测更加准确。 增加跳过连接 &nbsp; FPN 结合 Fast R-CNN 或 Faster R-CNN 　　在 FPN 中，我们生成了一个特征图的金字塔。用 RPN（详见上文）来生成 ROI。基于 ROI 的大小，我们选择最合适尺寸的特征图层来提取特征块。" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-15T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"https://www.cnblogs.com/cloud-ken/p/9470992.html https://www.cnblogs.com/cloud-ken/tag/DeeplearningAI/default.html?page=3 https://www.cnblogs.com/guoyaohua/p/8994246.html https://blog.csdn.net/jiongnima/article/details/79094159 （这个非常好，对比看看） （&nbsp;联合RPN和Fast R-CNN的训练。没看懂。） 一、基于候选区域的目标检测器（两阶段） &nbsp; &nbsp; 主要是RCNN系列。 1.1 &nbsp;滑动窗口检测器 &nbsp; 伪代码。我们创建很多窗口来检测不同位置的不同目标。要提升性能，一个显而易见的办法就是减少窗口数量。 for window in windows patch = get_patch(image, window) 　　results = detector(patch) 1.2 &nbsp;选择性搜索(SS) 　　我们不使用暴力方法，而是用候选区域方法（region proposal method）创建目标检测的感兴趣区域（ROI）。在选择性搜索（selective search，SS）中，我们首先将每个像素作为一组，计算每一组的纹理，并将两个最接近的组结合起来。但是为了避免单个区域吞噬其他区域，我们首先对较小的组进行分组。我们继续合并区域，直到所有区域都结合在一起。下图第一行展示了如何使区域增长，第二行中的蓝色矩形代表合并过程中所有可能的&nbsp;ROI。 1.3 &nbsp;R-CNN 通过使用更少且更高质量的 ROI，R-CNN 要比滑动窗口方法更快速、更准确。 如果我们有 2000 个候选区域，且每一个都需要独立地馈送到 CNN 中，那么对于不同的 ROI，我们需要重复提取 2000 次特征。（R-CNN很多卷积运算是重复的） ROIs = region_proposal(image) for ROI in ROIs: patch = get_patch(image, ROI) results = detector(pach) 边界框回归器(refinement) 　　候选区域方法有非常高的计算复杂度。为了加速这个过程，我们通常会使用计算量较少的候选区域选择方法构建 ROI，并在后面使用线性回归器（使用全连接层）进一步提炼边界框。 1.4 &nbsp;Fast R-CNN CNN 中的特征图以一种密集的方式表征空间特征，那么我们能直接使用特征图代替原图来检测目标吗？(能) 　Fast R-CNN 使用特征提取器（CNN）先提取整个图像的特征，而不是从头开始对每个图像块提取多次。然后，我们可以将创建候选区域的方法直接应用到提取到的特征图上。例如，Fast R-CNN 选择了 VGG16 中的卷积层 conv5 输出的 Feture Map 来生成 ROI，这些关注区域随后会结合对应的特征图以裁剪为特征图块，并用于目标检测任务中。我们使用 ROI 池化将特征图块转换为固定的大小，并馈送到全连接层进行分类和定位。因为 Fast-RCNN 不会重复提取特征，因此它能显著地减少处理时间。 将候选区域直接应用于特征图，并使用 ROI 池化将其转化为固定大小的特征图块（将 ROI（蓝色区域）与特征图重叠。） 在下面的伪代码中，计算量巨大的特征提取过程从 For 循环中移出来了，因此速度得到显著提升。Fast R-CNN 的训练速度是 R-CNN 的 10 倍，推断速度是后者的 150 倍。 feature_maps = process(image) ROIs = region_proposal(feature_maps) for ROI in ROIs: patch = roi_pooling(feature_maps, ROI) results = detector2(patch) 　　Fast R-CNN 最重要的一点就是包含特征提取器、分类器和边界框回归器在内的整个网络能通过多任务损失函数进行端到端的训练，这种多任务损失即结合了分类损失和定位损失的方法，大大提升了模型准确度。 ROI 池化（？？？？） 　　因为 Fast R-CNN 使用全连接层，所以我们应用 ROI 池化将不同大小的 ROI 转换为固定大小。 1.5 &nbsp;Faster R-CNN &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Faster R-CNN 采用与 Fast R-CNN 相同的设计，只是它用内部深层网络代替了候选区域方法。两个卷积网络，分布用于提取特征和生成ROIs（是否有目标，其位置及目标得分（不是类别得分））。新的候选区域网络（RPN）在生成 ROI 时效率更高，并且以每幅图像 10 毫秒的速度运行。 Faster R-CNN 的流程图与 Fast R-CNN 相同 feature_maps = process(image) ROIs = region_proposal(feature_maps) for ROI in ROIs patch = roi_pooling(feature_maps, ROI) class_scores, box = detector(patch) # Expensive! class_probabilities = softmax(class_scores) 候选区域网络(RPN) 　　候选区域网络（RPN）将第一个卷积网络的输出特征图作为输入。它在特征图上滑动一个 3×3 的卷积核，以使用卷积网络构建与类别无关的候选区域。如ZF 网络最后会输出 256 个值，它们将馈送到两个独立的全连接层，以预测边界框和两个 objectness 分数（边界框是否包含目标）。我们其实可以使用回归器计算单个 objectness 分数，但为简洁起见，Faster R-CNN 使用只有两个类别的分类器：即带有目标的类别和不带有目标的类别。 Faster R-CNN 不会创建随机边界框。相反，它会预测一些与左上角名为「锚点」（这是由经验固定比例的，yolo是聚类得到固定大小的）的参考框相关的偏移量（如x、y）： 首先第一个问题是为什么需要RoI Pooling？答案是在Fast R-CNN中，特征被共享卷积层一次性提取。因此，对于每个RoI而言，需要从共享卷积层上摘取对应的特征，并且送入全连接层进行分类。因此，RoI Pooling主要做了两件事，第一件是为每个RoI选取对应的特征，第二件事是为了满足全连接层的输入需求，将每个RoI对应的特征的维度转化成某个定值（对于每一个RoI，RoI Pooling Layer将其对应的特征从共享卷积层上拿出来，并转化成一样的大小(6×6)，类似对应roi坐标位置的分割？）。RoI Pooling示意图如 1.6 &nbsp;基于区域的全卷积神经网络（R-FCN）（没接触过，多看看） R-FCN 通过减少每个 ROI 所需的工作量实现加速（去掉了全连接层）。上面基于区域的特征图与 ROI 是独立的，可以在每个 ROI 之外单独计算。剩下的工作就比较简单了，因此 R-FCN 的速度比 Faster R-CNN 快。 假设我们只有一个特征图用来检测右眼。那么我们可以使用它定位人脸吗？应该可以。因为右眼应该在人脸图像的左上角，所以我们可以利用这一点定位整个人脸。如果我们还有其他用来检测左眼、鼻子或嘴巴的特征图，那么我们可以将检测结果结合起来，更好地定位人脸。 &nbsp; feature_maps = process(image) ROIs = region_proposal(feature_maps) score_maps = compute_score_map(feature_maps) for ROI in ROIs V = region_roi_pool(score_maps, ROI) class_scores, box = average(V) # Much simpler! class_probabilities = softmax(class_scores) &nbsp; &nbsp; &nbsp; （假设k=3）现在我们来看一下 5 × 5 的特征图 M，内部包含一个蓝色方块。我们将方块平均分成 3 × 3 个区域。现在，我们在 M 中创建了一个新的特征图，来检测方块的左上角（TL）。这个新的特征图如下图（右）所示。只有黄色的网格单元 [2, 2] 处于激活状态。 在左侧创建一个新的特征图，用于检测目标的左上角 　　我们将方块分成 9 个部分，由此创建了 9 个特征图，每个用来检测对应的目标区域。这些特征图叫作位置敏感得分图（position-sensitive score map），因为每个图检测目标的子区域（计算其得分）。 生成 9 个得分图 　　下图中红色虚线矩形是建议的 ROI。我们将其分割成 3 × 3 个区域，并询问每个区域包含目标对应部分的概率是多少。例如，左上角 ROI 区域包含左眼的概率。我们将结果存储成 3 × 3 vote 数组，如下图（右）所示。例如，vote_array[0][0] 包含左上角区域是否包含目标对应部分的得分。 将 ROI 应用到特征图上，输出一个 3 x 3 数组 　　将得分图(Feature Map)和 ROI 映射到 vote 数组的过程叫作位置敏感 ROI 池化（position-sensitive ROI-pool）。该过程与前面讨论过的 ROI 池化非常接近。 将 ROI 的一部分叠加到对应的得分图上，计算 V[i][j] 　　在计算出位置敏感 ROI 池化的所有值后，类别得分是其所有元素得分的平均值。 ROI 池化 　　假如我们有 C 个类别要检测。我们将其扩展为 C + 1 个类别，这样就为背景（非目标）增加了一个新的类别。每个类别有 3 × 3 个得分图，因此一共有 (C+1) × 3 × 3 个得分图。使用每个类别的得分图可以预测出该类别的类别得分。然后我们对这些得分应用 softmax 函数，计算出每个类别的概率。 二、单次目标检测器 　　单次目标检测器（SSD、YOLO、YOLOv2、YOLOv3）。1)分析FPN多尺度特征图如何提高准确率，特别是小目标的检测，其在单次检测器中的检测效果通常很差。2)分析 Focal loss 和 RetinaNet，是如何解决训练过程中的类别不平衡问题的。 &nbsp; &nbsp; &nbsp; 单次检测器通常需要在准确率和实时处理速度之间进行权衡。它们在检测太近距离或太小的目标时容易出现问题。 &nbsp; &nbsp; &nbsp; 作为替代，我们是否需要一个像RCNN系列 分离的候选区域步骤？我们可以直接在一个步骤内得到边界框和类别吗？ feature_maps = process(image) ROIs = region_proposal(feature_maps) for ROI in ROIs patch = roi_align(feature_maps, ROI) results = detector2(patch) # Reduce the amount of work here! 　　转化为： feature_maps = process(image) results = detector3(feature_maps) # No more separate step for ROIs &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;让我们再看一下滑动窗口检测器。我们可以通过在特征图上滑动窗口来检测目标。对于不同的目标类型，我们使用不同的窗口类型。以前的滑动窗口方法的致命错误在于使用窗口作为最终的边界框，这就需要非常多的形状来覆盖大部分目标。更有效的方法是将窗口当做初始猜想，这样我们就得到了从当前滑动窗口同时预测类别和边界框的检测器。 基于滑动窗口进行预测 &nbsp; 在特征图每个位置，我们有 k 个anchors（锚（先验框）是固定的初始边界框猜想），一个先验框对应一个特定位置（锚点）。我们使用相同的先验框形状仔细地选择锚点和每个位置。 　　以下是一个锚点的 4 个先验框（绿色）和 4 个对应预测（蓝色），每个预测对应一个特定先验框。（YOLOv3每个尺度特征图的每个锚点3个先验框，3个尺度，应用到这里就是一个锚点有3*3个预测框） 4 个预测，每个预测对应一个先验框 2.2 &nbsp;SSD (Single Shot MultiBox Detector) 　　SSD 是使用 VGG19 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样）的单次检测器。我们在该网络之后添加自定义卷积层（蓝色），并使用卷积核（绿色）执行预测。 同时对类别和位置执行单次预测 　　然而，卷积层降低了空间维度和分辨率。因此上述模型仅可以检测较大的目标。为了解决该问题，我们从多个特征图上执行独立的目标检测。采用多尺度特征图独立检测。 使用多尺度特征图用于检测 　　以下是特征图图示。 　　SSD 使用卷积网络中较深的层来检测目标。如果我们按接近真实的比例重绘上图，我们会发现图像的空间分辨率已经被显著降低，且可能已无法定位在低分辨率中难以检测的小目标。如果出现了这样的问题，我们需要增加输入图像的分辨率。 2.3 &nbsp;YOLO系列 &nbsp; &nbsp; &nbsp; YOLO &nbsp;在卷积层之后使用了 DarkNet 来做特征检测。 　　然而，它并没有使用多尺度特征图来做独立的检测。相反，它将特征图部分平滑化，并将其和另一个较低分辨率的特征图拼接。例如，YOLO 将一个 28 × 28 × 512 的层重塑为 14 × 14 × 2048，然后将它和 14 × 14 ×1024 的特征图拼接。之后，YOLO 在新的 14 × 14 × 3072 层上应用卷积核进行预测。 &nbsp; &nbsp; YOLOv2（YOLO9000）做出了很多实现上的改进，&nbsp;可以检测 9000 种不同类别的目标。 &nbsp; &nbsp; YOLOv3&nbsp;使用了更加复杂的骨干网络来提取特征。DarkNet-53 主要由 3 × 3 和 1× 1 的卷积核以及类似 ResNet 中的跳过连接构成。相比 ResNet-152，DarkNet 有更低的 BFLOP（十亿次浮点数运算），但能以 2 倍的速度得到相同的分类准确率。YOLOv3 还添加了特征金字塔，以更好地检测小目标。 &nbsp; &nbsp; FPN 提供了一条自上而下的路径，从语义丰富的层构建高分辨率的层。 自上而下重建空间分辨率 　　虽然该重建层的语义较强，但在经过所有的上采样和下采样之后，目标的位置不精确。在重建层和相应的特征图之间添加横向连接可以使位置侦测更加准确。 增加跳过连接 &nbsp; FPN 结合 Fast R-CNN 或 Faster R-CNN 　　在 FPN 中，我们生成了一个特征图的金字塔。用 RPN（详见上文）来生成 ROI。基于 ROI 的大小，我们选择最合适尺寸的特征图层来提取特征块。","@type":"BlogPosting","url":"/2019/01/15/7c71a96563d41deecaeebc2cedd1f65d.html","headline":"基于候选区域的目标检测器（两阶段）和单次目标检测器（一阶段）","dateModified":"2019-01-15T00:00:00+08:00","datePublished":"2019-01-15T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/01/15/7c71a96563d41deecaeebc2cedd1f65d.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>基于候选区域的目标检测器（两阶段）和单次目标检测器（一阶段）</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p><a href="https://www.cnblogs.com/cloud-ken/p/9470992.html" rel="nofollow">https://www.cnblogs.com/cloud-ken/p/9470992.html</a></p> 
  <p><a href="https://www.cnblogs.com/cloud-ken/tag/DeeplearningAI/default.html?page=3" rel="nofollow">https://www.cnblogs.com/cloud-ken/tag/DeeplearningAI/default.html?page=3</a></p> 
  <p><strong><a href="https://www.cnblogs.com/guoyaohua/p/8994246.html" rel="nofollow">https://www.cnblogs.com/guoyaohua/p/8994246.html</a></strong></p> 
  <p>https://blog.csdn.net/jiongnima/article/details/79094159 （这个非常好，对比看看）<br> （&nbsp;联合RPN和Fast R-CNN的训练。没看懂。）</p> 
  <h1>一、基于候选区域的目标检测器（两阶段）</h1> 
  <p>&nbsp; &nbsp; 主要是RCNN系列。</p> 
  <h2>1.1 &nbsp;滑动窗口检测器</h2> 
  <p><img alt="" class="has" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505111849348-1143701044.png"></p> 
  <p>&nbsp;</p> 
  <p>伪代码。我们创建很多窗口来检测不同位置的不同目标。要提升性能，一个显而易见的办法就是减少窗口数量。</p> 
  <pre class="has">
<code class="language-html">for window in windows 
    patch = get_patch(image, window) 
　　results = detector(patch)</code></pre> 
  <h2>1.2 &nbsp;选择性搜索(SS)</h2> 
  <p>　　我们不使用暴力方法，而是用<strong>候选区域方法（region proposal method）创建目标检测的感兴趣区域（ROI）</strong>。在<strong>选择性搜索（selective search，SS）</strong>中，我们首先将每个像素作为一组，计算每一组的<strong>纹理</strong>，并将两个最接近的组结合起来。但是为了避免单个区域吞噬其他区域，我们首先对较小的组进行分组。我们继续合并区域，直到所有区域都结合在一起。下图第一行展示了如何使区域增长，第二行中的蓝色矩形代表合并过程中所有可能的&nbsp;<strong>ROI</strong>。</p> 
  <h2>1.3 &nbsp;R-CNN</h2> 
  <p><img alt="" class="has" height="261" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505112934129-1540508900.png" width="871"></p> 
  <p>通过使用更少且更高质量的 ROI，R-CNN 要比滑动窗口方法更快速、更准确。</p> 
  <p>如果我们有 2000 个候选区域，且每一个都需要独立地馈送到 CNN 中，那么对于不同的 ROI，我们需要重复提取 2000 次特征。（<strong>R-CNN</strong><strong>很多卷积运算是重复的</strong>）</p> 
  <pre class="has">
<code class="language-html">ROIs = region_proposal(image)
for ROI in ROIs:
    patch = get_patch(image, ROI) 
    results = detector(pach)</code></pre> 
  <ul>
   <li><strong>边界框回归器(refinement)</strong></li> 
  </ul>
  <p>　　<strong>候选区域方法</strong>有非常高的计算复杂度。为了加速这个过程，我们通常会使用计算量较少的候选区域选择方法构建 ROI，并在后面使用线性回归器（使用全连接层）进一步提炼边界框。</p> 
  <h2>1.4 &nbsp;Fast R-CNN</h2> 
  <p>CNN 中的<strong>特征图以一种密集的方式表征空间特</strong>征，那么我们能直接<strong>使用特征图代替原图来检测目标</strong>吗？(能)</p> 
  <p>　<strong>Fast R-CNN 使用特征提取器（CNN）先提取整个图像的特征，而不是从头开始对每个图像块提取多次。</strong>然后，<strong>我们可以将创建候选区域的方法直接应用到提取到的特征图上</strong>。例如，Fast R-CNN 选择了 VGG16 中的卷积层 conv5 输出的 Feture Map 来生成 ROI，这些关注区域随后会结合对应的特征图以裁剪为特征图块，并用于目标检测任务中。我们<strong>使用 ROI 池化将特征图块转换为固定的大小</strong>，并馈送到全连接层进行分类和定位。因为 Fast-RCNN 不会重复提取特征，因此它能显著地减少处理时间。</p> 
  <p style="text-align:center;"><img alt="" class="has" height="395" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190115204652762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzM3NjQ0MDg1,size_16,color_FFFFFF,t_70" width="525"></p> 
  <p style="text-align:center;"><img alt="" class="has" height="249" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdnimg.cn/20190115210551693.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzM3NjQ0MDg1,size_16,color_FFFFFF,t_70" width="864"></p> 
  <p><em>将候选区域直接应用于特征图，并使用 ROI 池化将其转化为固定大小的特征图块（</em>将 ROI（蓝色区域）与特征图重叠。<em>）</em></p> 
  <p><img alt="" class="has" height="267" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505114125219-882795149.png" width="871"></p> 
  <p>在下面的伪代码中，计算量巨大的特征提取过程从 For 循环中移出来了，因此速度得到显著提升。<strong>Fast R-CNN 的训练速度是 R-CNN 的 10 倍，推断速度是后者的 150 倍。</strong></p> 
  <pre class="has">
<code class="language-html">feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs:
    patch = roi_pooling(feature_maps, ROI) 
    results = detector2(patch)</code></pre> 
  <p>　　<strong>Fast R-CNN 最重要的一点就是包含特征提取器、分类器和边界框回归器在内的整个网络能通过多任务损失函数进行端到端的训练，这种多任务损失即结合了分类损失和定位损失的方法，大大提升了模型准确度。</strong></p> 
  <ul>
   <li><strong>ROI 池化（？？？？）</strong></li> 
  </ul>
  <p>　　因为 Fast R-CNN 使用全连接层，所以我们应用 ROI 池化将不同大小的 ROI 转换为固定大小。</p> 
  <h2>1.5 &nbsp;Faster R-CNN</h2> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Faster R-CNN 采用与 Fast R-CNN <strong>相同</strong>的设计，<strong>只是它用内部深层网络代替了候选区域方法</strong>。<strong>两个卷积网络</strong>，分布用于提取特征和生成ROIs（是否有目标，其位置及目标得分（不是类别得分））。新的<strong>候选区域网络（RPN）</strong>在生成 ROI 时效率更高，并且以每幅图像 10 毫秒的速度运行。</p> 
  <p><img alt="" class="has" height="264" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505140852035-53224161.png" width="863"></p> 
  <p><strong>Faster R-CNN 的流程图与 Fast R-CNN 相同</strong></p> 
  <p><img alt="" class="has" height="244" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505140951802-712034784.png" width="868"></p> 
  <pre class="has">
<code class="language-html">feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs
    patch = roi_pooling(feature_maps, ROI)
    class_scores, box = detector(patch)         # Expensive!
    class_probabilities = softmax(class_scores)</code></pre> 
  <ul>
   <li><strong>候选区域网络(RPN)</strong></li> 
  </ul>
  <p>　　<strong>候选区域网络（RPN）</strong>将<strong>第一个卷积网络</strong>的输出特征图作为输入。它在特征图上滑动一个 3×3 的卷积核，以使用卷积网络构建<strong>与类别无关的候选区域</strong>。如ZF 网络最后会输出 256 个值，它们将馈送到两个独立的全连接层，以预测边界框和两个 objectness 分数（边界框是否包含目标）。我们其实可以使用回归器计算单个 objectness 分数，但为简洁起见，Faster R-CNN 使用只有两个类别的分类器：即带<strong>有目标的类别</strong>和<strong>不带有目标的类别</strong>。</p> 
  <p><img alt="" class="has" height="239" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505141538370-1305739714.png" width="872"></p> 
  <p>Faster R-CNN 不会创建随机边界框。相反，它会预测一些与左上角名为<strong>「锚点」（这是由经验固定比例的，yolo是聚类得到固定大小的）</strong>的参考框相关的偏移量（如x、y）：</p> 
  <p><img alt="" class="has" height="202" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505142155131-51553556.png" width="264"><img alt="" class="has" height="199" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505142221601-1484881357.png" width="195"><img alt="" class="has" height="253" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505142409837-738297303.png" width="408"></p> 
  <p>首先第一个问题是为什么需要RoI Pooling？答案是在Fast R-CNN中，特征被共享卷积层一次性提取。因此，对于每个RoI而言，需要从共享卷积层上摘取对应的特征，并且送入全连接层进行分类。因此，RoI Pooling主要做了两件事，第一件是为每个RoI选取对应的特征，第二件事是为了满足全连接层的输入需求，将每个RoI对应的特征的维度转化成某个定值（对于每一个RoI，RoI Pooling Layer将其对应的特征从共享卷积层上拿出来，并转化成一样的大小(6×6)，<strong>类似对应roi坐标位置的分割？</strong>）。RoI Pooling示意图如</p> 
  <p><img alt="" class="has" height="265" src="https://blog.uzzz.org.cn/_p?https://img-blog.csdn.net/20180120203716021?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvamlvbmduaW1h/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="337"></p> 
  <h2>1.6 &nbsp;基于区域的全卷积神经网络（R-FCN）（没接触过，多看看）</h2> 
  <p><strong>R-FCN 通过减少每个 ROI 所需的工作量实现加速（去掉了全连接层）</strong>。上面基于区域的特征图与 ROI 是独立的，可以在每个 ROI 之外单独计算。剩下的工作就比较简单了，因此 R-FCN 的速度比 Faster R-CNN 快。</p> 
  <p>假设我们只有一个特征图用来检测右眼。那么我们可以使用它定位人脸吗？应该可以。因为右眼应该在人脸图像的左上角，所以我们可以利用这一点定位整个人脸。如果我们还有其他用来检测左眼、鼻子或嘴巴的特征图，那么我们可以将检测结果结合起来，更好地定位人脸。</p> 
  <p><img alt="" class="has" height="216" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505142731192-1064481204.png" width="458"></p> 
  <p>&nbsp;</p> 
  <pre class="has">
<code class="language-html">feature_maps = process(image)
ROIs = region_proposal(feature_maps)         
score_maps = compute_score_map(feature_maps)
for ROI in ROIs
    V = region_roi_pool(score_maps, ROI)     
    class_scores, box = average(V)                   # Much simpler!
    class_probabilities = softmax(class_scores)</code></pre> 
  <p><img alt="" class="has" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505144433895-646844741.png"></p> 
  <p>&nbsp; &nbsp; &nbsp; （假设k=3）现在我们来看一下 5 × 5 的特征图 M，内部包含一个蓝色方块。我们将方块平均分成 3 × 3 个区域。现在，我们在 M 中创建了一个新的特征图，来检测方块的左上角（TL）。这个新的特征图如下图（右）所示。只有黄色的网格单元 [2, 2] 处于激活状态。</p> 
  <p><img alt="" class="has" height="202" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505143333476-553828348.png" width="550"></p> 
  <p><em>在左侧创建一个新的特征图，用于检测目标的左上角</em></p> 
  <p>　　我们将方块分成 9 个部分，由此创建了 9 个特征图，每个用来检测对应的目标区域。这些特征图叫作<strong>位置敏感得分图（position-sensitive score map）</strong>，因为每个图检测目标的子区域（计算其得分）。</p> 
  <p><img alt="" class="has" height="455" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505143440734-1537004993.png" width="579"></p> 
  <p><em>生成 9 个得分图</em></p> 
  <p>　　下图中红色虚线矩形是建议的 ROI。我们将其分割成 3 × 3 个区域，并询问每个区域包含目标对应部分的概率是多少。例如，左上角 ROI 区域包含左眼的概率。我们将结果存储成 3 × 3 vote 数组，如下图（右）所示。例如，vote_array[0][0] 包含左上角区域是否包含目标对应部分的得分。</p> 
  <p><img alt="" class="has" height="265" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505143707803-1747244764.png" width="793"></p> 
  <p><em>将 ROI 应用到特征图上，输出一个 3 x 3 数组</em></p> 
  <p>　　将得分图(Feature Map)和 ROI 映射到 vote 数组的过程叫作<strong>位置敏感 ROI 池化（position-sensitive ROI-pool）</strong>。该过程与前面讨论过的 ROI 池化非常接近。</p> 
  <p><img alt="" class="has" height="470" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505144039681-1211806338.png" width="553"></p> 
  <p><em>将 ROI 的一部分叠加到对应的得分图上，计算 V[i][j]</em></p> 
  <p>　　在计算出位置敏感 ROI 池化的所有值后，类别得分是其所有元素得分的平均值。</p> 
  <p><img alt="" class="has" height="218" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505144213719-1278140951.png" width="525"></p> 
  <p><em>ROI 池化</em></p> 
  <p>　　假如我们有 C 个类别要检测。我们将其扩展为 C + 1 个类别，这样就为背景（非目标）增加了一个新的类别。每个类别有 3 × 3 个得分图，因此一共有 (C+1) × 3 × 3 个得分图。使用每个类别的得分图可以预测出该类别的类别得分。然后我们对这些得分应用 softmax 函数，计算出每个类别的概率。</p> 
  <h1>二、单次目标检测器</h1> 
  <p>　　单次目标检测器（SSD、YOLO、YOLOv2、YOLOv3）。1)分析FPN多尺度特征图如何提高准确率，特别是小目标的检测，其在单次检测器中的检测效果通常很差。2)分析 Focal loss 和 RetinaNet，是如何解决训练过程中的类别不平衡问题的。</p> 
  <p><strong>&nbsp; &nbsp; &nbsp; 单次检测器通常需要在准确率和实时处理速度之间进行权衡</strong>。它们<strong>在检测太近距离或太小的目标时容易出现问题</strong>。</p> 
  <p>&nbsp; &nbsp; &nbsp;<strong> 作为替代，我们是否需要一个像RCNN系列 分离的候选区域步骤？</strong>我们可以直接在一个步骤内得到边界框和类别吗？</p> 
  <pre class="has">
<code class="language-html">feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs
    patch = roi_align(feature_maps, ROI)
    results = detector2(patch)    # Reduce the amount of work here!</code></pre> 
  <p>　　转化为：</p> 
  <pre class="has">
<code class="language-html">feature_maps = process(image)
results = detector3(feature_maps) # No more separate step for ROIs</code></pre> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;让我们再看一下滑动窗口检测器。我们可以通过在特征图上滑动窗口来检测目标。对于不同的目标类型，我们使用不同的窗口类型。以前的滑动窗口方法的致命错误在于使用窗口作为最终的边界框，这就需要非常多的形状来覆盖大部分目标。更有效的方法是将窗口当做初始猜想，这样我们就得到了从当前滑动窗口同时预测类别和边界框的检测器。</p> 
  <p><img alt="" class="has" height="261" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505150849466-1873550181.png" width="504"></p> 
  <p><em>基于滑动窗口进行预测</em></p> 
  <p>&nbsp;</p> 
  <p>在<strong>特征图每个位置，</strong>我们有 k 个anchors（<strong>锚（先验框）</strong>是固定的初始边界框猜想），一个先验框对应一个特定位置（锚点）。我们使用相同的先验框形状仔细地选择锚点和每个位置。</p> 
  <p>　　以下是一个锚点的 4 个先验框（绿色）和 4 个对应预测（蓝色），每个预测对应一个特定先验框。（YOLOv3每个尺度特征图的每个锚点3个先验框，3个尺度，应用到这里就是一个锚点有3*3个预测框）</p> 
  <p><img alt="" class="has" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505151142570-1755259728.png"></p> 
  <p><em>4 个预测，每个预测对应一个</em>先验框</p> 
  <p><img alt="" class="has" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505151523154-1980699118.png"></p> 
  <h2>2.2 &nbsp;SSD (Single Shot MultiBox Detector)</h2> 
  <p>　　SSD 是使用 VGG19 网络作为特征提取器（和 Faster R-CNN 中使用的 CNN 一样）的单次检测器。我们在该网络之后添加自定义卷积层（蓝色），并使用卷积核（绿色）执行预测。</p> 
  <p><img alt="" class="has" height="77" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505151811043-167769337.png" width="859"></p> 
  <p><em>同时对类别和位置执行单次预测</em></p> 
  <p>　　然而，<strong>卷积层降低了空间维度和分辨率。因此上述模型仅可以检测较大的目标</strong>。<strong>为了解决该问题，我们从多个特征图上执行独立的目标检测。采用多尺度特征图独立检测。</strong></p> 
  <p><img alt="" class="has" height="204" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505151959649-948297901.png" width="873"></p> 
  <p><em>使用多尺度特征图用于检测</em></p> 
  <p>　　以下是特征图图示。</p> 
  <p><img alt="" class="has" height="281" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505152113936-1996087392.png" width="862"></p> 
  <p>　　SSD 使用卷积网络中较深的层来检测目标。如果我们按接近真实的比例重绘上图，我们会发现图像的空间分辨率已经被显著降低，且可能已无法定位在低分辨率中难以检测的小目标。如果出现了这样的问题，我们需要增加输入图像的分辨率。</p> 
  <p><img alt="" class="has" height="349" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505152455865-701418716.png" width="517"></p> 
  <h2>2.3 &nbsp;YOLO系列</h2> 
  <p>&nbsp; &nbsp; &nbsp; <strong>YOLO &nbsp;</strong>在卷积层之后使用了 DarkNet 来做特征检测。</p> 
  <p><img alt="" class="has" height="110" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505152800163-2054035704.png" width="857"></p> 
  <p>　　然而，它并没有使用多尺度特征图来做独立的检测。相反，它将特征图部分平滑化，并将其和另一个较低分辨率的特征图拼接。例如，YOLO 将一个 28 × 28 × 512 的层重塑为 14 × 14 × 2048，然后将它和 14 × 14 ×1024 的特征图拼接。之后，YOLO 在新的 14 × 14 × 3072 层上应用卷积核进行预测。</p> 
  <p>&nbsp; &nbsp; <strong>YOLOv2（YOLO9000</strong>）做出了很多实现上的改进，&nbsp;可以检测 9000 种不同类别的目标。</p> 
  <p><strong>&nbsp; &nbsp; YOLOv3</strong>&nbsp;使用了更加复杂的骨干网络来提取特征。DarkNet-53 主要由 3 × 3 和 1× 1 的卷积核以及类似 ResNet 中的跳过连接构成。相比 ResNet-152，DarkNet 有更低的 BFLOP（十亿次浮点数运算），但能以 2 倍的速度得到相同的分类准确率。YOLOv3 还添加了<strong>特征金字塔</strong>，以更好地检测小目标。</p> 
  <p>&nbsp; &nbsp; FPN 提供了一条自上而下的路径，从语义丰富的层构建高分辨率的层。</p> 
  <p><img alt="" class="has" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505153759647-1349206532.png"></p> 
  <p><em>自上而下重建空间分辨率</em></p> 
  <p>　　虽然该重建层的语义较强，<strong>但在经过所有的上采样和下采样之后，目标的位置不精确。</strong>在重建层和相应的特征图之间<strong>添加横向连接</strong>可以使位置侦测更加准确。</p> 
  <p><img alt="" class="has" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505153843772-922492232.png"></p> 
  <p><em>增加跳过连接</em></p> 
  <p>&nbsp;</p> 
  <h3>FPN 结合 Fast R-CNN 或 Faster R-CNN</h3> 
  <p>　　在 FPN 中，我们生成了一个特征图的金字塔。用 RPN（详见上文）来生成 ROI。基于 ROI 的大小，我们选择最合适尺寸的特征图层来提取特征块。</p> 
  <p><img alt="" class="has" height="300" src="https://images2018.cnblogs.com/blog/1192699/201805/1192699-20180505154304324-944539261.png" width="869"></p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
