<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Pytorch入门之基本操作 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Pytorch入门之基本操作" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Pytorch入门之基本操作 继TensorFlow、keras之后，开始学习新的深度学习框架——Pytorch。整理只是为了方便以后查找。 学习内容来自一个印度小哥哥写的一个在GitHub上的深度学习教程，附上学习链接。 整理人：陈振庭 qq：2621336811 Tensor basics import numpy as np import torch 产生一个张量： x = torch.Tensor(3, 4) print(&quot;Type: {}&quot;.format(x.type())) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \n{}&quot;.format(x)) 结果： Type: torch.FloatTensor Size: torch.Size([3, 4]) Values: tensor([[1.1744e-35, 0.0000e+00, 2.8026e-44, 0.0000e+00], [ nan, 0.0000e+00, 1.3733e-14, 4.7429e+30], [1.9431e-19, 4.7429e+30, 5.0938e-14, 0.0000e+ 产生一个随机张量： x = torch.randn(2, 3) # normal distribution (rand(2,3) -&gt; uniform distribution) print (x) 结果： tensor([[ 0.7434, -1.0611, -0.3752], [ 0.2613, -1.7051, 0.9118]]) 产生一个全0或者全1张量： x = torch.zeros(2, 3) print (x) x = torch.ones(2, 3) print (x) 结果： tensor([[0., 0., 0.], [0., 0., 0.]]) tensor([[1., 1., 1.], [1., 1., 1.]]) 列表转化为张量： x = torch.Tensor([[1, 2, 3],[4, 5, 6]]) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \n{}&quot;.format(x)) 结果： Size: torch.Size([2, 3]) Values: tensor([[1., 2., 3.], [4., 5., 6.]]) Numpy数组转化为张量： x = torch.from_numpy(np.random.rand(2, 3)) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \n{}&quot;.format(x)) 结果： Size: torch.Size([2, 3]) Values: tensor([[0.0372, 0.6757, 0.9554], [0.5651, 0.2336, 0.8303]], dtype=torch.float64) 转换张量的类型： x = torch.Tensor(3, 4) print(&quot;Type: {}&quot;.format(x.type())) x = x.long() print(&quot;Type: {}&quot;.format(x.type())) 结果： Type: torch.FloatTensor Type: torch.LongTensor Tensor operations 张量相加： x = torch.randn(2, 3) y = torch.randn(2, 3) z = x + y print(&quot;Size: {}&quot;.format(z.shape)) print(&quot;Values: \n{}&quot;.format(z)) 结果： Size: torch.Size([2, 3]) Values: tensor([[ 0.5650, -0.0173, 1.1263], [ 3.4274, 1.3610, -0.9262]]) 张量点乘： x = torch.randn(2, 3) y = torch.randn(3, 2) z = torch.mm(x, y) print(&quot;Size: {}&quot;.format(z.shape)) print(&quot;Values: \n{}&quot;.format(z)) 结果： Size: torch.Size([2, 2]) Values: tensor([[ 1.3294, -2.4559], [-0.4337, 4.9667]]) 张量转置： x = torch.randn(2, 3) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \n{}&quot;.format(x)) y = torch.t(x) print(&quot;Size: {}&quot;.format(y.shape)) print(&quot;Values: \n{}&quot;.format(y)) 结果： Size: torch.Size([2, 3]) Values: tensor([[ 0.0257, -0.5716, -0.9207], [-1.0590, 0.2942, -0.7114]]) Size: torch.Size([3, 2]) Values: tensor([[ 0.0257, -1.0590], [-0.5716, 0.2942], [-0.9207, -0.7114]]) 转换张量形状： z = x.view(3, 2) print(&quot;Size: {}&quot;.format(z.shape)) print(&quot;Values: \n{}&quot;.format(z)) 结果： Size: torch.Size([3, 2]) Values: tensor([[ 0.0257, -0.5716], [-0.9207, -1.0590], [ 0.2942, -0.7114]]) 改变张量的形状一定要仔细，不然可能带来意外的结果： x = torch.tensor([ [[1,1,1,1], [2,2,2,2], [3,3,3,3]], [[10,10,10,10], [20,20,20,20], [30,30,30,30]] ]) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \n{}\n&quot;.format(x)) a = x.view(x.size(1), -1) print(&quot;Size: {}&quot;.format(a.shape)) print(&quot;Values: \n{}\n&quot;.format(a)) b = x.transpose(0,1).contiguous() print(&quot;Size: {}&quot;.format(b.shape)) print(&quot;Values: \n{}\n&quot;.format(b)) c = b.view(b.size(0), -1) print(&quot;Size: {}&quot;.format(c.shape)) print(&quot;Values: \n{}&quot;.format(c)) 结果： Size: torch.Size([2, 3, 4]) Values: tensor([[[ 1, 1, 1, 1], [ 2, 2, 2, 2], [ 3, 3, 3, 3]], [[10, 10, 10, 10], [20, 20, 20, 20], [30, 30, 30, 30]]]) Size: torch.Size([3, 8]) Values: tensor([[ 1, 1, 1, 1, 2, 2, 2, 2], [ 3, 3, 3, 3, 10, 10, 10, 10], [20, 20, 20, 20, 30, 30, 30, 30]]) Size: torch.Size([3, 2, 4]) Values: tensor([[[ 1, 1, 1, 1], [10, 10, 10, 10]], [[ 2, 2, 2, 2], [20, 20, 20, 20]], [[ 3, 3, 3, 3], [30, 30, 30, 30]]]) Size: torch.Size([3, 8]) Values: tensor([[ 1, 1, 1, 1, 10, 10, 10, 10], [ 2, 2, 2, 2, 20, 20, 20, 20], [ 3, 3, 3, 3, 30, 30, 30, 30]]) 张量的维度操作，即指定某个维度的轴对张量进行操作： x = torch.randn(2, 3) print(&quot;Values: \n{}&quot;.format(x)) y = torch.sum(x, dim=0) # add each row&#39;s value for every column print(&quot;Values: \n{}&quot;.format(y)) z = torch.sum(x, dim=1) # add each columns&#39;s value for every row print(&quot;Values: \n{}&quot;.format(z)) 结果： Values: tensor([[ 0.4295, 0.2223, 0.1772], [ 2.1602, -0.8891, -0.5011]]) Values: tensor([ 2.5897, -0.6667, -0.3239]) Values: tensor([0.8290, 0.7700]) Indexing, Splicing and Joining 张量的索引： x = torch.randn(3, 4) print(&quot;x: \n{}&quot;.format(x)) print (&quot;x[:1]: \n{}&quot;.format(x[:1])) print (&quot;x[:1, 1:3]: \n{}&quot;.format(x[:1, 1:3])) 结果： x: tensor([[-1.0305, 0.0368, 1.2809, 1.2346], [-0.8837, 1.3678, -0.0971, 1.2528], [ 0.3382, -1.4948, -0.7058, 1.3378]]) x[:1]: tensor([[-1.0305, 0.0368, 1.2809, 1.2346]]) x[:1, 1:3]: tensor([[0.0368, 1.2809]]) 张量的切片：用维度选择： x = torch.randn(2, 3) print(&quot;Values: \n{}&quot;.format(x)) col_indices = torch.LongTensor([0, 2]) chosen = torch.index_select(x, dim=1, index=col_indices) # values from column 0 &amp; 2 print(&quot;Values: \n{}&quot;.format(chosen)) row_indices = torch.LongTensor([0, 1]) chosen = x[row_indices, col_indices] # values from (0, 0) &amp; (2, 1) print(&quot;Values: \n{}&quot;.format(chosen)) 结果： Values: tensor([[ 0.0720, 0.4266, -0.5351], [ 0.9672, 0.3691, -0.7332]]) Values: tensor([[ 0.0720, -0.5351], [ 0.9672, -0.7332]]) Values: tensor([ 0.0720, -0.7332]) 张量的拼接： x = torch.randn(2, 3) print(&quot;Values: \n{}&quot;.format(x)) y = torch.cat([x, x], dim=0) # stack by rows (dim=1 to stack by columns) print(&quot;Values: \n{}&quot;.format(y)) 结果： Values: tensor([[-0.8443, 0.9883, 2.2796], [-0.0482, -0.1147, -0.5290]]) Values: tensor([[-0.8443, 0.9883, 2.2796], [-0.0482, -0.1147, -0.5290], [-0.8443, 0.9883, 2.2796], [-0.0482, -0.1147, -0.5290]]) Gradients 张量求梯度： x = torch.rand(3, 4, requires_grad=True) y = 3*x + 2 z = y.mean() z.backward() # z has to be scalar print(&quot;Values: \n{}&quot;.format(x)) print(&quot;x.grad: \n&quot;, x.grad) 结果： Values: tensor([[0.7014, 0.2477, 0.5928, 0.5314], [0.2832, 0.0825, 0.5684, 0.3090], [0.1591, 0.0049, 0.0439, 0.7602]], requires_grad=True) x.grad: tensor([[0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500]]) y = 3 x + 2 y = 3 x + 2 y=3x+2 z = ∑ y / N z = \sum y / N z=∑y/N ∂ ( z ) ∂ ( x ) = ∂ ( z ) ∂ ( y ) ∂ ( y ) ∂ ( x ) = 1 N ∗ 3 = 1 12 ∗ 3 = 0.25 \frac { \partial ( z ) } { \partial ( x ) } = \frac { \partial ( z ) } { \partial ( y ) } \frac { \partial ( y ) } { \partial ( x ) } = \frac { 1 } { N } * 3 = \frac { 1 } { 12 } * 3 = 0.25 ∂(x)∂(z)​=∂(y)∂(z)​∂(x)∂(y)​=N1​∗3=121​∗3=0.25 CUDA tensors 是否使用CUDA？： print (torch.cuda.is_available()) 结果： True 产生一个全0张量： x = torch.Tensor(3, 4).to(&quot;cpu&quot;) print(&quot;Type: {}&quot;.format(x.type())) 结果： Type: torch.FloatTensor 产生一个全0张量（CUDA下）： x = torch.Tensor(3, 4).to(&quot;cuda&quot;) print(&quot;Type: {}&quot;.format(x.type())) 结果： Type: torch.cuda.FloatTensor" />
<meta property="og:description" content="Pytorch入门之基本操作 继TensorFlow、keras之后，开始学习新的深度学习框架——Pytorch。整理只是为了方便以后查找。 学习内容来自一个印度小哥哥写的一个在GitHub上的深度学习教程，附上学习链接。 整理人：陈振庭 qq：2621336811 Tensor basics import numpy as np import torch 产生一个张量： x = torch.Tensor(3, 4) print(&quot;Type: {}&quot;.format(x.type())) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \n{}&quot;.format(x)) 结果： Type: torch.FloatTensor Size: torch.Size([3, 4]) Values: tensor([[1.1744e-35, 0.0000e+00, 2.8026e-44, 0.0000e+00], [ nan, 0.0000e+00, 1.3733e-14, 4.7429e+30], [1.9431e-19, 4.7429e+30, 5.0938e-14, 0.0000e+ 产生一个随机张量： x = torch.randn(2, 3) # normal distribution (rand(2,3) -&gt; uniform distribution) print (x) 结果： tensor([[ 0.7434, -1.0611, -0.3752], [ 0.2613, -1.7051, 0.9118]]) 产生一个全0或者全1张量： x = torch.zeros(2, 3) print (x) x = torch.ones(2, 3) print (x) 结果： tensor([[0., 0., 0.], [0., 0., 0.]]) tensor([[1., 1., 1.], [1., 1., 1.]]) 列表转化为张量： x = torch.Tensor([[1, 2, 3],[4, 5, 6]]) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \n{}&quot;.format(x)) 结果： Size: torch.Size([2, 3]) Values: tensor([[1., 2., 3.], [4., 5., 6.]]) Numpy数组转化为张量： x = torch.from_numpy(np.random.rand(2, 3)) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \n{}&quot;.format(x)) 结果： Size: torch.Size([2, 3]) Values: tensor([[0.0372, 0.6757, 0.9554], [0.5651, 0.2336, 0.8303]], dtype=torch.float64) 转换张量的类型： x = torch.Tensor(3, 4) print(&quot;Type: {}&quot;.format(x.type())) x = x.long() print(&quot;Type: {}&quot;.format(x.type())) 结果： Type: torch.FloatTensor Type: torch.LongTensor Tensor operations 张量相加： x = torch.randn(2, 3) y = torch.randn(2, 3) z = x + y print(&quot;Size: {}&quot;.format(z.shape)) print(&quot;Values: \n{}&quot;.format(z)) 结果： Size: torch.Size([2, 3]) Values: tensor([[ 0.5650, -0.0173, 1.1263], [ 3.4274, 1.3610, -0.9262]]) 张量点乘： x = torch.randn(2, 3) y = torch.randn(3, 2) z = torch.mm(x, y) print(&quot;Size: {}&quot;.format(z.shape)) print(&quot;Values: \n{}&quot;.format(z)) 结果： Size: torch.Size([2, 2]) Values: tensor([[ 1.3294, -2.4559], [-0.4337, 4.9667]]) 张量转置： x = torch.randn(2, 3) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \n{}&quot;.format(x)) y = torch.t(x) print(&quot;Size: {}&quot;.format(y.shape)) print(&quot;Values: \n{}&quot;.format(y)) 结果： Size: torch.Size([2, 3]) Values: tensor([[ 0.0257, -0.5716, -0.9207], [-1.0590, 0.2942, -0.7114]]) Size: torch.Size([3, 2]) Values: tensor([[ 0.0257, -1.0590], [-0.5716, 0.2942], [-0.9207, -0.7114]]) 转换张量形状： z = x.view(3, 2) print(&quot;Size: {}&quot;.format(z.shape)) print(&quot;Values: \n{}&quot;.format(z)) 结果： Size: torch.Size([3, 2]) Values: tensor([[ 0.0257, -0.5716], [-0.9207, -1.0590], [ 0.2942, -0.7114]]) 改变张量的形状一定要仔细，不然可能带来意外的结果： x = torch.tensor([ [[1,1,1,1], [2,2,2,2], [3,3,3,3]], [[10,10,10,10], [20,20,20,20], [30,30,30,30]] ]) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \n{}\n&quot;.format(x)) a = x.view(x.size(1), -1) print(&quot;Size: {}&quot;.format(a.shape)) print(&quot;Values: \n{}\n&quot;.format(a)) b = x.transpose(0,1).contiguous() print(&quot;Size: {}&quot;.format(b.shape)) print(&quot;Values: \n{}\n&quot;.format(b)) c = b.view(b.size(0), -1) print(&quot;Size: {}&quot;.format(c.shape)) print(&quot;Values: \n{}&quot;.format(c)) 结果： Size: torch.Size([2, 3, 4]) Values: tensor([[[ 1, 1, 1, 1], [ 2, 2, 2, 2], [ 3, 3, 3, 3]], [[10, 10, 10, 10], [20, 20, 20, 20], [30, 30, 30, 30]]]) Size: torch.Size([3, 8]) Values: tensor([[ 1, 1, 1, 1, 2, 2, 2, 2], [ 3, 3, 3, 3, 10, 10, 10, 10], [20, 20, 20, 20, 30, 30, 30, 30]]) Size: torch.Size([3, 2, 4]) Values: tensor([[[ 1, 1, 1, 1], [10, 10, 10, 10]], [[ 2, 2, 2, 2], [20, 20, 20, 20]], [[ 3, 3, 3, 3], [30, 30, 30, 30]]]) Size: torch.Size([3, 8]) Values: tensor([[ 1, 1, 1, 1, 10, 10, 10, 10], [ 2, 2, 2, 2, 20, 20, 20, 20], [ 3, 3, 3, 3, 30, 30, 30, 30]]) 张量的维度操作，即指定某个维度的轴对张量进行操作： x = torch.randn(2, 3) print(&quot;Values: \n{}&quot;.format(x)) y = torch.sum(x, dim=0) # add each row&#39;s value for every column print(&quot;Values: \n{}&quot;.format(y)) z = torch.sum(x, dim=1) # add each columns&#39;s value for every row print(&quot;Values: \n{}&quot;.format(z)) 结果： Values: tensor([[ 0.4295, 0.2223, 0.1772], [ 2.1602, -0.8891, -0.5011]]) Values: tensor([ 2.5897, -0.6667, -0.3239]) Values: tensor([0.8290, 0.7700]) Indexing, Splicing and Joining 张量的索引： x = torch.randn(3, 4) print(&quot;x: \n{}&quot;.format(x)) print (&quot;x[:1]: \n{}&quot;.format(x[:1])) print (&quot;x[:1, 1:3]: \n{}&quot;.format(x[:1, 1:3])) 结果： x: tensor([[-1.0305, 0.0368, 1.2809, 1.2346], [-0.8837, 1.3678, -0.0971, 1.2528], [ 0.3382, -1.4948, -0.7058, 1.3378]]) x[:1]: tensor([[-1.0305, 0.0368, 1.2809, 1.2346]]) x[:1, 1:3]: tensor([[0.0368, 1.2809]]) 张量的切片：用维度选择： x = torch.randn(2, 3) print(&quot;Values: \n{}&quot;.format(x)) col_indices = torch.LongTensor([0, 2]) chosen = torch.index_select(x, dim=1, index=col_indices) # values from column 0 &amp; 2 print(&quot;Values: \n{}&quot;.format(chosen)) row_indices = torch.LongTensor([0, 1]) chosen = x[row_indices, col_indices] # values from (0, 0) &amp; (2, 1) print(&quot;Values: \n{}&quot;.format(chosen)) 结果： Values: tensor([[ 0.0720, 0.4266, -0.5351], [ 0.9672, 0.3691, -0.7332]]) Values: tensor([[ 0.0720, -0.5351], [ 0.9672, -0.7332]]) Values: tensor([ 0.0720, -0.7332]) 张量的拼接： x = torch.randn(2, 3) print(&quot;Values: \n{}&quot;.format(x)) y = torch.cat([x, x], dim=0) # stack by rows (dim=1 to stack by columns) print(&quot;Values: \n{}&quot;.format(y)) 结果： Values: tensor([[-0.8443, 0.9883, 2.2796], [-0.0482, -0.1147, -0.5290]]) Values: tensor([[-0.8443, 0.9883, 2.2796], [-0.0482, -0.1147, -0.5290], [-0.8443, 0.9883, 2.2796], [-0.0482, -0.1147, -0.5290]]) Gradients 张量求梯度： x = torch.rand(3, 4, requires_grad=True) y = 3*x + 2 z = y.mean() z.backward() # z has to be scalar print(&quot;Values: \n{}&quot;.format(x)) print(&quot;x.grad: \n&quot;, x.grad) 结果： Values: tensor([[0.7014, 0.2477, 0.5928, 0.5314], [0.2832, 0.0825, 0.5684, 0.3090], [0.1591, 0.0049, 0.0439, 0.7602]], requires_grad=True) x.grad: tensor([[0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500]]) y = 3 x + 2 y = 3 x + 2 y=3x+2 z = ∑ y / N z = \sum y / N z=∑y/N ∂ ( z ) ∂ ( x ) = ∂ ( z ) ∂ ( y ) ∂ ( y ) ∂ ( x ) = 1 N ∗ 3 = 1 12 ∗ 3 = 0.25 \frac { \partial ( z ) } { \partial ( x ) } = \frac { \partial ( z ) } { \partial ( y ) } \frac { \partial ( y ) } { \partial ( x ) } = \frac { 1 } { N } * 3 = \frac { 1 } { 12 } * 3 = 0.25 ∂(x)∂(z)​=∂(y)∂(z)​∂(x)∂(y)​=N1​∗3=121​∗3=0.25 CUDA tensors 是否使用CUDA？： print (torch.cuda.is_available()) 结果： True 产生一个全0张量： x = torch.Tensor(3, 4).to(&quot;cpu&quot;) print(&quot;Type: {}&quot;.format(x.type())) 结果： Type: torch.FloatTensor 产生一个全0张量（CUDA下）： x = torch.Tensor(3, 4).to(&quot;cuda&quot;) print(&quot;Type: {}&quot;.format(x.type())) 结果： Type: torch.cuda.FloatTensor" />
<link rel="canonical" href="https://mlh.app/2019/01/15/d1c9298e788e49ada236b47cb0b9375d.html" />
<meta property="og:url" content="https://mlh.app/2019/01/15/d1c9298e788e49ada236b47cb0b9375d.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-15T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Pytorch入门之基本操作 继TensorFlow、keras之后，开始学习新的深度学习框架——Pytorch。整理只是为了方便以后查找。 学习内容来自一个印度小哥哥写的一个在GitHub上的深度学习教程，附上学习链接。 整理人：陈振庭 qq：2621336811 Tensor basics import numpy as np import torch 产生一个张量： x = torch.Tensor(3, 4) print(&quot;Type: {}&quot;.format(x.type())) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \\n{}&quot;.format(x)) 结果： Type: torch.FloatTensor Size: torch.Size([3, 4]) Values: tensor([[1.1744e-35, 0.0000e+00, 2.8026e-44, 0.0000e+00], [ nan, 0.0000e+00, 1.3733e-14, 4.7429e+30], [1.9431e-19, 4.7429e+30, 5.0938e-14, 0.0000e+ 产生一个随机张量： x = torch.randn(2, 3) # normal distribution (rand(2,3) -&gt; uniform distribution) print (x) 结果： tensor([[ 0.7434, -1.0611, -0.3752], [ 0.2613, -1.7051, 0.9118]]) 产生一个全0或者全1张量： x = torch.zeros(2, 3) print (x) x = torch.ones(2, 3) print (x) 结果： tensor([[0., 0., 0.], [0., 0., 0.]]) tensor([[1., 1., 1.], [1., 1., 1.]]) 列表转化为张量： x = torch.Tensor([[1, 2, 3],[4, 5, 6]]) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \\n{}&quot;.format(x)) 结果： Size: torch.Size([2, 3]) Values: tensor([[1., 2., 3.], [4., 5., 6.]]) Numpy数组转化为张量： x = torch.from_numpy(np.random.rand(2, 3)) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \\n{}&quot;.format(x)) 结果： Size: torch.Size([2, 3]) Values: tensor([[0.0372, 0.6757, 0.9554], [0.5651, 0.2336, 0.8303]], dtype=torch.float64) 转换张量的类型： x = torch.Tensor(3, 4) print(&quot;Type: {}&quot;.format(x.type())) x = x.long() print(&quot;Type: {}&quot;.format(x.type())) 结果： Type: torch.FloatTensor Type: torch.LongTensor Tensor operations 张量相加： x = torch.randn(2, 3) y = torch.randn(2, 3) z = x + y print(&quot;Size: {}&quot;.format(z.shape)) print(&quot;Values: \\n{}&quot;.format(z)) 结果： Size: torch.Size([2, 3]) Values: tensor([[ 0.5650, -0.0173, 1.1263], [ 3.4274, 1.3610, -0.9262]]) 张量点乘： x = torch.randn(2, 3) y = torch.randn(3, 2) z = torch.mm(x, y) print(&quot;Size: {}&quot;.format(z.shape)) print(&quot;Values: \\n{}&quot;.format(z)) 结果： Size: torch.Size([2, 2]) Values: tensor([[ 1.3294, -2.4559], [-0.4337, 4.9667]]) 张量转置： x = torch.randn(2, 3) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \\n{}&quot;.format(x)) y = torch.t(x) print(&quot;Size: {}&quot;.format(y.shape)) print(&quot;Values: \\n{}&quot;.format(y)) 结果： Size: torch.Size([2, 3]) Values: tensor([[ 0.0257, -0.5716, -0.9207], [-1.0590, 0.2942, -0.7114]]) Size: torch.Size([3, 2]) Values: tensor([[ 0.0257, -1.0590], [-0.5716, 0.2942], [-0.9207, -0.7114]]) 转换张量形状： z = x.view(3, 2) print(&quot;Size: {}&quot;.format(z.shape)) print(&quot;Values: \\n{}&quot;.format(z)) 结果： Size: torch.Size([3, 2]) Values: tensor([[ 0.0257, -0.5716], [-0.9207, -1.0590], [ 0.2942, -0.7114]]) 改变张量的形状一定要仔细，不然可能带来意外的结果： x = torch.tensor([ [[1,1,1,1], [2,2,2,2], [3,3,3,3]], [[10,10,10,10], [20,20,20,20], [30,30,30,30]] ]) print(&quot;Size: {}&quot;.format(x.shape)) print(&quot;Values: \\n{}\\n&quot;.format(x)) a = x.view(x.size(1), -1) print(&quot;Size: {}&quot;.format(a.shape)) print(&quot;Values: \\n{}\\n&quot;.format(a)) b = x.transpose(0,1).contiguous() print(&quot;Size: {}&quot;.format(b.shape)) print(&quot;Values: \\n{}\\n&quot;.format(b)) c = b.view(b.size(0), -1) print(&quot;Size: {}&quot;.format(c.shape)) print(&quot;Values: \\n{}&quot;.format(c)) 结果： Size: torch.Size([2, 3, 4]) Values: tensor([[[ 1, 1, 1, 1], [ 2, 2, 2, 2], [ 3, 3, 3, 3]], [[10, 10, 10, 10], [20, 20, 20, 20], [30, 30, 30, 30]]]) Size: torch.Size([3, 8]) Values: tensor([[ 1, 1, 1, 1, 2, 2, 2, 2], [ 3, 3, 3, 3, 10, 10, 10, 10], [20, 20, 20, 20, 30, 30, 30, 30]]) Size: torch.Size([3, 2, 4]) Values: tensor([[[ 1, 1, 1, 1], [10, 10, 10, 10]], [[ 2, 2, 2, 2], [20, 20, 20, 20]], [[ 3, 3, 3, 3], [30, 30, 30, 30]]]) Size: torch.Size([3, 8]) Values: tensor([[ 1, 1, 1, 1, 10, 10, 10, 10], [ 2, 2, 2, 2, 20, 20, 20, 20], [ 3, 3, 3, 3, 30, 30, 30, 30]]) 张量的维度操作，即指定某个维度的轴对张量进行操作： x = torch.randn(2, 3) print(&quot;Values: \\n{}&quot;.format(x)) y = torch.sum(x, dim=0) # add each row&#39;s value for every column print(&quot;Values: \\n{}&quot;.format(y)) z = torch.sum(x, dim=1) # add each columns&#39;s value for every row print(&quot;Values: \\n{}&quot;.format(z)) 结果： Values: tensor([[ 0.4295, 0.2223, 0.1772], [ 2.1602, -0.8891, -0.5011]]) Values: tensor([ 2.5897, -0.6667, -0.3239]) Values: tensor([0.8290, 0.7700]) Indexing, Splicing and Joining 张量的索引： x = torch.randn(3, 4) print(&quot;x: \\n{}&quot;.format(x)) print (&quot;x[:1]: \\n{}&quot;.format(x[:1])) print (&quot;x[:1, 1:3]: \\n{}&quot;.format(x[:1, 1:3])) 结果： x: tensor([[-1.0305, 0.0368, 1.2809, 1.2346], [-0.8837, 1.3678, -0.0971, 1.2528], [ 0.3382, -1.4948, -0.7058, 1.3378]]) x[:1]: tensor([[-1.0305, 0.0368, 1.2809, 1.2346]]) x[:1, 1:3]: tensor([[0.0368, 1.2809]]) 张量的切片：用维度选择： x = torch.randn(2, 3) print(&quot;Values: \\n{}&quot;.format(x)) col_indices = torch.LongTensor([0, 2]) chosen = torch.index_select(x, dim=1, index=col_indices) # values from column 0 &amp; 2 print(&quot;Values: \\n{}&quot;.format(chosen)) row_indices = torch.LongTensor([0, 1]) chosen = x[row_indices, col_indices] # values from (0, 0) &amp; (2, 1) print(&quot;Values: \\n{}&quot;.format(chosen)) 结果： Values: tensor([[ 0.0720, 0.4266, -0.5351], [ 0.9672, 0.3691, -0.7332]]) Values: tensor([[ 0.0720, -0.5351], [ 0.9672, -0.7332]]) Values: tensor([ 0.0720, -0.7332]) 张量的拼接： x = torch.randn(2, 3) print(&quot;Values: \\n{}&quot;.format(x)) y = torch.cat([x, x], dim=0) # stack by rows (dim=1 to stack by columns) print(&quot;Values: \\n{}&quot;.format(y)) 结果： Values: tensor([[-0.8443, 0.9883, 2.2796], [-0.0482, -0.1147, -0.5290]]) Values: tensor([[-0.8443, 0.9883, 2.2796], [-0.0482, -0.1147, -0.5290], [-0.8443, 0.9883, 2.2796], [-0.0482, -0.1147, -0.5290]]) Gradients 张量求梯度： x = torch.rand(3, 4, requires_grad=True) y = 3*x + 2 z = y.mean() z.backward() # z has to be scalar print(&quot;Values: \\n{}&quot;.format(x)) print(&quot;x.grad: \\n&quot;, x.grad) 结果： Values: tensor([[0.7014, 0.2477, 0.5928, 0.5314], [0.2832, 0.0825, 0.5684, 0.3090], [0.1591, 0.0049, 0.0439, 0.7602]], requires_grad=True) x.grad: tensor([[0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500], [0.2500, 0.2500, 0.2500, 0.2500]]) y = 3 x + 2 y = 3 x + 2 y=3x+2 z = ∑ y / N z = \\sum y / N z=∑y/N ∂ ( z ) ∂ ( x ) = ∂ ( z ) ∂ ( y ) ∂ ( y ) ∂ ( x ) = 1 N ∗ 3 = 1 12 ∗ 3 = 0.25 \\frac { \\partial ( z ) } { \\partial ( x ) } = \\frac { \\partial ( z ) } { \\partial ( y ) } \\frac { \\partial ( y ) } { \\partial ( x ) } = \\frac { 1 } { N } * 3 = \\frac { 1 } { 12 } * 3 = 0.25 ∂(x)∂(z)​=∂(y)∂(z)​∂(x)∂(y)​=N1​∗3=121​∗3=0.25 CUDA tensors 是否使用CUDA？： print (torch.cuda.is_available()) 结果： True 产生一个全0张量： x = torch.Tensor(3, 4).to(&quot;cpu&quot;) print(&quot;Type: {}&quot;.format(x.type())) 结果： Type: torch.FloatTensor 产生一个全0张量（CUDA下）： x = torch.Tensor(3, 4).to(&quot;cuda&quot;) print(&quot;Type: {}&quot;.format(x.type())) 结果： Type: torch.cuda.FloatTensor","@type":"BlogPosting","url":"https://mlh.app/2019/01/15/d1c9298e788e49ada236b47cb0b9375d.html","headline":"Pytorch入门之基本操作","dateModified":"2019-01-15T00:00:00+08:00","datePublished":"2019-01-15T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/01/15/d1c9298e788e49ada236b47cb0b9375d.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Pytorch入门之基本操作</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <h2><a id="Pytorch_0"></a>Pytorch入门之基本操作</h2> 
  <p>继TensorFlow、keras之后，开始学习新的深度学习框架——Pytorch。整理只是为了方便以后查找。<br> 学习内容来自一个印度小哥哥写的一个在GitHub上的深度学习教程，附上<a href="https://colab.research.google.com/github/GokuMohandas/practicalAI/blob/master/notebooks/07_PyTorch.ipynb#scrollTo=d-68-J6IYeMy" rel="nofollow">学习链接</a>。</p> 
  <p><strong>整理人：陈振庭<br> qq：2621336811</strong></p> 
  <h1><a id="Tensor_basics_7"></a>Tensor basics</h1> 
  <pre><code>import numpy as np
import torch
</code></pre> 
  <p>产生一个张量：</p> 
  <pre><code>x = torch.Tensor(3, 4)
print("Type: {}".format(x.type()))
print("Size: {}".format(x.shape))
print("Values: \n{}".format(x))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Type: torch.FloatTensor
Size: torch.Size([3, 4])
Values: 
tensor([[1.1744e-35, 0.0000e+00, 2.8026e-44, 0.0000e+00],
        [       nan, 0.0000e+00, 1.3733e-14, 4.7429e+30],
        [1.9431e-19, 4.7429e+30, 5.0938e-14, 0.0000e+
</code></pre> 
  <p>产生一个随机张量：</p> 
  <pre><code>x = torch.randn(2, 3) # normal distribution (rand(2,3) -&gt; uniform distribution)
print (x)
</code></pre> 
  <p>结果：</p> 
  <pre><code>tensor([[ 0.7434, -1.0611, -0.3752],
        [ 0.2613, -1.7051,  0.9118]])
</code></pre> 
  <p>产生一个全0或者全1张量：</p> 
  <pre><code>x = torch.zeros(2, 3)
print (x)
x = torch.ones(2, 3)
print (x)
</code></pre> 
  <p>结果：</p> 
  <pre><code>tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
</code></pre> 
  <p>列表转化为张量：</p> 
  <pre><code>x = torch.Tensor([[1, 2, 3],[4, 5, 6]])
print("Size: {}".format(x.shape)) 
print("Values: \n{}".format(x))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Size: torch.Size([2, 3])
Values: 
tensor([[1., 2., 3.],
        [4., 5., 6.]])
</code></pre> 
  <p>Numpy数组转化为张量：</p> 
  <pre><code>x = torch.from_numpy(np.random.rand(2, 3))
print("Size: {}".format(x.shape)) 
print("Values: \n{}".format(x))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Size: torch.Size([2, 3])
Values: 
tensor([[0.0372, 0.6757, 0.9554],
        [0.5651, 0.2336, 0.8303]], dtype=torch.float64)
</code></pre> 
  <p>转换张量的类型：</p> 
  <pre><code>x = torch.Tensor(3, 4)
print("Type: {}".format(x.type()))
x = x.long()
print("Type: {}".format(x.type()))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Type: torch.FloatTensor
Type: torch.LongTensor
</code></pre> 
  <h1><a id="Tensor_operations_97"></a>Tensor operations</h1> 
  <p>张量相加：</p> 
  <pre><code>x = torch.randn(2, 3)
y = torch.randn(2, 3)
z = x + y
print("Size: {}".format(z.shape)) 
print("Values: \n{}".format(z))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Size: torch.Size([2, 3])
Values: 
tensor([[ 0.5650, -0.0173,  1.1263],
        [ 3.4274,  1.3610, -0.9262]])
</code></pre> 
  <p>张量点乘：</p> 
  <pre><code>x = torch.randn(2, 3)
y = torch.randn(3, 2)
z = torch.mm(x, y)
print("Size: {}".format(z.shape)) 
print("Values: \n{}".format(z))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Size: torch.Size([2, 2])
Values: 
tensor([[ 1.3294, -2.4559],
        [-0.4337,  4.9667]])
</code></pre> 
  <p>张量转置：</p> 
  <pre><code>x = torch.randn(2, 3)
print("Size: {}".format(x.shape)) 
print("Values: \n{}".format(x))
y = torch.t(x)
print("Size: {}".format(y.shape)) 
print("Values: \n{}".format(y))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Size: torch.Size([2, 3])
Values: 
tensor([[ 0.0257, -0.5716, -0.9207],
        [-1.0590,  0.2942, -0.7114]])
Size: torch.Size([3, 2])
Values: 
tensor([[ 0.0257, -1.0590],
        [-0.5716,  0.2942],
        [-0.9207, -0.7114]])
</code></pre> 
  <p>转换张量形状：</p> 
  <pre><code>z = x.view(3, 2)
print("Size: {}".format(z.shape)) 
print("Values: \n{}".format(z))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Size: torch.Size([3, 2])
Values: 
tensor([[ 0.0257, -0.5716],
        [-0.9207, -1.0590],
        [ 0.2942, -0.7114]])
</code></pre> 
  <p>改变张量的形状一定要仔细，不然可能带来意外的结果：</p> 
  <pre><code>x = torch.tensor([
    [[1,1,1,1], [2,2,2,2], [3,3,3,3]],
    [[10,10,10,10], [20,20,20,20], [30,30,30,30]]
])
print("Size: {}".format(x.shape)) 
print("Values: \n{}\n".format(x))
a = x.view(x.size(1), -1)
print("Size: {}".format(a.shape)) 
print("Values: \n{}\n".format(a))
b = x.transpose(0,1).contiguous()
print("Size: {}".format(b.shape)) 
print("Values: \n{}\n".format(b))
c = b.view(b.size(0), -1)
print("Size: {}".format(c.shape)) 
print("Values: \n{}".format(c))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Size: torch.Size([2, 3, 4])
Values: 
tensor([[[ 1,  1,  1,  1],
         [ 2,  2,  2,  2],
         [ 3,  3,  3,  3]],

        [[10, 10, 10, 10],
         [20, 20, 20, 20],
         [30, 30, 30, 30]]])

Size: torch.Size([3, 8])
Values: 
tensor([[ 1,  1,  1,  1,  2,  2,  2,  2],
        [ 3,  3,  3,  3, 10, 10, 10, 10],
        [20, 20, 20, 20, 30, 30, 30, 30]])

Size: torch.Size([3, 2, 4])
Values: 
tensor([[[ 1,  1,  1,  1],
         [10, 10, 10, 10]],

        [[ 2,  2,  2,  2],
         [20, 20, 20, 20]],

        [[ 3,  3,  3,  3],
         [30, 30, 30, 30]]])

Size: torch.Size([3, 8])
Values: 
tensor([[ 1,  1,  1,  1, 10, 10, 10, 10],
        [ 2,  2,  2,  2, 20, 20, 20, 20],
        [ 3,  3,  3,  3, 30, 30, 30, 30]])
</code></pre> 
  <p>张量的维度操作，即指定某个维度的轴对张量进行操作：</p> 
  <pre><code>x = torch.randn(2, 3)
print("Values: \n{}".format(x))
y = torch.sum(x, dim=0) # add each row's value for every column
print("Values: \n{}".format(y))
z = torch.sum(x, dim=1) # add each columns's value for every row
print("Values: \n{}".format(z))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Values: 
tensor([[ 0.4295,  0.2223,  0.1772],
        [ 2.1602, -0.8891, -0.5011]])
Values: 
tensor([ 2.5897, -0.6667, -0.3239])
Values: 
tensor([0.8290, 0.7700])
</code></pre> 
  <h1><a id="Indexing_Splicing_and_Joining_245"></a>Indexing, Splicing and Joining</h1> 
  <p>张量的索引：</p> 
  <pre><code>x = torch.randn(3, 4)
print("x: \n{}".format(x))
print ("x[:1]: \n{}".format(x[:1]))
print ("x[:1, 1:3]: \n{}".format(x[:1, 1:3]))
</code></pre> 
  <p>结果：</p> 
  <pre><code>x: 
tensor([[-1.0305,  0.0368,  1.2809,  1.2346],
        [-0.8837,  1.3678, -0.0971,  1.2528],
        [ 0.3382, -1.4948, -0.7058,  1.3378]])
x[:1]: 
tensor([[-1.0305,  0.0368,  1.2809,  1.2346]])
x[:1, 1:3]: 
tensor([[0.0368, 1.2809]])
</code></pre> 
  <p>张量的切片：用维度选择：</p> 
  <pre><code>x = torch.randn(2, 3)
print("Values: \n{}".format(x))
col_indices = torch.LongTensor([0, 2])
chosen = torch.index_select(x, dim=1, index=col_indices) # values from column 0 &amp; 2
print("Values: \n{}".format(chosen)) 
row_indices = torch.LongTensor([0, 1])
chosen = x[row_indices, col_indices] # values from (0, 0) &amp; (2, 1)
print("Values: \n{}".format(chosen)) 
</code></pre> 
  <p>结果：</p> 
  <pre><code>Values: 
tensor([[ 0.0720,  0.4266, -0.5351],
        [ 0.9672,  0.3691, -0.7332]])
Values: 
tensor([[ 0.0720, -0.5351],
        [ 0.9672, -0.7332]])
Values: 
tensor([ 0.0720, -0.7332])
</code></pre> 
  <p>张量的拼接：</p> 
  <pre><code>x = torch.randn(2, 3)
print("Values: \n{}".format(x))
y = torch.cat([x, x], dim=0) # stack by rows (dim=1 to stack by columns)
print("Values: \n{}".format(y))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Values: 
tensor([[-0.8443,  0.9883,  2.2796],
        [-0.0482, -0.1147, -0.5290]])
Values: 
tensor([[-0.8443,  0.9883,  2.2796],
        [-0.0482, -0.1147, -0.5290],
        [-0.8443,  0.9883,  2.2796],
        [-0.0482, -0.1147, -0.5290]])
</code></pre> 
  <h1><a id="Gradients_309"></a>Gradients</h1> 
  <p>张量求梯度：</p> 
  <pre><code>x = torch.rand(3, 4, requires_grad=True)
y = 3*x + 2
z = y.mean()
z.backward() # z has to be scalar
print("Values: \n{}".format(x))
print("x.grad: \n", x.grad)
</code></pre> 
  <p>结果：</p> 
  <pre><code>Values: 
tensor([[0.7014, 0.2477, 0.5928, 0.5314],
        [0.2832, 0.0825, 0.5684, 0.3090],
        [0.1591, 0.0049, 0.0439, 0.7602]], requires_grad=True)
x.grad: 
 tensor([[0.2500, 0.2500, 0.2500, 0.2500],
        [0.2500, 0.2500, 0.2500, 0.2500],
        [0.2500, 0.2500, 0.2500, 0.2500]])
</code></pre> 
  <ul> 
   <li><span class="katex--inline"><span class="katex"><span class="katex-mathml">
       <math>
        <semantics>
         <mrow>
          <mi>
           y
          </mi>
          <mo>
           =
          </mo>
          <mn>
           3
          </mn>
          <mi>
           x
          </mi>
          <mo>
           +
          </mo>
          <mn>
           2
          </mn>
         </mrow>
         <annotation encoding="application/x-tex">
          y = 3 x + 2
         </annotation>
        </semantics>
       </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.19444em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mord mathit">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">2</span></span></span></span></span></li> 
   <li><span class="katex--inline"><span class="katex"><span class="katex-mathml">
       <math>
        <semantics>
         <mrow>
          <mi>
           z
          </mi>
          <mo>
           =
          </mo>
          <mo>
           ∑
          </mo>
          <mi>
           y
          </mi>
          <mi mathvariant="normal">
           /
          </mi>
          <mi>
           N
          </mi>
         </mrow>
         <annotation encoding="application/x-tex">
          z = \sum y / N
         </annotation>
        </semantics>
       </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathit" style="margin-right: 0.04398em;">z</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.00001em; vertical-align: -0.25001em;"></span><span class="mop op-symbol small-op" style="position: relative; top: -5e-06em;">∑</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathit" style="margin-right: 0.03588em;">y</span><span class="mord">/</span><span class="mord mathit" style="margin-right: 0.10903em;">N</span></span></span></span></span></li> 
   <li><span class="katex--inline"><span class="katex"><span class="katex-mathml">
       <math>
        <semantics>
         <mrow>
          <mfrac>
           <mrow>
            <mi mathvariant="normal">
             ∂
            </mi>
            <mo>
             (
            </mo>
            <mi>
             z
            </mi>
            <mo>
             )
            </mo>
           </mrow>
           <mrow>
            <mi mathvariant="normal">
             ∂
            </mi>
            <mo>
             (
            </mo>
            <mi>
             x
            </mi>
            <mo>
             )
            </mo>
           </mrow>
          </mfrac>
          <mo>
           =
          </mo>
          <mfrac>
           <mrow>
            <mi mathvariant="normal">
             ∂
            </mi>
            <mo>
             (
            </mo>
            <mi>
             z
            </mi>
            <mo>
             )
            </mo>
           </mrow>
           <mrow>
            <mi mathvariant="normal">
             ∂
            </mi>
            <mo>
             (
            </mo>
            <mi>
             y
            </mi>
            <mo>
             )
            </mo>
           </mrow>
          </mfrac>
          <mfrac>
           <mrow>
            <mi mathvariant="normal">
             ∂
            </mi>
            <mo>
             (
            </mo>
            <mi>
             y
            </mi>
            <mo>
             )
            </mo>
           </mrow>
           <mrow>
            <mi mathvariant="normal">
             ∂
            </mi>
            <mo>
             (
            </mo>
            <mi>
             x
            </mi>
            <mo>
             )
            </mo>
           </mrow>
          </mfrac>
          <mo>
           =
          </mo>
          <mfrac>
           <mn>
            1
           </mn>
           <mi>
            N
           </mi>
          </mfrac>
          <mo>
           ∗
          </mo>
          <mn>
           3
          </mn>
          <mo>
           =
          </mo>
          <mfrac>
           <mn>
            1
           </mn>
           <mn>
            12
           </mn>
          </mfrac>
          <mo>
           ∗
          </mo>
          <mn>
           3
          </mn>
          <mo>
           =
          </mo>
          <mn>
           0.25
          </mn>
         </mrow>
         <annotation encoding="application/x-tex">
          \frac { \partial ( z ) } { \partial ( x ) } = \frac { \partial ( z ) } { \partial ( y ) } \frac { \partial ( y ) } { \partial ( x ) } = \frac { 1 } { N } * 3 = \frac { 1 } { 12 } * 3 = 0.25
         </annotation>
        </semantics>
       </math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1.53em; vertical-align: -0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.01em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em;">∂</span><span class="mopen mtight">(</span><span class="mord mathit mtight">x</span><span class="mclose mtight">)</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.485em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em;">∂</span><span class="mopen mtight">(</span><span class="mord mathit mtight" style="margin-right: 0.04398em;">z</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.52em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.53em; vertical-align: -0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.01em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em;">∂</span><span class="mopen mtight">(</span><span class="mord mathit mtight" style="margin-right: 0.03588em;">y</span><span class="mclose mtight">)</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.485em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em;">∂</span><span class="mopen mtight">(</span><span class="mord mathit mtight" style="margin-right: 0.04398em;">z</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.52em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.01em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em;">∂</span><span class="mopen mtight">(</span><span class="mord mathit mtight">x</span><span class="mclose mtight">)</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.485em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em;">∂</span><span class="mopen mtight">(</span><span class="mord mathit mtight" style="margin-right: 0.03588em;">y</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.52em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.19011em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.845108em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right: 0.10903em;">N</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1.19011em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.845108em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">2</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">5</span></span></span></span></span></li> 
  </ul> 
  <h1><a id="CUDA_tensors_335"></a>CUDA tensors</h1> 
  <p>是否使用CUDA？：</p> 
  <pre><code>print (torch.cuda.is_available())
</code></pre> 
  <p>结果：</p> 
  <pre><code>True
</code></pre> 
  <p>产生一个全0张量：</p> 
  <pre><code>x = torch.Tensor(3, 4).to("cpu")
print("Type: {}".format(x.type()))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Type: torch.FloatTensor
</code></pre> 
  <p>产生一个全0张量（CUDA下）：</p> 
  <pre><code>x = torch.Tensor(3, 4).to("cuda")
print("Type: {}".format(x.type()))
</code></pre> 
  <p>结果：</p> 
  <pre><code>Type: torch.cuda.FloatTensor
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-7b4cdcb592.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
