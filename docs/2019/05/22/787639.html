<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Tensorflow实现卷积神经网络，用于人脸关键点识别 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Tensorflow实现卷积神经网络，用于人脸关键点识别" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 今年来人工智能的概念越来越火，AlphaGo以4：1击败李世石更是起到推波助澜的作用。作为一个开挖掘机的菜鸟，深深感到不学习一下deep learning早晚要被淘汰。 既然要开始学，当然是搭一个深度神经网络跑几个数据集感受一下作为入门最直观了。自己写代码实现的话debug的过程和运行效率都会很忧伤，我也不知道怎么调用GPU… 所以还是站在巨人的肩膀上，用现成的框架吧。粗略了解一下，现在比较知名的有caffe、mxnet、tensorflow等等。选哪个呢？对我来说选择的标准就两个，第一要容易安装(想尽快入门的迫切心情实在难以忍受一大堆的配置安装…)；第二文档要齐全(这应该是废话 - -)。这几个大名鼎鼎的框架文档都是比较齐全的，那就看最容易安装的。看了几个文档，tensorflow算是最容易安装的了。基本就是pip intall 给定的URL就可以了。安装方式的文档可以在tensorflow安装教程上查看。 tensorflow基本概念与用法 tensorflow直译过来就是张量流。去年google刚推出tensorflow的时候我就纳闷，为什么深度学习会牵扯到张量，以前学弹塑性力学的时候就是一大堆张量看的很烦…不过还好要理解tensorflow里的tensor完全不用理会那些。先来看一下官方文档的说明： &nbsp; class tf.Tensor&nbsp;&nbsp;&nbsp; Represents a value produced by an Operation.&nbsp;&nbsp;&nbsp; A Tensor is a symbolic handle to one of the outputs of an Operation. It does not hold the values of that operation’s output, but instead provides a means of computing those values in a TensorFlow Session. 首先，Tensor代表了执行一个操作(运算)所产生的值。其次，一个Tensor实例并不会保存具体的值，而只是代表了产生这些值的运算方式。好像有些拗口，也就是说假如有一个加法操作add，令c = add(1,1)。那么c就是一个tensor实例了，代表了1+1的结果，但是它并没有存储2这个具体的值，它只知道它代表1+1这个运算。从这里也可以看出，tensorflow里的api都是惰性求值，等真正需要知道具体的值的时候，才会执行计算，其他时候都是在定义计算的过程。 Tensor可以代表从常数一直到N维数组的值。 Flow指的是，指的是tensorflow这套框架里的数据传递全部都是tensor，也就是运算的输入，输出都是tensor。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 常用操作 这里只是简单介绍一下在后面定义卷积神经网络的时候会用到的东西。想要了解更详细的内容还得参考官网上的文档。 首先import tensorflow as tf，后面的tf就代表tensorflow啦。 常数 tf.constant 是一个Operation,用来产生常数,可以产生scalar与N-D array. a是一个tensor,代表了由constant这个Operation所产生的标量常数值的过程。 b就是代表了产生一个2*2的array的过程。 a = tf.constant(3)b = tf.constant(3,shape=[2,2])&nbsp; 1 2 变量 变量代表了神经网络中的参数，在优化计算的过程中需要被改变。tf.Variable当然也是一个Operation，用来产生一个变量，构造函数需要传入一个Tensor对象，传入的这个Tensor对象就决定了这个变量的值的类型(float 或 int)与shape。 变量虽然与Tensor有不同的类型，但是在计算过程中是与Tensor一样可以作为输入输出的。(可以理解为Tensor的派生类，但是实际上可能并不是这样，我还没有看源码) 变量在使用前都必须初始化。 w = tf.Variable(b) 1 Operation 其实Operation不应该单独拿出来说，因为之前的tf.constant和tf.Variable都是Op，不过还是说一下常规的操作，比如tf.matmul执行矩阵计算，tf.conv2d用于卷积计算，Op的详细用法以及其他的Op可以参考api文档。 tf.matmul(m,n)tf.conv2d(...) 1 2 TensorFlow的计算由不同的Operation组成，比如下图 定义了6*(3+5)这个计算过程。6、3、5其实也是Op，这在前面介绍过了。 卷积神经网络用于人脸关键点识别 写到这里终于要开始进入正题了，先从CNN做起吧。Tensorflow的tutorial里面有介绍用CNN(卷积神经网络)来识别手写数字，直接把那里的代码copy下来跑一遍也是可以的。但是那比较没有意思，kaggle上有一个人脸关键点识别的比赛，有数据集也比较有意思，就拿这个来练手了。 定义卷积神经网络 首先是定义网络结构，在这个例子里我用了3个卷积层，第一个卷积层用的max_pool。参数ksize定义pool窗口的大小，每个维度的意义与之前的strides相同，所以实际上我们设置第二个，第三个维度就可以了。 def max_pool_2x2(x):&nbsp;&nbsp;&nbsp; return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) 1 2 1 定义好产生权重、卷积、池化的函数以后就要开始组装这个卷积神经网络了。定义之前再定义一下输入样本x与对应的目标值y_。这里用了tf.placeholder表示此时的x与y_是指定shape的站位符，之后在定义网络结构的时候并不需要真的输入了具体的样本，只要在求值的时候feed进去就可以了。激活函数用relu，api也就是tf.nn.relu。 keep_prob是最后dropout的参数，dropout的目的是为了抗过拟合。 rmse是损失函数，因为这里的目的是为了检测人脸关键点的位置，是回归问题，所以用root-mean-square-error。并且最后的输出层不需要套softmax，直接输出y值就可以了。 这样就组装好了一个卷积神经网络。后续的步骤就是根据输入样本来train这些参数啦。 &nbsp;&nbsp;&nbsp; x = tf.placeholder(&quot;float&quot;, shape=[None, 96, 96, 1])&nbsp;&nbsp;&nbsp; y_ = tf.placeholder(&quot;float&quot;, shape=[None, 30])&nbsp;&nbsp;&nbsp; keep_prob = tf.placeholder(&quot;float&quot;)&nbsp;&nbsp;&nbsp; def model():&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv1 = weight_variable([3, 3, 1, 32])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv1 = bias_variable([32])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool1 = max_pool_2x2(h_conv1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv2 = weight_variable([2, 2, 32, 64])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv2 = bias_variable([64])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool2 = max_pool_2x2(h_conv2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv3 = weight_variable([2, 2, 64, 128])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv3 = bias_variable([128])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool3 = max_pool_2x2(h_conv3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc1 = weight_variable([11 * 11 * 128, 500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc1 = bias_variable([500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool3_flat = tf.reshape(h_pool3, [-1, 11 * 11 * 128])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc2 = weight_variable([500, 500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc2 = bias_variable([500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc3 = weight_variable([500, 30])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc3 = bias_variable([30])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_conv = tf.matmul(h_fc2_drop, W_fc3) + b_fc3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rmse = tf.sqrt(tf.reduce_mean(tf.square(y_ - y_conv)))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return y_conv, rmse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 1 训练卷积神经网络 读取训练数据 定义好卷积神经网络的结构之后，就要开始训练。训练首先是要读取训练样本。下面的代码用于读取样本。 &nbsp;&nbsp;&nbsp; import pandas as pd&nbsp;&nbsp;&nbsp; import numpy as np&nbsp;&nbsp;&nbsp; TRAIN_FILE = &#39;training.csv&#39;&nbsp;&nbsp;&nbsp; TEST_FILE = &#39;test.csv&#39;&nbsp;&nbsp;&nbsp; SAVE_PATH = &#39;model&#39;&nbsp;&nbsp;&nbsp; VALIDATION_SIZE = 100&nbsp;&nbsp;&nbsp; #验证集大小&nbsp;&nbsp;&nbsp; EPOCHS = 100&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #迭代次数&nbsp;&nbsp;&nbsp; BATCH_SIZE = 64&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #每个batch大小，稍微大一点的batch会更稳定&nbsp;&nbsp;&nbsp; EARLY_STOP_PATIENCE = 10 #控制early stopping的参数&nbsp;&nbsp;&nbsp; def input_data(test=False):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; file_name = TEST_FILE if test else TRAIN_FILE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df = pd.read_csv(file_name)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cols = df.columns[:-1]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #dropna()是丢弃有缺失数据的样本，这样最后7000多个样本只剩2140个可用的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df = df.dropna()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df[&#39;Image&#39;] = df[&#39;Image&#39;].apply(lambda img: np.fromstring(img, sep=&#39; &#39;) / 255.0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X = np.vstack(df[&#39;Image&#39;])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X = X.reshape((-1,96,96,1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if test:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = None&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = df[cols].values / 96.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #将y值缩放到[0,1]区间&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return X, y&nbsp;&nbsp;&nbsp; #最后生成提交结果的时候要用到&nbsp;&nbsp;&nbsp; keypoint_index = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_center_x&#39;:0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_center_y&#39;:1,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_center_x&#39;:2,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_center_y&#39;:3,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_inner_corner_x&#39;:4,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_inner_corner_y&#39;:5,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_outer_corner_x&#39;:6,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_outer_corner_y&#39;:7,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_inner_corner_x&#39;:8,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_inner_corner_y&#39;:9,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_outer_corner_x&#39;:10,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_outer_corner_y&#39;:11,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_inner_end_x&#39;:12,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_inner_end_y&#39;:13,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_outer_end_x&#39;:14,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_outer_end_y&#39;:15,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_inner_end_x&#39;:16,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_inner_end_y&#39;:17,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_outer_end_x&#39;:18,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_outer_end_y&#39;:19,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;nose_tip_x&#39;:20,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;nose_tip_y&#39;:21,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_left_corner_x&#39;:22,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_left_corner_y&#39;:23,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_right_corner_x&#39;:24,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_right_corner_y&#39;:25,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_top_lip_x&#39;:26,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_top_lip_y&#39;:27,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_bottom_lip_x&#39;:28,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_bottom_lip_y&#39;:29&nbsp;&nbsp;&nbsp; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 1 开始训练 执行训练的代码如下，save_model用于保存当前训练得到在验证集上loss最小的模型，方便以后直接拿来用。 tf.InteractiveSession()用来生成一个Session，(好像是废话…)。Session相当于一个引擎，TensorFlow框架要真正的进行计算，都要通过Session引擎来启动。 tf.train.AdamOptimizer是优化的算法，Adam的收敛速度会比较快,1e-3是learning rate,这里先简单的用固定的。minimize就是要最小化的目标，当然是最小化均方根误差了。 &nbsp;&nbsp;&nbsp; def save_model(saver,sess,save_path):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path = saver.save(sess, save_path)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;model save in :{0}&#39;.format(path)&nbsp;&nbsp;&nbsp; if __name__ == &#39;__main__&#39;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess = tf.InteractiveSession()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_conv, rmse = model()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_step = tf.train.AdamOptimizer(1e-3).minimize(rmse)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #变量都要初始化 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess.run(tf.initialize_all_variables())&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X,y = input_data()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_valid, y_valid = X[:VALIDATION_SIZE], y[:VALIDATION_SIZE]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X[VALIDATION_SIZE:], y[VALIDATION_SIZE:]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; best_validation_loss = 1000000.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; current_epoch = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TRAIN_SIZE = X_train.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_index = range(TRAIN_SIZE)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; random.shuffle(train_index)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X_train[train_index], y_train[train_index]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; saver = tf.train.Saver()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;begin training..., train dataset size:{0}&#39;.format(TRAIN_SIZE)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in xrange(EPOCHS):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; random.shuffle(train_index)&nbsp; #每个epoch都shuffle一下效果更好&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X_train[train_index], y_train[train_index]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for j in xrange(0,TRAIN_SIZE,BATCH_SIZE):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;epoch {0}, train {1} samples done...&#39;.format(i,j)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_step.run(feed_dict={x:X_train[j:j+BATCH_SIZE],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_:y_train[j:j+BATCH_SIZE], keep_prob:0.5})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #电脑太渣，用所有训练样本计算train_loss居然死机，只好注释了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #train_loss = rmse.eval(feed_dict={x:X_train, y_:y_train, keep_prob: 1.0})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; validation_loss = rmse.eval(feed_dict={x:X_valid, y_:y_valid, keep_prob: 1.0})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;epoch {0} done! validation loss:{1}&#39;.format(i, validation_loss*96.0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if validation_loss &lt; best_validation_loss:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; best_validation_loss = validation_loss&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; current_epoch = i&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; save_model(saver,sess,SAVE_PATH)&nbsp;&nbsp; #即时保存最好的结果&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; elif (i - current_epoch) &gt;= EARLY_STOP_PATIENCE:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;early stopping&#39;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 1 在测试集上预测 下面的代码用于预测test.csv里面的人脸关键点，最后的y值要乘以96，因为之前缩放到[0,1]区间了。 &nbsp;&nbsp;&nbsp; X,y = input_data(test=True)&nbsp;&nbsp;&nbsp; y_pred = []&nbsp;&nbsp;&nbsp; TEST_SIZE = X.shape[0]&nbsp;&nbsp;&nbsp; for j in xrange(0,TEST_SIZE,BATCH_SIZE):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_batch = y_conv.eval(feed_dict={x:X[j:j+BATCH_SIZE], keep_prob:1.0})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_pred.extend(y_batch)&nbsp;&nbsp;&nbsp; print &#39;predict test image done!&#39;&nbsp;&nbsp;&nbsp; output_file = open(&#39;submit.csv&#39;,&#39;w&#39;)&nbsp;&nbsp;&nbsp; output_file.write(&#39;RowId,Location\n&#39;)&nbsp;&nbsp;&nbsp; IdLookupTable = open(&#39;IdLookupTable.csv&#39;)&nbsp;&nbsp;&nbsp; IdLookupTable.readline()&nbsp;&nbsp;&nbsp; for line in IdLookupTable:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RowId,ImageId,FeatureName = line.rstrip().split(&#39;,&#39;)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_index = int(ImageId) - 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; feature_index = keypoint_index[FeatureName]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; feature_location = y_pred[image_index][feature_index] * 96&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; output_file.write(&#39;{0},{1}\n&#39;.format(RowId,feature_location))&nbsp;&nbsp;&nbsp; output_file.close()&nbsp;&nbsp;&nbsp; IdLookupTable.close() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 1 结果 用这个结构的卷积神经网络训练出来的模型，在测试集上预测的结果提交以后的成绩是3.4144，在kaggle的leaderboard上是41名，初试CNN，感觉还可以了。这只是数据，还是找一些现实的照片来试试这个模型如何，所以我找了一张anglababy的，标识出来的关键点感觉还算靠谱。基于TensorFlow的卷积神经网络先写到这了，有什么遗漏的想起来再补充，之后对深度学习更了解了，再写写CNN的原理，bp的推导过程之类的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 今年来人工智能的概念越来越火，AlphaGo以4：1击败李世石更是起到推波助澜的作用。作为一个开挖掘机的菜鸟，深深感到不学习一下deep learning早晚要被淘汰。 既然要开始学，当然是搭一个深度神经网络跑几个数据集感受一下作为入门最直观了。自己写代码实现的话debug的过程和运行效率都会很忧伤，我也不知道怎么调用GPU… 所以还是站在巨人的肩膀上，用现成的框架吧。粗略了解一下，现在比较知名的有caffe、mxnet、tensorflow等等。选哪个呢？对我来说选择的标准就两个，第一要容易安装(想尽快入门的迫切心情实在难以忍受一大堆的配置安装…)；第二文档要齐全(这应该是废话 - -)。这几个大名鼎鼎的框架文档都是比较齐全的，那就看最容易安装的。看了几个文档，tensorflow算是最容易安装的了。基本就是pip intall 给定的URL就可以了。安装方式的文档可以在tensorflow安装教程上查看。 tensorflow基本概念与用法 tensorflow直译过来就是张量流。去年google刚推出tensorflow的时候我就纳闷，为什么深度学习会牵扯到张量，以前学弹塑性力学的时候就是一大堆张量看的很烦…不过还好要理解tensorflow里的tensor完全不用理会那些。先来看一下官方文档的说明： &nbsp; class tf.Tensor&nbsp;&nbsp;&nbsp; Represents a value produced by an Operation.&nbsp;&nbsp;&nbsp; A Tensor is a symbolic handle to one of the outputs of an Operation. It does not hold the values of that operation’s output, but instead provides a means of computing those values in a TensorFlow Session. 首先，Tensor代表了执行一个操作(运算)所产生的值。其次，一个Tensor实例并不会保存具体的值，而只是代表了产生这些值的运算方式。好像有些拗口，也就是说假如有一个加法操作add，令c = add(1,1)。那么c就是一个tensor实例了，代表了1+1的结果，但是它并没有存储2这个具体的值，它只知道它代表1+1这个运算。从这里也可以看出，tensorflow里的api都是惰性求值，等真正需要知道具体的值的时候，才会执行计算，其他时候都是在定义计算的过程。 Tensor可以代表从常数一直到N维数组的值。 Flow指的是，指的是tensorflow这套框架里的数据传递全部都是tensor，也就是运算的输入，输出都是tensor。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 常用操作 这里只是简单介绍一下在后面定义卷积神经网络的时候会用到的东西。想要了解更详细的内容还得参考官网上的文档。 首先import tensorflow as tf，后面的tf就代表tensorflow啦。 常数 tf.constant 是一个Operation,用来产生常数,可以产生scalar与N-D array. a是一个tensor,代表了由constant这个Operation所产生的标量常数值的过程。 b就是代表了产生一个2*2的array的过程。 a = tf.constant(3)b = tf.constant(3,shape=[2,2])&nbsp; 1 2 变量 变量代表了神经网络中的参数，在优化计算的过程中需要被改变。tf.Variable当然也是一个Operation，用来产生一个变量，构造函数需要传入一个Tensor对象，传入的这个Tensor对象就决定了这个变量的值的类型(float 或 int)与shape。 变量虽然与Tensor有不同的类型，但是在计算过程中是与Tensor一样可以作为输入输出的。(可以理解为Tensor的派生类，但是实际上可能并不是这样，我还没有看源码) 变量在使用前都必须初始化。 w = tf.Variable(b) 1 Operation 其实Operation不应该单独拿出来说，因为之前的tf.constant和tf.Variable都是Op，不过还是说一下常规的操作，比如tf.matmul执行矩阵计算，tf.conv2d用于卷积计算，Op的详细用法以及其他的Op可以参考api文档。 tf.matmul(m,n)tf.conv2d(...) 1 2 TensorFlow的计算由不同的Operation组成，比如下图 定义了6*(3+5)这个计算过程。6、3、5其实也是Op，这在前面介绍过了。 卷积神经网络用于人脸关键点识别 写到这里终于要开始进入正题了，先从CNN做起吧。Tensorflow的tutorial里面有介绍用CNN(卷积神经网络)来识别手写数字，直接把那里的代码copy下来跑一遍也是可以的。但是那比较没有意思，kaggle上有一个人脸关键点识别的比赛，有数据集也比较有意思，就拿这个来练手了。 定义卷积神经网络 首先是定义网络结构，在这个例子里我用了3个卷积层，第一个卷积层用的max_pool。参数ksize定义pool窗口的大小，每个维度的意义与之前的strides相同，所以实际上我们设置第二个，第三个维度就可以了。 def max_pool_2x2(x):&nbsp;&nbsp;&nbsp; return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) 1 2 1 定义好产生权重、卷积、池化的函数以后就要开始组装这个卷积神经网络了。定义之前再定义一下输入样本x与对应的目标值y_。这里用了tf.placeholder表示此时的x与y_是指定shape的站位符，之后在定义网络结构的时候并不需要真的输入了具体的样本，只要在求值的时候feed进去就可以了。激活函数用relu，api也就是tf.nn.relu。 keep_prob是最后dropout的参数，dropout的目的是为了抗过拟合。 rmse是损失函数，因为这里的目的是为了检测人脸关键点的位置，是回归问题，所以用root-mean-square-error。并且最后的输出层不需要套softmax，直接输出y值就可以了。 这样就组装好了一个卷积神经网络。后续的步骤就是根据输入样本来train这些参数啦。 &nbsp;&nbsp;&nbsp; x = tf.placeholder(&quot;float&quot;, shape=[None, 96, 96, 1])&nbsp;&nbsp;&nbsp; y_ = tf.placeholder(&quot;float&quot;, shape=[None, 30])&nbsp;&nbsp;&nbsp; keep_prob = tf.placeholder(&quot;float&quot;)&nbsp;&nbsp;&nbsp; def model():&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv1 = weight_variable([3, 3, 1, 32])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv1 = bias_variable([32])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool1 = max_pool_2x2(h_conv1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv2 = weight_variable([2, 2, 32, 64])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv2 = bias_variable([64])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool2 = max_pool_2x2(h_conv2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv3 = weight_variable([2, 2, 64, 128])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv3 = bias_variable([128])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool3 = max_pool_2x2(h_conv3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc1 = weight_variable([11 * 11 * 128, 500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc1 = bias_variable([500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool3_flat = tf.reshape(h_pool3, [-1, 11 * 11 * 128])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc2 = weight_variable([500, 500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc2 = bias_variable([500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc3 = weight_variable([500, 30])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc3 = bias_variable([30])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_conv = tf.matmul(h_fc2_drop, W_fc3) + b_fc3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rmse = tf.sqrt(tf.reduce_mean(tf.square(y_ - y_conv)))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return y_conv, rmse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 1 训练卷积神经网络 读取训练数据 定义好卷积神经网络的结构之后，就要开始训练。训练首先是要读取训练样本。下面的代码用于读取样本。 &nbsp;&nbsp;&nbsp; import pandas as pd&nbsp;&nbsp;&nbsp; import numpy as np&nbsp;&nbsp;&nbsp; TRAIN_FILE = &#39;training.csv&#39;&nbsp;&nbsp;&nbsp; TEST_FILE = &#39;test.csv&#39;&nbsp;&nbsp;&nbsp; SAVE_PATH = &#39;model&#39;&nbsp;&nbsp;&nbsp; VALIDATION_SIZE = 100&nbsp;&nbsp;&nbsp; #验证集大小&nbsp;&nbsp;&nbsp; EPOCHS = 100&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #迭代次数&nbsp;&nbsp;&nbsp; BATCH_SIZE = 64&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #每个batch大小，稍微大一点的batch会更稳定&nbsp;&nbsp;&nbsp; EARLY_STOP_PATIENCE = 10 #控制early stopping的参数&nbsp;&nbsp;&nbsp; def input_data(test=False):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; file_name = TEST_FILE if test else TRAIN_FILE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df = pd.read_csv(file_name)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cols = df.columns[:-1]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #dropna()是丢弃有缺失数据的样本，这样最后7000多个样本只剩2140个可用的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df = df.dropna()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df[&#39;Image&#39;] = df[&#39;Image&#39;].apply(lambda img: np.fromstring(img, sep=&#39; &#39;) / 255.0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X = np.vstack(df[&#39;Image&#39;])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X = X.reshape((-1,96,96,1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if test:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = None&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = df[cols].values / 96.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #将y值缩放到[0,1]区间&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return X, y&nbsp;&nbsp;&nbsp; #最后生成提交结果的时候要用到&nbsp;&nbsp;&nbsp; keypoint_index = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_center_x&#39;:0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_center_y&#39;:1,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_center_x&#39;:2,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_center_y&#39;:3,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_inner_corner_x&#39;:4,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_inner_corner_y&#39;:5,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_outer_corner_x&#39;:6,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_outer_corner_y&#39;:7,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_inner_corner_x&#39;:8,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_inner_corner_y&#39;:9,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_outer_corner_x&#39;:10,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_outer_corner_y&#39;:11,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_inner_end_x&#39;:12,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_inner_end_y&#39;:13,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_outer_end_x&#39;:14,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_outer_end_y&#39;:15,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_inner_end_x&#39;:16,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_inner_end_y&#39;:17,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_outer_end_x&#39;:18,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_outer_end_y&#39;:19,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;nose_tip_x&#39;:20,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;nose_tip_y&#39;:21,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_left_corner_x&#39;:22,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_left_corner_y&#39;:23,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_right_corner_x&#39;:24,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_right_corner_y&#39;:25,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_top_lip_x&#39;:26,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_top_lip_y&#39;:27,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_bottom_lip_x&#39;:28,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_bottom_lip_y&#39;:29&nbsp;&nbsp;&nbsp; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 1 开始训练 执行训练的代码如下，save_model用于保存当前训练得到在验证集上loss最小的模型，方便以后直接拿来用。 tf.InteractiveSession()用来生成一个Session，(好像是废话…)。Session相当于一个引擎，TensorFlow框架要真正的进行计算，都要通过Session引擎来启动。 tf.train.AdamOptimizer是优化的算法，Adam的收敛速度会比较快,1e-3是learning rate,这里先简单的用固定的。minimize就是要最小化的目标，当然是最小化均方根误差了。 &nbsp;&nbsp;&nbsp; def save_model(saver,sess,save_path):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path = saver.save(sess, save_path)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;model save in :{0}&#39;.format(path)&nbsp;&nbsp;&nbsp; if __name__ == &#39;__main__&#39;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess = tf.InteractiveSession()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_conv, rmse = model()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_step = tf.train.AdamOptimizer(1e-3).minimize(rmse)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #变量都要初始化 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess.run(tf.initialize_all_variables())&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X,y = input_data()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_valid, y_valid = X[:VALIDATION_SIZE], y[:VALIDATION_SIZE]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X[VALIDATION_SIZE:], y[VALIDATION_SIZE:]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; best_validation_loss = 1000000.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; current_epoch = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TRAIN_SIZE = X_train.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_index = range(TRAIN_SIZE)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; random.shuffle(train_index)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X_train[train_index], y_train[train_index]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; saver = tf.train.Saver()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;begin training..., train dataset size:{0}&#39;.format(TRAIN_SIZE)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in xrange(EPOCHS):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; random.shuffle(train_index)&nbsp; #每个epoch都shuffle一下效果更好&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X_train[train_index], y_train[train_index]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for j in xrange(0,TRAIN_SIZE,BATCH_SIZE):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;epoch {0}, train {1} samples done...&#39;.format(i,j)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_step.run(feed_dict={x:X_train[j:j+BATCH_SIZE],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_:y_train[j:j+BATCH_SIZE], keep_prob:0.5})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #电脑太渣，用所有训练样本计算train_loss居然死机，只好注释了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #train_loss = rmse.eval(feed_dict={x:X_train, y_:y_train, keep_prob: 1.0})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; validation_loss = rmse.eval(feed_dict={x:X_valid, y_:y_valid, keep_prob: 1.0})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;epoch {0} done! validation loss:{1}&#39;.format(i, validation_loss*96.0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if validation_loss &lt; best_validation_loss:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; best_validation_loss = validation_loss&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; current_epoch = i&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; save_model(saver,sess,SAVE_PATH)&nbsp;&nbsp; #即时保存最好的结果&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; elif (i - current_epoch) &gt;= EARLY_STOP_PATIENCE:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;early stopping&#39;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 1 在测试集上预测 下面的代码用于预测test.csv里面的人脸关键点，最后的y值要乘以96，因为之前缩放到[0,1]区间了。 &nbsp;&nbsp;&nbsp; X,y = input_data(test=True)&nbsp;&nbsp;&nbsp; y_pred = []&nbsp;&nbsp;&nbsp; TEST_SIZE = X.shape[0]&nbsp;&nbsp;&nbsp; for j in xrange(0,TEST_SIZE,BATCH_SIZE):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_batch = y_conv.eval(feed_dict={x:X[j:j+BATCH_SIZE], keep_prob:1.0})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_pred.extend(y_batch)&nbsp;&nbsp;&nbsp; print &#39;predict test image done!&#39;&nbsp;&nbsp;&nbsp; output_file = open(&#39;submit.csv&#39;,&#39;w&#39;)&nbsp;&nbsp;&nbsp; output_file.write(&#39;RowId,Location\n&#39;)&nbsp;&nbsp;&nbsp; IdLookupTable = open(&#39;IdLookupTable.csv&#39;)&nbsp;&nbsp;&nbsp; IdLookupTable.readline()&nbsp;&nbsp;&nbsp; for line in IdLookupTable:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RowId,ImageId,FeatureName = line.rstrip().split(&#39;,&#39;)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_index = int(ImageId) - 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; feature_index = keypoint_index[FeatureName]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; feature_location = y_pred[image_index][feature_index] * 96&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; output_file.write(&#39;{0},{1}\n&#39;.format(RowId,feature_location))&nbsp;&nbsp;&nbsp; output_file.close()&nbsp;&nbsp;&nbsp; IdLookupTable.close() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 1 结果 用这个结构的卷积神经网络训练出来的模型，在测试集上预测的结果提交以后的成绩是3.4144，在kaggle的leaderboard上是41名，初试CNN，感觉还可以了。这只是数据，还是找一些现实的照片来试试这个模型如何，所以我找了一张anglababy的，标识出来的关键点感觉还算靠谱。基于TensorFlow的卷积神经网络先写到这了，有什么遗漏的想起来再补充，之后对深度学习更了解了，再写写CNN的原理，bp的推导过程之类的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/22/787639.html" />
<meta property="og:url" content="https://mlh.app/2019/05/22/787639.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-22T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 今年来人工智能的概念越来越火，AlphaGo以4：1击败李世石更是起到推波助澜的作用。作为一个开挖掘机的菜鸟，深深感到不学习一下deep learning早晚要被淘汰。 既然要开始学，当然是搭一个深度神经网络跑几个数据集感受一下作为入门最直观了。自己写代码实现的话debug的过程和运行效率都会很忧伤，我也不知道怎么调用GPU… 所以还是站在巨人的肩膀上，用现成的框架吧。粗略了解一下，现在比较知名的有caffe、mxnet、tensorflow等等。选哪个呢？对我来说选择的标准就两个，第一要容易安装(想尽快入门的迫切心情实在难以忍受一大堆的配置安装…)；第二文档要齐全(这应该是废话 - -)。这几个大名鼎鼎的框架文档都是比较齐全的，那就看最容易安装的。看了几个文档，tensorflow算是最容易安装的了。基本就是pip intall 给定的URL就可以了。安装方式的文档可以在tensorflow安装教程上查看。 tensorflow基本概念与用法 tensorflow直译过来就是张量流。去年google刚推出tensorflow的时候我就纳闷，为什么深度学习会牵扯到张量，以前学弹塑性力学的时候就是一大堆张量看的很烦…不过还好要理解tensorflow里的tensor完全不用理会那些。先来看一下官方文档的说明： &nbsp; class tf.Tensor&nbsp;&nbsp;&nbsp; Represents a value produced by an Operation.&nbsp;&nbsp;&nbsp; A Tensor is a symbolic handle to one of the outputs of an Operation. It does not hold the values of that operation’s output, but instead provides a means of computing those values in a TensorFlow Session. 首先，Tensor代表了执行一个操作(运算)所产生的值。其次，一个Tensor实例并不会保存具体的值，而只是代表了产生这些值的运算方式。好像有些拗口，也就是说假如有一个加法操作add，令c = add(1,1)。那么c就是一个tensor实例了，代表了1+1的结果，但是它并没有存储2这个具体的值，它只知道它代表1+1这个运算。从这里也可以看出，tensorflow里的api都是惰性求值，等真正需要知道具体的值的时候，才会执行计算，其他时候都是在定义计算的过程。 Tensor可以代表从常数一直到N维数组的值。 Flow指的是，指的是tensorflow这套框架里的数据传递全部都是tensor，也就是运算的输入，输出都是tensor。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 常用操作 这里只是简单介绍一下在后面定义卷积神经网络的时候会用到的东西。想要了解更详细的内容还得参考官网上的文档。 首先import tensorflow as tf，后面的tf就代表tensorflow啦。 常数 tf.constant 是一个Operation,用来产生常数,可以产生scalar与N-D array. a是一个tensor,代表了由constant这个Operation所产生的标量常数值的过程。 b就是代表了产生一个2*2的array的过程。 a = tf.constant(3)b = tf.constant(3,shape=[2,2])&nbsp; 1 2 变量 变量代表了神经网络中的参数，在优化计算的过程中需要被改变。tf.Variable当然也是一个Operation，用来产生一个变量，构造函数需要传入一个Tensor对象，传入的这个Tensor对象就决定了这个变量的值的类型(float 或 int)与shape。 变量虽然与Tensor有不同的类型，但是在计算过程中是与Tensor一样可以作为输入输出的。(可以理解为Tensor的派生类，但是实际上可能并不是这样，我还没有看源码) 变量在使用前都必须初始化。 w = tf.Variable(b) 1 Operation 其实Operation不应该单独拿出来说，因为之前的tf.constant和tf.Variable都是Op，不过还是说一下常规的操作，比如tf.matmul执行矩阵计算，tf.conv2d用于卷积计算，Op的详细用法以及其他的Op可以参考api文档。 tf.matmul(m,n)tf.conv2d(...) 1 2 TensorFlow的计算由不同的Operation组成，比如下图 定义了6*(3+5)这个计算过程。6、3、5其实也是Op，这在前面介绍过了。 卷积神经网络用于人脸关键点识别 写到这里终于要开始进入正题了，先从CNN做起吧。Tensorflow的tutorial里面有介绍用CNN(卷积神经网络)来识别手写数字，直接把那里的代码copy下来跑一遍也是可以的。但是那比较没有意思，kaggle上有一个人脸关键点识别的比赛，有数据集也比较有意思，就拿这个来练手了。 定义卷积神经网络 首先是定义网络结构，在这个例子里我用了3个卷积层，第一个卷积层用的max_pool。参数ksize定义pool窗口的大小，每个维度的意义与之前的strides相同，所以实际上我们设置第二个，第三个维度就可以了。 def max_pool_2x2(x):&nbsp;&nbsp;&nbsp; return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding=&#39;SAME&#39;) 1 2 1 定义好产生权重、卷积、池化的函数以后就要开始组装这个卷积神经网络了。定义之前再定义一下输入样本x与对应的目标值y_。这里用了tf.placeholder表示此时的x与y_是指定shape的站位符，之后在定义网络结构的时候并不需要真的输入了具体的样本，只要在求值的时候feed进去就可以了。激活函数用relu，api也就是tf.nn.relu。 keep_prob是最后dropout的参数，dropout的目的是为了抗过拟合。 rmse是损失函数，因为这里的目的是为了检测人脸关键点的位置，是回归问题，所以用root-mean-square-error。并且最后的输出层不需要套softmax，直接输出y值就可以了。 这样就组装好了一个卷积神经网络。后续的步骤就是根据输入样本来train这些参数啦。 &nbsp;&nbsp;&nbsp; x = tf.placeholder(&quot;float&quot;, shape=[None, 96, 96, 1])&nbsp;&nbsp;&nbsp; y_ = tf.placeholder(&quot;float&quot;, shape=[None, 30])&nbsp;&nbsp;&nbsp; keep_prob = tf.placeholder(&quot;float&quot;)&nbsp;&nbsp;&nbsp; def model():&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv1 = weight_variable([3, 3, 1, 32])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv1 = bias_variable([32])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool1 = max_pool_2x2(h_conv1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv2 = weight_variable([2, 2, 32, 64])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv2 = bias_variable([64])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool2 = max_pool_2x2(h_conv2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv3 = weight_variable([2, 2, 64, 128])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv3 = bias_variable([128])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool3 = max_pool_2x2(h_conv3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc1 = weight_variable([11 * 11 * 128, 500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc1 = bias_variable([500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool3_flat = tf.reshape(h_pool3, [-1, 11 * 11 * 128])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc2 = weight_variable([500, 500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc2 = bias_variable([500])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc3 = weight_variable([500, 30])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc3 = bias_variable([30])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_conv = tf.matmul(h_fc2_drop, W_fc3) + b_fc3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rmse = tf.sqrt(tf.reduce_mean(tf.square(y_ - y_conv)))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return y_conv, rmse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 1 训练卷积神经网络 读取训练数据 定义好卷积神经网络的结构之后，就要开始训练。训练首先是要读取训练样本。下面的代码用于读取样本。 &nbsp;&nbsp;&nbsp; import pandas as pd&nbsp;&nbsp;&nbsp; import numpy as np&nbsp;&nbsp;&nbsp; TRAIN_FILE = &#39;training.csv&#39;&nbsp;&nbsp;&nbsp; TEST_FILE = &#39;test.csv&#39;&nbsp;&nbsp;&nbsp; SAVE_PATH = &#39;model&#39;&nbsp;&nbsp;&nbsp; VALIDATION_SIZE = 100&nbsp;&nbsp;&nbsp; #验证集大小&nbsp;&nbsp;&nbsp; EPOCHS = 100&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #迭代次数&nbsp;&nbsp;&nbsp; BATCH_SIZE = 64&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #每个batch大小，稍微大一点的batch会更稳定&nbsp;&nbsp;&nbsp; EARLY_STOP_PATIENCE = 10 #控制early stopping的参数&nbsp;&nbsp;&nbsp; def input_data(test=False):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; file_name = TEST_FILE if test else TRAIN_FILE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df = pd.read_csv(file_name)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cols = df.columns[:-1]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #dropna()是丢弃有缺失数据的样本，这样最后7000多个样本只剩2140个可用的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df = df.dropna()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df[&#39;Image&#39;] = df[&#39;Image&#39;].apply(lambda img: np.fromstring(img, sep=&#39; &#39;) / 255.0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X = np.vstack(df[&#39;Image&#39;])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X = X.reshape((-1,96,96,1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if test:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = None&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = df[cols].values / 96.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #将y值缩放到[0,1]区间&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return X, y&nbsp;&nbsp;&nbsp; #最后生成提交结果的时候要用到&nbsp;&nbsp;&nbsp; keypoint_index = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_center_x&#39;:0,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_center_y&#39;:1,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_center_x&#39;:2,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_center_y&#39;:3,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_inner_corner_x&#39;:4,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_inner_corner_y&#39;:5,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_outer_corner_x&#39;:6,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eye_outer_corner_y&#39;:7,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_inner_corner_x&#39;:8,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_inner_corner_y&#39;:9,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_outer_corner_x&#39;:10,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eye_outer_corner_y&#39;:11,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_inner_end_x&#39;:12,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_inner_end_y&#39;:13,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_outer_end_x&#39;:14,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;left_eyebrow_outer_end_y&#39;:15,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_inner_end_x&#39;:16,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_inner_end_y&#39;:17,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_outer_end_x&#39;:18,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;right_eyebrow_outer_end_y&#39;:19,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;nose_tip_x&#39;:20,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;nose_tip_y&#39;:21,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_left_corner_x&#39;:22,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_left_corner_y&#39;:23,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_right_corner_x&#39;:24,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_right_corner_y&#39;:25,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_top_lip_x&#39;:26,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_top_lip_y&#39;:27,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_bottom_lip_x&#39;:28,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#39;mouth_center_bottom_lip_y&#39;:29&nbsp;&nbsp;&nbsp; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 1 开始训练 执行训练的代码如下，save_model用于保存当前训练得到在验证集上loss最小的模型，方便以后直接拿来用。 tf.InteractiveSession()用来生成一个Session，(好像是废话…)。Session相当于一个引擎，TensorFlow框架要真正的进行计算，都要通过Session引擎来启动。 tf.train.AdamOptimizer是优化的算法，Adam的收敛速度会比较快,1e-3是learning rate,这里先简单的用固定的。minimize就是要最小化的目标，当然是最小化均方根误差了。 &nbsp;&nbsp;&nbsp; def save_model(saver,sess,save_path):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path = saver.save(sess, save_path)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;model save in :{0}&#39;.format(path)&nbsp;&nbsp;&nbsp; if __name__ == &#39;__main__&#39;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess = tf.InteractiveSession()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_conv, rmse = model()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_step = tf.train.AdamOptimizer(1e-3).minimize(rmse)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #变量都要初始化 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess.run(tf.initialize_all_variables())&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X,y = input_data()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_valid, y_valid = X[:VALIDATION_SIZE], y[:VALIDATION_SIZE]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X[VALIDATION_SIZE:], y[VALIDATION_SIZE:]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; best_validation_loss = 1000000.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; current_epoch = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TRAIN_SIZE = X_train.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_index = range(TRAIN_SIZE)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; random.shuffle(train_index)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X_train[train_index], y_train[train_index]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; saver = tf.train.Saver()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;begin training..., train dataset size:{0}&#39;.format(TRAIN_SIZE)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in xrange(EPOCHS):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; random.shuffle(train_index)&nbsp; #每个epoch都shuffle一下效果更好&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X_train[train_index], y_train[train_index]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for j in xrange(0,TRAIN_SIZE,BATCH_SIZE):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;epoch {0}, train {1} samples done...&#39;.format(i,j)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_step.run(feed_dict={x:X_train[j:j+BATCH_SIZE],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_:y_train[j:j+BATCH_SIZE], keep_prob:0.5})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #电脑太渣，用所有训练样本计算train_loss居然死机，只好注释了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; #train_loss = rmse.eval(feed_dict={x:X_train, y_:y_train, keep_prob: 1.0})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; validation_loss = rmse.eval(feed_dict={x:X_valid, y_:y_valid, keep_prob: 1.0})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;epoch {0} done! validation loss:{1}&#39;.format(i, validation_loss*96.0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if validation_loss &lt; best_validation_loss:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; best_validation_loss = validation_loss&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; current_epoch = i&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; save_model(saver,sess,SAVE_PATH)&nbsp;&nbsp; #即时保存最好的结果&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; elif (i - current_epoch) &gt;= EARLY_STOP_PATIENCE:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;early stopping&#39;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 1 在测试集上预测 下面的代码用于预测test.csv里面的人脸关键点，最后的y值要乘以96，因为之前缩放到[0,1]区间了。 &nbsp;&nbsp;&nbsp; X,y = input_data(test=True)&nbsp;&nbsp;&nbsp; y_pred = []&nbsp;&nbsp;&nbsp; TEST_SIZE = X.shape[0]&nbsp;&nbsp;&nbsp; for j in xrange(0,TEST_SIZE,BATCH_SIZE):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_batch = y_conv.eval(feed_dict={x:X[j:j+BATCH_SIZE], keep_prob:1.0})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_pred.extend(y_batch)&nbsp;&nbsp;&nbsp; print &#39;predict test image done!&#39;&nbsp;&nbsp;&nbsp; output_file = open(&#39;submit.csv&#39;,&#39;w&#39;)&nbsp;&nbsp;&nbsp; output_file.write(&#39;RowId,Location\\n&#39;)&nbsp;&nbsp;&nbsp; IdLookupTable = open(&#39;IdLookupTable.csv&#39;)&nbsp;&nbsp;&nbsp; IdLookupTable.readline()&nbsp;&nbsp;&nbsp; for line in IdLookupTable:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RowId,ImageId,FeatureName = line.rstrip().split(&#39;,&#39;)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_index = int(ImageId) - 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; feature_index = keypoint_index[FeatureName]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; feature_location = y_pred[image_index][feature_index] * 96&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; output_file.write(&#39;{0},{1}\\n&#39;.format(RowId,feature_location))&nbsp;&nbsp;&nbsp; output_file.close()&nbsp;&nbsp;&nbsp; IdLookupTable.close() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 1 结果 用这个结构的卷积神经网络训练出来的模型，在测试集上预测的结果提交以后的成绩是3.4144，在kaggle的leaderboard上是41名，初试CNN，感觉还可以了。这只是数据，还是找一些现实的照片来试试这个模型如何，所以我找了一张anglababy的，标识出来的关键点感觉还算靠谱。基于TensorFlow的卷积神经网络先写到这了，有什么遗漏的想起来再补充，之后对深度学习更了解了，再写写CNN的原理，bp的推导过程之类的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/22/787639.html","headline":"Tensorflow实现卷积神经网络，用于人脸关键点识别","dateModified":"2019-05-22T00:00:00+08:00","datePublished":"2019-05-22T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/22/787639.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Tensorflow实现卷积神经网络，用于人脸关键点识别</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <div class="markdown_views" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
   <div class="markdown_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <!-- flowchart &amp;#31661;&amp;#22836;&amp;#22270;&amp;#26631; &amp;#21247;&amp;#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <p>今年来人工智能的概念越来越火，AlphaGo以4：1击败李世石更是起到推波助澜的作用。作为一个开挖掘机的菜鸟，深深感到不学习一下deep learning早晚要被淘汰。</p>
    <p>既然要开始学，当然是搭一个深度神经网络跑几个数据集感受一下作为入门最直观了。自己写代码实现的话debug的过程和运行效率都会很忧伤，我也不知道怎么调用GPU… 所以还是站在巨人的肩膀上，用现成的框架吧。粗略了解一下，现在比较知名的有caffe、mxnet、tensorflow等等。选哪个呢？对我来说选择的标准就两个，第一要容易安装(想尽快入门的迫切心情实在难以忍受一大堆的配置安装…)；第二文档要齐全(这应该是废话 - -)。这几个大名鼎鼎的框架文档都是比较齐全的，那就看最容易安装的。看了几个文档，tensorflow算是最容易安装的了。基本就是<code>pip intall 给定的URL</code>就可以了。安装方式的文档可以在<a href="https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#download-and-setup" rel="nofollow" target="_blank">tensorflow安装教程</a>上查看。</p>
   </div>
   <h2 id="tensorflow基本概念与用法"><a></a><a target="_blank"></a>tensorflow基本概念与用法</h2>
   <p>tensorflow直译过来就是张量流。去年google刚推出tensorflow的时候我就纳闷，为什么深度学习会牵扯到张量，以前学弹塑性力学的时候就是一大堆张量看的很烦…不过还好要理解tensorflow里的tensor完全不用理会那些。先来看一下<a href="https://www.tensorflow.org/versions/r0.7/api_docs/index.html" rel="nofollow" target="_blank">官方文档</a>的说明：</p>
   <blockquote>
    &nbsp; 
    <p>class tf.Tensor</p>&nbsp;&nbsp;&nbsp; 
    <p>Represents a value produced by an Operation.</p>&nbsp;&nbsp;&nbsp; 
    <p>A Tensor is a symbolic handle to one of the outputs of an Operation. It does not hold the values of that operation’s output, but instead provides a means of computing those values in a TensorFlow Session.</p>
   </blockquote>
   <p>首先，Tensor代表了执行一个操作(运算)所产生的值。其次，一个Tensor实例并不会保存具体的值，而只是代表了产生这些值的运算方式。好像有些拗口，也就是说假如有一个加法操作<code>add</code>，令<code>c = add(1,1)</code>。那么<code>c</code>就是一个tensor实例了，代表了<code>1+1</code>的结果，但是它并没有存储<code>2</code>这个具体的值，它只知道它代表<code>1+1</code>这个运算。从这里也可以看出，tensorflow里的api都是惰性求值，等真正需要知道具体的值的时候，才会执行计算，其他时候都是在定义计算的过程。</p>
   <p>Tensor可以代表从常数一直到N维数组的值。</p>
   <p><strong>Flow</strong>指的是，指的是tensorflow这套框架里的数据传递全部都是tensor，也就是运算的输入，输出都是tensor。</p>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <h3 id="常用操作"><a></a><a target="_blank"></a>常用操作</h3>
   <p>这里只是简单介绍一下在后面定义卷积神经网络的时候会用到的东西。想要了解更详细的内容还得参考官网上的文档。</p>
   <p>首先<code>import tensorflow as tf</code>，后面的<code>tf</code>就代表tensorflow啦。</p>
   <h4 id="常数">常数</h4>
   <p><code>tf.constant</code> 是一个Operation,用来产生常数,可以产生scalar与N-D array. a是一个tensor,代表了由constant这个Operation所产生的标量常数值的过程。 b就是代表了产生一个2*2的array的过程。</p>
   <pre class="prettyprint"><code class="has-numbering">a = tf.constant(3)b = tf.constant(3,shape=[2,2])&nbsp; </code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
    </ul>
    <ul class="pre-numbering"></ul></pre>
   <h4 id="变量">变量</h4>
   <p>变量代表了神经网络中的参数，在优化计算的过程中需要被改变。<code>tf.Variable</code>当然也是一个Operation，用来产生一个变量，构造函数需要传入一个Tensor对象，传入的这个Tensor对象就决定了这个变量的值的类型(float 或 int)与shape。</p>
   <p>变量虽然与Tensor有不同的类型，但是在计算过程中是与Tensor一样可以作为输入输出的。(<em>可以理解为Tensor的派生类，但是实际上可能并不是这样，我还没有看源码</em>)</p>
   <p>变量在使用前都必须初始化。</p>
   <pre class="prettyprint"><code class="has-numbering">w = tf.Variable(b)</code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
    </ul>
    <ul class="pre-numbering"></ul></pre>
   <h4 id="operation">Operation</h4>
   <p>其实Operation不应该单独拿出来说，因为之前的tf.constant和tf.Variable都是Op，不过还是说一下常规的操作，比如<code>tf.matmul</code>执行矩阵计算，<code>tf.conv2d</code>用于卷积计算，Op的详细用法以及其他的Op可以参考api文档。</p>
   <pre class="prettyprint"><code class="has-numbering">tf.matmul(m,n)tf.conv2d(...)</code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
    </ul>
    <ul class="pre-numbering"></ul></pre>
   <p>TensorFlow的计算由不同的Operation组成，比如下图 <br><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20160316222135608"></p>
   <p>定义了6*(3+5)这个计算过程。6、3、5其实也是Op，这在前面介绍过了。</p>
   <h2 id="卷积神经网络用于人脸关键点识别"><a></a><a target="_blank"></a>卷积神经网络用于人脸关键点识别</h2>
   <p>写到这里终于要开始进入正题了，先从CNN做起吧。Tensorflow的tutorial里面有介绍用CNN(卷积神经网络)来识别手写数字，直接把那里的代码copy下来跑一遍也是可以的。但是那比较没有意思，<a href="https://www.kaggle.com" rel="nofollow" target="_blank">kaggle</a>上有一个人脸关键点识别的比赛，有数据集也比较有意思，就拿这个来练手了。</p>
   <h3 id="定义卷积神经网络"><a></a><a target="_blank"></a>定义卷积神经网络</h3>
   <p>首先是定义网络结构，在这个例子里我用了3个卷积层，第一个卷积层用<span class="MathJax_Preview"></span>的max_pool。参数<code>ksize</code>定义pool窗口的大小，每个维度的意义与之前的strides相同，所以实际上我们设置第二个，第三个维度就可以了。</p>
   <pre class="prettyprint"><code class="language-python hljs&nbsp; has-numbering"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">max_pool_2x2</span><span class="hljs-params">(x)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],strides=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], padding=<span class="hljs-string">'SAME'</span>)</code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>定义好产生权重、卷积、池化的函数以后就要开始<strong>组装</strong>这个卷积神经网络了。定义之前再定义一下输入样本<code>x</code>与对应的目标值<code>y_</code>。这里用了<code>tf.placeholder</code>表示此时的<code>x</code>与<code>y_</code>是指定shape的站位符，之后在定义网络结构的时候并不需要真的输入了具体的样本，只要在求值的时候feed进去就可以了。激活函数用<code>relu</code>，api也就是<code>tf.nn.relu</code>。 <br><code>keep_prob</code>是最后dropout的参数，dropout的目的是为了抗过拟合。</p>
   <p><code>rmse</code>是损失函数，因为这里的目的是为了检测人脸关键点的位置，是回归问题，所以用root-mean-square-error。并且最后的输出层不需要套softmax，直接输出y值就可以了。</p>
   <p>这样就组装好了一个卷积神经网络。后续的步骤就是根据输入样本来train这些参数啦。</p>
   <pre class="prettyprint"><code class="language-python hljs&nbsp; has-numbering">&nbsp;&nbsp;&nbsp; x = tf.placeholder(<span class="hljs-string">"float"</span>, shape=[<span class="hljs-keyword">None</span>, <span class="hljs-number">96</span>, <span class="hljs-number">96</span>, <span class="hljs-number">1</span>])&nbsp;&nbsp;&nbsp; y_ = tf.placeholder(<span class="hljs-string">"float"</span>, shape=[<span class="hljs-keyword">None</span>, <span class="hljs-number">30</span>])&nbsp;&nbsp;&nbsp; keep_prob = tf.placeholder(<span class="hljs-string">"float"</span>)&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model</span><span class="hljs-params">()</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv1 = weight_variable([<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv1 = bias_variable([<span class="hljs-number">32</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool1 = max_pool_2x2(h_conv1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv2 = weight_variable([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv2 = bias_variable([<span class="hljs-number">64</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool2 = max_pool_2x2(h_conv2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_conv3 = weight_variable([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">64</span>, <span class="hljs-number">128</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_conv3 = bias_variable([<span class="hljs-number">128</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool3 = max_pool_2x2(h_conv3)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc1 = weight_variable([<span class="hljs-number">11</span> * <span class="hljs-number">11</span> * <span class="hljs-number">128</span>, <span class="hljs-number">500</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc1 = bias_variable([<span class="hljs-number">500</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_pool3_flat = tf.reshape(h_pool3, [-<span class="hljs-number">1</span>, <span class="hljs-number">11</span> * <span class="hljs-number">11</span> * <span class="hljs-number">128</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc2 = weight_variable([<span class="hljs-number">500</span>, <span class="hljs-number">500</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc2 = bias_variable([<span class="hljs-number">500</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; W_fc3 = weight_variable([<span class="hljs-number">500</span>, <span class="hljs-number">30</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; b_fc3 = bias_variable([<span class="hljs-number">30</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_conv = tf.matmul(h_fc2_drop, W_fc3) + b_fc3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; rmse = tf.sqrt(tf.reduce_mean(tf.square(y_ - y_conv)))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> y_conv, rmse</code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
     <li>26</li>
     <li>27</li>
     <li>28</li>
     <li>29</li>
     <li>30</li>
     <li>31</li>
     <li>32</li>
     <li>33</li>
     <li>34</li>
     <li>35</li>
     <li>36</li>
     <li>37</li>
     <li>38</li>
     <li>39</li>
     <li>40</li>
     <li>41</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <h3 id="训练卷积神经网络"><a></a><a target="_blank"></a>训练卷积神经网络</h3>
   <h4 id="读取训练数据">读取训练数据</h4>
   <p>定义好卷积神经网络的结构之后，就要开始训练。训练首先是要读取训练样本。下面的代码用于读取样本。</p>
   <pre class="prettyprint"><code class="language-python hljs&nbsp; has-numbering">&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np&nbsp;&nbsp;&nbsp; TRAIN_FILE = <span class="hljs-string">'training.csv'</span>&nbsp;&nbsp;&nbsp; TEST_FILE = <span class="hljs-string">'test.csv'</span>&nbsp;&nbsp;&nbsp; SAVE_PATH = <span class="hljs-string">'model'</span>&nbsp;&nbsp;&nbsp; VALIDATION_SIZE = <span class="hljs-number">100</span>&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#验证集大小</span>&nbsp;&nbsp;&nbsp; EPOCHS = <span class="hljs-number">100</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#迭代次数</span>&nbsp;&nbsp;&nbsp; BATCH_SIZE = <span class="hljs-number">64</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#每个batch大小，稍微大一点的batch会更稳定</span>&nbsp;&nbsp;&nbsp; EARLY_STOP_PATIENCE = <span class="hljs-number">10</span> <span class="hljs-comment">#控制early stopping的参数</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">input_data</span><span class="hljs-params">(test=False)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; file_name = TEST_FILE <span class="hljs-keyword">if</span> test <span class="hljs-keyword">else</span> TRAIN_FILE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df = pd.read_csv(file_name)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cols = df.columns[:-<span class="hljs-number">1</span>]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#dropna()是丢弃有缺失数据的样本，这样最后7000多个样本只剩2140个可用的。</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df = df.dropna()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df[<span class="hljs-string">'Image'</span>] = df[<span class="hljs-string">'Image'</span>].apply(<span class="hljs-keyword">lambda</span> img: np.fromstring(img, sep=<span class="hljs-string">' '</span>) / <span class="hljs-number">255.0</span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X = np.vstack(df[<span class="hljs-string">'Image'</span>])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X = X.reshape((-<span class="hljs-number">1</span>,<span class="hljs-number">96</span>,<span class="hljs-number">96</span>,<span class="hljs-number">1</span>))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">if</span> test:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = <span class="hljs-keyword">None</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">else</span>:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = df[cols].values / <span class="hljs-number">96.0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#将y值缩放到[0,1]区间</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> X, y&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#最后生成提交结果的时候要用到</span>&nbsp;&nbsp;&nbsp; keypoint_index = {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'left_eye_center_x'</span>:<span class="hljs-number">0</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'left_eye_center_y'</span>:<span class="hljs-number">1</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'right_eye_center_x'</span>:<span class="hljs-number">2</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'right_eye_center_y'</span>:<span class="hljs-number">3</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'left_eye_inner_corner_x'</span>:<span class="hljs-number">4</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'left_eye_inner_corner_y'</span>:<span class="hljs-number">5</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'left_eye_outer_corner_x'</span>:<span class="hljs-number">6</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'left_eye_outer_corner_y'</span>:<span class="hljs-number">7</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'right_eye_inner_corner_x'</span>:<span class="hljs-number">8</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'right_eye_inner_corner_y'</span>:<span class="hljs-number">9</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'right_eye_outer_corner_x'</span>:<span class="hljs-number">10</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'right_eye_outer_corner_y'</span>:<span class="hljs-number">11</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'left_eyebrow_inner_end_x'</span>:<span class="hljs-number">12</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'left_eyebrow_inner_end_y'</span>:<span class="hljs-number">13</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'left_eyebrow_outer_end_x'</span>:<span class="hljs-number">14</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'left_eyebrow_outer_end_y'</span>:<span class="hljs-number">15</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'right_eyebrow_inner_end_x'</span>:<span class="hljs-number">16</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'right_eyebrow_inner_end_y'</span>:<span class="hljs-number">17</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'right_eyebrow_outer_end_x'</span>:<span class="hljs-number">18</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'right_eyebrow_outer_end_y'</span>:<span class="hljs-number">19</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'nose_tip_x'</span>:<span class="hljs-number">20</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'nose_tip_y'</span>:<span class="hljs-number">21</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'mouth_left_corner_x'</span>:<span class="hljs-number">22</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'mouth_left_corner_y'</span>:<span class="hljs-number">23</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'mouth_right_corner_x'</span>:<span class="hljs-number">24</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'mouth_right_corner_y'</span>:<span class="hljs-number">25</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'mouth_center_top_lip_x'</span>:<span class="hljs-number">26</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'mouth_center_top_lip_y'</span>:<span class="hljs-number">27</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'mouth_center_bottom_lip_x'</span>:<span class="hljs-number">28</span>,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-string">'mouth_center_bottom_lip_y'</span>:<span class="hljs-number">29</span>&nbsp;&nbsp;&nbsp; }</code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
     <li>26</li>
     <li>27</li>
     <li>28</li>
     <li>29</li>
     <li>30</li>
     <li>31</li>
     <li>32</li>
     <li>33</li>
     <li>34</li>
     <li>35</li>
     <li>36</li>
     <li>37</li>
     <li>38</li>
     <li>39</li>
     <li>40</li>
     <li>41</li>
     <li>42</li>
     <li>43</li>
     <li>44</li>
     <li>45</li>
     <li>46</li>
     <li>47</li>
     <li>48</li>
     <li>49</li>
     <li>50</li>
     <li>51</li>
     <li>52</li>
     <li>53</li>
     <li>54</li>
     <li>55</li>
     <li>56</li>
     <li>57</li>
     <li>58</li>
     <li>59</li>
     <li>60</li>
     <li>61</li>
     <li>62</li>
     <li>63</li>
     <li>64</li>
     <li>65</li>
     <li>66</li>
     <li>67</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <h4 id="开始训练">开始训练</h4>
   <p>执行训练的代码如下，<code>save_model</code>用于保存当前训练得到在验证集上loss最小的模型，方便以后直接拿来用。</p>
   <p><code>tf.InteractiveSession()</code>用来生成一个Session，(好像是废话…)。Session相当于一个引擎，TensorFlow框架要真正的进行计算，都要通过Session引擎来启动。</p>
   <p><code>tf.train.AdamOptimizer</code>是优化的算法，Adam的收敛速度会比较快,<code>1e-3</code>是learning rate,这里先简单的用固定的。minimize就是要最小化的目标，当然是最小化均方根误差了。</p>
   <pre class="prettyprint"><code class="language-python hljs&nbsp; has-numbering">&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save_model</span><span class="hljs-params">(saver,sess,save_path)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; path = saver.save(sess, save_path)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">'model save in :{0}'</span>.format(path)&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess = tf.InteractiveSession()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_conv, rmse = model()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_step = tf.train.AdamOptimizer(<span class="hljs-number">1e-3</span>).minimize(rmse)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#变量都要初始化 </span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess.run(tf.initialize_all_variables())&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X,y = input_data()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_valid, y_valid = X[:VALIDATION_SIZE], y[:VALIDATION_SIZE]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X[VALIDATION_SIZE:], y[VALIDATION_SIZE:]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; best_validation_loss = <span class="hljs-number">1000000.0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; current_epoch = <span class="hljs-number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TRAIN_SIZE = X_train.shape[<span class="hljs-number">0</span>]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_index = range(TRAIN_SIZE)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; random.shuffle(train_index)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X_train[train_index], y_train[train_index]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; saver = tf.train.Saver()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">'begin training..., train dataset size:{0}'</span>.format(TRAIN_SIZE)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> xrange(EPOCHS):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; random.shuffle(train_index)&nbsp; <span class="hljs-comment">#每个epoch都shuffle一下效果更好</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; X_train, y_train = X_train[train_index], y_train[train_index]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">0</span>,TRAIN_SIZE,BATCH_SIZE):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">'epoch {0}, train {1} samples done...'</span>.format(i,j)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_step.run(feed_dict={x:X_train[j:j+BATCH_SIZE],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_:y_train[j:j+BATCH_SIZE], keep_prob:<span class="hljs-number">0.5</span>})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#电脑太渣，用所有训练样本计算train_loss居然死机，只好注释了。</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#train_loss = rmse.eval(feed_dict={x:X_train, y_:y_train, keep_prob: 1.0})</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; validation_loss = rmse.eval(feed_dict={x:X_valid, y_:y_valid, keep_prob: <span class="hljs-number">1.0</span>})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">'epoch {0} done! validation loss:{1}'</span>.format(i, validation_loss*<span class="hljs-number">96.0</span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">if</span> validation_loss &lt; best_validation_loss:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; best_validation_loss = validation_loss&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; current_epoch = i&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; save_model(saver,sess,SAVE_PATH)&nbsp;&nbsp; <span class="hljs-comment">#即时保存最好的结果</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">elif</span> (i - current_epoch) &gt;= EARLY_STOP_PATIENCE:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">'early stopping'</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">break</span></code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
     <li>26</li>
     <li>27</li>
     <li>28</li>
     <li>29</li>
     <li>30</li>
     <li>31</li>
     <li>32</li>
     <li>33</li>
     <li>34</li>
     <li>35</li>
     <li>36</li>
     <li>37</li>
     <li>38</li>
     <li>39</li>
     <li>40</li>
     <li>41</li>
     <li>42</li>
     <li>43</li>
     <li>44</li>
     <li>45</li>
     <li>46</li>
     <li>47</li>
     <li>48</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <h3 id="在测试集上预测"><a></a><a target="_blank"></a>在测试集上预测</h3>
   <p>下面的代码用于预测<code>test.csv</code>里面的人脸关键点，最后的y值要乘以96，因为之前缩放到[0,1]区间了。</p>
   <pre class="prettyprint"><code class="language-python hljs&nbsp; has-numbering">&nbsp;&nbsp;&nbsp; X,y = input_data(test=<span class="hljs-keyword">True</span>)&nbsp;&nbsp;&nbsp; y_pred = []&nbsp;&nbsp;&nbsp; TEST_SIZE = X.shape[<span class="hljs-number">0</span>]&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">0</span>,TEST_SIZE,BATCH_SIZE):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_batch = y_conv.eval(feed_dict={x:X[j:j+BATCH_SIZE], keep_prob:<span class="hljs-number">1.0</span>})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y_pred.extend(y_batch)&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">'predict test image done!'</span>&nbsp;&nbsp;&nbsp; output_file = open(<span class="hljs-string">'submit.csv'</span>,<span class="hljs-string">'w'</span>)&nbsp;&nbsp;&nbsp; output_file.write(<span class="hljs-string">'RowId,Location\n'</span>)&nbsp;&nbsp;&nbsp; IdLookupTable = open(<span class="hljs-string">'IdLookupTable.csv'</span>)&nbsp;&nbsp;&nbsp; IdLookupTable.readline()&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> IdLookupTable:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RowId,ImageId,FeatureName = line.rstrip().split(<span class="hljs-string">','</span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; image_index = int(ImageId) - <span class="hljs-number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; feature_index = keypoint_index[FeatureName]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; feature_location = y_pred[image_index][feature_index] * <span class="hljs-number">96</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; output_file.write(<span class="hljs-string">'{0},{1}\n'</span>.format(RowId,feature_location))&nbsp;&nbsp;&nbsp; output_file.close()&nbsp;&nbsp;&nbsp; IdLookupTable.close()</code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <h3 id="结果"><a></a><a target="_blank"></a>结果</h3>
   <p>用这个结构的卷积神经网络训练出来的模型，在测试集上预测的结果提交以后的成绩是3.4144，在kaggle的leaderboard上是41名，初试CNN，感觉还可以了。这只是数据，还是找一些现实的照片来试试这个模型如何，所以我找了一张anglababy的，标识出来的关键点感觉还算靠谱。基于TensorFlow的卷积神经网络先写到这了，有什么遗漏的想起来再补充，之后对深度学习更了解了，再写写CNN的原理，bp的推导过程之类的。</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20160316222058610"></p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
