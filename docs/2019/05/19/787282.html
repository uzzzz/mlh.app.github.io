<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>前向传播算法 Forward propagation 与反向传播算法 Back propagation | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="前向传播算法 Forward propagation 与反向传播算法 Back propagation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 虽然学深度学习有一段时间了，但是对于一些算法的具体实现还是模糊不清，用了很久也不是很了解。因此特意先对深度学习中的相关基础概念做一下总结。先看看前向传播算法(Forward propagation)与反向传播算法(Back propagation)。 1.前向传播 如图所示，这里讲得已经很清楚了，前向传播的思想比较简单。&nbsp; 举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。&nbsp; 最终不断的通过这种方法一层层的运算，得到输出层结果。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 对于前向传播来说，不管维度多高，其过程都可以用如下公式表示： 的值。 大家也许已经注意到，这样做是十分冗余的，因为很多路径被重复访问了。比如上图中，a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。 同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。 正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的。 从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放”些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以”层”为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。 以上图为例，节点c接受e发送的1*2并堆放起来，节点d接受e发送的1*3并堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送2*1并对堆放起来，节点c向b发送2*1并堆放起来，节点d向b发送3*1并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为2*1+3*1=5, 即顶点e对b的偏导数为5. 举个不太恰当的例子，如果把上图中的箭头表示欠钱的关系，即c→e表示e欠c的钱。以a, b为例，直接计算e对它们俩的偏导相当于a, b各自去讨薪。a向c讨薪，c说e欠我钱，你向他要。于是a又跨过c去找e。b先向c讨薪，同样又转向e，b又向d讨薪，再次转向e。可以看到，追款之路，充满艰辛，而且还有重复，即a, b 都从c转向e。 而BP算法就是主动还款。e把所欠之钱还给c，d。c，d收到钱，乐呵地把钱转发给了a，b，皆大欢喜。 3.反向传播具体计算过程推导 为了方便起见，这里我定义了三层网络，输入层（第0层），隐藏层（第1层），输出层（第二层）。并且每个结点没有偏置（有偏置原理完全一样），激活函数为sigmod函数（不同的激活函数，求导不同），符号说明如下： 对应网络如下： 其中对应的矩阵表示如下 首先我们先走一遍正向传播，公式与相应的数据对应如下： 那么： 同理可以得到： 那么最终的损失为 ，我们当然是希望这个值越小越好。这也是我们为什么要进行训练，调节参数，使得最终的损失最小。这就用到了我们的反向传播算法，实际上反向传播就是梯度下降法中链式法则的使用。 下面我们看如何反向传播 根据公式，我们有： 这个时候我们需要求出C对w的偏导，则根据链式法则有： 同理有： 到此我们已经算出了最后一层的参数偏导了.我们继续往前面链式推导： 我们现在还需要求 ，下面给出一个推导其它全都类似 同理可得其它几个式子： 则最终的结果为： 再按照这个权重参数进行一遍正向传播得出来的Error为0.165 而这个值比原来的0.19要小，则继续迭代，不断修正权值，使得代价函数越来越小，预测值不断逼近0.5.我迭代了100次的结果，Error为5.92944818e-07（已经很小了，说明预测值与真实值非常接近了），最后的权值为： bp过程可能差不多就是这样了，可能此文需要你以前接触过bp算法，只是还有疑惑，一步步推导后，会有较深的理解。 上面的python代码实现如下： #!/usr/bin/env python#coding:utf-8import numpy as npdef nonlin(x, deriv = False):&nbsp;&nbsp;&nbsp; if(deriv == True):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return x * (1 - x)&nbsp;&nbsp;&nbsp; return 1 / (1 + np.exp(-x))X = np.array([[0.35], [0.9]])y = np.array([[0.5]])np.random.seed(1)W0 = np.array([[0.1, 0.8], [0.4, 0.6]])W1 = np.array([[0.3, 0.9]])print &#39;original &#39;, W0, &#39;\n&#39;, W1for j in xrange(100):&nbsp;&nbsp;&nbsp; l0 = X&nbsp;&nbsp;&nbsp; l1 = nonlin(np.dot(W0, l0))&nbsp;&nbsp;&nbsp; l2 = nonlin(np.dot(W1, l1))&nbsp;&nbsp;&nbsp; l2_error = y - l2&nbsp;&nbsp;&nbsp; Error = 1 / 2.0 * (y-l2)**2&nbsp;&nbsp;&nbsp; print &#39;Error:&#39;, Error&nbsp;&nbsp;&nbsp; l2_delta = l2_error * nonlin(l2, deriv=True)&nbsp;&nbsp;&nbsp; l1_error = l2_delta * W1 #back propagation&nbsp;&nbsp;&nbsp; l1_delta = l1_error * nonlin(l1, deriv=True)&nbsp;&nbsp;&nbsp; W1 += l2_delta * l1.T&nbsp;&nbsp;&nbsp; W0 += l0.T.dot(l1_delta)&nbsp;&nbsp;&nbsp; print W0, &#39;\n&#39;, W1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 虽然学深度学习有一段时间了，但是对于一些算法的具体实现还是模糊不清，用了很久也不是很了解。因此特意先对深度学习中的相关基础概念做一下总结。先看看前向传播算法(Forward propagation)与反向传播算法(Back propagation)。 1.前向传播 如图所示，这里讲得已经很清楚了，前向传播的思想比较简单。&nbsp; 举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。&nbsp; 最终不断的通过这种方法一层层的运算，得到输出层结果。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 对于前向传播来说，不管维度多高，其过程都可以用如下公式表示： 的值。 大家也许已经注意到，这样做是十分冗余的，因为很多路径被重复访问了。比如上图中，a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。 同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。 正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的。 从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放”些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以”层”为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。 以上图为例，节点c接受e发送的1*2并堆放起来，节点d接受e发送的1*3并堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送2*1并对堆放起来，节点c向b发送2*1并堆放起来，节点d向b发送3*1并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为2*1+3*1=5, 即顶点e对b的偏导数为5. 举个不太恰当的例子，如果把上图中的箭头表示欠钱的关系，即c→e表示e欠c的钱。以a, b为例，直接计算e对它们俩的偏导相当于a, b各自去讨薪。a向c讨薪，c说e欠我钱，你向他要。于是a又跨过c去找e。b先向c讨薪，同样又转向e，b又向d讨薪，再次转向e。可以看到，追款之路，充满艰辛，而且还有重复，即a, b 都从c转向e。 而BP算法就是主动还款。e把所欠之钱还给c，d。c，d收到钱，乐呵地把钱转发给了a，b，皆大欢喜。 3.反向传播具体计算过程推导 为了方便起见，这里我定义了三层网络，输入层（第0层），隐藏层（第1层），输出层（第二层）。并且每个结点没有偏置（有偏置原理完全一样），激活函数为sigmod函数（不同的激活函数，求导不同），符号说明如下： 对应网络如下： 其中对应的矩阵表示如下 首先我们先走一遍正向传播，公式与相应的数据对应如下： 那么： 同理可以得到： 那么最终的损失为 ，我们当然是希望这个值越小越好。这也是我们为什么要进行训练，调节参数，使得最终的损失最小。这就用到了我们的反向传播算法，实际上反向传播就是梯度下降法中链式法则的使用。 下面我们看如何反向传播 根据公式，我们有： 这个时候我们需要求出C对w的偏导，则根据链式法则有： 同理有： 到此我们已经算出了最后一层的参数偏导了.我们继续往前面链式推导： 我们现在还需要求 ，下面给出一个推导其它全都类似 同理可得其它几个式子： 则最终的结果为： 再按照这个权重参数进行一遍正向传播得出来的Error为0.165 而这个值比原来的0.19要小，则继续迭代，不断修正权值，使得代价函数越来越小，预测值不断逼近0.5.我迭代了100次的结果，Error为5.92944818e-07（已经很小了，说明预测值与真实值非常接近了），最后的权值为： bp过程可能差不多就是这样了，可能此文需要你以前接触过bp算法，只是还有疑惑，一步步推导后，会有较深的理解。 上面的python代码实现如下： #!/usr/bin/env python#coding:utf-8import numpy as npdef nonlin(x, deriv = False):&nbsp;&nbsp;&nbsp; if(deriv == True):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return x * (1 - x)&nbsp;&nbsp;&nbsp; return 1 / (1 + np.exp(-x))X = np.array([[0.35], [0.9]])y = np.array([[0.5]])np.random.seed(1)W0 = np.array([[0.1, 0.8], [0.4, 0.6]])W1 = np.array([[0.3, 0.9]])print &#39;original &#39;, W0, &#39;\n&#39;, W1for j in xrange(100):&nbsp;&nbsp;&nbsp; l0 = X&nbsp;&nbsp;&nbsp; l1 = nonlin(np.dot(W0, l0))&nbsp;&nbsp;&nbsp; l2 = nonlin(np.dot(W1, l1))&nbsp;&nbsp;&nbsp; l2_error = y - l2&nbsp;&nbsp;&nbsp; Error = 1 / 2.0 * (y-l2)**2&nbsp;&nbsp;&nbsp; print &#39;Error:&#39;, Error&nbsp;&nbsp;&nbsp; l2_delta = l2_error * nonlin(l2, deriv=True)&nbsp;&nbsp;&nbsp; l1_error = l2_delta * W1 #back propagation&nbsp;&nbsp;&nbsp; l1_delta = l1_error * nonlin(l1, deriv=True)&nbsp;&nbsp;&nbsp; W1 += l2_delta * l1.T&nbsp;&nbsp;&nbsp; W0 += l0.T.dot(l1_delta)&nbsp;&nbsp;&nbsp; print W0, &#39;\n&#39;, W1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/19/787282.html" />
<meta property="og:url" content="https://mlh.app/2019/05/19/787282.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-19T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 虽然学深度学习有一段时间了，但是对于一些算法的具体实现还是模糊不清，用了很久也不是很了解。因此特意先对深度学习中的相关基础概念做一下总结。先看看前向传播算法(Forward propagation)与反向传播算法(Back propagation)。 1.前向传播 如图所示，这里讲得已经很清楚了，前向传播的思想比较简单。&nbsp; 举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。&nbsp; 最终不断的通过这种方法一层层的运算，得到输出层结果。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 对于前向传播来说，不管维度多高，其过程都可以用如下公式表示： 的值。 大家也许已经注意到，这样做是十分冗余的，因为很多路径被重复访问了。比如上图中，a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。 同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。 正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的。 从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放”些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以”层”为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。 以上图为例，节点c接受e发送的1*2并堆放起来，节点d接受e发送的1*3并堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送2*1并对堆放起来，节点c向b发送2*1并堆放起来，节点d向b发送3*1并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为2*1+3*1=5, 即顶点e对b的偏导数为5. 举个不太恰当的例子，如果把上图中的箭头表示欠钱的关系，即c→e表示e欠c的钱。以a, b为例，直接计算e对它们俩的偏导相当于a, b各自去讨薪。a向c讨薪，c说e欠我钱，你向他要。于是a又跨过c去找e。b先向c讨薪，同样又转向e，b又向d讨薪，再次转向e。可以看到，追款之路，充满艰辛，而且还有重复，即a, b 都从c转向e。 而BP算法就是主动还款。e把所欠之钱还给c，d。c，d收到钱，乐呵地把钱转发给了a，b，皆大欢喜。 3.反向传播具体计算过程推导 为了方便起见，这里我定义了三层网络，输入层（第0层），隐藏层（第1层），输出层（第二层）。并且每个结点没有偏置（有偏置原理完全一样），激活函数为sigmod函数（不同的激活函数，求导不同），符号说明如下： 对应网络如下： 其中对应的矩阵表示如下 首先我们先走一遍正向传播，公式与相应的数据对应如下： 那么： 同理可以得到： 那么最终的损失为 ，我们当然是希望这个值越小越好。这也是我们为什么要进行训练，调节参数，使得最终的损失最小。这就用到了我们的反向传播算法，实际上反向传播就是梯度下降法中链式法则的使用。 下面我们看如何反向传播 根据公式，我们有： 这个时候我们需要求出C对w的偏导，则根据链式法则有： 同理有： 到此我们已经算出了最后一层的参数偏导了.我们继续往前面链式推导： 我们现在还需要求 ，下面给出一个推导其它全都类似 同理可得其它几个式子： 则最终的结果为： 再按照这个权重参数进行一遍正向传播得出来的Error为0.165 而这个值比原来的0.19要小，则继续迭代，不断修正权值，使得代价函数越来越小，预测值不断逼近0.5.我迭代了100次的结果，Error为5.92944818e-07（已经很小了，说明预测值与真实值非常接近了），最后的权值为： bp过程可能差不多就是这样了，可能此文需要你以前接触过bp算法，只是还有疑惑，一步步推导后，会有较深的理解。 上面的python代码实现如下： #!/usr/bin/env python#coding:utf-8import numpy as npdef nonlin(x, deriv = False):&nbsp;&nbsp;&nbsp; if(deriv == True):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return x * (1 - x)&nbsp;&nbsp;&nbsp; return 1 / (1 + np.exp(-x))X = np.array([[0.35], [0.9]])y = np.array([[0.5]])np.random.seed(1)W0 = np.array([[0.1, 0.8], [0.4, 0.6]])W1 = np.array([[0.3, 0.9]])print &#39;original &#39;, W0, &#39;\\n&#39;, W1for j in xrange(100):&nbsp;&nbsp;&nbsp; l0 = X&nbsp;&nbsp;&nbsp; l1 = nonlin(np.dot(W0, l0))&nbsp;&nbsp;&nbsp; l2 = nonlin(np.dot(W1, l1))&nbsp;&nbsp;&nbsp; l2_error = y - l2&nbsp;&nbsp;&nbsp; Error = 1 / 2.0 * (y-l2)**2&nbsp;&nbsp;&nbsp; print &#39;Error:&#39;, Error&nbsp;&nbsp;&nbsp; l2_delta = l2_error * nonlin(l2, deriv=True)&nbsp;&nbsp;&nbsp; l1_error = l2_delta * W1 #back propagation&nbsp;&nbsp;&nbsp; l1_delta = l1_error * nonlin(l1, deriv=True)&nbsp;&nbsp;&nbsp; W1 += l2_delta * l1.T&nbsp;&nbsp;&nbsp; W0 += l0.T.dot(l1_delta)&nbsp;&nbsp;&nbsp; print W0, &#39;\\n&#39;, W1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/19/787282.html","headline":"前向传播算法 Forward propagation 与反向传播算法 Back propagation","dateModified":"2019-05-19T00:00:00+08:00","datePublished":"2019-05-19T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/19/787282.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>前向传播算法 Forward propagation 与反向传播算法 Back propagation</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <div class="markdown_views" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
   <div class="markdown_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <!-- flowchart &amp;#31661;&amp;#22836;&amp;#22270;&amp;#26631; &amp;#21247;&amp;#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <p>虽然学深度学习有一段时间了，但是对于一些算法的具体实现还是模糊不清，用了很久也不是很了解。因此特意先对深度学习中的相关基础概念做一下总结。先看看前向传播算法(Forward propagation)与反向传播算法(Back propagation)。</p>
    <h2 id="1前向传播"><a></a><a target="_blank"></a>1.前向传播</h2>
    <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216112534046?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
    <p>如图所示，这里讲得已经很清楚了，前向传播的思想比较简单。&nbsp; <br>举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。&nbsp; <br>最终不断的通过这种方法一层层的运算，得到输出层结果。</p>
   </div>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <p>对于前向传播来说，不管维度多高，其过程都可以用如下公式表示： <br><span class="MathJax_Preview"></span></p>
   <div class="MathJax_Display"></div>
   <span class="MathJax_Preview"></span>
   <div class="MathJax_Display"></div>的值。
   <p>大家也许已经注意到，这样做是十分冗余的，因为很多路径被重复访问了。比如上图中，a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。</p>
   <p>同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。 <br>正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的。</p>
   <p>从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放”些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以”层”为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。</p>
   <p>以上图为例，节点c接受e发送的1*2并堆放起来，节点d接受e发送的1*3并堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送2*1并对堆放起来，节点c向b发送2*1并堆放起来，节点d向b发送3*1并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为2*1+3*1=5, 即顶点e对b的偏导数为5.</p>
   <p>举个不太恰当的例子，如果把上图中的箭头表示欠钱的关系，即c→e表示e欠c的钱。以a, b为例，直接计算e对它们俩的偏导相当于a, b各自去讨薪。a向c讨薪，c说e欠我钱，你向他要。于是a又跨过c去找e。b先向c讨薪，同样又转向e，b又向d讨薪，再次转向e。可以看到，追款之路，充满艰辛，而且还有重复，即a, b 都从c转向e。</p>
   <p>而BP算法就是主动还款。e把所欠之钱还给c，d。c，d收到钱，乐呵地把钱转发给了a，b，皆大欢喜。</p>
   <h2 id="3反向传播具体计算过程推导"><a></a><a target="_blank"></a>3.反向传播具体计算过程推导</h2>
   <p>为了方便起见，这里我定义了三层网络，输入层（第0层），隐藏层（第1层），输出层（第二层）。并且每个结点没有偏置（有偏置原理完全一样），激活函数为sigmod函数（不同的激活函数，求导不同），符号说明如下： <br><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134213577?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>对应网络如下：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134239053?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>其中对应的矩阵表示如下</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134307332?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>首先我们先走一遍正向传播，公式与相应的数据对应如下：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134333133?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>那么：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134357804?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>同理可以得到：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134439370?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>那么最终的损失为</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134503371?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>，我们当然是希望这个值越小越好。这也是我们为什么要进行训练，调节参数，使得最终的损失最小。这就用到了我们的反向传播算法，实际上反向传播就是梯度下降法中链式法则的使用。</p>
   <p>下面我们看如何反向传播</p>
   <p>根据公式，我们有：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134649167?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>这个时候我们需要求出C对w的偏导，则根据链式法则有：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134714473?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>同理有：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134735675?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>到此我们已经算出了最后一层的参数偏导了.我们继续往前面链式推导：</p>
   <p>我们现在还需要求</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134804098?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>，下面给出一个推导其它全都类似</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134836373?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>同理可得其它几个式子：</p>
   <p>则最终的结果为：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134903785?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>再按照这个权重参数进行一遍正向传播得出来的Error为0.165</p>
   <p>而这个值比原来的0.19要小，则继续迭代，不断修正权值，使得代价函数越来越小，预测值不断逼近0.5.我迭代了100次的结果，Error为5.92944818e-07（已经很小了，说明预测值与真实值非常接近了），最后的权值为：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20171216134952597?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYml0Y2FybWFubGVl/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></p>
   <p>bp过程可能差不多就是这样了，可能此文需要你以前接触过bp算法，只是还有疑惑，一步步推导后，会有较深的理解。</p>
   <p>上面的python代码实现如下：</p>
   <pre class="prettyprint"><code class="hljs lua has-numbering">#!/usr/bin/env python#coding:utf-<span class="hljs-number">8</span>import numpy as npdef nonlin(x, deriv = False):&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">if</span>(deriv == True):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> - x)&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))X = np.array(<span class="hljs-string">[[0.35], [0.9]]</span>)y = np.array(<span class="hljs-string">[[0.5]]</span>)np.random.seed(<span class="hljs-number">1</span>)W0 = np.array(<span class="hljs-string">[[0.1, 0.8], [0.4, 0.6]]</span>)W1 = np.array(<span class="hljs-string">[[0.3, 0.9]]</span>)<span class="hljs-built_in">print</span> <span class="hljs-string">'original '</span>, W0, <span class="hljs-string">'\n'</span>, W1<span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">100</span>):&nbsp;&nbsp;&nbsp; l0 = X&nbsp;&nbsp;&nbsp; l1 = nonlin(np.dot(W0, l0))&nbsp;&nbsp;&nbsp; l2 = nonlin(np.dot(W1, l1))&nbsp;&nbsp;&nbsp; l2_error = y - l2&nbsp;&nbsp;&nbsp; Error = <span class="hljs-number">1</span> / <span class="hljs-number">2.0</span> * (y-l2)**<span class="hljs-number">2</span>&nbsp;&nbsp;&nbsp; <span class="hljs-built_in">print</span> <span class="hljs-string">'Error:'</span>, Error&nbsp;&nbsp;&nbsp; l2_delta = l2_error * nonlin(l2, deriv=True)&nbsp;&nbsp;&nbsp; l1_error = l2_delta * W1 #back propagation&nbsp;&nbsp;&nbsp; l1_delta = l1_error * nonlin(l1, deriv=True)&nbsp;&nbsp;&nbsp; W1 += l2_delta * l1.T&nbsp;&nbsp;&nbsp; W0 += l0.T.dot(l1_delta)&nbsp;&nbsp;&nbsp; <span class="hljs-built_in">print</span> W0, <span class="hljs-string">'\n'</span>, W1</code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
     <li>26</li>
     <li>27</li>
     <li>28</li>
     <li>29</li>
     <li>30</li>
     <li>31</li>
     <li>32</li>
     <li>33</li>
     <li>34</li>
     <li>35</li>
     <li>36</li>
     <li>37</li>
     <li>38</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
