<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>深度学习（DL） 卷积神经网络（CNN） 从原理到实现 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="深度学习（DL） 卷积神经网络（CNN） 从原理到实现" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 序 深度学习现在大火，虽然自己上过深度学习课程、用过keras做过一些实验，始终觉得理解不透彻。最近仔细学习前辈和学者的著作，感谢他们的无私奉献，整理得到本文，共勉。 1.前言 （1）神经网络的缺陷 在神经网络一文中简单介绍了其原理，可以发现不同层之间是全连接的，当神经网络的深度、节点数变大，会导致过拟合、参数过多等问题。 （2）计算机视觉（图像）背景 通过抽取只依赖图像里小的子区域的局部特征，然后利用这些特征的信息就可以融合到后续处理阶段中，从而检测更高级的特征，最后产生图像整体的信息。 距离较近的像素的相关性要远大于距离较远像素的相关性。 对于图像的一个区域有用的局部特征可能对于图像的其他区域也有用，例如感兴趣的物体发生平移的情形。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 2.卷积神经网络（CNN）特性 根据前言中的两方面，这里介绍卷积神经网络的两个特性。 （1）局部感知 图1：全连接网络。如果L1层有1000×1000像素的图像，L2层有1000,000个隐层神经元，每个隐层神经元都连接L1层图像的每一个像素点，就有1000x1000x1000,000=10^12个连接，也就是10^12个权值参数。 图2：局部连接网络。L2层每一个节点与L1层节点同位置附近10×10的窗口相连接，则1百万个隐层神经元就只有100w乘以100，即10^8个参数。其权值连接个数比原来减少了四个数量级。 （2）权值共享 就图2来说，权值共享，不是说，所有的红色线标注的连接权值相同。而是说，每一个颜色的线都有一个红色线的权值与之相等，所以第二层的每个节点，其从上一层进行卷积的参数都是相同的。 图2中隐层的每一个神经元都连接10×10个图像区域，也就是说每一个神经元存在10×10=100个连接权值参数。如果我们每个神经元这100个参数是相同的？也就是说每个神经元用的是同一个卷积核去卷积图像。这样L1层我们就只有100个参数。但是这样，只提取了图像一种特征？如果需要提取不同的特征，就加多几种卷积核。所以假设我们加到100种卷积核，也就是1万个参数。 每种卷积核的参数不一样，表示它提出输入图像的不同特征（不同的边缘）。这样每种卷积核去卷积图像就得到对图像的不同特征的放映，我们称之为Feature Map，也就是特征图。 3.网络结构 以LeCun的LeNet-5为例，不包含输入，LeNet-5共有7层，每层都包含连接权值（可训练参数）。输入图像为32*32大小。我们先要明确一点：每个层有多个特征图，每个特征图通过一种卷积滤波器提取输入的一种特征，然后每个特征图有多个神经元。 C1、C3、C5是卷积层，S2、S4、S6是下采样层。利用图像局部相关性的原理，对图像进行下抽样，可以减少数据处理量同时保留有用信息。 图3 4.前向传播 在神经网络一文中已经详细介绍过全连接和激励层的前向传播过程，这里主要介绍卷积层、下采样（池化）层。 （1）卷积层 如图4所示，输入图片是一个5×5的图片，用一个3×3的卷积核对该图片进行卷积操作。本质上是一个点积操作。举例：1×1+0×1+1×1+0×0+1×1+0×1+1×0+0×0+1×1=4 图4 def conv2(X, k):&nbsp;&nbsp;&nbsp; x_row, x_col = X.shape&nbsp;&nbsp;&nbsp; k_row, k_col = k.shape&nbsp;&nbsp;&nbsp; ret_row, ret_col = x_row - k_row + 1, x_col - k_col + 1&nbsp;&nbsp;&nbsp; ret = np.empty((ret_row, ret_col))&nbsp;&nbsp;&nbsp; for y in range(ret_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sub = X[y : y + k_row, x : x + k_col]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y,x] = np.sum(sub * k)&nbsp;&nbsp;&nbsp; return retclass ConvLayer:&nbsp;&nbsp;&nbsp; def __init__(self, in_channel, out_channel, kernel_size):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_channel, out_channel, kernel_size, kernel_size)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_channel))&nbsp;&nbsp;&nbsp; def _relu(self, x):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x[x &lt; 0] = 0&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # assume the first index is channel index&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_channel, kernel_row, kernel_col = self.w.shape[1], self.w.shape[2], self.w.shape[3]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = np.zeros((out_channel, in_row - kernel_row + 1, in_col - kernel_col + 1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for j in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] += conv2(in_data[i], self.w[i, j])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] += self.b[j]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] = self._relu(self.topval[j])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 1 （2）下采样（池化）层 下采样，即池化，目的是减小特征图，池化规模一般为2×2。常用的池化方法有： 最大池化（Max Pooling）。如图5所示。 均值池化（Mean Pooling）。如图6所示。 高斯池化。借鉴高斯模糊的方法。 可训练池化。训练函数 ff ，接受4个点为输入，输出1个点。 图5 图6 class MaxPoolingLayer:&nbsp;&nbsp;&nbsp; def __init__(self, kernel_size, name=&#39;MaxPool&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.kernel_size = kernel_size&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row = in_row / k + (1 if in_row % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_col = in_col / k + (1 if in_col % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag = np.zeros_like(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = np.empty((in_batch, in_channel, out_row, out_col))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for c in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for oy in range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for ox in range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k if (oy + 1) * k &lt;= in_row else in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k if (ox + 1) * k &lt;= in_col else in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; idx = np.argmax(in_data[b_id, c, oy * k: oy * k + height, ox * k: ox * k + width])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_r = idx / width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_c = idx % width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag[b_id, c, oy * k + offset_r, ox * k + offset_c] = 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[b_id, c, oy, ox] = in_data[b_id, c, oy * k + offset_r, ox * k + offset_c]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return ret 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 1 5.后向传播 在神经网络一文中已经详细介绍过全连接和激励层的后向传播过程，这里主要介绍卷积层、下采样（池化）层。 （1）卷积层 当一个卷积层L的下一层(L+1)为采样层，并假设我们已经计算得到了采样层的残差，现在计算该卷积层的残差。从最上面的网络结构图我们知道，采样层（L+1）的map大小是卷积层L的1/（scale*scale），以scale=2为例，但这两层的map个数是一样的，卷积层L的某个map中的4个单元与L+1层对应map的一个单元关联，可以对采样层的残差与一个scale*scale的全1矩阵进行克罗内克积 进行扩充，使得采样层的残差的维度与上一层的输出map的维度一致。 扩展过程： 图7 利用卷积计算卷积层的残差： 图8 def backward(self, residual):&nbsp;&nbsp;&nbsp; in_channel, out_channel, kernel_size = self.w.shape&nbsp;&nbsp;&nbsp; in_batch = residual.shape[0]&nbsp;&nbsp;&nbsp; # gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; self.gradient_b = residual.sum(axis=3).sum(axis=2).sum(axis=0) / self.batch_size&nbsp;&nbsp;&nbsp; # gradient_w&nbsp;&nbsp;&nbsp; self.gradient_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w[i, o] += conv2(self.bottom_val[b_id], residual[o])&nbsp;&nbsp;&nbsp; self.gradient_w /= self.batch_size&nbsp;&nbsp;&nbsp; # gradient_x&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(self.bottom_val)&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, i] += conv2(padding(residual, kernel_size - 1), rot180(self.w[i, o]))&nbsp;&nbsp;&nbsp; gradient_x /= self.batch_size&nbsp;&nbsp;&nbsp; # update&nbsp;&nbsp;&nbsp; self.prev_gradient_w = self.prev_gradient_w * self.momentum - self.gradient_w&nbsp;&nbsp;&nbsp; self.w += self.lr * self.prev_gradient_w&nbsp;&nbsp;&nbsp; self.prev_gradient_b = self.prev_gradient_b * self.momentum - self.gradient_b&nbsp;&nbsp;&nbsp; self.b += self.lr * self.prev_gradient_b&nbsp;&nbsp;&nbsp; return gradient_x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 1 （2）下采样（池化）层 当某个采样层L的下一层是卷积层(L+1)，并假设我们已经计算出L+1层的残差，现在计算L层的残差。采样层到卷积层直接的连接是有权重和偏置参数的，因此不像卷积层到采样层那样简单。现再假设L层第j个map Mj与L+1层的M2j关联，按照BP的原理，L层的残差Dj是L+1层残差D2j的加权和，但是这里的困难在于，我们很难理清M2j的那些单元通过哪些权重与Mj的哪些单元关联，这里需要两个小的变换（rot180°和padding）： rot180°：旋转：表示对矩阵进行180度旋转（可通过行对称交换和列对称交换完成） def rot180(in_data):&nbsp;&nbsp;&nbsp; ret = in_data.copy()&nbsp;&nbsp;&nbsp; yEnd = ret.shape[0] - 1&nbsp;&nbsp;&nbsp; xEnd = ret.shape[1] - 1&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[yEnd - y][x] = ret[y][x]&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y][xEnd - x] = ret[y][x]&nbsp;&nbsp;&nbsp; return ret 1 2 3 4 5 6 7 8 9 10 11 1 padding:扩充 def padding(in_data, size):&nbsp;&nbsp;&nbsp; cur_r, cur_w = in_data.shape[0], in_data.shape[1]&nbsp;&nbsp;&nbsp; new_r = cur_r + size * 2&nbsp;&nbsp;&nbsp; new_w = cur_w + size * 2&nbsp;&nbsp;&nbsp; ret = np.zeros((new_r, new_w))&nbsp;&nbsp;&nbsp; ret[size:cur_r + size, size:cur_w+size] = in_data&nbsp;&nbsp;&nbsp; return ret 1 2 3 4 5 6 7 1 图9 6.核心代码（demo版） import numpy as npimport sysdef conv2(X, k):&nbsp;&nbsp;&nbsp; # as a demo code, here we ignore the shape check&nbsp;&nbsp;&nbsp; x_row, x_col = X.shape&nbsp;&nbsp;&nbsp; k_row, k_col = k.shape&nbsp;&nbsp;&nbsp; ret_row, ret_col = x_row - k_row + 1, x_col - k_col + 1&nbsp;&nbsp;&nbsp; ret = np.empty((ret_row, ret_col))&nbsp;&nbsp;&nbsp; for y in range(ret_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sub = X[y : y + k_row, x : x + k_col]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y,x] = np.sum(sub * k)&nbsp;&nbsp;&nbsp; return retdef rot180(in_data):&nbsp;&nbsp;&nbsp; ret = in_data.copy()&nbsp;&nbsp;&nbsp; yEnd = ret.shape[0] - 1&nbsp;&nbsp;&nbsp; xEnd = ret.shape[1] - 1&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[yEnd - y][x] = ret[y][x]&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y][xEnd - x] = ret[y][x]&nbsp;&nbsp;&nbsp; return retdef padding(in_data, size):&nbsp;&nbsp;&nbsp; cur_r, cur_w = in_data.shape[0], in_data.shape[1]&nbsp;&nbsp;&nbsp; new_r = cur_r + size * 2&nbsp;&nbsp;&nbsp; new_w = cur_w + size * 2&nbsp;&nbsp;&nbsp; ret = np.zeros((new_r, new_w))&nbsp;&nbsp;&nbsp; ret[size:cur_r + size, size:cur_w+size] = in_data&nbsp;&nbsp;&nbsp; return retdef discreterize(in_data, size):&nbsp;&nbsp;&nbsp; num = in_data.shape[0]&nbsp;&nbsp;&nbsp; ret = np.zeros((num, size))&nbsp;&nbsp;&nbsp; for i, idx in enumerate(in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[i, idx] = 1&nbsp;&nbsp;&nbsp; return retclass ConvLayer:&nbsp;&nbsp;&nbsp; def __init__(self, in_channel, out_channel, kernel_size, lr=0.01, momentum=0.9, name=&#39;Conv&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_channel, out_channel, kernel_size, kernel_size)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_channel))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layer_name = name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.lr = lr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.momentum = momentum&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_b = np.zeros_like(self.b)&nbsp;&nbsp;&nbsp; # def _relu(self, x):&nbsp;&nbsp;&nbsp; #&nbsp;&nbsp;&nbsp;&nbsp; x[x &lt; 0] = 0&nbsp;&nbsp;&nbsp; #&nbsp;&nbsp;&nbsp;&nbsp; return x&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # assume the first index is channel index&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;conv forward:&#39; + str(in_data.shape)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_channel, kernel_size = self.w.shape[1], self.w.shape[2]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = np.zeros((in_batch, out_channel, in_row - kernel_size + 1, in_col - kernel_size + 1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.bottom_val = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[b_id, o] += conv2(in_data[b_id, i], self.w[i, o])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[b_id, o] += self.b[o]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_channel, out_channel, kernel_size = self.w.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch = residual.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_b = residual.sum(axis=3).sum(axis=2).sum(axis=0) / self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # gradient_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w[i, o] += conv2(self.bottom_val[b_id], residual[o])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w /= self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # gradient_x&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(self.bottom_val)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, i] += conv2(padding(residual, kernel_size - 1), rot180(self.w[i, o]))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x /= self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # update&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_w = self.prev_gradient_w * self.momentum - self.gradient_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w += self.lr * self.prev_gradient_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_b = self.prev_gradient_b * self.momentum - self.gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b += self.lr * self.prev_gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return gradient_xclass FCLayer:&nbsp;&nbsp;&nbsp; def __init__(self, in_num, out_num, lr = 0.01, momentum=0.9):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self._in_num = in_num&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self._out_num = out_num&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_num, out_num)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_num, 1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.lr = lr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.momentum = momentum&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_b = np.zeros_like(self.b)&nbsp;&nbsp;&nbsp; # def _sigmoid(self, in_data):&nbsp;&nbsp;&nbsp; #&nbsp;&nbsp;&nbsp;&nbsp; return 1 / (1 + np.exp(-in_data))&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;fc forward=&#39; + str(in_data.shape)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.topVal = np.dot(self.w.T, in_data) + self.b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.bottomVal = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.topVal&nbsp;&nbsp;&nbsp; def backward(self, loss):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; batch_size = loss.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # residual_z = loss * self.topVal * (1 - self.topVal)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; grad_w = np.dot(self.bottomVal, loss.T) / batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; grad_b = np.sum(loss) / batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_x = np.dot(self.w, loss)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_w = self.prev_grad_w * momentum - grad_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_b = self.prev_grad_b * momentum - grad_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w -= self.lr * self.prev_grad_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b -= self.lr * self.prev_grad_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return residual_xclass ReLULayer:&nbsp;&nbsp;&nbsp; def __init__(self, name=&#39;ReLU&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pass&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = in_data.copy()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[ret &lt; 0] = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return ret&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = residual.copy()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[self.top_val &lt; 0] = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return gradient_xclass MaxPoolingLayer:&nbsp;&nbsp;&nbsp; def __init__(self, kernel_size, name=&#39;MaxPool&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.kernel_size = kernel_size&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row = in_row / k + (1 if in_row % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_col = in_col / k + (1 if in_col % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag = np.zeros_like(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = np.empty((in_batch, in_channel, out_row, out_col))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for c in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for oy in range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for ox in range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k if (oy + 1) * k &lt;= in_row else in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k if (ox + 1) * k &lt;= in_col else in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; idx = np.argmax(in_data[b_id, c, oy * k: oy * k + height, ox * k: ox * k + width])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_r = idx / width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_c = idx % width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag[b_id, c, oy * k + offset_r, ox * k + offset_c] = 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[b_id, c, oy, ox] = in_data[b_id, c, oy * k + offset_r, ox * k + offset_c]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return ret&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = self.flag&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row, out_col = residual.shape[2], residual.shape[3]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(self.flag)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for c in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for oy in range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for ox in range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k if (oy + 1) * k &lt;= in_row else in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k if (ox + 1) * k &lt;= in_col else in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, c, oy * k + offset_r, ox * k + offset_c] = residual[b_id, c, oy, ox]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[self.flag == 0] = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return gradient_xclass FlattenLayer:&nbsp;&nbsp;&nbsp; def __init__(self, name=&#39;Flatten&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pass&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.in_batch, self.in_channel, self.r, self.c = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return in_data.reshape(self.in_batch, self.in_channel * self.r * self.c)&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return residual.reshape(self.in_batch, self.in_channel, self.r, self.c)class SoftmaxLayer:&nbsp;&nbsp;&nbsp; def __init__(self, name=&#39;Softmax&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pass&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; exp_out = np.exp(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = exp_out / np.sum(exp_out, axis=1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val - residualclass Net:&nbsp;&nbsp;&nbsp; def __init__(self):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layers = []&nbsp;&nbsp;&nbsp; def addLayer(self, layer):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layers.append(layer)&nbsp;&nbsp;&nbsp; def train(self, trainData, trainLabel, validData, validLabel, batch_size, iteration):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_num = trainData.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for iter in range(iteration):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;iter=&#39; + str(iter)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for batch_iter in range(0, train_num, batch_size):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if batch_iter + batch_size &lt; train_num:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.train_inner(trainData[batch_iter: batch_iter + batch_size],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; trainLabel[batch_iter: batch_iter + batch_size])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.train_inner(trainData[batch_iter: train_num],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; trainLabel[batch_iter: train_num])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &quot;eval=&quot; + str(self.eval(validData, validLabel))&nbsp;&nbsp;&nbsp; def train_inner(self, data, label):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lay_num = len(self.layers)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(lay_num):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_data = self.layers[i].forward(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = out_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_in = label&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(0, lay_num, -1):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_out = self.layers[i].backward(residual_in)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_in = residual_out&nbsp;&nbsp;&nbsp; def eval(self, data, label):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lay_num = len(self.layers)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(lay_num):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_data = self.layers[i].forward(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = out_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_idx = np.argmax(in_data, axis=1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; label_idx = np.argmax(label, axis=1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return np.sum(out_idx == label_idx) / float(out_idx.shape[0]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 序 深度学习现在大火，虽然自己上过深度学习课程、用过keras做过一些实验，始终觉得理解不透彻。最近仔细学习前辈和学者的著作，感谢他们的无私奉献，整理得到本文，共勉。 1.前言 （1）神经网络的缺陷 在神经网络一文中简单介绍了其原理，可以发现不同层之间是全连接的，当神经网络的深度、节点数变大，会导致过拟合、参数过多等问题。 （2）计算机视觉（图像）背景 通过抽取只依赖图像里小的子区域的局部特征，然后利用这些特征的信息就可以融合到后续处理阶段中，从而检测更高级的特征，最后产生图像整体的信息。 距离较近的像素的相关性要远大于距离较远像素的相关性。 对于图像的一个区域有用的局部特征可能对于图像的其他区域也有用，例如感兴趣的物体发生平移的情形。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 2.卷积神经网络（CNN）特性 根据前言中的两方面，这里介绍卷积神经网络的两个特性。 （1）局部感知 图1：全连接网络。如果L1层有1000×1000像素的图像，L2层有1000,000个隐层神经元，每个隐层神经元都连接L1层图像的每一个像素点，就有1000x1000x1000,000=10^12个连接，也就是10^12个权值参数。 图2：局部连接网络。L2层每一个节点与L1层节点同位置附近10×10的窗口相连接，则1百万个隐层神经元就只有100w乘以100，即10^8个参数。其权值连接个数比原来减少了四个数量级。 （2）权值共享 就图2来说，权值共享，不是说，所有的红色线标注的连接权值相同。而是说，每一个颜色的线都有一个红色线的权值与之相等，所以第二层的每个节点，其从上一层进行卷积的参数都是相同的。 图2中隐层的每一个神经元都连接10×10个图像区域，也就是说每一个神经元存在10×10=100个连接权值参数。如果我们每个神经元这100个参数是相同的？也就是说每个神经元用的是同一个卷积核去卷积图像。这样L1层我们就只有100个参数。但是这样，只提取了图像一种特征？如果需要提取不同的特征，就加多几种卷积核。所以假设我们加到100种卷积核，也就是1万个参数。 每种卷积核的参数不一样，表示它提出输入图像的不同特征（不同的边缘）。这样每种卷积核去卷积图像就得到对图像的不同特征的放映，我们称之为Feature Map，也就是特征图。 3.网络结构 以LeCun的LeNet-5为例，不包含输入，LeNet-5共有7层，每层都包含连接权值（可训练参数）。输入图像为32*32大小。我们先要明确一点：每个层有多个特征图，每个特征图通过一种卷积滤波器提取输入的一种特征，然后每个特征图有多个神经元。 C1、C3、C5是卷积层，S2、S4、S6是下采样层。利用图像局部相关性的原理，对图像进行下抽样，可以减少数据处理量同时保留有用信息。 图3 4.前向传播 在神经网络一文中已经详细介绍过全连接和激励层的前向传播过程，这里主要介绍卷积层、下采样（池化）层。 （1）卷积层 如图4所示，输入图片是一个5×5的图片，用一个3×3的卷积核对该图片进行卷积操作。本质上是一个点积操作。举例：1×1+0×1+1×1+0×0+1×1+0×1+1×0+0×0+1×1=4 图4 def conv2(X, k):&nbsp;&nbsp;&nbsp; x_row, x_col = X.shape&nbsp;&nbsp;&nbsp; k_row, k_col = k.shape&nbsp;&nbsp;&nbsp; ret_row, ret_col = x_row - k_row + 1, x_col - k_col + 1&nbsp;&nbsp;&nbsp; ret = np.empty((ret_row, ret_col))&nbsp;&nbsp;&nbsp; for y in range(ret_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sub = X[y : y + k_row, x : x + k_col]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y,x] = np.sum(sub * k)&nbsp;&nbsp;&nbsp; return retclass ConvLayer:&nbsp;&nbsp;&nbsp; def __init__(self, in_channel, out_channel, kernel_size):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_channel, out_channel, kernel_size, kernel_size)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_channel))&nbsp;&nbsp;&nbsp; def _relu(self, x):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x[x &lt; 0] = 0&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # assume the first index is channel index&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_channel, kernel_row, kernel_col = self.w.shape[1], self.w.shape[2], self.w.shape[3]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = np.zeros((out_channel, in_row - kernel_row + 1, in_col - kernel_col + 1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for j in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] += conv2(in_data[i], self.w[i, j])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] += self.b[j]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] = self._relu(self.topval[j])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 1 （2）下采样（池化）层 下采样，即池化，目的是减小特征图，池化规模一般为2×2。常用的池化方法有： 最大池化（Max Pooling）。如图5所示。 均值池化（Mean Pooling）。如图6所示。 高斯池化。借鉴高斯模糊的方法。 可训练池化。训练函数 ff ，接受4个点为输入，输出1个点。 图5 图6 class MaxPoolingLayer:&nbsp;&nbsp;&nbsp; def __init__(self, kernel_size, name=&#39;MaxPool&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.kernel_size = kernel_size&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row = in_row / k + (1 if in_row % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_col = in_col / k + (1 if in_col % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag = np.zeros_like(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = np.empty((in_batch, in_channel, out_row, out_col))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for c in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for oy in range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for ox in range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k if (oy + 1) * k &lt;= in_row else in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k if (ox + 1) * k &lt;= in_col else in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; idx = np.argmax(in_data[b_id, c, oy * k: oy * k + height, ox * k: ox * k + width])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_r = idx / width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_c = idx % width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag[b_id, c, oy * k + offset_r, ox * k + offset_c] = 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[b_id, c, oy, ox] = in_data[b_id, c, oy * k + offset_r, ox * k + offset_c]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return ret 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 1 5.后向传播 在神经网络一文中已经详细介绍过全连接和激励层的后向传播过程，这里主要介绍卷积层、下采样（池化）层。 （1）卷积层 当一个卷积层L的下一层(L+1)为采样层，并假设我们已经计算得到了采样层的残差，现在计算该卷积层的残差。从最上面的网络结构图我们知道，采样层（L+1）的map大小是卷积层L的1/（scale*scale），以scale=2为例，但这两层的map个数是一样的，卷积层L的某个map中的4个单元与L+1层对应map的一个单元关联，可以对采样层的残差与一个scale*scale的全1矩阵进行克罗内克积 进行扩充，使得采样层的残差的维度与上一层的输出map的维度一致。 扩展过程： 图7 利用卷积计算卷积层的残差： 图8 def backward(self, residual):&nbsp;&nbsp;&nbsp; in_channel, out_channel, kernel_size = self.w.shape&nbsp;&nbsp;&nbsp; in_batch = residual.shape[0]&nbsp;&nbsp;&nbsp; # gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; self.gradient_b = residual.sum(axis=3).sum(axis=2).sum(axis=0) / self.batch_size&nbsp;&nbsp;&nbsp; # gradient_w&nbsp;&nbsp;&nbsp; self.gradient_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w[i, o] += conv2(self.bottom_val[b_id], residual[o])&nbsp;&nbsp;&nbsp; self.gradient_w /= self.batch_size&nbsp;&nbsp;&nbsp; # gradient_x&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(self.bottom_val)&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, i] += conv2(padding(residual, kernel_size - 1), rot180(self.w[i, o]))&nbsp;&nbsp;&nbsp; gradient_x /= self.batch_size&nbsp;&nbsp;&nbsp; # update&nbsp;&nbsp;&nbsp; self.prev_gradient_w = self.prev_gradient_w * self.momentum - self.gradient_w&nbsp;&nbsp;&nbsp; self.w += self.lr * self.prev_gradient_w&nbsp;&nbsp;&nbsp; self.prev_gradient_b = self.prev_gradient_b * self.momentum - self.gradient_b&nbsp;&nbsp;&nbsp; self.b += self.lr * self.prev_gradient_b&nbsp;&nbsp;&nbsp; return gradient_x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 1 （2）下采样（池化）层 当某个采样层L的下一层是卷积层(L+1)，并假设我们已经计算出L+1层的残差，现在计算L层的残差。采样层到卷积层直接的连接是有权重和偏置参数的，因此不像卷积层到采样层那样简单。现再假设L层第j个map Mj与L+1层的M2j关联，按照BP的原理，L层的残差Dj是L+1层残差D2j的加权和，但是这里的困难在于，我们很难理清M2j的那些单元通过哪些权重与Mj的哪些单元关联，这里需要两个小的变换（rot180°和padding）： rot180°：旋转：表示对矩阵进行180度旋转（可通过行对称交换和列对称交换完成） def rot180(in_data):&nbsp;&nbsp;&nbsp; ret = in_data.copy()&nbsp;&nbsp;&nbsp; yEnd = ret.shape[0] - 1&nbsp;&nbsp;&nbsp; xEnd = ret.shape[1] - 1&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[yEnd - y][x] = ret[y][x]&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y][xEnd - x] = ret[y][x]&nbsp;&nbsp;&nbsp; return ret 1 2 3 4 5 6 7 8 9 10 11 1 padding:扩充 def padding(in_data, size):&nbsp;&nbsp;&nbsp; cur_r, cur_w = in_data.shape[0], in_data.shape[1]&nbsp;&nbsp;&nbsp; new_r = cur_r + size * 2&nbsp;&nbsp;&nbsp; new_w = cur_w + size * 2&nbsp;&nbsp;&nbsp; ret = np.zeros((new_r, new_w))&nbsp;&nbsp;&nbsp; ret[size:cur_r + size, size:cur_w+size] = in_data&nbsp;&nbsp;&nbsp; return ret 1 2 3 4 5 6 7 1 图9 6.核心代码（demo版） import numpy as npimport sysdef conv2(X, k):&nbsp;&nbsp;&nbsp; # as a demo code, here we ignore the shape check&nbsp;&nbsp;&nbsp; x_row, x_col = X.shape&nbsp;&nbsp;&nbsp; k_row, k_col = k.shape&nbsp;&nbsp;&nbsp; ret_row, ret_col = x_row - k_row + 1, x_col - k_col + 1&nbsp;&nbsp;&nbsp; ret = np.empty((ret_row, ret_col))&nbsp;&nbsp;&nbsp; for y in range(ret_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sub = X[y : y + k_row, x : x + k_col]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y,x] = np.sum(sub * k)&nbsp;&nbsp;&nbsp; return retdef rot180(in_data):&nbsp;&nbsp;&nbsp; ret = in_data.copy()&nbsp;&nbsp;&nbsp; yEnd = ret.shape[0] - 1&nbsp;&nbsp;&nbsp; xEnd = ret.shape[1] - 1&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[yEnd - y][x] = ret[y][x]&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y][xEnd - x] = ret[y][x]&nbsp;&nbsp;&nbsp; return retdef padding(in_data, size):&nbsp;&nbsp;&nbsp; cur_r, cur_w = in_data.shape[0], in_data.shape[1]&nbsp;&nbsp;&nbsp; new_r = cur_r + size * 2&nbsp;&nbsp;&nbsp; new_w = cur_w + size * 2&nbsp;&nbsp;&nbsp; ret = np.zeros((new_r, new_w))&nbsp;&nbsp;&nbsp; ret[size:cur_r + size, size:cur_w+size] = in_data&nbsp;&nbsp;&nbsp; return retdef discreterize(in_data, size):&nbsp;&nbsp;&nbsp; num = in_data.shape[0]&nbsp;&nbsp;&nbsp; ret = np.zeros((num, size))&nbsp;&nbsp;&nbsp; for i, idx in enumerate(in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[i, idx] = 1&nbsp;&nbsp;&nbsp; return retclass ConvLayer:&nbsp;&nbsp;&nbsp; def __init__(self, in_channel, out_channel, kernel_size, lr=0.01, momentum=0.9, name=&#39;Conv&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_channel, out_channel, kernel_size, kernel_size)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_channel))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layer_name = name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.lr = lr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.momentum = momentum&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_b = np.zeros_like(self.b)&nbsp;&nbsp;&nbsp; # def _relu(self, x):&nbsp;&nbsp;&nbsp; #&nbsp;&nbsp;&nbsp;&nbsp; x[x &lt; 0] = 0&nbsp;&nbsp;&nbsp; #&nbsp;&nbsp;&nbsp;&nbsp; return x&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # assume the first index is channel index&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;conv forward:&#39; + str(in_data.shape)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_channel, kernel_size = self.w.shape[1], self.w.shape[2]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = np.zeros((in_batch, out_channel, in_row - kernel_size + 1, in_col - kernel_size + 1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.bottom_val = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[b_id, o] += conv2(in_data[b_id, i], self.w[i, o])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[b_id, o] += self.b[o]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_channel, out_channel, kernel_size = self.w.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch = residual.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_b = residual.sum(axis=3).sum(axis=2).sum(axis=0) / self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # gradient_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w[i, o] += conv2(self.bottom_val[b_id], residual[o])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w /= self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # gradient_x&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(self.bottom_val)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, i] += conv2(padding(residual, kernel_size - 1), rot180(self.w[i, o]))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x /= self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # update&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_w = self.prev_gradient_w * self.momentum - self.gradient_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w += self.lr * self.prev_gradient_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_b = self.prev_gradient_b * self.momentum - self.gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b += self.lr * self.prev_gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return gradient_xclass FCLayer:&nbsp;&nbsp;&nbsp; def __init__(self, in_num, out_num, lr = 0.01, momentum=0.9):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self._in_num = in_num&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self._out_num = out_num&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_num, out_num)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_num, 1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.lr = lr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.momentum = momentum&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_b = np.zeros_like(self.b)&nbsp;&nbsp;&nbsp; # def _sigmoid(self, in_data):&nbsp;&nbsp;&nbsp; #&nbsp;&nbsp;&nbsp;&nbsp; return 1 / (1 + np.exp(-in_data))&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;fc forward=&#39; + str(in_data.shape)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.topVal = np.dot(self.w.T, in_data) + self.b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.bottomVal = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.topVal&nbsp;&nbsp;&nbsp; def backward(self, loss):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; batch_size = loss.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # residual_z = loss * self.topVal * (1 - self.topVal)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; grad_w = np.dot(self.bottomVal, loss.T) / batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; grad_b = np.sum(loss) / batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_x = np.dot(self.w, loss)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_w = self.prev_grad_w * momentum - grad_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_b = self.prev_grad_b * momentum - grad_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w -= self.lr * self.prev_grad_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b -= self.lr * self.prev_grad_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return residual_xclass ReLULayer:&nbsp;&nbsp;&nbsp; def __init__(self, name=&#39;ReLU&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pass&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = in_data.copy()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[ret &lt; 0] = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return ret&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = residual.copy()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[self.top_val &lt; 0] = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return gradient_xclass MaxPoolingLayer:&nbsp;&nbsp;&nbsp; def __init__(self, kernel_size, name=&#39;MaxPool&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.kernel_size = kernel_size&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row = in_row / k + (1 if in_row % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_col = in_col / k + (1 if in_col % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag = np.zeros_like(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = np.empty((in_batch, in_channel, out_row, out_col))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for c in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for oy in range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for ox in range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k if (oy + 1) * k &lt;= in_row else in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k if (ox + 1) * k &lt;= in_col else in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; idx = np.argmax(in_data[b_id, c, oy * k: oy * k + height, ox * k: ox * k + width])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_r = idx / width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_c = idx % width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag[b_id, c, oy * k + offset_r, ox * k + offset_c] = 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[b_id, c, oy, ox] = in_data[b_id, c, oy * k + offset_r, ox * k + offset_c]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return ret&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = self.flag&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row, out_col = residual.shape[2], residual.shape[3]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(self.flag)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for c in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for oy in range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for ox in range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k if (oy + 1) * k &lt;= in_row else in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k if (ox + 1) * k &lt;= in_col else in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, c, oy * k + offset_r, ox * k + offset_c] = residual[b_id, c, oy, ox]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[self.flag == 0] = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return gradient_xclass FlattenLayer:&nbsp;&nbsp;&nbsp; def __init__(self, name=&#39;Flatten&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pass&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.in_batch, self.in_channel, self.r, self.c = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return in_data.reshape(self.in_batch, self.in_channel * self.r * self.c)&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return residual.reshape(self.in_batch, self.in_channel, self.r, self.c)class SoftmaxLayer:&nbsp;&nbsp;&nbsp; def __init__(self, name=&#39;Softmax&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pass&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; exp_out = np.exp(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = exp_out / np.sum(exp_out, axis=1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val - residualclass Net:&nbsp;&nbsp;&nbsp; def __init__(self):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layers = []&nbsp;&nbsp;&nbsp; def addLayer(self, layer):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layers.append(layer)&nbsp;&nbsp;&nbsp; def train(self, trainData, trainLabel, validData, validLabel, batch_size, iteration):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_num = trainData.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for iter in range(iteration):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;iter=&#39; + str(iter)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for batch_iter in range(0, train_num, batch_size):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if batch_iter + batch_size &lt; train_num:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.train_inner(trainData[batch_iter: batch_iter + batch_size],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; trainLabel[batch_iter: batch_iter + batch_size])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.train_inner(trainData[batch_iter: train_num],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; trainLabel[batch_iter: train_num])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &quot;eval=&quot; + str(self.eval(validData, validLabel))&nbsp;&nbsp;&nbsp; def train_inner(self, data, label):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lay_num = len(self.layers)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(lay_num):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_data = self.layers[i].forward(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = out_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_in = label&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(0, lay_num, -1):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_out = self.layers[i].backward(residual_in)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_in = residual_out&nbsp;&nbsp;&nbsp; def eval(self, data, label):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lay_num = len(self.layers)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(lay_num):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_data = self.layers[i].forward(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = out_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_idx = np.argmax(in_data, axis=1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; label_idx = np.argmax(label, axis=1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return np.sum(out_idx == label_idx) / float(out_idx.shape[0]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/19/787279.html" />
<meta property="og:url" content="https://mlh.app/2019/05/19/787279.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-19T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 序 深度学习现在大火，虽然自己上过深度学习课程、用过keras做过一些实验，始终觉得理解不透彻。最近仔细学习前辈和学者的著作，感谢他们的无私奉献，整理得到本文，共勉。 1.前言 （1）神经网络的缺陷 在神经网络一文中简单介绍了其原理，可以发现不同层之间是全连接的，当神经网络的深度、节点数变大，会导致过拟合、参数过多等问题。 （2）计算机视觉（图像）背景 通过抽取只依赖图像里小的子区域的局部特征，然后利用这些特征的信息就可以融合到后续处理阶段中，从而检测更高级的特征，最后产生图像整体的信息。 距离较近的像素的相关性要远大于距离较远像素的相关性。 对于图像的一个区域有用的局部特征可能对于图像的其他区域也有用，例如感兴趣的物体发生平移的情形。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 2.卷积神经网络（CNN）特性 根据前言中的两方面，这里介绍卷积神经网络的两个特性。 （1）局部感知 图1：全连接网络。如果L1层有1000×1000像素的图像，L2层有1000,000个隐层神经元，每个隐层神经元都连接L1层图像的每一个像素点，就有1000x1000x1000,000=10^12个连接，也就是10^12个权值参数。 图2：局部连接网络。L2层每一个节点与L1层节点同位置附近10×10的窗口相连接，则1百万个隐层神经元就只有100w乘以100，即10^8个参数。其权值连接个数比原来减少了四个数量级。 （2）权值共享 就图2来说，权值共享，不是说，所有的红色线标注的连接权值相同。而是说，每一个颜色的线都有一个红色线的权值与之相等，所以第二层的每个节点，其从上一层进行卷积的参数都是相同的。 图2中隐层的每一个神经元都连接10×10个图像区域，也就是说每一个神经元存在10×10=100个连接权值参数。如果我们每个神经元这100个参数是相同的？也就是说每个神经元用的是同一个卷积核去卷积图像。这样L1层我们就只有100个参数。但是这样，只提取了图像一种特征？如果需要提取不同的特征，就加多几种卷积核。所以假设我们加到100种卷积核，也就是1万个参数。 每种卷积核的参数不一样，表示它提出输入图像的不同特征（不同的边缘）。这样每种卷积核去卷积图像就得到对图像的不同特征的放映，我们称之为Feature Map，也就是特征图。 3.网络结构 以LeCun的LeNet-5为例，不包含输入，LeNet-5共有7层，每层都包含连接权值（可训练参数）。输入图像为32*32大小。我们先要明确一点：每个层有多个特征图，每个特征图通过一种卷积滤波器提取输入的一种特征，然后每个特征图有多个神经元。 C1、C3、C5是卷积层，S2、S4、S6是下采样层。利用图像局部相关性的原理，对图像进行下抽样，可以减少数据处理量同时保留有用信息。 图3 4.前向传播 在神经网络一文中已经详细介绍过全连接和激励层的前向传播过程，这里主要介绍卷积层、下采样（池化）层。 （1）卷积层 如图4所示，输入图片是一个5×5的图片，用一个3×3的卷积核对该图片进行卷积操作。本质上是一个点积操作。举例：1×1+0×1+1×1+0×0+1×1+0×1+1×0+0×0+1×1=4 图4 def conv2(X, k):&nbsp;&nbsp;&nbsp; x_row, x_col = X.shape&nbsp;&nbsp;&nbsp; k_row, k_col = k.shape&nbsp;&nbsp;&nbsp; ret_row, ret_col = x_row - k_row + 1, x_col - k_col + 1&nbsp;&nbsp;&nbsp; ret = np.empty((ret_row, ret_col))&nbsp;&nbsp;&nbsp; for y in range(ret_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sub = X[y : y + k_row, x : x + k_col]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y,x] = np.sum(sub * k)&nbsp;&nbsp;&nbsp; return retclass ConvLayer:&nbsp;&nbsp;&nbsp; def __init__(self, in_channel, out_channel, kernel_size):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_channel, out_channel, kernel_size, kernel_size)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_channel))&nbsp;&nbsp;&nbsp; def _relu(self, x):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x[x &lt; 0] = 0&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # assume the first index is channel index&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_channel, kernel_row, kernel_col = self.w.shape[1], self.w.shape[2], self.w.shape[3]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = np.zeros((out_channel, in_row - kernel_row + 1, in_col - kernel_col + 1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for j in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] += conv2(in_data[i], self.w[i, j])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] += self.b[j]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] = self._relu(self.topval[j])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 1 （2）下采样（池化）层 下采样，即池化，目的是减小特征图，池化规模一般为2×2。常用的池化方法有： 最大池化（Max Pooling）。如图5所示。 均值池化（Mean Pooling）。如图6所示。 高斯池化。借鉴高斯模糊的方法。 可训练池化。训练函数 ff ，接受4个点为输入，输出1个点。 图5 图6 class MaxPoolingLayer:&nbsp;&nbsp;&nbsp; def __init__(self, kernel_size, name=&#39;MaxPool&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.kernel_size = kernel_size&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row = in_row / k + (1 if in_row % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_col = in_col / k + (1 if in_col % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag = np.zeros_like(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = np.empty((in_batch, in_channel, out_row, out_col))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for c in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for oy in range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for ox in range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k if (oy + 1) * k &lt;= in_row else in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k if (ox + 1) * k &lt;= in_col else in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; idx = np.argmax(in_data[b_id, c, oy * k: oy * k + height, ox * k: ox * k + width])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_r = idx / width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_c = idx % width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag[b_id, c, oy * k + offset_r, ox * k + offset_c] = 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[b_id, c, oy, ox] = in_data[b_id, c, oy * k + offset_r, ox * k + offset_c]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return ret 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 1 5.后向传播 在神经网络一文中已经详细介绍过全连接和激励层的后向传播过程，这里主要介绍卷积层、下采样（池化）层。 （1）卷积层 当一个卷积层L的下一层(L+1)为采样层，并假设我们已经计算得到了采样层的残差，现在计算该卷积层的残差。从最上面的网络结构图我们知道，采样层（L+1）的map大小是卷积层L的1/（scale*scale），以scale=2为例，但这两层的map个数是一样的，卷积层L的某个map中的4个单元与L+1层对应map的一个单元关联，可以对采样层的残差与一个scale*scale的全1矩阵进行克罗内克积 进行扩充，使得采样层的残差的维度与上一层的输出map的维度一致。 扩展过程： 图7 利用卷积计算卷积层的残差： 图8 def backward(self, residual):&nbsp;&nbsp;&nbsp; in_channel, out_channel, kernel_size = self.w.shape&nbsp;&nbsp;&nbsp; in_batch = residual.shape[0]&nbsp;&nbsp;&nbsp; # gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; self.gradient_b = residual.sum(axis=3).sum(axis=2).sum(axis=0) / self.batch_size&nbsp;&nbsp;&nbsp; # gradient_w&nbsp;&nbsp;&nbsp; self.gradient_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w[i, o] += conv2(self.bottom_val[b_id], residual[o])&nbsp;&nbsp;&nbsp; self.gradient_w /= self.batch_size&nbsp;&nbsp;&nbsp; # gradient_x&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(self.bottom_val)&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, i] += conv2(padding(residual, kernel_size - 1), rot180(self.w[i, o]))&nbsp;&nbsp;&nbsp; gradient_x /= self.batch_size&nbsp;&nbsp;&nbsp; # update&nbsp;&nbsp;&nbsp; self.prev_gradient_w = self.prev_gradient_w * self.momentum - self.gradient_w&nbsp;&nbsp;&nbsp; self.w += self.lr * self.prev_gradient_w&nbsp;&nbsp;&nbsp; self.prev_gradient_b = self.prev_gradient_b * self.momentum - self.gradient_b&nbsp;&nbsp;&nbsp; self.b += self.lr * self.prev_gradient_b&nbsp;&nbsp;&nbsp; return gradient_x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 1 （2）下采样（池化）层 当某个采样层L的下一层是卷积层(L+1)，并假设我们已经计算出L+1层的残差，现在计算L层的残差。采样层到卷积层直接的连接是有权重和偏置参数的，因此不像卷积层到采样层那样简单。现再假设L层第j个map Mj与L+1层的M2j关联，按照BP的原理，L层的残差Dj是L+1层残差D2j的加权和，但是这里的困难在于，我们很难理清M2j的那些单元通过哪些权重与Mj的哪些单元关联，这里需要两个小的变换（rot180°和padding）： rot180°：旋转：表示对矩阵进行180度旋转（可通过行对称交换和列对称交换完成） def rot180(in_data):&nbsp;&nbsp;&nbsp; ret = in_data.copy()&nbsp;&nbsp;&nbsp; yEnd = ret.shape[0] - 1&nbsp;&nbsp;&nbsp; xEnd = ret.shape[1] - 1&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[yEnd - y][x] = ret[y][x]&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y][xEnd - x] = ret[y][x]&nbsp;&nbsp;&nbsp; return ret 1 2 3 4 5 6 7 8 9 10 11 1 padding:扩充 def padding(in_data, size):&nbsp;&nbsp;&nbsp; cur_r, cur_w = in_data.shape[0], in_data.shape[1]&nbsp;&nbsp;&nbsp; new_r = cur_r + size * 2&nbsp;&nbsp;&nbsp; new_w = cur_w + size * 2&nbsp;&nbsp;&nbsp; ret = np.zeros((new_r, new_w))&nbsp;&nbsp;&nbsp; ret[size:cur_r + size, size:cur_w+size] = in_data&nbsp;&nbsp;&nbsp; return ret 1 2 3 4 5 6 7 1 图9 6.核心代码（demo版） import numpy as npimport sysdef conv2(X, k):&nbsp;&nbsp;&nbsp; # as a demo code, here we ignore the shape check&nbsp;&nbsp;&nbsp; x_row, x_col = X.shape&nbsp;&nbsp;&nbsp; k_row, k_col = k.shape&nbsp;&nbsp;&nbsp; ret_row, ret_col = x_row - k_row + 1, x_col - k_col + 1&nbsp;&nbsp;&nbsp; ret = np.empty((ret_row, ret_col))&nbsp;&nbsp;&nbsp; for y in range(ret_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sub = X[y : y + k_row, x : x + k_col]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y,x] = np.sum(sub * k)&nbsp;&nbsp;&nbsp; return retdef rot180(in_data):&nbsp;&nbsp;&nbsp; ret = in_data.copy()&nbsp;&nbsp;&nbsp; yEnd = ret.shape[0] - 1&nbsp;&nbsp;&nbsp; xEnd = ret.shape[1] - 1&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[yEnd - y][x] = ret[y][x]&nbsp;&nbsp;&nbsp; for y in range(ret.shape[0]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for x in range(ret.shape[1] / 2):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y][xEnd - x] = ret[y][x]&nbsp;&nbsp;&nbsp; return retdef padding(in_data, size):&nbsp;&nbsp;&nbsp; cur_r, cur_w = in_data.shape[0], in_data.shape[1]&nbsp;&nbsp;&nbsp; new_r = cur_r + size * 2&nbsp;&nbsp;&nbsp; new_w = cur_w + size * 2&nbsp;&nbsp;&nbsp; ret = np.zeros((new_r, new_w))&nbsp;&nbsp;&nbsp; ret[size:cur_r + size, size:cur_w+size] = in_data&nbsp;&nbsp;&nbsp; return retdef discreterize(in_data, size):&nbsp;&nbsp;&nbsp; num = in_data.shape[0]&nbsp;&nbsp;&nbsp; ret = np.zeros((num, size))&nbsp;&nbsp;&nbsp; for i, idx in enumerate(in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[i, idx] = 1&nbsp;&nbsp;&nbsp; return retclass ConvLayer:&nbsp;&nbsp;&nbsp; def __init__(self, in_channel, out_channel, kernel_size, lr=0.01, momentum=0.9, name=&#39;Conv&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_channel, out_channel, kernel_size, kernel_size)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_channel))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layer_name = name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.lr = lr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.momentum = momentum&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_b = np.zeros_like(self.b)&nbsp;&nbsp;&nbsp; # def _relu(self, x):&nbsp;&nbsp;&nbsp; #&nbsp;&nbsp;&nbsp;&nbsp; x[x &lt; 0] = 0&nbsp;&nbsp;&nbsp; #&nbsp;&nbsp;&nbsp;&nbsp; return x&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # assume the first index is channel index&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;conv forward:&#39; + str(in_data.shape)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_channel, kernel_size = self.w.shape[1], self.w.shape[2]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = np.zeros((in_batch, out_channel, in_row - kernel_size + 1, in_col - kernel_size + 1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.bottom_val = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[b_id, o] += conv2(in_data[b_id, i], self.w[i, o])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[b_id, o] += self.b[o]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_channel, out_channel, kernel_size = self.w.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch = residual.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_b = residual.sum(axis=3).sum(axis=2).sum(axis=0) / self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # gradient_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w[i, o] += conv2(self.bottom_val[b_id], residual[o])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w /= self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # gradient_x&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(self.bottom_val)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for o in range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, i] += conv2(padding(residual, kernel_size - 1), rot180(self.w[i, o]))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x /= self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # update&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_w = self.prev_gradient_w * self.momentum - self.gradient_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w += self.lr * self.prev_gradient_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_b = self.prev_gradient_b * self.momentum - self.gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b += self.lr * self.prev_gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return gradient_xclass FCLayer:&nbsp;&nbsp;&nbsp; def __init__(self, in_num, out_num, lr = 0.01, momentum=0.9):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self._in_num = in_num&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self._out_num = out_num&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_num, out_num)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_num, 1))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.lr = lr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.momentum = momentum&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_b = np.zeros_like(self.b)&nbsp;&nbsp;&nbsp; # def _sigmoid(self, in_data):&nbsp;&nbsp;&nbsp; #&nbsp;&nbsp;&nbsp;&nbsp; return 1 / (1 + np.exp(-in_data))&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;fc forward=&#39; + str(in_data.shape)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.topVal = np.dot(self.w.T, in_data) + self.b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.bottomVal = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.topVal&nbsp;&nbsp;&nbsp; def backward(self, loss):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; batch_size = loss.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # residual_z = loss * self.topVal * (1 - self.topVal)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; grad_w = np.dot(self.bottomVal, loss.T) / batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; grad_b = np.sum(loss) / batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_x = np.dot(self.w, loss)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_w = self.prev_grad_w * momentum - grad_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_b = self.prev_grad_b * momentum - grad_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w -= self.lr * self.prev_grad_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b -= self.lr * self.prev_grad_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return residual_xclass ReLULayer:&nbsp;&nbsp;&nbsp; def __init__(self, name=&#39;ReLU&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pass&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = in_data.copy()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[ret &lt; 0] = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return ret&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = residual.copy()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[self.top_val &lt; 0] = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return gradient_xclass MaxPoolingLayer:&nbsp;&nbsp;&nbsp; def __init__(self, kernel_size, name=&#39;MaxPool&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.kernel_size = kernel_size&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row = in_row / k + (1 if in_row % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_col = in_col / k + (1 if in_col % k != 0 else 0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag = np.zeros_like(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = np.empty((in_batch, in_channel, out_row, out_col))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for c in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for oy in range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for ox in range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k if (oy + 1) * k &lt;= in_row else in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k if (ox + 1) * k &lt;= in_col else in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; idx = np.argmax(in_data[b_id, c, oy * k: oy * k + height, ox * k: ox * k + width])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_r = idx / width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_c = idx % width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag[b_id, c, oy * k + offset_r, ox * k + offset_c] = 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[b_id, c, oy, ox] = in_data[b_id, c, oy * k + offset_r, ox * k + offset_c]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return ret&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = self.flag&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row, out_col = residual.shape[2], residual.shape[3]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(self.flag)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for b_id in range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for c in range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for oy in range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for ox in range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k if (oy + 1) * k &lt;= in_row else in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k if (ox + 1) * k &lt;= in_col else in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, c, oy * k + offset_r, ox * k + offset_c] = residual[b_id, c, oy, ox]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[self.flag == 0] = 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return gradient_xclass FlattenLayer:&nbsp;&nbsp;&nbsp; def __init__(self, name=&#39;Flatten&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pass&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.in_batch, self.in_channel, self.r, self.c = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return in_data.reshape(self.in_batch, self.in_channel * self.r * self.c)&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return residual.reshape(self.in_batch, self.in_channel, self.r, self.c)class SoftmaxLayer:&nbsp;&nbsp;&nbsp; def __init__(self, name=&#39;Softmax&#39;):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pass&nbsp;&nbsp;&nbsp; def forward(self, in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; exp_out = np.exp(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = exp_out / np.sum(exp_out, axis=1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val&nbsp;&nbsp;&nbsp; def backward(self, residual):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return self.top_val - residualclass Net:&nbsp;&nbsp;&nbsp; def __init__(self):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layers = []&nbsp;&nbsp;&nbsp; def addLayer(self, layer):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layers.append(layer)&nbsp;&nbsp;&nbsp; def train(self, trainData, trainLabel, validData, validLabel, batch_size, iteration):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_num = trainData.shape[0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for iter in range(iteration):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &#39;iter=&#39; + str(iter)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for batch_iter in range(0, train_num, batch_size):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if batch_iter + batch_size &lt; train_num:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.train_inner(trainData[batch_iter: batch_iter + batch_size],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; trainLabel[batch_iter: batch_iter + batch_size])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; else:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.train_inner(trainData[batch_iter: train_num],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; trainLabel[batch_iter: train_num])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print &quot;eval=&quot; + str(self.eval(validData, validLabel))&nbsp;&nbsp;&nbsp; def train_inner(self, data, label):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lay_num = len(self.layers)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(lay_num):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_data = self.layers[i].forward(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = out_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_in = label&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(0, lay_num, -1):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_out = self.layers[i].backward(residual_in)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_in = residual_out&nbsp;&nbsp;&nbsp; def eval(self, data, label):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lay_num = len(self.layers)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for i in range(lay_num):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_data = self.layers[i].forward(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = out_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_idx = np.argmax(in_data, axis=1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; label_idx = np.argmax(label, axis=1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return np.sum(out_idx == label_idx) / float(out_idx.shape[0]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/19/787279.html","headline":"深度学习（DL） 卷积神经网络（CNN） 从原理到实现","dateModified":"2019-05-19T00:00:00+08:00","datePublished":"2019-05-19T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/19/787279.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>深度学习（DL） 卷积神经网络（CNN） 从原理到实现</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <div class="markdown_views" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
   <div class="markdown_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <!-- flowchart &amp;#31661;&amp;#22836;&amp;#22270;&amp;#26631; &amp;#21247;&amp;#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <h2 id="序"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">序</font></strong></h2>
    <p><font size="3">深度学习现在大火，虽然自己上过深度学习课程、用过keras做过一些实验，始终觉得理解不透彻。最近仔细学习前辈和学者的著作，感谢他们的无私奉献，整理得到本文，共勉。</font></p>
    <h2 id="1前言-1"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">1.前言</font></strong></h2>
    <p><font color="#009900" size="3">（1）神经网络的缺陷</font></p>
    <p><font size="3">在<a href="http://blog.csdn.net/a819825294/article/details/53393837" rel="nofollow" target="_blank">神经网络</a>一文中简单介绍了其原理，可以发现不同层之间是全连接的，当神经网络的深度、节点数变大，会导致过拟合、参数过多等问题。</font></p>
    <p><font color="#009900" size="3">（2）计算机视觉（图像）背景</font></p>
    <ul>
     <li><font size="3">通过抽取只依赖图像里小的子区域的局部特征，然后利用这些特征的信息就可以融合到后续处理阶段中，从而检测更高级的特征，最后产生图像整体的信息。</font></li>
     <li><font size="3">距离较近的像素的相关性要远大于距离较远像素的相关性。</font></li>
     <li><font size="3">对于图像的一个区域有用的局部特征可能对于图像的其他区域也有用，例如感兴趣的物体发生平移的情形。</font></li>
    </ul>
   </div>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <h2 id="2卷积神经网络cnn特性"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">2.卷积神经网络（CNN）特性</font></strong></h2>
   <p><font size="3">根据前言中的两方面，这里介绍卷积神经网络的两个特性。</font></p>
   <p></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161201192409769">
   </center>
   <p></p>
   <p><font color="#009900" size="3">（1）局部感知</font></p>
   <p><font size="3">图1：全连接网络。如果L1层有1000×1000像素的图像，L2层有1000,000个隐层神经元，每个隐层神经元都连接L1层图像的每一个像素点，就有1000x1000x1000,000=10^12个连接，也就是10^12个权值参数。</font></p>
   <p><font size="3">图2：局部连接网络。L2层每一个节点与L1层节点同位置附近10×10的窗口相连接，则1百万个隐层神经元就只有100w乘以100，即10^8个参数。其权值连接个数比原来减少了四个数量级。</font></p>
   <p><font color="#009900" size="3">（2）权值共享</font></p>
   <p><font size="3">就图2来说，权值共享，不是说，所有的红色线标注的连接权值相同。而是说，每一个颜色的线都有一个红色线的权值与之相等，所以第二层的每个节点，其从上一层进行卷积的参数都是相同的。</font></p>
   <p><font size="3">图2中隐层的每一个神经元都连接10×10个图像区域，也就是说每一个神经元存在10×10=100个连接权值参数。如果我们每个神经元这100个参数是相同的？也就是说每个神经元用的是同一个卷积核去卷积图像。这样L1层我们就只有100个参数。但是这样，只提取了图像一种特征？<font color="#ff0000">如果需要提取不同的特征，就加多几种卷积核。</font>所以假设我们加到100种卷积核，也就是1万个参数。</font></p>
   <p><font size="3">每种卷积核的参数不一样，表示它提出输入图像的不同特征（不同的边缘）。这样每种卷积核去卷积图像就得到对图像的不同特征的放映，我们称之为Feature Map，也就是特征图。</font></p>
   <h2 id="3网络结构"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">3.网络结构</font></strong></h2>
   <p><font size="3">以LeCun的LeNet-5为例，不包含输入，LeNet-5共有7层，每层都包含连接权值（可训练参数）。输入图像为32*32大小。我们先要明确一点：每个层有多个特征图，每个特征图通过一种卷积滤波器提取输入的一种特征，然后每个特征图有多个神经元。</font></p>
   <p><font size="3">C1、C3、C5是卷积层，S2、S4、S6是下采样层。利用图像局部相关性的原理，对图像进行下抽样，可以减少数据处理量同时保留有用信息。</font></p>
   <p></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161201193243538"> 
    <br>
    <center>
     <font size="4">图3</font>
    </center>
   </center>
   <p></p>
   <h2 id="4前向传播"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">4.前向传播</font></strong></h2>
   <p><font size="3">在<a href="http://blog.csdn.net/a819825294/article/details/53393837" rel="nofollow" target="_blank">神经网络</a>一文中已经详细介绍过全连接和激励层的前向传播过程，这里主要介绍卷积层、下采样（池化）层。</font></p>
   <p><font color="#009900" size="3">（1）卷积层</font></p>
   <p><font size="3">如图4所示，输入图片是一个5×5的图片，用一个3×3的卷积核对该图片进行卷积操作。本质上是一个点积操作。举例：1×1+0×1+1×1+0×0+1×1+0×1+1×0+0×0+1×1=4</font></p>
   <p></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161201194121152"> 
    <br>
    <center>
     <font size="4">图4</font>
    </center>
   </center>
   <p></p>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">conv2</span><span class="hljs-params">(X, k)</span>:</span>&nbsp;&nbsp;&nbsp; x_row, x_col = X.shape&nbsp;&nbsp;&nbsp; k_row, k_col = k.shape&nbsp;&nbsp;&nbsp; ret_row, ret_col = x_row - k_row + <span class="hljs-number">1</span>, x_col - k_col + <span class="hljs-number">1</span>&nbsp;&nbsp;&nbsp; ret = np.empty((ret_row, ret_col))&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> range(ret_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(ret_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sub = X[y : y + k_row, x : x + k_col]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y,x] = np.sum(sub * k)&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> ret<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ConvLayer</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_channel, out_channel, kernel_size)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_channel, out_channel, kernel_size, kernel_size)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_channel))&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_relu</span><span class="hljs-params">(self, x)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x[x &lt; <span class="hljs-number">0</span>] = <span class="hljs-number">0</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, in_data)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># assume the first index is channel index</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_channel, kernel_row, kernel_col = self.w.shape[<span class="hljs-number">1</span>], self.w.shape[<span class="hljs-number">2</span>], self.w.shape[<span class="hljs-number">3</span>]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = np.zeros((out_channel, in_row - kernel_row + <span class="hljs-number">1</span>, in_col - kernel_col + <span class="hljs-number">1</span>))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] += conv2(in_data[i], self.w[i, j])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] += self.b[j]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[j] = self._relu(self.topval[j])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> self.top_val</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
     <li>26</li>
     <li>27</li>
     <li>28</li>
     <li>29</li>
     <li>30</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p><font color="#009900" size="3">（2）下采样（池化）层</font></p>
   <p><font size="3">下采样，即池化，目的是减小特征图，池化规模一般为2×2。常用的池化方法有：</font></p>
   <ul>
    <li><font size="3">最大池化（Max Pooling）。如图5所示。</font></li>
    <li><font size="3">均值池化（Mean Pooling）。如图6所示。</font></li>
    <li><font size="3">高斯池化。借鉴高斯模糊的方法。</font></li>
    <li><font size="3">可训练池化。训练函数 ff ，接受4个点为输入，输出1个点。</font></li>
   </ul>
   <p></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161201195153225"> 
    <br>
    <center>
     <font size="4">图5</font>
    </center>
   </center>
   <p></p>
   <p></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161201195315180"> 
    <br>
    <center>
     <font size="4">图6</font>
    </center>
   </center>
   <p></p>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MaxPoolingLayer</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, kernel_size, name=<span class="hljs-string">'MaxPool'</span>)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.kernel_size = kernel_size&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, in_data)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row = in_row / k + (<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> in_row % k != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_col = in_col / k + (<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> in_col % k != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag = np.zeros_like(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = np.empty((in_batch, in_channel, out_row, out_col))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b_id <span class="hljs-keyword">in</span> range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> oy <span class="hljs-keyword">in</span> range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> ox <span class="hljs-keyword">in</span> range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k <span class="hljs-keyword">if</span> (oy + <span class="hljs-number">1</span>) * k &lt;= in_row <span class="hljs-keyword">else</span> in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k <span class="hljs-keyword">if</span> (ox + <span class="hljs-number">1</span>) * k &lt;= in_col <span class="hljs-keyword">else</span> in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; idx = np.argmax(in_data[b_id, c, oy * k: oy * k + height, ox * k: ox * k + width])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_r = idx / width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_c = idx % width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag[b_id, c, oy * k + offset_r, ox * k + offset_c] = <span class="hljs-number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[b_id, c, oy, ox] = in_data[b_id, c, oy * k + offset_r, ox * k + offset_c]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> ret</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <h2 id="5后向传播"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">5.后向传播</font></strong></h2>
   <p><font size="3">在<a href="http://blog.csdn.net/a819825294/article/details/53393837" rel="nofollow" target="_blank">神经网络</a>一文中已经详细介绍过全连接和激励层的后向传播过程，这里主要介绍卷积层、下采样（池化）层。</font></p>
   <p><font color="#009900" size="3">（1）卷积层</font></p>
   <p><font size="3">当一个卷积层L的下一层(L+1)为采样层，并假设我们已经计算得到了采样层的残差，现在计算该卷积层的残差。从最上面的网络结构图我们知道，采样层（L+1）的map大小是卷积层L的1/（scale*scale），以scale=2为例，但这两层的map个数是一样的，卷积层L的某个map中的4个单元与L+1层对应map的一个单元关联，可以对采样层的残差与一个scale*scale的全1矩阵进行<a href="http://baike.baidu.com/link?url=SGmdAmegpAH-ihmWfu2NP9Ni_1mCWgLQxGn3W3jMi5D7hiemgl1tVStDWNbu-FgmP1WWnO8EK2menzPKgmo68UG74ffRRBF33lCtCcIDOak8Aq2y3PMn2UC2Z5HHQlHwaVv9-tCwCN1ng-C-W1NjZK" rel="nofollow" target="_blank">克罗内克积</a> 进行扩充，使得采样层的残差的维度与上一层的输出map的维度一致。</font></p>
   <p><font size="3">扩展过程：</font></p>
   <p></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161201200605820"> 
    <br>
    <center>
     <font size="4">图7</font>
    </center>
   </center>
   <p></p>
   <p><font size="3">利用卷积计算卷积层的残差：</font></p>
   <p></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161201200823946"> 
    <br>
    <center>
     <font size="4">图8</font>
    </center>
   </center>
   <p></p>
   <pre class="prettyprint"><code class="hljs ruby has-numbering"><span class="hljs-function"><span class="hljs-keyword">def</span> </span>backward(<span class="hljs-keyword">self</span>, residual)<span class="hljs-symbol">:</span>&nbsp;&nbsp;&nbsp; in_channel, out_channel, kernel_size = <span class="hljs-keyword">self</span>.w.shape&nbsp;&nbsp;&nbsp; in_batch = residual.shape[<span class="hljs-number">0</span>]&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">self</span>.gradient_b = residual.sum(axis=<span class="hljs-number">3</span>).sum(axis=<span class="hljs-number">2</span>).sum(axis=<span class="hljs-number">0</span>) / <span class="hljs-keyword">self</span>.batch_size&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># gradient_w</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">self</span>.gradient_w = np.zeros_like(<span class="hljs-keyword">self</span>.w)&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b_id <span class="hljs-keyword">in</span> range(in_batch)<span class="hljs-symbol">:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(in_channel)<span class="hljs-symbol">:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> range(out_channel)<span class="hljs-symbol">:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">self</span>.gradient_w[i, o] += conv2(<span class="hljs-keyword">self</span>.bottom_val[b_id], residual[o])&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">self</span>.gradient_w /= <span class="hljs-keyword">self</span>.batch_size&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># gradient_x</span>&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(<span class="hljs-keyword">self</span>.bottom_val)&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b_id <span class="hljs-keyword">in</span> range(in_batch)<span class="hljs-symbol">:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(in_channel)<span class="hljs-symbol">:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> range(out_channel)<span class="hljs-symbol">:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, i] += conv2(padding(residual, kernel_size - <span class="hljs-number">1</span>), rot18<span class="hljs-number">0</span>(<span class="hljs-keyword">self</span>.w[i, o]))&nbsp;&nbsp;&nbsp; gradient_x /= <span class="hljs-keyword">self</span>.batch_size&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># update</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">self</span>.prev_gradient_w = <span class="hljs-keyword">self</span>.prev_gradient_w * <span class="hljs-keyword">self</span>.momentum - <span class="hljs-keyword">self</span>.gradient_w&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">self</span>.w += <span class="hljs-keyword">self</span>.lr * <span class="hljs-keyword">self</span>.prev_gradient_w&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">self</span>.prev_gradient_b = <span class="hljs-keyword">self</span>.prev_gradient_b * <span class="hljs-keyword">self</span>.momentum - <span class="hljs-keyword">self</span>.gradient_b&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">self</span>.b += <span class="hljs-keyword">self</span>.lr * <span class="hljs-keyword">self</span>.prev_gradient_b&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> gradient_x</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p><font color="#009900" size="3">（2）下采样（池化）层</font></p>
   <p><font size="3">当某个采样层L的下一层是卷积层(L+1)，并假设我们已经计算出L+1层的残差，现在计算L层的残差。采样层到卷积层直接的连接是有权重和偏置参数的，因此不像卷积层到采样层那样简单。现再假设L层第j个map Mj与L+1层的M2j关联，按照BP的原理，L层的残差Dj是L+1层残差D2j的加权和，但是这里的困难在于，我们很难理清M2j的那些单元通过哪些权重与Mj的哪些单元关联，这里需要两个小的变换（rot180°和padding）：</font></p>
   <p><font size="3">rot180°：旋转：表示对矩阵进行180度旋转（可通过行对称交换和列对称交换完成）</font></p>
   <pre class="prettyprint"><code class="hljs avrasm has-numbering">def rot180(in_data):&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">ret</span> = in_data<span class="hljs-preprocessor">.copy</span>()&nbsp;&nbsp;&nbsp; yEnd = <span class="hljs-keyword">ret</span><span class="hljs-preprocessor">.shape</span>[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>&nbsp;&nbsp;&nbsp; xEnd = <span class="hljs-keyword">ret</span><span class="hljs-preprocessor">.shape</span>[<span class="hljs-number">1</span>] - <span class="hljs-number">1</span>&nbsp;&nbsp;&nbsp; for <span class="hljs-built_in">y</span> <span class="hljs-keyword">in</span> range(<span class="hljs-keyword">ret</span><span class="hljs-preprocessor">.shape</span>[<span class="hljs-number">0</span>] / <span class="hljs-number">2</span>):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for <span class="hljs-built_in">x</span> <span class="hljs-keyword">in</span> range(<span class="hljs-keyword">ret</span><span class="hljs-preprocessor">.shape</span>[<span class="hljs-number">1</span>]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">ret</span>[yEnd - <span class="hljs-built_in">y</span>][<span class="hljs-built_in">x</span>] = <span class="hljs-keyword">ret</span>[<span class="hljs-built_in">y</span>][<span class="hljs-built_in">x</span>]&nbsp;&nbsp;&nbsp; for <span class="hljs-built_in">y</span> <span class="hljs-keyword">in</span> range(<span class="hljs-keyword">ret</span><span class="hljs-preprocessor">.shape</span>[<span class="hljs-number">0</span>]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for <span class="hljs-built_in">x</span> <span class="hljs-keyword">in</span> range(<span class="hljs-keyword">ret</span><span class="hljs-preprocessor">.shape</span>[<span class="hljs-number">1</span>] / <span class="hljs-number">2</span>):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">ret</span>[<span class="hljs-built_in">y</span>][xEnd - <span class="hljs-built_in">x</span>] = <span class="hljs-keyword">ret</span>[<span class="hljs-built_in">y</span>][<span class="hljs-built_in">x</span>]&nbsp;&nbsp;&nbsp; return <span class="hljs-keyword">ret</span></code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p><font size="3">padding:扩充</font></p>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">padding</span><span class="hljs-params">(in_data, size)</span>:</span>&nbsp;&nbsp;&nbsp; cur_r, cur_w = in_data.shape[<span class="hljs-number">0</span>], in_data.shape[<span class="hljs-number">1</span>]&nbsp;&nbsp;&nbsp; new_r = cur_r + size * <span class="hljs-number">2</span>&nbsp;&nbsp;&nbsp; new_w = cur_w + size * <span class="hljs-number">2</span>&nbsp;&nbsp;&nbsp; ret = np.zeros((new_r, new_w))&nbsp;&nbsp;&nbsp; ret[size:cur_r + size, size:cur_w+size] = in_data&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> ret</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p></p>
   <center>
    <img title="" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20161201201518053"> 
    <br>
    <center>
     <font size="4">图9</font>
    </center>
   </center>
   <p></p>
   <h2 id="6核心代码demo版"><a></a><a target="_blank"></a><strong><font color="#0099ff" face="黑体" size="5">6.核心代码（demo版）</font></strong></h2>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> sys<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">conv2</span><span class="hljs-params">(X, k)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># as a demo code, here we ignore the shape check</span>&nbsp;&nbsp;&nbsp; x_row, x_col = X.shape&nbsp;&nbsp;&nbsp; k_row, k_col = k.shape&nbsp;&nbsp;&nbsp; ret_row, ret_col = x_row - k_row + <span class="hljs-number">1</span>, x_col - k_col + <span class="hljs-number">1</span>&nbsp;&nbsp;&nbsp; ret = np.empty((ret_row, ret_col))&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> range(ret_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(ret_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sub = X[y : y + k_row, x : x + k_col]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y,x] = np.sum(sub * k)&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> ret<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rot180</span><span class="hljs-params">(in_data)</span>:</span>&nbsp;&nbsp;&nbsp; ret = in_data.copy()&nbsp;&nbsp;&nbsp; yEnd = ret.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>&nbsp;&nbsp;&nbsp; xEnd = ret.shape[<span class="hljs-number">1</span>] - <span class="hljs-number">1</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> range(ret.shape[<span class="hljs-number">0</span>] / <span class="hljs-number">2</span>):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(ret.shape[<span class="hljs-number">1</span>]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[yEnd - y][x] = ret[y][x]&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> range(ret.shape[<span class="hljs-number">0</span>]):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(ret.shape[<span class="hljs-number">1</span>] / <span class="hljs-number">2</span>):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[y][xEnd - x] = ret[y][x]&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> ret<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">padding</span><span class="hljs-params">(in_data, size)</span>:</span>&nbsp;&nbsp;&nbsp; cur_r, cur_w = in_data.shape[<span class="hljs-number">0</span>], in_data.shape[<span class="hljs-number">1</span>]&nbsp;&nbsp;&nbsp; new_r = cur_r + size * <span class="hljs-number">2</span>&nbsp;&nbsp;&nbsp; new_w = cur_w + size * <span class="hljs-number">2</span>&nbsp;&nbsp;&nbsp; ret = np.zeros((new_r, new_w))&nbsp;&nbsp;&nbsp; ret[size:cur_r + size, size:cur_w+size] = in_data&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> ret<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">discreterize</span><span class="hljs-params">(in_data, size)</span>:</span>&nbsp;&nbsp;&nbsp; num = in_data.shape[<span class="hljs-number">0</span>]&nbsp;&nbsp;&nbsp; ret = np.zeros((num, size))&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> i, idx <span class="hljs-keyword">in</span> enumerate(in_data):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[i, idx] = <span class="hljs-number">1</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> ret<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ConvLayer</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_channel, out_channel, kernel_size, lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.9</span>, name=<span class="hljs-string">'Conv'</span>)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_channel, out_channel, kernel_size, kernel_size)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_channel))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layer_name = name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.lr = lr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.momentum = momentum&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_b = np.zeros_like(self.b)&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># def _relu(self, x):</span>&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#&nbsp;&nbsp;&nbsp;&nbsp; x[x &lt; 0] = 0</span>&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#&nbsp;&nbsp;&nbsp;&nbsp; return x</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, in_data)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># assume the first index is channel index</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">'conv forward:'</span> + str(in_data.shape)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_channel, kernel_size = self.w.shape[<span class="hljs-number">1</span>], self.w.shape[<span class="hljs-number">2</span>]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = np.zeros((in_batch, out_channel, in_row - kernel_size + <span class="hljs-number">1</span>, in_col - kernel_size + <span class="hljs-number">1</span>))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.bottom_val = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b_id <span class="hljs-keyword">in</span> range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[b_id, o] += conv2(in_data[b_id, i], self.w[i, o])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val[b_id, o] += self.b[o]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> self.top_val&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self, residual)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_channel, out_channel, kernel_size = self.w.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch = residual.shape[<span class="hljs-number">0</span>]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_b = residual.sum(axis=<span class="hljs-number">3</span>).sum(axis=<span class="hljs-number">2</span>).sum(axis=<span class="hljs-number">0</span>) / self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># gradient_w</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b_id <span class="hljs-keyword">in</span> range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w[i, o] += conv2(self.bottom_val[b_id], residual[o])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.gradient_w /= self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># gradient_x</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(self.bottom_val)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b_id <span class="hljs-keyword">in</span> range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> range(out_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, i] += conv2(padding(residual, kernel_size - <span class="hljs-number">1</span>), rot180(self.w[i, o]))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x /= self.batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># update</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_w = self.prev_gradient_w * self.momentum - self.gradient_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w += self.lr * self.prev_gradient_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_gradient_b = self.prev_gradient_b * self.momentum - self.gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b += self.lr * self.prev_gradient_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> gradient_x<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FCLayer</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_num, out_num, lr = <span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.9</span>)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self._in_num = in_num&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self._out_num = out_num&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w = np.random.randn(in_num, out_num)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b = np.zeros((out_num, <span class="hljs-number">1</span>))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.lr = lr&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.momentum = momentum&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_w = np.zeros_like(self.w)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_b = np.zeros_like(self.b)&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># def _sigmoid(self, in_data):</span>&nbsp;&nbsp;&nbsp; <span class="hljs-comment">#&nbsp;&nbsp;&nbsp;&nbsp; return 1 / (1 + np.exp(-in_data))</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, in_data)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">'fc forward='</span> + str(in_data.shape)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.topVal = np.dot(self.w.T, in_data) + self.b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.bottomVal = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> self.topVal&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self, loss)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; batch_size = loss.shape[<span class="hljs-number">0</span>]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment"># residual_z = loss * self.topVal * (1 - self.topVal)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; grad_w = np.dot(self.bottomVal, loss.T) / batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; grad_b = np.sum(loss) / batch_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_x = np.dot(self.w, loss)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_w = self.prev_grad_w * momentum - grad_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.prev_grad_b = self.prev_grad_b * momentum - grad_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.w -= self.lr * self.prev_grad_w&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.b -= self.lr * self.prev_grad_b&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> residual_x<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ReLULayer</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, name=<span class="hljs-string">'ReLU'</span>)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">pass</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, in_data)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = in_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = in_data.copy()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[ret &lt; <span class="hljs-number">0</span>] = <span class="hljs-number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> ret&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self, residual)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = residual.copy()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[self.top_val &lt; <span class="hljs-number">0</span>] = <span class="hljs-number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> gradient_x<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MaxPoolingLayer</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, kernel_size, name=<span class="hljs-string">'MaxPool'</span>)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.kernel_size = kernel_size&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, in_data)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row = in_row / k + (<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> in_row % k != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_col = in_col / k + (<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> in_col % k != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag = np.zeros_like(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret = np.empty((in_batch, in_channel, out_row, out_col))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b_id <span class="hljs-keyword">in</span> range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> oy <span class="hljs-keyword">in</span> range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> ox <span class="hljs-keyword">in</span> range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k <span class="hljs-keyword">if</span> (oy + <span class="hljs-number">1</span>) * k &lt;= in_row <span class="hljs-keyword">else</span> in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k <span class="hljs-keyword">if</span> (ox + <span class="hljs-number">1</span>) * k &lt;= in_col <span class="hljs-keyword">else</span> in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; idx = np.argmax(in_data[b_id, c, oy * k: oy * k + height, ox * k: ox * k + width])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_r = idx / width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; offset_c = idx % width&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.flag[b_id, c, oy * k + offset_r, ox * k + offset_c] = <span class="hljs-number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret[b_id, c, oy, ox] = in_data[b_id, c, oy * k + offset_r, ox * k + offset_c]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> ret&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self, residual)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_batch, in_channel, in_row, in_col = self.flag&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; k = self.kernel_size&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_row, out_col = residual.shape[<span class="hljs-number">2</span>], residual.shape[<span class="hljs-number">3</span>]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x = np.zeros_like(self.flag)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> b_id <span class="hljs-keyword">in</span> range(in_batch):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> range(in_channel):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> oy <span class="hljs-keyword">in</span> range(out_row):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> ox <span class="hljs-keyword">in</span> range(out_col):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; height = k <span class="hljs-keyword">if</span> (oy + <span class="hljs-number">1</span>) * k &lt;= in_row <span class="hljs-keyword">else</span> in_row - oy * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; width = k <span class="hljs-keyword">if</span> (ox + <span class="hljs-number">1</span>) * k &lt;= in_col <span class="hljs-keyword">else</span> in_col - ox * k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[b_id, c, oy * k + offset_r, ox * k + offset_c] = residual[b_id, c, oy, ox]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gradient_x[self.flag == <span class="hljs-number">0</span>] = <span class="hljs-number">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> gradient_x<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FlattenLayer</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, name=<span class="hljs-string">'Flatten'</span>)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">pass</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, in_data)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.in_batch, self.in_channel, self.r, self.c = in_data.shape&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> in_data.reshape(self.in_batch, self.in_channel * self.r * self.c)&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self, residual)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> residual.reshape(self.in_batch, self.in_channel, self.r, self.c)<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SoftmaxLayer</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, name=<span class="hljs-string">'Softmax'</span>)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">pass</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, in_data)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; exp_out = np.exp(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.top_val = exp_out / np.sum(exp_out, axis=<span class="hljs-number">1</span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> self.top_val&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">backward</span><span class="hljs-params">(self, residual)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> self.top_val - residual<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layers = []&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">addLayer</span><span class="hljs-params">(self, layer)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.layers.append(layer)&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(self, trainData, trainLabel, validData, validLabel, batch_size, iteration)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train_num = trainData.shape[<span class="hljs-number">0</span>]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> iter <span class="hljs-keyword">in</span> range(iteration):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">'iter='</span> + str(iter)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> batch_iter <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, train_num, batch_size):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">if</span> batch_iter + batch_size &lt; train_num:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.train_inner(trainData[batch_iter: batch_iter + batch_size],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; trainLabel[batch_iter: batch_iter + batch_size])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">else</span>:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; self.train_inner(trainData[batch_iter: train_num],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; trainLabel[batch_iter: train_num])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">print</span> <span class="hljs-string">"eval="</span> + str(self.eval(validData, validLabel))&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_inner</span><span class="hljs-params">(self, data, label)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lay_num = len(self.layers)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(lay_num):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_data = self.layers[i].forward(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = out_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_in = label&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, lay_num, -<span class="hljs-number">1</span>):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_out = self.layers[i].backward(residual_in)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; residual_in = residual_out&nbsp;&nbsp;&nbsp; <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">eval</span><span class="hljs-params">(self, data, label)</span>:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lay_num = len(self.layers)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(lay_num):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_data = self.layers[i].forward(in_data)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; in_data = out_data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; out_idx = np.argmax(in_data, axis=<span class="hljs-number">1</span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; label_idx = np.argmax(label, axis=<span class="hljs-number">1</span>)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> np.sum(out_idx == label_idx) / float(out_idx.shape[<span class="hljs-number">0</span>])</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
     <li>26</li>
     <li>27</li>
     <li>28</li>
     <li>29</li>
     <li>30</li>
     <li>31</li>
     <li>32</li>
     <li>33</li>
     <li>34</li>
     <li>35</li>
     <li>36</li>
     <li>37</li>
     <li>38</li>
     <li>39</li>
     <li>40</li>
     <li>41</li>
     <li>42</li>
     <li>43</li>
     <li>44</li>
     <li>45</li>
     <li>46</li>
     <li>47</li>
     <li>48</li>
     <li>49</li>
     <li>50</li>
     <li>51</li>
     <li>52</li>
     <li>53</li>
     <li>54</li>
     <li>55</li>
     <li>56</li>
     <li>57</li>
     <li>58</li>
     <li>59</li>
     <li>60</li>
     <li>61</li>
     <li>62</li>
     <li>63</li>
     <li>64</li>
     <li>65</li>
     <li>66</li>
     <li>67</li>
     <li>68</li>
     <li>69</li>
     <li>70</li>
     <li>71</li>
     <li>72</li>
     <li>73</li>
     <li>74</li>
     <li>75</li>
     <li>76</li>
     <li>77</li>
     <li>78</li>
     <li>79</li>
     <li>80</li>
     <li>81</li>
     <li>82</li>
     <li>83</li>
     <li>84</li>
     <li>85</li>
     <li>86</li>
     <li>87</li>
     <li>88</li>
     <li>89</li>
     <li>90</li>
     <li>91</li>
     <li>92</li>
     <li>93</li>
     <li>94</li>
     <li>95</li>
     <li>96</li>
     <li>97</li>
     <li>98</li>
     <li>99</li>
     <li>100</li>
     <li>101</li>
     <li>102</li>
     <li>103</li>
     <li>104</li>
     <li>105</li>
     <li>106</li>
     <li>107</li>
     <li>108</li>
     <li>109</li>
     <li>110</li>
     <li>111</li>
     <li>112</li>
     <li>113</li>
     <li>114</li>
     <li>115</li>
     <li>116</li>
     <li>117</li>
     <li>118</li>
     <li>119</li>
     <li>120</li>
     <li>121</li>
     <li>122</li>
     <li>123</li>
     <li>124</li>
     <li>125</li>
     <li>126</li>
     <li>127</li>
     <li>128</li>
     <li>129</li>
     <li>130</li>
     <li>131</li>
     <li>132</li>
     <li>133</li>
     <li>134</li>
     <li>135</li>
     <li>136</li>
     <li>137</li>
     <li>138</li>
     <li>139</li>
     <li>140</li>
     <li>141</li>
     <li>142</li>
     <li>143</li>
     <li>144</li>
     <li>145</li>
     <li>146</li>
     <li>147</li>
     <li>148</li>
     <li>149</li>
     <li>150</li>
     <li>151</li>
     <li>152</li>
     <li>153</li>
     <li>154</li>
     <li>155</li>
     <li>156</li>
     <li>157</li>
     <li>158</li>
     <li>159</li>
     <li>160</li>
     <li>161</li>
     <li>162</li>
     <li>163</li>
     <li>164</li>
     <li>165</li>
     <li>166</li>
     <li>167</li>
     <li>168</li>
     <li>169</li>
     <li>170</li>
     <li>171</li>
     <li>172</li>
     <li>173</li>
     <li>174</li>
     <li>175</li>
     <li>176</li>
     <li>177</li>
     <li>178</li>
     <li>179</li>
     <li>180</li>
     <li>181</li>
     <li>182</li>
     <li>183</li>
     <li>184</li>
     <li>185</li>
     <li>186</li>
     <li>187</li>
     <li>188</li>
     <li>189</li>
     <li>190</li>
     <li>191</li>
     <li>192</li>
     <li>193</li>
     <li>194</li>
     <li>195</li>
     <li>196</li>
     <li>197</li>
     <li>198</li>
     <li>199</li>
     <li>200</li>
     <li>201</li>
     <li>202</li>
     <li>203</li>
     <li>204</li>
     <li>205</li>
     <li>206</li>
     <li>207</li>
     <li>208</li>
     <li>209</li>
     <li>210</li>
     <li>211</li>
     <li>212</li>
     <li>213</li>
     <li>214</li>
     <li>215</li>
     <li>216</li>
     <li>217</li>
     <li>218</li>
     <li>219</li>
     <li>220</li>
     <li>221</li>
     <li>222</li>
     <li>223</li>
     <li>224</li>
     <li>225</li>
     <li>226</li>
     <li>227</li>
     <li>228</li>
     <li>229</li>
     <li>230</li>
     <li>231</li>
     <li>232</li>
     <li>233</li>
     <li>234</li>
     <li>235</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
