<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>自然语言处理之朴素贝叶斯 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="自然语言处理之朴素贝叶斯" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="一 、朴素贝叶斯基本原理 基于朴素贝叶斯公式，比较出后验概率的最大值来进行分类，后验概率的计算是由先验概率与类条件概率的乘积得出，先验概率和类条件概率要通过训练数据集得出，即为朴素贝叶斯分类模型，将其保存为中间结果，测试文档进行分类时调用这个中间结果得出后验概率。 1、基本定义 分类是把一个事物分到某个类别中。一个事物具有很多属性，把它的众多属性看作一个向量，即，用x这个向量来代表这个事物，x的集合记为X，称为属性集。类别也有很多种，用集合表示。一般X和C的关系是不确定的，可以将X和C看作是随机变量，P(C|X)称为C的后验概率，与之相对的，P(C)称为C的先验概率。 根据贝叶斯公式，后验概率P(C|X)=P(X|C)P(C)/P(X)，但在比较不同C值的后验概率时，分母P(X)总是常数，忽略掉，后验概率P(C|X)=P(X|C)P(C)，先验概率P(C)可以通过计算训练集中属于每一个类的训练样本所占的比例，对类条件概率P(X|C)的估计，我们只谈论朴素贝叶斯分类器方法，因为朴素贝叶斯假设事物属性之间相互条件独立，。 2、模型原理与训练 朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)即为词频型和伯努利模型(Bernoulli model)即文档型，还有一种高斯模型。 前二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。 这里暂不考虑特征抽取、为避免消除测试文档时类条件概率中有为0现象而做的取对数等问题。 （1）高斯模型 有些特征可能是连续型变量，比如说人的身高，物体的长度，这些特征可以转换成离散型的值，比如如果身高在160cm以下，特征值为1；在160cm和170cm之间，特征值为2；在170cm之上，特征值为3。也可以这样转换，将身高转换为3个特征，分别是f1、f2、f3，如果身高是160cm以下，这三个特征的值分别是1、0、0，若身高在170cm之上，这三个特征的值分别是0、0、1。不过这些方式都不够细腻，高斯模型可以解决这个问题。高斯模型假设这些一个特征的所有属于某个类别的观测值符合高斯分布，也就是： from sklearn.naive_bayes import GaussianNB #高斯贝叶斯 def train_model_GaussianNB(): pass clf3 = GaussianNB() clf3.fit(X[499:], y[499:])#训练模型 predict_labels = clf3.predict(X[0:499]) # 预测对了几个？ n = 0 for i in range(len(predict_labels)): if (predict_labels[i] == y[i]): n = n + 1 print(&quot;高斯贝叶斯:&quot;) # 正确率 print n / 499.0 # 混淆矩阵 confusion_matrix(y[0:499], predict_labels) return 2）多项式模型 在多项式模型中，设某文档d=(t1,t2,…,tk)，tk是该文档中出现过的单词，允许重复，则先验概率P(c)= 类c下单词总数/整个训练样本的单词总数。类条件概率P(tk|c)=(类c下单词tk在各个文档中出现过的次数之和+1)/(类c下单词总数+|V|)。 其中V是训练样本的单词表（即抽取单词，单词出现多次，只算一个），|V|则表示训练样本包含多少种单词。P(tk|c)可以看作是单词tk在证明d属于类c上提供了多大的证据，而P(c)则可以认为是类别c在整体上占多大比例(有多大可能性)。 &nbsp; from sklearn.naive_bayes import MultinomialNB #多项式贝叶斯 def train_model_MultinomialNB(): pass clf = MultinomialNB() #训练模型 clf.fit(X[499:],y[499:]) #预测训练集 predict_labels = clf.predict(X[0:499]) #预测对了几个？ n = 0 for i in range(len(predict_labels)): if(predict_labels[i] == y[i]): n = n + 1 print(&quot;多项式贝叶斯:&quot;) #正确率 print n/499.0 #混淆矩阵 confusion_matrix(y[0:499], predict_labels) return （3）伯努利模型 P(c)= 类c下文件总数/整个训练样本的文件总数&nbsp; P(tk|c)=(类c下包含单词tk的文件数+1)/(类c下包含的文件+2) from sklearn.naive_bayes import BernoulliNB #伯努利贝叶斯 def train_model_BernoulliNB(): pass clf2 = BernoulliNB() clf2.fit(X[499:], y[499:]) predict_labels = clf2.predict(X[0:499]) # 预测对了几个？ n = 0 for i in range(len(predict_labels)): if (predict_labels[i] == y[i]): n = n + 1 print(&quot;伯努利贝叶斯:&quot;) # 正确率 print n / 499.0 # 混淆矩阵 confusion_matrix(y[0:499], predict_labels) return &nbsp; 文本分类是作为离散型数据的。朴素贝叶斯用于很多方面，数据就会有连续和离散的，连续型时可用正态分布，还可用区间，将数据的各属性分成几个区间段进行概率计算，测试时看其属性的值在哪个区间就用哪个条件概率。再有TF、TDIDF，这些只是描述事物属性时的不同计算方法，例如文本分类时，可以用单词在本文档中出现的次数描述一个文档，可以用出现还是没出现即0和1来描述，还可以用单词在本类文档中出现的次数与这个单词在剩余类出现的次数（降低此属性对某类的重要性）相结合来表述。 以上参见原文：https://blog.csdn.net/u013710265/article/details/72780520&nbsp; &nbsp; 4）文本分类实践 搜狗新闻数据源：http://www.sogou.com/labs/resource/ca.php 从搜狗下载的数据是类似XML的带标签对的数据，因此需要使用正则表达式或者BeautifulSoup等工具处理为dataframe格式 import pandas as pd import jieba from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB #从本地读取数据集，并切分训练、测试集 data = pd.read_table(&#39;news_data/news_data.txt&#39;) x_train, x_test, y_train, y_test = train_test_split(data[&#39;新闻内容&#39;], data[&#39;类别&#39;], random_state=1) #分词 def fenci(train_data): words_df = train_data.apply(lambda x:&#39; &#39;.join(jieba.cut(x))) return words_df x_train_fenci = fenci(x_train) x_train_fenci[:5] #引入停用词 infile = open(&quot;stopwords.txt&quot;,encoding=&#39;utf-8&#39;) stopwords_lst = infile.readlines() stopwords = [x.strip() for x in stopwords_lst] #文本特征提取（词库表示法） vectorizer = CountVectorizer(stop_words=stopwords, max_features=5000) vectorizer.fit(x_train_fenci) #建模 classifier = MultinomialNB() #模型训练 classifier.fit(vectorizer.transform(x_train_fenci), y_train) #使用训练好的模型进行预测 classifier.score(vectorizer.transform(fenci(x_test)), y_test) PS：（在文本特征提取中CountVectorizer旨在通过计数来将一个文档转换为向量。当不存在先验字典时，Countvectorizer作为Estimator提取词汇进行训练，并生成一个CountVectorizerModel用于存储相应的词汇向量空间。该模型产生文档关于词语的稀疏表示。）下面举个例子： #用于转词向量的语料 yuliao = [&#39;dog cat fish dog dog dog&#39;,&#39;cat eat fish&#39;,&#39;i like eat fish&#39;] #sklearn库CountVectorizer转词向量 from sklearn.feature_extraction.text import CountVectorizer cv = CountVectorizer() vector = cv.fit_transform(yuliao) vector.todense() 接下来我们看下提取到的特征分别是 {&#39;cat&#39;: 0, &#39;dog&#39;: 1, &#39;eat&#39;: 2, &#39;fish&#39;: 3, &#39;like&#39;: 4} 从上面的例子可以看出，语料中每个词作为一个特征，词频数作为特征的值，如第一句中dog出现了4次，因此特征值为4。 &nbsp; Tips1： 加入文本特征提取(TF-IDF)：TF-IDF（term frequency–inverse document frequency），词频-逆文件频率。ps:（ta是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降） 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章. 下面直接给出一个词x的IDF的基本公式如下： &nbsp;其中，N代表语料库中文本的总数，而N(x)代表语料库中包含词x的文本总数 TF−IDF(x)=TF(x)∗IDF(x)&nbsp;其中TF(x)指词x在当前文本中的词频 词库表示法的缺点：一些普遍出现的词，词频较高，看起来似乎是更重要的特征，但因为这个词普遍出现，这个词可能不是非常的重要。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。用scikit-learn进行TF-IDF预处理 from sklearn.feature_extraction.text import TfidfVectorizer #使用tf-idf把文本转为向量 tv = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase = False) tv.fit(x_train_fenci) #模型训练 classifier.fit(tv.transform(fenci(x_train)), y_train) #利用训练好的模型测试 classifier.score(tv.transform(fenci(x_test)), y_test) Tips2： N-gram模型 在朴素贝叶斯算法中，为了避免维度灾难，有一个大胆的假设，即X的n个维度之间相互独立: 这个假设下，条件分布大大的简化了，但是这也可能带来预测的不准确性。n个维度相互独立，就是完全没有考虑上下文语境，把每个词拆开单独看，这么看的话，猫吃鱼、鱼吃猫得到的特征向量是完全一样的。 N-gram模型就是假设与附近n个词是相关的，比如当n=(1,2)时，猫吃鱼提取到的特征就是[‘猫’,‘吃’,‘鱼’,‘猫吃’,‘吃鱼’]，为了平衡计算量和上下文关系，N一般取2或者3。朴素贝叶斯n个维度之间完全相互独立的假设，就是N-gram的n=1时的情况 #转词向量 tv_2gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,2),lowercase = False) tv_2gram.fit(x_train_fenci) #训练模型 clf_2gram = MultinomialNB() clf_2gram.fit(tv_2gram.transform(fenci(x_train)), y_train) #预测 clf_2gram.score(tv_2gram.transform(fenci(x_test)), y_test) &nbsp;" />
<meta property="og:description" content="一 、朴素贝叶斯基本原理 基于朴素贝叶斯公式，比较出后验概率的最大值来进行分类，后验概率的计算是由先验概率与类条件概率的乘积得出，先验概率和类条件概率要通过训练数据集得出，即为朴素贝叶斯分类模型，将其保存为中间结果，测试文档进行分类时调用这个中间结果得出后验概率。 1、基本定义 分类是把一个事物分到某个类别中。一个事物具有很多属性，把它的众多属性看作一个向量，即，用x这个向量来代表这个事物，x的集合记为X，称为属性集。类别也有很多种，用集合表示。一般X和C的关系是不确定的，可以将X和C看作是随机变量，P(C|X)称为C的后验概率，与之相对的，P(C)称为C的先验概率。 根据贝叶斯公式，后验概率P(C|X)=P(X|C)P(C)/P(X)，但在比较不同C值的后验概率时，分母P(X)总是常数，忽略掉，后验概率P(C|X)=P(X|C)P(C)，先验概率P(C)可以通过计算训练集中属于每一个类的训练样本所占的比例，对类条件概率P(X|C)的估计，我们只谈论朴素贝叶斯分类器方法，因为朴素贝叶斯假设事物属性之间相互条件独立，。 2、模型原理与训练 朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)即为词频型和伯努利模型(Bernoulli model)即文档型，还有一种高斯模型。 前二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。 这里暂不考虑特征抽取、为避免消除测试文档时类条件概率中有为0现象而做的取对数等问题。 （1）高斯模型 有些特征可能是连续型变量，比如说人的身高，物体的长度，这些特征可以转换成离散型的值，比如如果身高在160cm以下，特征值为1；在160cm和170cm之间，特征值为2；在170cm之上，特征值为3。也可以这样转换，将身高转换为3个特征，分别是f1、f2、f3，如果身高是160cm以下，这三个特征的值分别是1、0、0，若身高在170cm之上，这三个特征的值分别是0、0、1。不过这些方式都不够细腻，高斯模型可以解决这个问题。高斯模型假设这些一个特征的所有属于某个类别的观测值符合高斯分布，也就是： from sklearn.naive_bayes import GaussianNB #高斯贝叶斯 def train_model_GaussianNB(): pass clf3 = GaussianNB() clf3.fit(X[499:], y[499:])#训练模型 predict_labels = clf3.predict(X[0:499]) # 预测对了几个？ n = 0 for i in range(len(predict_labels)): if (predict_labels[i] == y[i]): n = n + 1 print(&quot;高斯贝叶斯:&quot;) # 正确率 print n / 499.0 # 混淆矩阵 confusion_matrix(y[0:499], predict_labels) return 2）多项式模型 在多项式模型中，设某文档d=(t1,t2,…,tk)，tk是该文档中出现过的单词，允许重复，则先验概率P(c)= 类c下单词总数/整个训练样本的单词总数。类条件概率P(tk|c)=(类c下单词tk在各个文档中出现过的次数之和+1)/(类c下单词总数+|V|)。 其中V是训练样本的单词表（即抽取单词，单词出现多次，只算一个），|V|则表示训练样本包含多少种单词。P(tk|c)可以看作是单词tk在证明d属于类c上提供了多大的证据，而P(c)则可以认为是类别c在整体上占多大比例(有多大可能性)。 &nbsp; from sklearn.naive_bayes import MultinomialNB #多项式贝叶斯 def train_model_MultinomialNB(): pass clf = MultinomialNB() #训练模型 clf.fit(X[499:],y[499:]) #预测训练集 predict_labels = clf.predict(X[0:499]) #预测对了几个？ n = 0 for i in range(len(predict_labels)): if(predict_labels[i] == y[i]): n = n + 1 print(&quot;多项式贝叶斯:&quot;) #正确率 print n/499.0 #混淆矩阵 confusion_matrix(y[0:499], predict_labels) return （3）伯努利模型 P(c)= 类c下文件总数/整个训练样本的文件总数&nbsp; P(tk|c)=(类c下包含单词tk的文件数+1)/(类c下包含的文件+2) from sklearn.naive_bayes import BernoulliNB #伯努利贝叶斯 def train_model_BernoulliNB(): pass clf2 = BernoulliNB() clf2.fit(X[499:], y[499:]) predict_labels = clf2.predict(X[0:499]) # 预测对了几个？ n = 0 for i in range(len(predict_labels)): if (predict_labels[i] == y[i]): n = n + 1 print(&quot;伯努利贝叶斯:&quot;) # 正确率 print n / 499.0 # 混淆矩阵 confusion_matrix(y[0:499], predict_labels) return &nbsp; 文本分类是作为离散型数据的。朴素贝叶斯用于很多方面，数据就会有连续和离散的，连续型时可用正态分布，还可用区间，将数据的各属性分成几个区间段进行概率计算，测试时看其属性的值在哪个区间就用哪个条件概率。再有TF、TDIDF，这些只是描述事物属性时的不同计算方法，例如文本分类时，可以用单词在本文档中出现的次数描述一个文档，可以用出现还是没出现即0和1来描述，还可以用单词在本类文档中出现的次数与这个单词在剩余类出现的次数（降低此属性对某类的重要性）相结合来表述。 以上参见原文：https://blog.csdn.net/u013710265/article/details/72780520&nbsp; &nbsp; 4）文本分类实践 搜狗新闻数据源：http://www.sogou.com/labs/resource/ca.php 从搜狗下载的数据是类似XML的带标签对的数据，因此需要使用正则表达式或者BeautifulSoup等工具处理为dataframe格式 import pandas as pd import jieba from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB #从本地读取数据集，并切分训练、测试集 data = pd.read_table(&#39;news_data/news_data.txt&#39;) x_train, x_test, y_train, y_test = train_test_split(data[&#39;新闻内容&#39;], data[&#39;类别&#39;], random_state=1) #分词 def fenci(train_data): words_df = train_data.apply(lambda x:&#39; &#39;.join(jieba.cut(x))) return words_df x_train_fenci = fenci(x_train) x_train_fenci[:5] #引入停用词 infile = open(&quot;stopwords.txt&quot;,encoding=&#39;utf-8&#39;) stopwords_lst = infile.readlines() stopwords = [x.strip() for x in stopwords_lst] #文本特征提取（词库表示法） vectorizer = CountVectorizer(stop_words=stopwords, max_features=5000) vectorizer.fit(x_train_fenci) #建模 classifier = MultinomialNB() #模型训练 classifier.fit(vectorizer.transform(x_train_fenci), y_train) #使用训练好的模型进行预测 classifier.score(vectorizer.transform(fenci(x_test)), y_test) PS：（在文本特征提取中CountVectorizer旨在通过计数来将一个文档转换为向量。当不存在先验字典时，Countvectorizer作为Estimator提取词汇进行训练，并生成一个CountVectorizerModel用于存储相应的词汇向量空间。该模型产生文档关于词语的稀疏表示。）下面举个例子： #用于转词向量的语料 yuliao = [&#39;dog cat fish dog dog dog&#39;,&#39;cat eat fish&#39;,&#39;i like eat fish&#39;] #sklearn库CountVectorizer转词向量 from sklearn.feature_extraction.text import CountVectorizer cv = CountVectorizer() vector = cv.fit_transform(yuliao) vector.todense() 接下来我们看下提取到的特征分别是 {&#39;cat&#39;: 0, &#39;dog&#39;: 1, &#39;eat&#39;: 2, &#39;fish&#39;: 3, &#39;like&#39;: 4} 从上面的例子可以看出，语料中每个词作为一个特征，词频数作为特征的值，如第一句中dog出现了4次，因此特征值为4。 &nbsp; Tips1： 加入文本特征提取(TF-IDF)：TF-IDF（term frequency–inverse document frequency），词频-逆文件频率。ps:（ta是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降） 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章. 下面直接给出一个词x的IDF的基本公式如下： &nbsp;其中，N代表语料库中文本的总数，而N(x)代表语料库中包含词x的文本总数 TF−IDF(x)=TF(x)∗IDF(x)&nbsp;其中TF(x)指词x在当前文本中的词频 词库表示法的缺点：一些普遍出现的词，词频较高，看起来似乎是更重要的特征，但因为这个词普遍出现，这个词可能不是非常的重要。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。用scikit-learn进行TF-IDF预处理 from sklearn.feature_extraction.text import TfidfVectorizer #使用tf-idf把文本转为向量 tv = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase = False) tv.fit(x_train_fenci) #模型训练 classifier.fit(tv.transform(fenci(x_train)), y_train) #利用训练好的模型测试 classifier.score(tv.transform(fenci(x_test)), y_test) Tips2： N-gram模型 在朴素贝叶斯算法中，为了避免维度灾难，有一个大胆的假设，即X的n个维度之间相互独立: 这个假设下，条件分布大大的简化了，但是这也可能带来预测的不准确性。n个维度相互独立，就是完全没有考虑上下文语境，把每个词拆开单独看，这么看的话，猫吃鱼、鱼吃猫得到的特征向量是完全一样的。 N-gram模型就是假设与附近n个词是相关的，比如当n=(1,2)时，猫吃鱼提取到的特征就是[‘猫’,‘吃’,‘鱼’,‘猫吃’,‘吃鱼’]，为了平衡计算量和上下文关系，N一般取2或者3。朴素贝叶斯n个维度之间完全相互独立的假设，就是N-gram的n=1时的情况 #转词向量 tv_2gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,2),lowercase = False) tv_2gram.fit(x_train_fenci) #训练模型 clf_2gram = MultinomialNB() clf_2gram.fit(tv_2gram.transform(fenci(x_train)), y_train) #预测 clf_2gram.score(tv_2gram.transform(fenci(x_test)), y_test) &nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/19/787277.html" />
<meta property="og:url" content="https://mlh.app/2019/05/19/787277.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-19T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"一 、朴素贝叶斯基本原理 基于朴素贝叶斯公式，比较出后验概率的最大值来进行分类，后验概率的计算是由先验概率与类条件概率的乘积得出，先验概率和类条件概率要通过训练数据集得出，即为朴素贝叶斯分类模型，将其保存为中间结果，测试文档进行分类时调用这个中间结果得出后验概率。 1、基本定义 分类是把一个事物分到某个类别中。一个事物具有很多属性，把它的众多属性看作一个向量，即，用x这个向量来代表这个事物，x的集合记为X，称为属性集。类别也有很多种，用集合表示。一般X和C的关系是不确定的，可以将X和C看作是随机变量，P(C|X)称为C的后验概率，与之相对的，P(C)称为C的先验概率。 根据贝叶斯公式，后验概率P(C|X)=P(X|C)P(C)/P(X)，但在比较不同C值的后验概率时，分母P(X)总是常数，忽略掉，后验概率P(C|X)=P(X|C)P(C)，先验概率P(C)可以通过计算训练集中属于每一个类的训练样本所占的比例，对类条件概率P(X|C)的估计，我们只谈论朴素贝叶斯分类器方法，因为朴素贝叶斯假设事物属性之间相互条件独立，。 2、模型原理与训练 朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)即为词频型和伯努利模型(Bernoulli model)即文档型，还有一种高斯模型。 前二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。 这里暂不考虑特征抽取、为避免消除测试文档时类条件概率中有为0现象而做的取对数等问题。 （1）高斯模型 有些特征可能是连续型变量，比如说人的身高，物体的长度，这些特征可以转换成离散型的值，比如如果身高在160cm以下，特征值为1；在160cm和170cm之间，特征值为2；在170cm之上，特征值为3。也可以这样转换，将身高转换为3个特征，分别是f1、f2、f3，如果身高是160cm以下，这三个特征的值分别是1、0、0，若身高在170cm之上，这三个特征的值分别是0、0、1。不过这些方式都不够细腻，高斯模型可以解决这个问题。高斯模型假设这些一个特征的所有属于某个类别的观测值符合高斯分布，也就是： from sklearn.naive_bayes import GaussianNB #高斯贝叶斯 def train_model_GaussianNB(): pass clf3 = GaussianNB() clf3.fit(X[499:], y[499:])#训练模型 predict_labels = clf3.predict(X[0:499]) # 预测对了几个？ n = 0 for i in range(len(predict_labels)): if (predict_labels[i] == y[i]): n = n + 1 print(&quot;高斯贝叶斯:&quot;) # 正确率 print n / 499.0 # 混淆矩阵 confusion_matrix(y[0:499], predict_labels) return 2）多项式模型 在多项式模型中，设某文档d=(t1,t2,…,tk)，tk是该文档中出现过的单词，允许重复，则先验概率P(c)= 类c下单词总数/整个训练样本的单词总数。类条件概率P(tk|c)=(类c下单词tk在各个文档中出现过的次数之和+1)/(类c下单词总数+|V|)。 其中V是训练样本的单词表（即抽取单词，单词出现多次，只算一个），|V|则表示训练样本包含多少种单词。P(tk|c)可以看作是单词tk在证明d属于类c上提供了多大的证据，而P(c)则可以认为是类别c在整体上占多大比例(有多大可能性)。 &nbsp; from sklearn.naive_bayes import MultinomialNB #多项式贝叶斯 def train_model_MultinomialNB(): pass clf = MultinomialNB() #训练模型 clf.fit(X[499:],y[499:]) #预测训练集 predict_labels = clf.predict(X[0:499]) #预测对了几个？ n = 0 for i in range(len(predict_labels)): if(predict_labels[i] == y[i]): n = n + 1 print(&quot;多项式贝叶斯:&quot;) #正确率 print n/499.0 #混淆矩阵 confusion_matrix(y[0:499], predict_labels) return （3）伯努利模型 P(c)= 类c下文件总数/整个训练样本的文件总数&nbsp; P(tk|c)=(类c下包含单词tk的文件数+1)/(类c下包含的文件+2) from sklearn.naive_bayes import BernoulliNB #伯努利贝叶斯 def train_model_BernoulliNB(): pass clf2 = BernoulliNB() clf2.fit(X[499:], y[499:]) predict_labels = clf2.predict(X[0:499]) # 预测对了几个？ n = 0 for i in range(len(predict_labels)): if (predict_labels[i] == y[i]): n = n + 1 print(&quot;伯努利贝叶斯:&quot;) # 正确率 print n / 499.0 # 混淆矩阵 confusion_matrix(y[0:499], predict_labels) return &nbsp; 文本分类是作为离散型数据的。朴素贝叶斯用于很多方面，数据就会有连续和离散的，连续型时可用正态分布，还可用区间，将数据的各属性分成几个区间段进行概率计算，测试时看其属性的值在哪个区间就用哪个条件概率。再有TF、TDIDF，这些只是描述事物属性时的不同计算方法，例如文本分类时，可以用单词在本文档中出现的次数描述一个文档，可以用出现还是没出现即0和1来描述，还可以用单词在本类文档中出现的次数与这个单词在剩余类出现的次数（降低此属性对某类的重要性）相结合来表述。 以上参见原文：https://blog.csdn.net/u013710265/article/details/72780520&nbsp; &nbsp; 4）文本分类实践 搜狗新闻数据源：http://www.sogou.com/labs/resource/ca.php 从搜狗下载的数据是类似XML的带标签对的数据，因此需要使用正则表达式或者BeautifulSoup等工具处理为dataframe格式 import pandas as pd import jieba from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB #从本地读取数据集，并切分训练、测试集 data = pd.read_table(&#39;news_data/news_data.txt&#39;) x_train, x_test, y_train, y_test = train_test_split(data[&#39;新闻内容&#39;], data[&#39;类别&#39;], random_state=1) #分词 def fenci(train_data): words_df = train_data.apply(lambda x:&#39; &#39;.join(jieba.cut(x))) return words_df x_train_fenci = fenci(x_train) x_train_fenci[:5] #引入停用词 infile = open(&quot;stopwords.txt&quot;,encoding=&#39;utf-8&#39;) stopwords_lst = infile.readlines() stopwords = [x.strip() for x in stopwords_lst] #文本特征提取（词库表示法） vectorizer = CountVectorizer(stop_words=stopwords, max_features=5000) vectorizer.fit(x_train_fenci) #建模 classifier = MultinomialNB() #模型训练 classifier.fit(vectorizer.transform(x_train_fenci), y_train) #使用训练好的模型进行预测 classifier.score(vectorizer.transform(fenci(x_test)), y_test) PS：（在文本特征提取中CountVectorizer旨在通过计数来将一个文档转换为向量。当不存在先验字典时，Countvectorizer作为Estimator提取词汇进行训练，并生成一个CountVectorizerModel用于存储相应的词汇向量空间。该模型产生文档关于词语的稀疏表示。）下面举个例子： #用于转词向量的语料 yuliao = [&#39;dog cat fish dog dog dog&#39;,&#39;cat eat fish&#39;,&#39;i like eat fish&#39;] #sklearn库CountVectorizer转词向量 from sklearn.feature_extraction.text import CountVectorizer cv = CountVectorizer() vector = cv.fit_transform(yuliao) vector.todense() 接下来我们看下提取到的特征分别是 {&#39;cat&#39;: 0, &#39;dog&#39;: 1, &#39;eat&#39;: 2, &#39;fish&#39;: 3, &#39;like&#39;: 4} 从上面的例子可以看出，语料中每个词作为一个特征，词频数作为特征的值，如第一句中dog出现了4次，因此特征值为4。 &nbsp; Tips1： 加入文本特征提取(TF-IDF)：TF-IDF（term frequency–inverse document frequency），词频-逆文件频率。ps:（ta是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降） 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章. 下面直接给出一个词x的IDF的基本公式如下： &nbsp;其中，N代表语料库中文本的总数，而N(x)代表语料库中包含词x的文本总数 TF−IDF(x)=TF(x)∗IDF(x)&nbsp;其中TF(x)指词x在当前文本中的词频 词库表示法的缺点：一些普遍出现的词，词频较高，看起来似乎是更重要的特征，但因为这个词普遍出现，这个词可能不是非常的重要。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。用scikit-learn进行TF-IDF预处理 from sklearn.feature_extraction.text import TfidfVectorizer #使用tf-idf把文本转为向量 tv = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase = False) tv.fit(x_train_fenci) #模型训练 classifier.fit(tv.transform(fenci(x_train)), y_train) #利用训练好的模型测试 classifier.score(tv.transform(fenci(x_test)), y_test) Tips2： N-gram模型 在朴素贝叶斯算法中，为了避免维度灾难，有一个大胆的假设，即X的n个维度之间相互独立: 这个假设下，条件分布大大的简化了，但是这也可能带来预测的不准确性。n个维度相互独立，就是完全没有考虑上下文语境，把每个词拆开单独看，这么看的话，猫吃鱼、鱼吃猫得到的特征向量是完全一样的。 N-gram模型就是假设与附近n个词是相关的，比如当n=(1,2)时，猫吃鱼提取到的特征就是[‘猫’,‘吃’,‘鱼’,‘猫吃’,‘吃鱼’]，为了平衡计算量和上下文关系，N一般取2或者3。朴素贝叶斯n个维度之间完全相互独立的假设，就是N-gram的n=1时的情况 #转词向量 tv_2gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,2),lowercase = False) tv_2gram.fit(x_train_fenci) #训练模型 clf_2gram = MultinomialNB() clf_2gram.fit(tv_2gram.transform(fenci(x_train)), y_train) #预测 clf_2gram.score(tv_2gram.transform(fenci(x_test)), y_test) &nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/19/787277.html","headline":"自然语言处理之朴素贝叶斯","dateModified":"2019-05-19T00:00:00+08:00","datePublished":"2019-05-19T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/19/787277.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>自然语言处理之朴素贝叶斯</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h1>一 、朴素贝叶斯基本原理</h1> 
  <ul>
   <li>基于朴素贝叶斯公式，比较出后验概率的最大值来进行分类，后验概率的计算是由先验概率与类条件概率的乘积得出，先验概率和类条件概率要通过训练数据集得出，即为朴素贝叶斯分类模型，将其保存为中间结果，测试文档进行分类时调用这个中间结果得出后验概率。</li> 
  </ul>
  <h2>1、基本定义</h2> 
  <ul>
   <li>分类是把一个事物分到某个类别中。一个事物具有很多属性，把它的众多属性看作一个向量，即<img alt="X=(x_{1},x_{2},x_{3},\cdot \cdot \cdot ,x_{n})" class="mathcode" src="https://private.codecogs.com/gif.latex?X%3D%28x_%7B1%7D%2Cx_%7B2%7D%2Cx_%7B3%7D%2C%5Ccdot%20%5Ccdot%20%5Ccdot%20%2Cx_%7Bn%7D%29">，用x这个向量来代表这个事物，x的集合记为X，称为属性集。类别也有很多种，用集合<img alt="C={c{_{1}},c{_{2}},\cdot \cdot \cdot ,c{_{m}}}" class="mathcode" src="https://private.codecogs.com/gif.latex?C%3D%7Bc%7B_%7B1%7D%7D%2Cc%7B_%7B2%7D%7D%2C%5Ccdot%20%5Ccdot%20%5Ccdot%20%2Cc%7B_%7Bm%7D%7D%7D">表示。一般X和C的关系是不确定的，可以将X和C看作是随机变量，P(C|X)称为C的后验概率，与之相对的，P(C)称为C的先验概率。</li> 
   <li>根据贝叶斯公式，后验概率P(C|X)=P(X|C)P(C)/P(X)，但在比较不同C值的后验概率时，分母P(X)总是常数，忽略掉，后验概率P(C|X)=P(X|C)P(C)，先验概率P(C)可以通过计算训练集中属于每一个类的训练样本所占的比例，对类条件概率P(X|C)的估计，我们只谈论朴素贝叶斯分类器方法，因为朴素贝叶斯假设事物属性之间相互条件独立，<img alt="P(X|C)=\prod P(x_{i}|c_{i})" class="mathcode" src="https://private.codecogs.com/gif.latex?P%28X%7CC%29%3D%5Cprod%20P%28x_%7Bi%7D%7Cc_%7Bi%7D%29">。</li> 
  </ul>
  <h2>2、模型原理与训练</h2> 
  <ul>
   <li>朴素贝叶斯分类器是一种有监督学习，常见有两种模型，多项式模型(multinomial model)即为词频型和伯努利模型(Bernoulli model)即文档型，还有一种高斯模型。</li> 
   <li>前二者的计算粒度不一样，多项式模型以单词为粒度，伯努利模型以文件为粒度，因此二者的先验概率和类条件概率的计算方法都不同。计算后验概率时，对于一个文档d，多项式模型中，只有在d中出现过的单词，才会参与后验概率计算，伯努利模型中，没有在d中出现，但是在全局单词表中出现的单词，也会参与计算，不过是作为“反方”参与的。</li> 
   <li>这里暂不考虑特征抽取、为避免消除测试文档时类条件概率中有为0现象而做的取对数等问题。</li> 
  </ul>
  <h3>（1）高斯模型</h3> 
  <p>有些特征可能是连续型变量，比如说人的身高，物体的长度，这些特征可以转换成离散型的值，比如如果身高在160cm以下，特征值为1；在160cm和170cm之间，特征值为2；在170cm之上，特征值为3。也可以这样转换，将身高转换为3个特征，分别是f1、f2、f3，如果身高是160cm以下，这三个特征的值分别是1、0、0，若身高在170cm之上，这三个特征的值分别是0、0、1。不过这些方式都不够细腻，高斯模型可以解决这个问题。高斯模型假设这些一个特征的所有属于某个类别的观测值符合高斯分布，也就是：</p> 
  <pre class="has">
<code>from sklearn.naive_bayes import GaussianNB
#高斯贝叶斯
def train_model_GaussianNB():
    pass
    clf3 = GaussianNB()
    clf3.fit(X[499:], y[499:])#训练模型
    predict_labels = clf3.predict(X[0:499])
    # 预测对了几个？
    n = 0
    for i in range(len(predict_labels)):
        if (predict_labels[i] == y[i]):
            n = n + 1
    print("高斯贝叶斯:")
    # 正确率
    print n / 499.0
    # 混淆矩阵
    confusion_matrix(y[0:499], predict_labels)
    return
</code></pre> 
  <h3>2）多项式模型</h3> 
  <p><br> 在多项式模型中，设某文档d=(t1,t2,…,tk)，tk是该文档中出现过的单词，允许重复，则先验概率P(c)= 类c下单词总数/整个训练样本的单词总数。类条件概率P(tk|c)=(类c下单词tk在各个文档中出现过的次数之和+1)/(类c下单词总数+|V|)。<br> 其中V是训练样本的单词表（即抽取单词，单词出现多次，只算一个），|V|则表示训练样本包含多少种单词。P(tk|c)可以看作是单词tk在证明d属于类c上提供了多大的证据，而P(c)则可以认为是类别c在整体上占多大比例(有多大可能性)。</p> 
  <p>&nbsp;</p> 
  <pre class="has">
<code>
from sklearn.naive_bayes import MultinomialNB
#多项式贝叶斯
def train_model_MultinomialNB():
    pass
    clf = MultinomialNB()
    #训练模型
    clf.fit(X[499:],y[499:])
    #预测训练集
    predict_labels = clf.predict(X[0:499])
    #预测对了几个？
    n = 0
    for i in range(len(predict_labels)):
        if(predict_labels[i] == y[i]):
            n = n + 1
    print("多项式贝叶斯:")
    #正确率
    print n/499.0
    #混淆矩阵
    confusion_matrix(y[0:499], predict_labels)
    return
</code></pre> 
  <h3>（3）伯努利模型</h3> 
  <p><br> P(c)= 类c下文件总数/整个训练样本的文件总数&nbsp;<br> P(tk|c)=(类c下包含单词tk的文件数+1)/(类c下包含的文件+2)</p> 
  <pre class="has">
<code>from sklearn.naive_bayes import BernoulliNB
#伯努利贝叶斯
def train_model_BernoulliNB():
    pass
    clf2 = BernoulliNB()
    clf2.fit(X[499:], y[499:])
    predict_labels = clf2.predict(X[0:499])
    # 预测对了几个？
    n = 0
    for i in range(len(predict_labels)):
        if (predict_labels[i] == y[i]):
            n = n + 1
    print("伯努利贝叶斯:")
    # 正确率
    print n / 499.0
    # 混淆矩阵
    confusion_matrix(y[0:499], predict_labels)
    return
</code></pre> 
  <p>&nbsp;</p> 
  <p>文本分类是作为离散型数据的。朴素贝叶斯用于很多方面，数据就会有连续和离散的，连续型时可用正态分布，还可用区间，将数据的各属性分成几个区间段进行概率计算，测试时看其属性的值在哪个区间就用哪个条件概率。再有TF、TDIDF，这些只是描述事物属性时的不同计算方法，例如文本分类时，可以用单词在本文档中出现的次数描述一个文档，可以用出现还是没出现即0和1来描述，还可以用单词在本类文档中出现的次数与这个单词在剩余类出现的次数（降低此属性对某类的重要性）相结合来表述。<br> 以上参见原文：https://blog.csdn.net/u013710265/article/details/72780520&nbsp;<br> &nbsp;</p> 
  <h3>4）文本分类实践</h3> 
  <p>搜狗新闻数据源：<a href="http://www.sogou.com/labs/resource/ca.php" rel="nofollow">http://www.sogou.com/labs/resource/ca.php</a></p> 
  <p>从搜狗下载的数据是类似XML的带标签对的数据，因此需要使用正则表达式或者BeautifulSoup等工具处理为dataframe格式</p> 
  <pre class="has">
<code>import pandas as pd
import jieba
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
#从本地读取数据集，并切分训练、测试集
data = pd.read_table('news_data/news_data.txt')
x_train, x_test, y_train, y_test = train_test_split(data['新闻内容'], data['类别'], random_state=1)
#分词
def fenci(train_data):
    words_df = train_data.apply(lambda x:' '.join(jieba.cut(x)))
    return words_df

x_train_fenci = fenci(x_train)
x_train_fenci[:5]
#引入停用词
infile = open("stopwords.txt",encoding='utf-8')
stopwords_lst = infile.readlines()
stopwords = [x.strip() for x in stopwords_lst]
#文本特征提取（词库表示法）
vectorizer = CountVectorizer(stop_words=stopwords, max_features=5000)
vectorizer.fit(x_train_fenci)
#建模

classifier = MultinomialNB()
#模型训练
classifier.fit(vectorizer.transform(x_train_fenci), y_train)
#使用训练好的模型进行预测
classifier.score(vectorizer.transform(fenci(x_test)), y_test)
</code></pre> 
  <p>PS：（在文本特征提取中CountVectorizer旨在通过计数来将一个文档转换为向量。当不存在先验字典时，Countvectorizer作为Estimator提取词汇进行训练，并生成一个CountVectorizerModel用于存储相应的词汇向量空间。该模型产生文档关于词语的稀疏表示。）下面举个例子：</p> 
  <pre class="has">
<code>#用于转词向量的语料
yuliao = ['dog cat fish dog dog dog','cat eat fish','i like eat fish']

#sklearn库CountVectorizer转词向量
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
vector = cv.fit_transform(yuliao)
vector.todense()
</code></pre> 
  <p>接下来我们看下提取到的特征分别是</p> 
  <pre class="has">
<code>{'cat': 0, 'dog': 1, 'eat': 2, 'fish': 3, 'like': 4}</code></pre> 
  <p>从上面的例子可以看出，语料中每个词作为一个特征，词频数作为特征的值，如第一句中dog出现了4次，因此特征值为4。</p> 
  <p>&nbsp;</p> 
  <h3>Tips1：</h3> 
  <ul>
   <li>加入文本特征提取(TF-IDF)：<strong>TF-IDF</strong>（term frequency–inverse document frequency），词频-逆文件频率。ps:（ta是一种用于资讯检索与资讯探勘的常用加权技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降）</li> 
  </ul>
  <p>一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.</p> 
  <p>下面直接给出一个词x的IDF的基本公式如下：</p> 
  <p><img alt="IDF(x)=lg(N/N(x))" class="mathcode" src="https://private.codecogs.com/gif.latex?IDF%28x%29%3Dlg%28N/N%28x%29%29">&nbsp;其中，N代表语料库中文本的总数，而N(x)代表语料库中包含词x的文本总数</p> 
  <p>TF−IDF(x)=TF(x)∗IDF(x)&nbsp;其中TF(x)指词x在当前文本中的词频</p> 
  <p>词库表示法的缺点：一些普遍出现的词，词频较高，看起来似乎是更重要的特征，但因为这个词普遍出现，这个词可能不是非常的重要。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。<br><strong>用scikit-learn进行TF-IDF预处理</strong></p> 
  <pre class="has">
<code>from sklearn.feature_extraction.text import TfidfVectorizer
#使用tf-idf把文本转为向量
tv = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase = False)
tv.fit(x_train_fenci)
#模型训练
classifier.fit(tv.transform(fenci(x_train)), y_train)
#利用训练好的模型测试
classifier.score(tv.transform(fenci(x_test)), y_test)
</code></pre> 
  <h3>Tips2：</h3> 
  <p>N-gram模型</p> 
  <p>在朴素贝叶斯算法中，为了避免维度灾难，有一个大胆的假设，即X的n个维度之间相互独立:</p> 
  <p><img alt="" class="has" height="65" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519205723697.png" width="1015"></p> 
  <p>这个假设下，条件分布大大的简化了，但是这也可能带来预测的不准确性。n个维度相互独立，就是完全没有考虑上下文语境，把每个词拆开单独看，这么看的话，猫吃鱼、鱼吃猫得到的特征向量是完全一样的。</p> 
  <p>N-gram模型就是假设<img alt="X{_{n}}" class="mathcode" src="https://private.codecogs.com/gif.latex?X%7B_%7Bn%7D%7D">与附近n个词是相关的，比如当n=(1,2)时，猫吃鱼提取到的特征就是[‘猫’,‘吃’,‘鱼’,‘猫吃’,‘吃鱼’]，为了平衡计算量和上下文关系，N一般取2或者3。朴素贝叶斯n个维度之间完全相互独立的假设，就是N-gram的n=1时的情况</p> 
  <pre class="has">
<code>#转词向量
tv_2gram = TfidfVectorizer(stop_words=stopwords, max_features=5000, ngram_range=(1,2),lowercase = False)
tv_2gram.fit(x_train_fenci)
#训练模型
clf_2gram = MultinomialNB()
clf_2gram.fit(tv_2gram.transform(fenci(x_train)), y_train)
#预测
clf_2gram.score(tv_2gram.transform(fenci(x_test)), y_test)
</code></pre> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
