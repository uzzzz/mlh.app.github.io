<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>【目标检测 深度学习】3.Yolo系列算法原理 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="【目标检测 深度学习】3.Yolo系列算法原理" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="目录 1.YoloV1 1.1综述 1.2算法原理 1.3yolov1网络结构图 1.4网络结构分析 1.5损失函数定义 1.6网络训练 1.7yolov1网络存在的问题 1.8yolov1网络性能 2.YoloV2 2.1算法原理 2.2算法网络结构 2.3算法性能对比 3.Yolo9000 4.YoloV3 4.1YoloV3介绍： 4.2改进策略 4.2主干网络 4.3多尺度预测（类FPN） 4.4分类器 4.5网络性能 4.6darknet框架 5.Yolo系列网络优缺点 1.YoloV1 1.1综述 同时预测多个box位置和类别 端到端的目标检测和识别 速度更快 实现回归功能的CNN并不需要复杂的设计过程 hijack选用整图训练模型，更好地区分目标和背景区域 1.2算法原理 图像被分成S×S个格子，对于每一个格子 包含GT物体中心的格子负责检测相应的物体，如上图狗所在中心落在某一个格子上，则这个格子负责预测狗。 每个格子预测B（超参数）个检测框及其置信度（x,y,w,h,c），以及C个类别概率。置信度表面了预测的bbox中含有目标物体的可能性以及当前bbox预测得有多准。检测框及置信度对应bbox，类别对应当前的格子。每个格子预测的向量长度为5×B×C bbox信息（x,y,w,h）为物体的中心位置相对格子位置的偏移及宽度和高度，均被归一化。 置信度反映是否包含物体以及包含物体情况下位置的准确性，定义为 1.3yolov1网络结构图 yolov1的网络结构采用直接回归的方式来获取最终所需要检测的物体的位置和类别，它只包含了CNN网络结构。原始的输入图像经过多层卷积之后最终再通过一个FC层。原论文里S的大小为7，B为2。 通过这样的回归，最终可以得到每一个格子所包含的bbox的置信度以及是否包含目标物体，如果包含的话，当前物体的偏移量是多少，以及长宽是多少。对FC层通过的结果进行解码，得到上图中下半部分的两幅图，对于第一个图，虽然每个格子预测B个BBOX，但是只选择IOU最高的BBOX作为物体检测的输出，这也是yolo算法的缺陷，图像中可能会包含多个小的目标，但由于每一个格子只预测一个物体，如果格子同时出现多个物体的时候，效果就会变差。网络回归之后得到每一个bbox对应的类别和位置信息，通过NMS进行筛选后得到最终检测的结果。 1.4网络结构分析 网络使用小卷积，即1×1和3×3 FC输出为：S×S(B×5+C） 网络比VGG16快，准确率稍差 1.5损失函数定义 Loss函数：均方和误差 坐标误差、IOU误差和分类误差 权重考量 1.6网络训练 预训练 ImageNet1000类数据预训练 使用预训练参数（20个conv）来初始化yolo，并训练VOC20 将输入图像分辨率从224×224Resize到448×448。对于预训练模型，如果我们仅仅是使用了卷积层，则对feature map的大小不敏感 训练时B个bbox的GT设置相同 1.7yolov1网络存在的问题 输入尺寸固定（检测层没有采用多尺度输入） 小目标检测效果差 同一格子包含多个目标时，仅预测一个（IOU最高） 损失函数中，没有区分大物体的IOU和小物体的IOU的误差对于网络训练loss的贡献，实际上小物体的IOU误差会对网络优化过程造成更大的影响，进而降低物体定位的准确性。 出现不常见长宽比目标时，算法效果较差。 1.8yolov1网络性能 五种尺度训练的精度与其他网络的精度对比： 2.YoloV2 2.1算法原理 相比Yolov1： 引入了anchor box思想 输出层：卷积层替代YOLO V1的全连接层 联合使用coco和Imagenet物体分类标注数据 识别种类、精度、速度和定位准确性等都有大大提升 具体地改进之处： batch normalization 进行归一化处理让整个过程变得更稳定，收敛速度变得更快，达到模型规范化的效果 v1中也大量用了BN，同时在FC中运用dropout防止过拟合 v2中取消了dropout，均使用BN . 高分辨率分类器 v1中使用224×224预训练分类网络，448×448用于检测网络 v2以448×448的分辨率微调最初的分类网络，保证两个模型在分布上的一致性。 . anchor boxes 预测bbox的偏移，使用卷积代替FC。最后卷积层长宽为S×S，通道数为B×5+C。则与以前效果一样。 输入尺度：416。因为图片中的物体倾向于出现在图像中间的位置，如果是448×448的输入，那么最后下采样之后是14×14，没有中心，如果是416×416的输入，32倍下采样后是13×13（奇数输出），有中心，可以预测那些倾向于出现在中心的大物体。 max pooling下采样 . 预测超过1000个。比如13×13 然后每个anchor9个预测值，一共就预测1521个 mAP小幅度降低，recall显著提高 对于anchor box的宽、高、维度的选择往往是先验框，在训练的过程虽然也可以调整box的宽高维度，最终得到准确的bbox，但是作者希望我们开始选择的时候就选具有代表性的bbox的宽高维度，这样可以通过网络更容易学习到预测的位置，作者采用kmeans的方法对bbox进行回归，自动找到更好的bbox的宽高维度比，距离的度量采用IOU，这样误差与bbox尺寸无关，防止欧式距离中较大尺寸的框所得距离比较小尺寸的大。 最终anchor box预测数量为5最合适。 . 细粒度特征 添加pass through layer,把浅层特征图（26×26）连接到深层特征图 用Resnet中的identity mapping方法 把26×26×512的特征图叠加成13×13×2048的特征图，与深层特征图相连接，增加细粒度特征 性能获得1%的提升 multi-scale training，在结尾处用卷积代替FC,则输入图像尺寸就可以变化，所以可以改变图像尺寸进行多尺度训练。每隔几次迭代后就会微调网络的输入尺寸，输入图像尺寸{320,352，…608} 2.2算法网络结构 主干网络：Darknet-19 主要使用3×3卷积 pooling之后channel数增加 global average pooling 即用1×1卷积核压缩特征 1×1卷积压缩特征表示 batch normalization 每个box包含4个坐标值，1个置信度和classes个条件类别概率。在v1中每个cell预测一个类别概率，现在是每个bbox预测一个类别概率，所以之前每个格子预测的维度是5×B+C，在v2中，每个box预测5+C个向量。 2.3算法性能对比 下图是不同尺度训练的精度与其他网络的精度对比： 3.Yolo9000 yolo9000是在yolov2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。它采用了wordTree的结构，用它来混合检测数据集和识别数据集中的数据，来达到分类和检测联合训练的效果。 对于分类任务而言，同样是猫，但分类标签可能会有猫的类别的划分，比如波斯猫等等更细粒度的猫的标签，而对于检测任务而言，仅仅是识别猫、狗这种粗粒度上的概念，如果将分类和回归采用简单方法进行融合就会同时出现猫的label和波斯猫的label这种情况。word tree则是将这两种label来构建它们的粒度关系，将分类和检测任务的数据集融合，在检测任务中不仅要完成物体类别的回归，同样需要对物体类别进行判定，而在分类数据集上，对物体类别进行分类而且物体类别可能更细，通过wordl tree能够将label的层次关系表示出来。 . 通过联合训练策略，Yolo9000可以快速检测出超过9000个类别的物体，总体mAP值为19.7%。 4.YoloV3 4.1YoloV3介绍： 速度和精度最均衡的目标检测网络 融合多种先进方法，改进YoloV1/V2的缺点，且效果更优 重点解决了小物体检测问题 4.2改进策略 更好的主干网络（类ResNet） 多尺度预测（类FPN） 更好的分类器 4.2主干网络 主干网络对比如下： 4.3多尺度预测（类FPN） 聚类来得到Bbox的先验，聚类之后得到9个聚类中心，将这9个聚类中心平均分到3种尺度上，每种尺度预测3个box，另外对于每种尺度，作者引入卷积层来进一步提取特征，之后再输出box的信息。 如下图对于尺度1而言，作者通过卷积之后直接得到后续box的信息 对于尺度2而言，作者在进行输出之前会对尺度1输出的卷积进行上采样，然后同尺度2的feature map进行相加，相加之后再输出到后续的box信息，整个feature map的大小比scale 1扩大了两倍 尺度3比尺度2也扩大了两倍，如下图所示。 4.4分类器 更好的分类器：binary cross-entropy loss 因为softmax不适用于多标签分类 softmax可被独立的多个logistic分类器替代，且准确率不会下降 4.5网络性能 4.6darknet框架 由C语言和CUDA实现 GPU显存利用效率极高 第三方库的依赖较少 容易移植到其他平台，如windows或嵌入式设备 5.Yolo系列网络优缺点 优点： 快速，pipline简单 背景误检率低 通用性强 但相比RCNN系列物体检测方法，YOLO具有以下缺点（主要因为每个网格预测固定数量的物体使候选框数量减少）： 识别物体位置精准性差 召回率低" />
<meta property="og:description" content="目录 1.YoloV1 1.1综述 1.2算法原理 1.3yolov1网络结构图 1.4网络结构分析 1.5损失函数定义 1.6网络训练 1.7yolov1网络存在的问题 1.8yolov1网络性能 2.YoloV2 2.1算法原理 2.2算法网络结构 2.3算法性能对比 3.Yolo9000 4.YoloV3 4.1YoloV3介绍： 4.2改进策略 4.2主干网络 4.3多尺度预测（类FPN） 4.4分类器 4.5网络性能 4.6darknet框架 5.Yolo系列网络优缺点 1.YoloV1 1.1综述 同时预测多个box位置和类别 端到端的目标检测和识别 速度更快 实现回归功能的CNN并不需要复杂的设计过程 hijack选用整图训练模型，更好地区分目标和背景区域 1.2算法原理 图像被分成S×S个格子，对于每一个格子 包含GT物体中心的格子负责检测相应的物体，如上图狗所在中心落在某一个格子上，则这个格子负责预测狗。 每个格子预测B（超参数）个检测框及其置信度（x,y,w,h,c），以及C个类别概率。置信度表面了预测的bbox中含有目标物体的可能性以及当前bbox预测得有多准。检测框及置信度对应bbox，类别对应当前的格子。每个格子预测的向量长度为5×B×C bbox信息（x,y,w,h）为物体的中心位置相对格子位置的偏移及宽度和高度，均被归一化。 置信度反映是否包含物体以及包含物体情况下位置的准确性，定义为 1.3yolov1网络结构图 yolov1的网络结构采用直接回归的方式来获取最终所需要检测的物体的位置和类别，它只包含了CNN网络结构。原始的输入图像经过多层卷积之后最终再通过一个FC层。原论文里S的大小为7，B为2。 通过这样的回归，最终可以得到每一个格子所包含的bbox的置信度以及是否包含目标物体，如果包含的话，当前物体的偏移量是多少，以及长宽是多少。对FC层通过的结果进行解码，得到上图中下半部分的两幅图，对于第一个图，虽然每个格子预测B个BBOX，但是只选择IOU最高的BBOX作为物体检测的输出，这也是yolo算法的缺陷，图像中可能会包含多个小的目标，但由于每一个格子只预测一个物体，如果格子同时出现多个物体的时候，效果就会变差。网络回归之后得到每一个bbox对应的类别和位置信息，通过NMS进行筛选后得到最终检测的结果。 1.4网络结构分析 网络使用小卷积，即1×1和3×3 FC输出为：S×S(B×5+C） 网络比VGG16快，准确率稍差 1.5损失函数定义 Loss函数：均方和误差 坐标误差、IOU误差和分类误差 权重考量 1.6网络训练 预训练 ImageNet1000类数据预训练 使用预训练参数（20个conv）来初始化yolo，并训练VOC20 将输入图像分辨率从224×224Resize到448×448。对于预训练模型，如果我们仅仅是使用了卷积层，则对feature map的大小不敏感 训练时B个bbox的GT设置相同 1.7yolov1网络存在的问题 输入尺寸固定（检测层没有采用多尺度输入） 小目标检测效果差 同一格子包含多个目标时，仅预测一个（IOU最高） 损失函数中，没有区分大物体的IOU和小物体的IOU的误差对于网络训练loss的贡献，实际上小物体的IOU误差会对网络优化过程造成更大的影响，进而降低物体定位的准确性。 出现不常见长宽比目标时，算法效果较差。 1.8yolov1网络性能 五种尺度训练的精度与其他网络的精度对比： 2.YoloV2 2.1算法原理 相比Yolov1： 引入了anchor box思想 输出层：卷积层替代YOLO V1的全连接层 联合使用coco和Imagenet物体分类标注数据 识别种类、精度、速度和定位准确性等都有大大提升 具体地改进之处： batch normalization 进行归一化处理让整个过程变得更稳定，收敛速度变得更快，达到模型规范化的效果 v1中也大量用了BN，同时在FC中运用dropout防止过拟合 v2中取消了dropout，均使用BN . 高分辨率分类器 v1中使用224×224预训练分类网络，448×448用于检测网络 v2以448×448的分辨率微调最初的分类网络，保证两个模型在分布上的一致性。 . anchor boxes 预测bbox的偏移，使用卷积代替FC。最后卷积层长宽为S×S，通道数为B×5+C。则与以前效果一样。 输入尺度：416。因为图片中的物体倾向于出现在图像中间的位置，如果是448×448的输入，那么最后下采样之后是14×14，没有中心，如果是416×416的输入，32倍下采样后是13×13（奇数输出），有中心，可以预测那些倾向于出现在中心的大物体。 max pooling下采样 . 预测超过1000个。比如13×13 然后每个anchor9个预测值，一共就预测1521个 mAP小幅度降低，recall显著提高 对于anchor box的宽、高、维度的选择往往是先验框，在训练的过程虽然也可以调整box的宽高维度，最终得到准确的bbox，但是作者希望我们开始选择的时候就选具有代表性的bbox的宽高维度，这样可以通过网络更容易学习到预测的位置，作者采用kmeans的方法对bbox进行回归，自动找到更好的bbox的宽高维度比，距离的度量采用IOU，这样误差与bbox尺寸无关，防止欧式距离中较大尺寸的框所得距离比较小尺寸的大。 最终anchor box预测数量为5最合适。 . 细粒度特征 添加pass through layer,把浅层特征图（26×26）连接到深层特征图 用Resnet中的identity mapping方法 把26×26×512的特征图叠加成13×13×2048的特征图，与深层特征图相连接，增加细粒度特征 性能获得1%的提升 multi-scale training，在结尾处用卷积代替FC,则输入图像尺寸就可以变化，所以可以改变图像尺寸进行多尺度训练。每隔几次迭代后就会微调网络的输入尺寸，输入图像尺寸{320,352，…608} 2.2算法网络结构 主干网络：Darknet-19 主要使用3×3卷积 pooling之后channel数增加 global average pooling 即用1×1卷积核压缩特征 1×1卷积压缩特征表示 batch normalization 每个box包含4个坐标值，1个置信度和classes个条件类别概率。在v1中每个cell预测一个类别概率，现在是每个bbox预测一个类别概率，所以之前每个格子预测的维度是5×B+C，在v2中，每个box预测5+C个向量。 2.3算法性能对比 下图是不同尺度训练的精度与其他网络的精度对比： 3.Yolo9000 yolo9000是在yolov2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。它采用了wordTree的结构，用它来混合检测数据集和识别数据集中的数据，来达到分类和检测联合训练的效果。 对于分类任务而言，同样是猫，但分类标签可能会有猫的类别的划分，比如波斯猫等等更细粒度的猫的标签，而对于检测任务而言，仅仅是识别猫、狗这种粗粒度上的概念，如果将分类和回归采用简单方法进行融合就会同时出现猫的label和波斯猫的label这种情况。word tree则是将这两种label来构建它们的粒度关系，将分类和检测任务的数据集融合，在检测任务中不仅要完成物体类别的回归，同样需要对物体类别进行判定，而在分类数据集上，对物体类别进行分类而且物体类别可能更细，通过wordl tree能够将label的层次关系表示出来。 . 通过联合训练策略，Yolo9000可以快速检测出超过9000个类别的物体，总体mAP值为19.7%。 4.YoloV3 4.1YoloV3介绍： 速度和精度最均衡的目标检测网络 融合多种先进方法，改进YoloV1/V2的缺点，且效果更优 重点解决了小物体检测问题 4.2改进策略 更好的主干网络（类ResNet） 多尺度预测（类FPN） 更好的分类器 4.2主干网络 主干网络对比如下： 4.3多尺度预测（类FPN） 聚类来得到Bbox的先验，聚类之后得到9个聚类中心，将这9个聚类中心平均分到3种尺度上，每种尺度预测3个box，另外对于每种尺度，作者引入卷积层来进一步提取特征，之后再输出box的信息。 如下图对于尺度1而言，作者通过卷积之后直接得到后续box的信息 对于尺度2而言，作者在进行输出之前会对尺度1输出的卷积进行上采样，然后同尺度2的feature map进行相加，相加之后再输出到后续的box信息，整个feature map的大小比scale 1扩大了两倍 尺度3比尺度2也扩大了两倍，如下图所示。 4.4分类器 更好的分类器：binary cross-entropy loss 因为softmax不适用于多标签分类 softmax可被独立的多个logistic分类器替代，且准确率不会下降 4.5网络性能 4.6darknet框架 由C语言和CUDA实现 GPU显存利用效率极高 第三方库的依赖较少 容易移植到其他平台，如windows或嵌入式设备 5.Yolo系列网络优缺点 优点： 快速，pipline简单 背景误检率低 通用性强 但相比RCNN系列物体检测方法，YOLO具有以下缺点（主要因为每个网格预测固定数量的物体使候选框数量减少）： 识别物体位置精准性差 召回率低" />
<link rel="canonical" href="https://mlh.app/2019/05/19/787278.html" />
<meta property="og:url" content="https://mlh.app/2019/05/19/787278.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-19T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"目录 1.YoloV1 1.1综述 1.2算法原理 1.3yolov1网络结构图 1.4网络结构分析 1.5损失函数定义 1.6网络训练 1.7yolov1网络存在的问题 1.8yolov1网络性能 2.YoloV2 2.1算法原理 2.2算法网络结构 2.3算法性能对比 3.Yolo9000 4.YoloV3 4.1YoloV3介绍： 4.2改进策略 4.2主干网络 4.3多尺度预测（类FPN） 4.4分类器 4.5网络性能 4.6darknet框架 5.Yolo系列网络优缺点 1.YoloV1 1.1综述 同时预测多个box位置和类别 端到端的目标检测和识别 速度更快 实现回归功能的CNN并不需要复杂的设计过程 hijack选用整图训练模型，更好地区分目标和背景区域 1.2算法原理 图像被分成S×S个格子，对于每一个格子 包含GT物体中心的格子负责检测相应的物体，如上图狗所在中心落在某一个格子上，则这个格子负责预测狗。 每个格子预测B（超参数）个检测框及其置信度（x,y,w,h,c），以及C个类别概率。置信度表面了预测的bbox中含有目标物体的可能性以及当前bbox预测得有多准。检测框及置信度对应bbox，类别对应当前的格子。每个格子预测的向量长度为5×B×C bbox信息（x,y,w,h）为物体的中心位置相对格子位置的偏移及宽度和高度，均被归一化。 置信度反映是否包含物体以及包含物体情况下位置的准确性，定义为 1.3yolov1网络结构图 yolov1的网络结构采用直接回归的方式来获取最终所需要检测的物体的位置和类别，它只包含了CNN网络结构。原始的输入图像经过多层卷积之后最终再通过一个FC层。原论文里S的大小为7，B为2。 通过这样的回归，最终可以得到每一个格子所包含的bbox的置信度以及是否包含目标物体，如果包含的话，当前物体的偏移量是多少，以及长宽是多少。对FC层通过的结果进行解码，得到上图中下半部分的两幅图，对于第一个图，虽然每个格子预测B个BBOX，但是只选择IOU最高的BBOX作为物体检测的输出，这也是yolo算法的缺陷，图像中可能会包含多个小的目标，但由于每一个格子只预测一个物体，如果格子同时出现多个物体的时候，效果就会变差。网络回归之后得到每一个bbox对应的类别和位置信息，通过NMS进行筛选后得到最终检测的结果。 1.4网络结构分析 网络使用小卷积，即1×1和3×3 FC输出为：S×S(B×5+C） 网络比VGG16快，准确率稍差 1.5损失函数定义 Loss函数：均方和误差 坐标误差、IOU误差和分类误差 权重考量 1.6网络训练 预训练 ImageNet1000类数据预训练 使用预训练参数（20个conv）来初始化yolo，并训练VOC20 将输入图像分辨率从224×224Resize到448×448。对于预训练模型，如果我们仅仅是使用了卷积层，则对feature map的大小不敏感 训练时B个bbox的GT设置相同 1.7yolov1网络存在的问题 输入尺寸固定（检测层没有采用多尺度输入） 小目标检测效果差 同一格子包含多个目标时，仅预测一个（IOU最高） 损失函数中，没有区分大物体的IOU和小物体的IOU的误差对于网络训练loss的贡献，实际上小物体的IOU误差会对网络优化过程造成更大的影响，进而降低物体定位的准确性。 出现不常见长宽比目标时，算法效果较差。 1.8yolov1网络性能 五种尺度训练的精度与其他网络的精度对比： 2.YoloV2 2.1算法原理 相比Yolov1： 引入了anchor box思想 输出层：卷积层替代YOLO V1的全连接层 联合使用coco和Imagenet物体分类标注数据 识别种类、精度、速度和定位准确性等都有大大提升 具体地改进之处： batch normalization 进行归一化处理让整个过程变得更稳定，收敛速度变得更快，达到模型规范化的效果 v1中也大量用了BN，同时在FC中运用dropout防止过拟合 v2中取消了dropout，均使用BN . 高分辨率分类器 v1中使用224×224预训练分类网络，448×448用于检测网络 v2以448×448的分辨率微调最初的分类网络，保证两个模型在分布上的一致性。 . anchor boxes 预测bbox的偏移，使用卷积代替FC。最后卷积层长宽为S×S，通道数为B×5+C。则与以前效果一样。 输入尺度：416。因为图片中的物体倾向于出现在图像中间的位置，如果是448×448的输入，那么最后下采样之后是14×14，没有中心，如果是416×416的输入，32倍下采样后是13×13（奇数输出），有中心，可以预测那些倾向于出现在中心的大物体。 max pooling下采样 . 预测超过1000个。比如13×13 然后每个anchor9个预测值，一共就预测1521个 mAP小幅度降低，recall显著提高 对于anchor box的宽、高、维度的选择往往是先验框，在训练的过程虽然也可以调整box的宽高维度，最终得到准确的bbox，但是作者希望我们开始选择的时候就选具有代表性的bbox的宽高维度，这样可以通过网络更容易学习到预测的位置，作者采用kmeans的方法对bbox进行回归，自动找到更好的bbox的宽高维度比，距离的度量采用IOU，这样误差与bbox尺寸无关，防止欧式距离中较大尺寸的框所得距离比较小尺寸的大。 最终anchor box预测数量为5最合适。 . 细粒度特征 添加pass through layer,把浅层特征图（26×26）连接到深层特征图 用Resnet中的identity mapping方法 把26×26×512的特征图叠加成13×13×2048的特征图，与深层特征图相连接，增加细粒度特征 性能获得1%的提升 multi-scale training，在结尾处用卷积代替FC,则输入图像尺寸就可以变化，所以可以改变图像尺寸进行多尺度训练。每隔几次迭代后就会微调网络的输入尺寸，输入图像尺寸{320,352，…608} 2.2算法网络结构 主干网络：Darknet-19 主要使用3×3卷积 pooling之后channel数增加 global average pooling 即用1×1卷积核压缩特征 1×1卷积压缩特征表示 batch normalization 每个box包含4个坐标值，1个置信度和classes个条件类别概率。在v1中每个cell预测一个类别概率，现在是每个bbox预测一个类别概率，所以之前每个格子预测的维度是5×B+C，在v2中，每个box预测5+C个向量。 2.3算法性能对比 下图是不同尺度训练的精度与其他网络的精度对比： 3.Yolo9000 yolo9000是在yolov2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。它采用了wordTree的结构，用它来混合检测数据集和识别数据集中的数据，来达到分类和检测联合训练的效果。 对于分类任务而言，同样是猫，但分类标签可能会有猫的类别的划分，比如波斯猫等等更细粒度的猫的标签，而对于检测任务而言，仅仅是识别猫、狗这种粗粒度上的概念，如果将分类和回归采用简单方法进行融合就会同时出现猫的label和波斯猫的label这种情况。word tree则是将这两种label来构建它们的粒度关系，将分类和检测任务的数据集融合，在检测任务中不仅要完成物体类别的回归，同样需要对物体类别进行判定，而在分类数据集上，对物体类别进行分类而且物体类别可能更细，通过wordl tree能够将label的层次关系表示出来。 . 通过联合训练策略，Yolo9000可以快速检测出超过9000个类别的物体，总体mAP值为19.7%。 4.YoloV3 4.1YoloV3介绍： 速度和精度最均衡的目标检测网络 融合多种先进方法，改进YoloV1/V2的缺点，且效果更优 重点解决了小物体检测问题 4.2改进策略 更好的主干网络（类ResNet） 多尺度预测（类FPN） 更好的分类器 4.2主干网络 主干网络对比如下： 4.3多尺度预测（类FPN） 聚类来得到Bbox的先验，聚类之后得到9个聚类中心，将这9个聚类中心平均分到3种尺度上，每种尺度预测3个box，另外对于每种尺度，作者引入卷积层来进一步提取特征，之后再输出box的信息。 如下图对于尺度1而言，作者通过卷积之后直接得到后续box的信息 对于尺度2而言，作者在进行输出之前会对尺度1输出的卷积进行上采样，然后同尺度2的feature map进行相加，相加之后再输出到后续的box信息，整个feature map的大小比scale 1扩大了两倍 尺度3比尺度2也扩大了两倍，如下图所示。 4.4分类器 更好的分类器：binary cross-entropy loss 因为softmax不适用于多标签分类 softmax可被独立的多个logistic分类器替代，且准确率不会下降 4.5网络性能 4.6darknet框架 由C语言和CUDA实现 GPU显存利用效率极高 第三方库的依赖较少 容易移植到其他平台，如windows或嵌入式设备 5.Yolo系列网络优缺点 优点： 快速，pipline简单 背景误检率低 通用性强 但相比RCNN系列物体检测方法，YOLO具有以下缺点（主要因为每个网格预测固定数量的物体使候选框数量减少）： 识别物体位置精准性差 召回率低","@type":"BlogPosting","url":"https://mlh.app/2019/05/19/787278.html","headline":"【目标检测 深度学习】3.Yolo系列算法原理","dateModified":"2019-05-19T00:00:00+08:00","datePublished":"2019-05-19T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/19/787278.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>【目标检测 深度学习】3.Yolo系列算法原理</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>目录</h3>
   <ul>
    <li><a href="#1YoloV1_3" rel="nofollow">1.YoloV1</a></li>
    <ul>
     <li><a href="#11_4" rel="nofollow">1.1综述</a></li>
     <li><a href="#12_11" rel="nofollow">1.2算法原理</a></li>
     <li><a href="#13yolov1_19" rel="nofollow">1.3yolov1网络结构图</a></li>
     <li><a href="#14_23" rel="nofollow">1.4网络结构分析</a></li>
     <li><a href="#15_28" rel="nofollow">1.5损失函数定义</a></li>
     <li><a href="#16_33" rel="nofollow">1.6网络训练</a></li>
     <li><a href="#17yolov1_40" rel="nofollow">1.7yolov1网络存在的问题</a></li>
     <li><a href="#18yolov1_47" rel="nofollow">1.8yolov1网络性能</a></li>
    </ul>
    <li><a href="#2YoloV2_51" rel="nofollow">2.YoloV2</a></li>
    <ul>
     <li><a href="#21_52" rel="nofollow">2.1算法原理</a></li>
     <li><a href="#22_87" rel="nofollow">2.2算法网络结构</a></li>
    </ul>
    <li><a href="#23_97" rel="nofollow">2.3算法性能对比</a></li>
    <li><a href="#3Yolo9000_102" rel="nofollow">3.Yolo9000</a></li>
    <li><a href="#4YoloV3_110" rel="nofollow">4.YoloV3</a></li>
    <ul>
     <li><a href="#41YoloV3_111" rel="nofollow">4.1YoloV3介绍：</a></li>
     <li><a href="#42_116" rel="nofollow">4.2改进策略</a></li>
     <li><a href="#42_121" rel="nofollow">4.2主干网络</a></li>
     <li><a href="#43FPN_124" rel="nofollow">4.3多尺度预测（类FPN）</a></li>
     <li><a href="#44_132" rel="nofollow">4.4分类器</a></li>
     <li><a href="#45_137" rel="nofollow">4.5网络性能</a></li>
     <li><a href="#46darknet_140" rel="nofollow">4.6darknet框架</a></li>
    </ul>
    <li><a href="#5Yolo_146" rel="nofollow">5.Yolo系列网络优缺点</a></li>
   </ul>
  </div>
  <p></p> 
  <hr> 
  <h1><a id="1YoloV1_3"></a>1.YoloV1</h1> 
  <h2><a id="11_4"></a>1.1综述</h2> 
  <ul> 
   <li>同时预测多个box位置和类别</li> 
   <li>端到端的目标检测和识别</li> 
   <li>速度更快 
    <ul> 
     <li>实现回归功能的CNN并不需要复杂的设计过程</li> 
     <li>hijack选用整图训练模型，更好地区分目标和背景区域</li> 
    </ul> </li> 
  </ul> 
  <h2><a id="12_11"></a>1.2算法原理</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190515204733286.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzE0NTA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <ul> 
   <li>图像被分成S×S个格子，对于每一个格子</li> 
   <li>包含GT物体中心的格子负责检测相应的物体，如上图狗所在中心落在某一个格子上，则这个格子负责预测狗。</li> 
   <li>每个格子预测B（超参数）个检测框及其置信度（x,y,w,h,c），以及C个类别概率。置信度表面了预测的bbox中含有目标物体的可能性以及当前bbox预测得有多准。检测框及置信度对应bbox，类别对应当前的格子。每个格子预测的向量长度为5×B×C</li> 
   <li>bbox信息（x,y,w,h）为物体的中心位置相对格子位置的偏移及宽度和高度，均被归一化。</li> 
   <li>置信度反映是否包含物体以及包含物体情况下位置的准确性，定义为 
    <ul> 
     <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519163100282.png" alt="在这里插入图片描述"></li> 
    </ul> </li> 
  </ul> 
  <h2><a id="13yolov1_19"></a>1.3yolov1网络结构图</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519163254391.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzE0NTA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> yolov1的网络结构采用直接回归的方式来获取最终所需要检测的物体的位置和类别，它只包含了CNN网络结构。原始的输入图像经过多层卷积之后最终再通过一个FC层。原论文里S的大小为7，B为2。 通过这样的回归，最终可以得到每一个格子所包含的bbox的置信度以及是否包含目标物体，如果包含的话，当前物体的偏移量是多少，以及长宽是多少。对FC层通过的结果进行解码，得到上图中下半部分的两幅图，对于第一个图，虽然每个格子预测B个BBOX，但是只选择IOU最高的BBOX作为物体检测的输出，这也是yolo算法的缺陷，图像中可能会包含多个小的目标，但由于每一个格子只预测一个物体，如果格子同时出现多个物体的时候，效果就会变差。网络回归之后得到每一个bbox对应的类别和位置信息，通过NMS进行筛选后得到最终检测的结果。</p> 
  <h2><a id="14_23"></a>1.4网络结构分析</h2> 
  <ul> 
   <li>网络使用小卷积，即1×1和3×3</li> 
   <li>FC输出为：S×S(B×5+C）</li> 
   <li>网络比VGG16快，准确率稍差</li> 
  </ul> 
  <h2><a id="15_28"></a>1.5损失函数定义</h2> 
  <ul> 
   <li>Loss函数：均方和误差</li> 
   <li>坐标误差、IOU误差和分类误差</li> 
   <li>权重考量</li> 
   <li><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519165428264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzE0NTA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li> 
  </ul> 
  <h2><a id="16_33"></a>1.6网络训练</h2> 
  <ul> 
   <li>预训练 
    <ul> 
     <li>ImageNet1000类数据预训练</li> 
    </ul> </li> 
   <li>使用预训练参数（20个conv）来初始化yolo，并训练VOC20</li> 
   <li>将输入图像分辨率从224×224Resize到448×448。对于预训练模型，如果我们仅仅是使用了卷积层，则对feature map的大小不敏感</li> 
   <li>训练时B个bbox的GT设置相同</li> 
  </ul> 
  <h2><a id="17yolov1_40"></a>1.7yolov1网络存在的问题</h2> 
  <ul> 
   <li>输入尺寸固定（检测层没有采用多尺度输入）</li> 
   <li>小目标检测效果差 
    <ul> 
     <li>同一格子包含多个目标时，仅预测一个（IOU最高）</li> 
    </ul> </li> 
   <li>损失函数中，没有区分大物体的IOU和小物体的IOU的误差对于网络训练loss的贡献，实际上小物体的IOU误差会对网络优化过程造成更大的影响，进而降低物体定位的准确性。</li> 
   <li>出现不常见长宽比目标时，算法效果较差。</li> 
  </ul> 
  <h2><a id="18yolov1_47"></a>1.8yolov1网络性能</h2> 
  <p>五种尺度训练的精度与其他网络的精度对比：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519170854543.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzE0NTA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h1><a id="2YoloV2_51"></a>2.YoloV2</h1> 
  <h2><a id="21_52"></a>2.1算法原理</h2> 
  <p>相比Yolov1：</p> 
  <ul> 
   <li>引入了anchor box思想</li> 
   <li>输出层：卷积层替代YOLO V1的全连接层</li> 
   <li>联合使用coco和Imagenet物体分类标注数据</li> 
   <li>识别种类、精度、速度和定位准确性等都有大大提升</li> 
  </ul> 
  <p>具体地改进之处：</p> 
  <ol> 
   <li> <p>batch normalization</p> 
    <ul> 
     <li>进行归一化处理让整个过程变得更稳定，收敛速度变得更快，达到模型规范化的效果</li> 
     <li>v1中也大量用了BN，同时在FC中运用dropout防止过拟合</li> 
     <li>v2中取消了dropout，均使用BN<br> .</li> 
    </ul> </li> 
   <li> <p>高分辨率分类器</p> 
    <ul> 
     <li>v1中使用224×224预训练分类网络，448×448用于检测网络</li> 
     <li>v2以448×448的分辨率微调最初的分类网络，保证两个模型在分布上的一致性。<br> .</li> 
    </ul> </li> 
   <li> <p>anchor boxes</p> 
    <ul> 
     <li>预测bbox的偏移，使用卷积代替FC。最后卷积层长宽为<mark>S×S</mark>，通道数为<mark>B×5+C</mark>。则与以前效果一样。</li> 
     <li>输入尺度：416。因为图片中的物体倾向于出现在图像中间的位置，如果是448×448的输入，那么最后下采样之后是14×14，没有中心，如果是416×416的输入，32倍下采样后是13×13（<mark>奇数输出</mark>），有中心，可以预测那些倾向于出现在中心的大物体。</li> 
    </ul> </li> 
   <li> <p>max pooling下采样<br> .</p> </li> 
   <li> <p>预测超过1000个。比如13×13 然后每个anchor9个预测值，一共就预测1521个</p> 
    <ul> 
     <li>mAP小幅度降低，recall显著提高</li> 
     <li>对于anchor box的宽、高、维度的选择往往是先验框，在训练的过程虽然也可以调整box的宽高维度，最终得到准确的bbox，但是作者希望我们开始选择的时候就选具有代表性的bbox的宽高维度，这样可以通过网络更容易学习到预测的位置，作者采用kmeans的方法对bbox进行回归，自动找到更好的bbox的宽高维度比，距离的度量采用IOU，这样误差与bbox尺寸无关，防止欧式距离中较大尺寸的框所得距离比较小尺寸的大。</li> 
     <li>最终anchor box预测数量为5最合适。</li> 
    </ul> <p>.</p> </li> 
   <li> <p>细粒度特征</p> 
    <ul> 
     <li>添加pass through layer,把浅层特征图（26×26）连接到深层特征图 
      <ul> 
       <li>用Resnet中的identity mapping方法</li> 
       <li>把26×26×512的特征图叠加成13×13×2048的特征图，与深层特征图相连接，增加细粒度特征</li> 
       <li>性能获得1%的提升</li> 
      </ul> </li> 
    </ul> </li> 
   <li> <p>multi-scale training，在结尾处用卷积代替FC,则输入图像尺寸就可以变化，所以可以改变图像尺寸进行多尺度训练。每隔几次迭代后就会微调网络的输入尺寸，输入图像尺寸{320,352，…608}</p> </li> 
  </ol> 
  <h2><a id="22_87"></a>2.2算法网络结构</h2> 
  <ol> 
   <li>主干网络：Darknet-19 
    <ul> 
     <li>主要使用3×3卷积</li> 
     <li>pooling之后channel数增加</li> 
     <li>global average pooling 即用1×1卷积核压缩特征</li> 
     <li>1×1卷积压缩特征表示</li> 
     <li>batch normalization<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519183440957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzE0NTA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li> 
     <li>每个box包含4个坐标值，1个置信度和classes个条件类别概率。在v1中每个cell预测一个类别概率，现在是每个bbox预测一个类别概率，所以之前每个格子预测的维度是5×B+C，在v2中，每个box预测5+C个向量。</li> 
    </ul> </li> 
  </ol> 
  <h1><a id="23_97"></a>2.3算法性能对比</h1> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519183928454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzE0NTA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 下图是不同尺度训练的精度与其他网络的精度对比：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519184043349.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzE0NTA3,size_16,color_FFFFFF,t_70" alt=" "></p> 
  <h1><a id="3Yolo9000_102"></a>3.Yolo9000</h1> 
  <p>yolo9000是在yolov2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。它采用了wordTree的结构，用它来混合检测数据集和识别数据集中的数据，来达到分类和检测联合训练的效果。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519184407621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzE0NTA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 对于分类任务而言，同样是猫，但分类标签可能会有猫的类别的划分，比如波斯猫等等更细粒度的猫的标签，而对于检测任务而言，仅仅是识别猫、狗这种粗粒度上的概念，如果将分类和回归采用简单方法进行融合就会同时出现猫的label和波斯猫的label这种情况。word tree则是将这两种label来构建它们的粒度关系，将分类和检测任务的数据集融合，在检测任务中不仅要完成物体类别的回归，同样需要对物体类别进行判定，而在分类数据集上，对物体类别进行分类而且物体类别可能更细，通过wordl tree能够将label的层次关系表示出来。<br> .<br> 通过联合训练策略，Yolo9000可以快速检测出超过9000个类别的物体，总体mAP值为19.7%。<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519185622133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzE0NTA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h1><a id="4YoloV3_110"></a>4.YoloV3</h1> 
  <h2><a id="41YoloV3_111"></a>4.1YoloV3介绍：</h2> 
  <ul> 
   <li>速度和精度最均衡的目标检测网络</li> 
   <li>融合多种先进方法，改进YoloV1/V2的缺点，且效果更优 
    <ul> 
     <li>重点解决了小物体检测问题</li> 
    </ul> </li> 
  </ul> 
  <h2><a id="42_116"></a>4.2改进策略</h2> 
  <ul> 
   <li>更好的主干网络（类ResNet）</li> 
   <li>多尺度预测（类FPN）</li> 
   <li>更好的分类器</li> 
  </ul> 
  <h2><a id="42_121"></a>4.2主干网络</h2> 
  <p>主干网络对比如下：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519190716700.png" alt="在这里插入图片描述"></p> 
  <h2><a id="43FPN_124"></a>4.3多尺度预测（类FPN）</h2> 
  <ul> 
   <li>聚类来得到Bbox的先验，聚类之后得到9个聚类中心，将这9个聚类中心平均分到3种尺度上，每种尺度预测3个box，另外对于每种尺度，作者引入卷积层来进一步提取特征，之后再输出box的信息。</li> 
   <li>如下图对于尺度1而言，作者通过卷积之后直接得到后续box的信息</li> 
   <li>对于尺度2而言，作者在进行输出之前会对尺度1输出的卷积进行上采样，然后同尺度2的feature map进行相加，相加之后再输出到后续的box信息，整个feature map的大小比scale 1扩大了两倍</li> 
   <li>尺度3比尺度2也扩大了两倍，如下图所示。</li> 
  </ul> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190519190912842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzE0NTA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="44_132"></a>4.4分类器</h2> 
  <p>更好的分类器：binary cross-entropy loss<br> 因为softmax不适用于多标签分类<br> softmax可被独立的多个logistic分类器替代，且准确率不会下降</p> 
  <h2><a id="45_137"></a>4.5网络性能</h2> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019051919165553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwMzE0NTA3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <h2><a id="46darknet_140"></a>4.6darknet框架</h2> 
  <ul> 
   <li>由C语言和CUDA实现</li> 
   <li>GPU显存利用效率极高</li> 
   <li>第三方库的依赖较少</li> 
   <li>容易移植到其他平台，如windows或嵌入式设备</li> 
  </ul> 
  <h1><a id="5Yolo_146"></a>5.Yolo系列网络优缺点</h1> 
  <p>优点：</p> 
  <ol> 
   <li>快速，pipline简单</li> 
   <li>背景误检率低</li> 
   <li>通用性强</li> 
  </ol> 
  <p>但相比RCNN系列物体检测方法，YOLO具有以下缺点（主要因为每个网格预测固定数量的物体使候选框数量减少）：</p> 
  <ol> 
   <li>识别物体位置精准性差</li> 
   <li>召回率低</li> 
  </ol> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
