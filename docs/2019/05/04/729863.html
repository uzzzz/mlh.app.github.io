<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>kafka 命令、API | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="kafka 命令、API" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="大数据组件使用 总文章 &nbsp; 1.使用控制台运行 &nbsp;&nbsp; &nbsp;1.创建一个topic主题 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-topics.sh &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --create --zookeeper IP:2181 --replication-factor 副本数 --partitions 分片数/分区数 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --create --zookeeper NODE1:2181 --replication-factor 1 --partitions 1 --topic order &nbsp;&nbsp; &nbsp;2.编写代码启动一个生产者，生产数据 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-console-producer.sh &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-producer.sh --broker-list IP:9092 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-producer.sh --broker-list NODE1:9092 --topic order &nbsp;&nbsp; &nbsp;3.编写代码启动给一个消费者，消费数据 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-console-consumer.sh &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-consumer.sh --zookeeper IP:2181 --from-beginning --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-consumer.sh --zookeeper NODE1:2181 --from-beginning --topic order &nbsp;&nbsp; &nbsp;4.Kafka 常用操作命令 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.查看当前服务器中的所有 topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --list --zookeeper zookeeper的IP:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --list --zookeeper NODE1:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.创建 topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --create --zookeeper zookeeper的IP:2181 --replication-factor 副本数 --partitions 分片数/分区数 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --create --zookeeper NODE1:2181 --replication-factor 1 --partitions 1 --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.删除 topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;需要在kafka集群中的每个kafka服务器中的 vim /root/kafka/config/server.properties 中设置 delete.topic.enable=true 否则只是标记删除或者直接重启 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --delete --zookeeper zookeeper的IP:2181 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --delete --zookeeper NODE1:2181 --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;4.通过 shell 命令发送消息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-producer.sh --broker-list kafka的IP:9092 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-producer.sh --broker-list NODE1:9092 --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;5.通过 shell 消费消息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-consumer.sh --zookeeper zookeeper的IP:2181 --from-beginning --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-consumer.sh --zookeeper NODE1:2181 --from-beginning --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;6.查看消费位置 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper zookeeper的IP:2181 --group testGroup &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper NODE1:2181 --group testGroup &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;7.查看某个 Topic 的详情 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --topic 主题名 --describe --zookeeper zookeeper的IP:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --topic 主题名 --describe --zookeeper NODE1:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;8.对分区数进行修改 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --zookeeper zookeeper的IP --alter --partitions 2 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --zookeeper NODE1 --alter --partitions 分片数/分区数 --topic 主题名 2.使用Java api运行 &nbsp;&nbsp; &nbsp;1.java工程-maven，依赖 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;dependency&gt; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;version&gt;0.11.0.1&lt;/version&gt; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;/dependency&gt; &nbsp;&nbsp; &nbsp;2.生产者（先启动消费者等待接收数据，然后再启动生产者进行发送数据） &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者：订单的生产者代码 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public class OrderProducer &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public static void main(String[] args) throws InterruptedException &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 通过Properties配置文件的方式 配置&quot;连接集群&quot;的信息 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Properties props = new Properties(); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;bootstrap.servers&quot;, &quot;NODE1:9092&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;acks&quot;, &quot;all&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;retries&quot;, 0); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;batch.size&quot;, 16384); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;linger.ms&quot;, 1); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;buffer.memory&quot;, 33554432); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;String, String&gt;(props); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (int i = 0; i &lt; 10000; i++) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:可以指定数据发往哪个partition,当ProducerRecord 的构造参数中有partition的时候，就可以发送到对应partition上 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;(&quot;topic主题名&quot;,partition,&quot;key&quot;, &quot;value&quot;) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; partition = new ProducerRecord&lt;String, String&gt;(&quot;order&quot;, 0, &quot;key&quot;, &quot;订单&quot;+i); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:如果生产者没有指定partition，但是发送消息中有key，根据key的hash值的方式发送数据到那个partition分区/分片中 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;(&quot;topic主题名&quot;,&quot;key&quot;, &quot;value&quot;) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; key = new ProducerRecord&lt;String, String&gt;(&quot;order&quot;, &quot;key&quot;, &quot;value&quot;+i); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:既没有指定partition，也没有key的情况下如何发送数据。使用轮询的方式发送数据到那个partition分区/分片中 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;(&quot;topic主题名&quot;, &quot;value&quot;) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; value = new ProducerRecord&lt;String, String&gt;(&quot;order&quot;, &quot;订单信息！&quot;+i); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者发送数据 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kafkaProducer.send(value); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;3.消费者（先启动消费者等待接收数据，然后再启动生产者进行发送数据）&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka消费者：订单的消费者代码 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public class OrderConsumer &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public static void main(String[] args) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //1.通过Properties配置文件的方式 配置&quot;连接集群&quot;的信息 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Properties props = new Properties(); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;bootstrap.servers&quot;, &quot;NODE1:9092&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //配置group.id：多个Consumer的group.id都相同的话，表示多个Consumer都在同一个消费组group中 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;group.id&quot;, &quot;test&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // Kafka消费者 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(props); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //2.订阅某个topic主题下的相关消息数据用于消费。可以订阅多个topic主题，封装到一个 List中，可以使用Arrays.asList进行封装 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kafkaConsumer.subscribe(Arrays.asList(&quot;order&quot;)); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; while (true) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //jdk queue的操作方法：offer插入元素、poll获取元素。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //blockingqueue的操作方法：put插入元素，take获取元素。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(100); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //此处需要等待到有数据才能消费进行获取打印 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println(&quot;消费的数据为：&quot; + record.value()); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; 3.topic主体中的分区partition 和 group.id消费者组 的关系 &nbsp;&nbsp; &nbsp;Consumer Group（CG）消费者组： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这是 kafka 用来实现一个 topic(主题) 消息的广播（发给所有的 consumer消费者）和 单播（发给任意一个 consumer消费者）的手段。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个 topic(主题) 可以有多个 CG消费者组。topic(主题) 的消息会复制（不是真的复制，是概念上的）到所有的 CG消费者组， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;但每个 partion(分区) 只会把消息发给该 CG 中的一个consumer消费者。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果需要实现广播，只要每个 consumer消费者 有一个独立的 CG 就可以了。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;要实现单播只要所有的 consumer 在同一个 CG。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;用 CG消费者组 还可以将 consumer消费者 进行自由的分组，而不需要多次发送消息到不同的 topic(主题)。 &nbsp;&nbsp; &nbsp;Broker：一台 kafka 服务器就是一个 broker(中间人)。一个kafka集群(cluster) 由多个 broker(中间人) 组成。一个 broker 可以容纳多个 topic(主题)。 &nbsp;&nbsp; &nbsp;Partition(分区)： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;为了实现扩展性，一个非常大的 topic(主题) 可以分布到多个 broker（即服务器）上，一个topic(主题) 可以分为多个 partition(分区)， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;每个 partition(分区) 是一个有序的队列(Queue)。partition(分区) 中的每条消息都会被分配一个有序的 id（offset）。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;kafka 只保证按一个 partition(分区) 中的顺序 将消息发给 consumer消费者，不保证一个 topic 的整体（多个 partition分区 间）的顺序 &nbsp; &nbsp;&nbsp; &nbsp;情况一：同一个topic的一个partition只能被同一个customerGroup的一个customer消费；group里多于partition数量的customer会空闲； &nbsp;&nbsp; &nbsp;情况二：同一个topic的partition数量多于同一个customerGroup的customer数量时，会有一个customer消费多个partition，这样也就没法保证customer接收到的消息的顺序性，kafka只保证在一个partition上数据是有序的，无法保证topic全局数据的顺序性； &nbsp;&nbsp; &nbsp;情况三：一个topic 的partitions被多个customerGroup消费时，可以并行重复消费； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;kafka topic&nbsp;partition被同一个GROUPID的多个消费者消费，只有一个能收到消息的原因一般有如下： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.只有一个partition分区； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;同一个topic的同一个partition分区只允许同一个customerGroup的一个消费者消费信息，一个partition上不允许同一个消费者组的多个消费者并发， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;但同一个partition上是可以多个不同消费者组种的消费者并发消费的； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.多个partition分区，但是，消息在生产时只发往到了一个partiton上； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;key的hashCode%partitionNum相同导致，或者自定义了分区策略； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;导致这种严重的数据倾斜； &nbsp; &nbsp;&nbsp; &nbsp;1.发送到Kafka的消息会根据key，发送到对应topic的partition中，有默认的分发规则（也可以自己重写分发规则）， &nbsp;&nbsp; &nbsp; &nbsp;基本上就是相同的key发送到一个partition中，不同的key有可能发送到相同的partition中。 &nbsp;&nbsp; &nbsp;2.group是消费者中的概念，按照group（组）对消费者进行区分。 &nbsp;&nbsp; &nbsp; &nbsp;对于每个group，需要先指定订阅哪个topic的消息，然后该topic下的partition会平均分配到group下面的consumer上。 &nbsp;&nbsp; &nbsp; &nbsp;所以会出现以下这些情况： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.一个topic被多个group订阅，那么一条消息就会被不同group中的多个consumer处理。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果需要实现广播，只要每个 consumer消费者 有一个独立的 CG 就可以了。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;一个 topic(主题) 可以有多个 CG消费者组。topic(主题) 的消息会复制（不是真的复制，是概念上的）到所有的 CG消费者组， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;但每个 partion(分区) 只会把消息发给该 CG 中的一个consumer消费者。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.同一个group中，每个partition只会被一个consumer处理，这个consumer处理的消息不一定是同一个key的。所以需要在处理的地方判断。 &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;要实现单播只要所有的 consumer 在同一个 CG。 &nbsp;&nbsp; &nbsp;3.例子，现在有一个用户信息修改的回调消息扔到消息队列里，有两个业务要处理，一个是更新数据库，一个是更新es索引信息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;topic：user_update_topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;key：user_update_key_cid //cid标志公司的区分信息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样子的话，同一个公司的用户更新会被分配到一个partition中，同一个公司的用户更新能保证前后顺序不变 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group1中的consumer更新数据库，group：consumer1_group &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group2中的consumer更新es索引信息，group：consumer2_group &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group1、group2这个两个group会分别消费这个topic下的数据，对于每个group，内部的consumer会平分topic下的partition， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;相当于group中的每个consumer会处理多个公司的数据，但处理的公司不会有重叠。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;以上是topic中partition多余group中的消费者的时候，如果group下面有3个消费者，但是分区partition只有一个，那么三个消费者中只有一个会消费消息。 &nbsp; &nbsp;&nbsp; &nbsp;消费者组 (Consumer Group) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;consumer group是kafka提供的可扩展且具有容错性的消费者机制。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;既然是一个组，那么组内必然可以有多个消费者或消费者实例(consumer instance)，它们共享一个公共的ID，即group ID。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;当然，每个分区只能由同一个消费组内的一个consumer来消费(当然该分区还可以同时分配给其他group中的某个consumer来消费)。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;个人认为，理解consumer group记住下面这三个特性就好了： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.consumer group下可以有一个或多个consumer instance，consumer instance可以是一个进程，也可以是一个线程 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.group.id是一个字符串，唯一标识一个consumer group &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.consumer group下订阅的topic下的每个分区只能分配给某个group下的一个consumer， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;当然该分区还可以同时分配给其他group中的某个consumer来消费。 &nbsp;&nbsp; &nbsp;kafka consumer： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;消费者可以从多个broker中读取数据。消费者可以消费多个topic中的数据。 &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;因为Kafka的broker是无状态的，所以consumer必须使用partition offset来记录消费了多少数据。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果一个consumer指定了一个topic的offset，意味着该consumer已经消费了该offset之前的所有数据。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;consumer可以通过指定offset，从topic的指定位置开始消费数据。consumer的offset存储在Zookeeper中。 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;offset： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;用来保存消费进度。offset表示在当前topic，当前groupID下消费到的位置。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;offset为earliest并不代表offset=1。在不进行过期配置的情况下，kafka消息默认7天时间就会过期。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;过期后其offset也就随之发生变化，使得用数字进行配置的消费进度并不准确。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.earliest：自动重置到最早的offset。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.latest：看上去重置到最晚的offset。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.none：如果边更早的offset也没有的话，就抛出异常给consumer，告诉consumer在整个consumer group中都没有发现有这样的offset。 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;groupID： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个字符串用来指示一组consumer所在的组。相同的groupID表示在一个组里。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;相同的groupID消费记录offset时，记录的是同一个offset。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;所以，此处需要注意&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;（1）如果多个地方都使用相同的groupid，可能造成个别消费者消费不到的情况 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;（2）如果单个消费者消费能力不足的话，可以启动多个相同groupid的consumer消费，处理相同的逻辑。 &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;produce方面： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果有多个分区，发送的时候按照key值hashCode%partitionNum哈希取模分区数来决定该条信息发往哪个partition,&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这里可以自定义partition的分发策略，只要实现Partitioner接口就好，可以自定义成随机分发或者fangwang发往指定分区； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;customer方面： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;对于topic中的partition来说，一个partition只允许一个customer来消费，同一个partition上不允许customer并发； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;Partition数量 &gt; customer数量时： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个consumer会对应于多个partitions，这里主要合理分配consumer数和partition数，否则会导致partition里面的数据被取的不均匀&nbsp;。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;最好partiton数目是consumer数目的整数倍，所以partition数目很重要，比如取24，就很容易设定consumer数目&nbsp;。 &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;Partition数量 &lt; customer数量时： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;就会有剩余的customer闲置，造成浪费； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;如果一个consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同； &nbsp;&nbsp; &nbsp;kafka只保证在一个上数据是有序的（每个partition都有segment文件记录消息的顺序性），无法保证topic全局数据的顺序行； &nbsp;&nbsp; &nbsp;增减consumer，broker，partition会导致rebalance，所以rebalance后consumer对应的partition会发生变化。 4.storm消费kafka时并行度设置问题 &nbsp;&nbsp; &nbsp;1.首先明确的一点是，storm的并行度都是executor即线程级别的并行； &nbsp;&nbsp; &nbsp; &nbsp;包括work(进程)，executor(线程)的设置，具体体现在works,spout,bolt设置上，同一个executor上设置多个task还是会串行化执行，并不能提高执行效率， &nbsp;&nbsp; &nbsp; &nbsp;这也是由于并行是线程并行，一个线程的多个task肯定是有先后执行顺序的，有顺序那就不是并行； &nbsp;&nbsp; &nbsp; &nbsp;关于node，work，executor，task关系和work，spout，bolt，并行度设置网上有很多资料，挺详细； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;2.这里记录下我遇到的自己关系的另外两个问题： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个是从kafka消费消息是spout并行度设置，另一个ack响应开启的是线程还是进程以及如何设置其数量； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.第一个问题： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;其实理解了上面kafka customer和partition的关系第一个问题也就解决了，spout的并发度实例数量设置最好和partition数量一样， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样能保证一个spout消费者实例对应一个partition，即实现了一个partition中消息消费的顺序性（有时消息的顺序性要求并不是很高） &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;也能很好地提高整个topology的执行效率，至少对拓扑执行效率来说，瓶颈不会卡在spout（数据源）这里； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.第二个问题： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;通过Storm UI发现，work和spout,bolt并行度不变的情况下，多开几个acker_executors，works的数量并没有增加，反而是executors数量增加， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样就确定了acker_executors如其名一样只是线程，并不像有些网友说的ack的执行是会单独开启ack进程再在该进程里运行ack响应线程。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;他其实就是一个普通的ack线程，运行在已有的work进程里； &nbsp;&nbsp; &nbsp;3.另外通过测试发现，我设置了4个work，4个spout，4个bolt，没有设置acker_executors，Storm UI上显示Num workers是4，Num executors却是12 &nbsp;&nbsp; &nbsp; （4个spout，4个bolt 这里一共是8个executors），所以默认情况下一个work里会有一个acker_executors。 &nbsp;&nbsp; &nbsp;4.默认情况下 一个work会有一个executor，一个executor会有一个task，如果设置了他们的数量，就会按照设置的数量来生成对应实例； &nbsp;&nbsp; &nbsp; &nbsp;如开了4个work，2个spout，3个bolt，那spout和bolt的executors一共就会有5个（spout executors 2个，bolt&nbsp;executors 3个，）， &nbsp;&nbsp; &nbsp; &nbsp;相当于有2个work里的每个work都有1个spout executor和1个bolt executor，另外还有1个work里只有1个bolt executor，另外还有一个work里啥也没有； &nbsp;&nbsp; &nbsp; &nbsp;其实这种配置会导致多开一个啥活也不干的work进程，有些浪费； 5.kafka 多个消费者在同一个groupID消费者组中，只有一个消费者能收到消息，解决方案如下 &nbsp; &nbsp;&nbsp; &nbsp;1.业务需求： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;将一个业务逻辑分拆出来单独部署多个服务器上，然后与主程序之间通过kafka队列通信，每个业务实例上都有一个消费者在监听队列， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;且他们的groupid相同。我们的原意是主程序 往 队列上发送的命令参数（数据值）会被其中随机的一个消费者收到，从而实现一个负载均衡的效果， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;但是后来发现主程序发送往队列上的控制消息（数据值）始终是被其中固定的一个服务器上的消费者收到，其他的消费者从头到尾没有收到过。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样自然就达不到我们负载均衡的效果了。 &nbsp;&nbsp; &nbsp;2.后来发现，造成这个结果的原因可能有两个： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.分区数(Partition)设成了1 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.发送消息的时候指定了key值 &nbsp;&nbsp; &nbsp;3.原因一的解决方案： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.原因：分区数(Partition)设成了1 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;因为kafka为了保证了消息的一致性，同一个分区的消息只会被于此关联的同一个消费者接收到(大致就是这个意思)。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果只有一个分区的话，如果第一条消息被其中一个消费者收到后，后面的消息始终会被这个消费者收到。所以应保证分区的数目大于消费者的数目。 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.解决方案： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#查看partition数据 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;bin/kafka-topics.sh --zookeeper zk1,zk2,zk3 --describe --partitions 20 --topic topic_test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#增加partition数目 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#增加partition数目后，如果使用的是java的kafka-client的Producer的话，会由于producer内部的缓存机制， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#导致过几分钟后才被producer感知到partition数目的变化 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;bin/kafka-topics.sh --zookeeper zk1,zk2,zk3 --alter --partitions 20 --topic topic_test &nbsp;&nbsp; &nbsp;4.原因二的解决方案： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.原因：发送消息的时候指定了key值 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;我们改了partitions 的数目后，再次测试，还是始终被其中一个消费者收到。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;后来查看了producer的send方法，发现了是因为我们把key设置为了””,也就是指定了key值为空字符串。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.send方法发送 代码分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;producer.send(new ProducerRecord&lt;String, String&gt;(&quot;test&quot;,&quot;&quot;,&quot;&quot;),new Callback() { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void onCompletion(RecordMetadata metadata, Exception exception) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;}); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;4.send方法的实现 和 partition方法的调用 代码分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // first make sure the metadata for the topic is available &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; long waitedOnMetadataMs = waitOnMetadata(record.topic(), this.maxBlockTimeMs); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; long remainingWaitMs = Math.max(0, this.maxBlockTimeMs - waitedOnMetadataMs); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; byte[] serializedKey; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; serializedKey = keySerializer.serialize(record.topic(), record.key()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ClassCastException cce) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new SerializationException(&quot;Can&#39;t convert key of class &quot; + record.key().getClass().getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; specified in key.serializer&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; byte[] serializedValue; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; serializedValue = valueSerializer.serialize(record.topic(), record.value()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ClassCastException cce) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new SerializationException(&quot;Can&#39;t convert value of class &quot; + record.value().getClass().getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; specified in value.serializer&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;//可以看出 生产者往哪个partition分区 发送消息，实际是由 partition()方法得出的 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int partition = partition(record, serializedKey, serializedValue, metadata.fetch()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; ensureValidRecordSize(serializedSize); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; TopicPartition tp = new TopicPartition(record.topic(), partition); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.trace(&quot;Sending record {} with callback {} to topic {} partition {}&quot;, record, callback, record.topic(), partition); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; RecordAccumulator.RecordAppendResult result = accumulator.append(tp, serializedKey, serializedValue, callback, remainingWaitMs); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (result.batchIsFull || result.newBatchCreated) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.trace(&quot;Waking up the sender since topic {} partition {} is either full or getting a new batch&quot;, record.topic(), partition); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.sender.wakeup(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return result.future; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // handling exceptions and record the errors; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // for API exceptions return them in the future, &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // for other exceptions throw directly &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ApiException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.debug(&quot;Exception occurred during message send:&quot;, e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (callback != null) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; callback.onCompletion(null, e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return new FutureFailure(e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (InterruptedException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new InterruptException(e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (BufferExhaustedException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.metrics.sensor(&quot;buffer-exhausted-records&quot;).record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw e; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (KafkaException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw e; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;5.partition方法的实现 和 ProducerRecord对象 代码分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey , byte[] serializedValue, Cluster cluster) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;Integer partition = record.partition(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;if (partition != null) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(record.topic()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int numPartitions = partitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;// they have given us a partition, use it &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;if (partition &lt; 0 || partition &gt;= numPartitions) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;throw new IllegalArgumentException(&quot;Invalid partition given with record: &quot; + partition &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + &quot; is not in the range [0...&quot; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + numPartitions &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + &quot;].&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return partition; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return this.partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue, &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;cluster); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果发送的时候已经指定了parition的话，就会发送到指定的partition上。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;怎么指定呢？？我们发送的是一个ProducerRecoed对象。看它的构造方法，第二个参数就是partition。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public ProducerRecord(String topic, Integer partition, K key, V value) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (topic == null) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new IllegalArgumentException(&quot;Topic cannot be null&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.topic = topic; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.partition = partition; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.key = key; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.value = value; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有指定的parition的话，就调用： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;this.partitioner.partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有配置partitioner.class的话，那么partitioner默认是org.apache.kafka.clients.producer.internals.DefaultPartitioner的一个对象(参考文章末尾)， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;查看一下它的partition()方法： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int numPartitions = partitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (keyBytes == null) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int nextValue = counter.getAndIncrement(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (availablePartitions.size() &gt; 0) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int part = DefaultPartitioner.toPositive(nextValue) % availablePartitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return availablePartitions.get(part).partition(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } else { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // no partitions are available, give a non-available partition &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return DefaultPartitioner.toPositive(nextValue) % numPartitions; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } else { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // hash the keyBytes to choose a partition &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return DefaultPartitioner.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;从上面代码就看出了如果指定了key值的话，partition的值实际上是由Utils.murmur2(keyBytes)哈希计算出来，这样自然每次都是被同一个消费者接收到。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有指定的话，就会通过轮询的方式逐个发送。这里有个问题就是，如果我们每次都打印出partition的值的话， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;可能会看到peoducer发送并不一定会按照1、2、3、4、5这样的顺序发送，这个从上面代码中能看出最终返回的不是part的值， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;而是availablePartitions.get(part).partition()的值，而availablePartitions里的PartitionInfo的顺序本身就不一定是严格按照顺序排列的。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;在org.apache.kafka.clients.producer.ProducerConfig类中可以看到kafkaproducer的配置以及默认值，其中有这么一行： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;.define(PARTITIONER_CLASS_CONFIG, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Type.CLASS, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; DefaultPartitioner.class.getName(), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Importance.MEDIUM, PARTITIONER_CLASS_DOC) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;其中静态变量 PARTITIONER_CLASS_CONFIG的值是”partitioner.class”,可以看出其默认对应的类是 DefaultPartitioner.class，当然， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这里可以自定义partition的分发策略，只要实现Partitioner接口就好，例如下面代码实现了随机选择partition。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public class MyPartitioner implements Partitioner{ &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void configure(Map&lt;String, ?&gt; configs) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public int partition(String topic, Object key, byte[] keyBytes, &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;Object value, byte[] valueBytes, Cluster cluster) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int numPartitions = partitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int randomNum = new Random().nextInt(numPartitions); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return partitions.get(randomNum).partition(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void close() { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;} &nbsp;" />
<meta property="og:description" content="大数据组件使用 总文章 &nbsp; 1.使用控制台运行 &nbsp;&nbsp; &nbsp;1.创建一个topic主题 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-topics.sh &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --create --zookeeper IP:2181 --replication-factor 副本数 --partitions 分片数/分区数 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --create --zookeeper NODE1:2181 --replication-factor 1 --partitions 1 --topic order &nbsp;&nbsp; &nbsp;2.编写代码启动一个生产者，生产数据 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-console-producer.sh &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-producer.sh --broker-list IP:9092 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-producer.sh --broker-list NODE1:9092 --topic order &nbsp;&nbsp; &nbsp;3.编写代码启动给一个消费者，消费数据 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-console-consumer.sh &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-consumer.sh --zookeeper IP:2181 --from-beginning --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-consumer.sh --zookeeper NODE1:2181 --from-beginning --topic order &nbsp;&nbsp; &nbsp;4.Kafka 常用操作命令 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.查看当前服务器中的所有 topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --list --zookeeper zookeeper的IP:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --list --zookeeper NODE1:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.创建 topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --create --zookeeper zookeeper的IP:2181 --replication-factor 副本数 --partitions 分片数/分区数 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --create --zookeeper NODE1:2181 --replication-factor 1 --partitions 1 --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.删除 topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;需要在kafka集群中的每个kafka服务器中的 vim /root/kafka/config/server.properties 中设置 delete.topic.enable=true 否则只是标记删除或者直接重启 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --delete --zookeeper zookeeper的IP:2181 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --delete --zookeeper NODE1:2181 --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;4.通过 shell 命令发送消息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-producer.sh --broker-list kafka的IP:9092 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-producer.sh --broker-list NODE1:9092 --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;5.通过 shell 消费消息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-consumer.sh --zookeeper zookeeper的IP:2181 --from-beginning --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-consumer.sh --zookeeper NODE1:2181 --from-beginning --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;6.查看消费位置 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper zookeeper的IP:2181 --group testGroup &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper NODE1:2181 --group testGroup &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;7.查看某个 Topic 的详情 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --topic 主题名 --describe --zookeeper zookeeper的IP:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --topic 主题名 --describe --zookeeper NODE1:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;8.对分区数进行修改 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --zookeeper zookeeper的IP --alter --partitions 2 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --zookeeper NODE1 --alter --partitions 分片数/分区数 --topic 主题名 2.使用Java api运行 &nbsp;&nbsp; &nbsp;1.java工程-maven，依赖 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;dependency&gt; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;version&gt;0.11.0.1&lt;/version&gt; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;/dependency&gt; &nbsp;&nbsp; &nbsp;2.生产者（先启动消费者等待接收数据，然后再启动生产者进行发送数据） &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者：订单的生产者代码 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public class OrderProducer &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public static void main(String[] args) throws InterruptedException &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 通过Properties配置文件的方式 配置&quot;连接集群&quot;的信息 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Properties props = new Properties(); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;bootstrap.servers&quot;, &quot;NODE1:9092&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;acks&quot;, &quot;all&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;retries&quot;, 0); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;batch.size&quot;, 16384); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;linger.ms&quot;, 1); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;buffer.memory&quot;, 33554432); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;String, String&gt;(props); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (int i = 0; i &lt; 10000; i++) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:可以指定数据发往哪个partition,当ProducerRecord 的构造参数中有partition的时候，就可以发送到对应partition上 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;(&quot;topic主题名&quot;,partition,&quot;key&quot;, &quot;value&quot;) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; partition = new ProducerRecord&lt;String, String&gt;(&quot;order&quot;, 0, &quot;key&quot;, &quot;订单&quot;+i); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:如果生产者没有指定partition，但是发送消息中有key，根据key的hash值的方式发送数据到那个partition分区/分片中 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;(&quot;topic主题名&quot;,&quot;key&quot;, &quot;value&quot;) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; key = new ProducerRecord&lt;String, String&gt;(&quot;order&quot;, &quot;key&quot;, &quot;value&quot;+i); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:既没有指定partition，也没有key的情况下如何发送数据。使用轮询的方式发送数据到那个partition分区/分片中 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;(&quot;topic主题名&quot;, &quot;value&quot;) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; value = new ProducerRecord&lt;String, String&gt;(&quot;order&quot;, &quot;订单信息！&quot;+i); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者发送数据 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kafkaProducer.send(value); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;3.消费者（先启动消费者等待接收数据，然后再启动生产者进行发送数据）&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka消费者：订单的消费者代码 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public class OrderConsumer &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public static void main(String[] args) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //1.通过Properties配置文件的方式 配置&quot;连接集群&quot;的信息 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Properties props = new Properties(); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;bootstrap.servers&quot;, &quot;NODE1:9092&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //配置group.id：多个Consumer的group.id都相同的话，表示多个Consumer都在同一个消费组group中 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;group.id&quot;, &quot;test&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // Kafka消费者 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(props); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //2.订阅某个topic主题下的相关消息数据用于消费。可以订阅多个topic主题，封装到一个 List中，可以使用Arrays.asList进行封装 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kafkaConsumer.subscribe(Arrays.asList(&quot;order&quot;)); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; while (true) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //jdk queue的操作方法：offer插入元素、poll获取元素。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //blockingqueue的操作方法：put插入元素，take获取元素。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(100); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //此处需要等待到有数据才能消费进行获取打印 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println(&quot;消费的数据为：&quot; + record.value()); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; 3.topic主体中的分区partition 和 group.id消费者组 的关系 &nbsp;&nbsp; &nbsp;Consumer Group（CG）消费者组： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这是 kafka 用来实现一个 topic(主题) 消息的广播（发给所有的 consumer消费者）和 单播（发给任意一个 consumer消费者）的手段。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个 topic(主题) 可以有多个 CG消费者组。topic(主题) 的消息会复制（不是真的复制，是概念上的）到所有的 CG消费者组， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;但每个 partion(分区) 只会把消息发给该 CG 中的一个consumer消费者。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果需要实现广播，只要每个 consumer消费者 有一个独立的 CG 就可以了。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;要实现单播只要所有的 consumer 在同一个 CG。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;用 CG消费者组 还可以将 consumer消费者 进行自由的分组，而不需要多次发送消息到不同的 topic(主题)。 &nbsp;&nbsp; &nbsp;Broker：一台 kafka 服务器就是一个 broker(中间人)。一个kafka集群(cluster) 由多个 broker(中间人) 组成。一个 broker 可以容纳多个 topic(主题)。 &nbsp;&nbsp; &nbsp;Partition(分区)： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;为了实现扩展性，一个非常大的 topic(主题) 可以分布到多个 broker（即服务器）上，一个topic(主题) 可以分为多个 partition(分区)， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;每个 partition(分区) 是一个有序的队列(Queue)。partition(分区) 中的每条消息都会被分配一个有序的 id（offset）。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;kafka 只保证按一个 partition(分区) 中的顺序 将消息发给 consumer消费者，不保证一个 topic 的整体（多个 partition分区 间）的顺序 &nbsp; &nbsp;&nbsp; &nbsp;情况一：同一个topic的一个partition只能被同一个customerGroup的一个customer消费；group里多于partition数量的customer会空闲； &nbsp;&nbsp; &nbsp;情况二：同一个topic的partition数量多于同一个customerGroup的customer数量时，会有一个customer消费多个partition，这样也就没法保证customer接收到的消息的顺序性，kafka只保证在一个partition上数据是有序的，无法保证topic全局数据的顺序性； &nbsp;&nbsp; &nbsp;情况三：一个topic 的partitions被多个customerGroup消费时，可以并行重复消费； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;kafka topic&nbsp;partition被同一个GROUPID的多个消费者消费，只有一个能收到消息的原因一般有如下： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.只有一个partition分区； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;同一个topic的同一个partition分区只允许同一个customerGroup的一个消费者消费信息，一个partition上不允许同一个消费者组的多个消费者并发， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;但同一个partition上是可以多个不同消费者组种的消费者并发消费的； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.多个partition分区，但是，消息在生产时只发往到了一个partiton上； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;key的hashCode%partitionNum相同导致，或者自定义了分区策略； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;导致这种严重的数据倾斜； &nbsp; &nbsp;&nbsp; &nbsp;1.发送到Kafka的消息会根据key，发送到对应topic的partition中，有默认的分发规则（也可以自己重写分发规则）， &nbsp;&nbsp; &nbsp; &nbsp;基本上就是相同的key发送到一个partition中，不同的key有可能发送到相同的partition中。 &nbsp;&nbsp; &nbsp;2.group是消费者中的概念，按照group（组）对消费者进行区分。 &nbsp;&nbsp; &nbsp; &nbsp;对于每个group，需要先指定订阅哪个topic的消息，然后该topic下的partition会平均分配到group下面的consumer上。 &nbsp;&nbsp; &nbsp; &nbsp;所以会出现以下这些情况： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.一个topic被多个group订阅，那么一条消息就会被不同group中的多个consumer处理。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果需要实现广播，只要每个 consumer消费者 有一个独立的 CG 就可以了。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;一个 topic(主题) 可以有多个 CG消费者组。topic(主题) 的消息会复制（不是真的复制，是概念上的）到所有的 CG消费者组， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;但每个 partion(分区) 只会把消息发给该 CG 中的一个consumer消费者。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.同一个group中，每个partition只会被一个consumer处理，这个consumer处理的消息不一定是同一个key的。所以需要在处理的地方判断。 &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;要实现单播只要所有的 consumer 在同一个 CG。 &nbsp;&nbsp; &nbsp;3.例子，现在有一个用户信息修改的回调消息扔到消息队列里，有两个业务要处理，一个是更新数据库，一个是更新es索引信息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;topic：user_update_topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;key：user_update_key_cid //cid标志公司的区分信息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样子的话，同一个公司的用户更新会被分配到一个partition中，同一个公司的用户更新能保证前后顺序不变 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group1中的consumer更新数据库，group：consumer1_group &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group2中的consumer更新es索引信息，group：consumer2_group &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group1、group2这个两个group会分别消费这个topic下的数据，对于每个group，内部的consumer会平分topic下的partition， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;相当于group中的每个consumer会处理多个公司的数据，但处理的公司不会有重叠。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;以上是topic中partition多余group中的消费者的时候，如果group下面有3个消费者，但是分区partition只有一个，那么三个消费者中只有一个会消费消息。 &nbsp; &nbsp;&nbsp; &nbsp;消费者组 (Consumer Group) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;consumer group是kafka提供的可扩展且具有容错性的消费者机制。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;既然是一个组，那么组内必然可以有多个消费者或消费者实例(consumer instance)，它们共享一个公共的ID，即group ID。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;当然，每个分区只能由同一个消费组内的一个consumer来消费(当然该分区还可以同时分配给其他group中的某个consumer来消费)。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;个人认为，理解consumer group记住下面这三个特性就好了： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.consumer group下可以有一个或多个consumer instance，consumer instance可以是一个进程，也可以是一个线程 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.group.id是一个字符串，唯一标识一个consumer group &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.consumer group下订阅的topic下的每个分区只能分配给某个group下的一个consumer， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;当然该分区还可以同时分配给其他group中的某个consumer来消费。 &nbsp;&nbsp; &nbsp;kafka consumer： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;消费者可以从多个broker中读取数据。消费者可以消费多个topic中的数据。 &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;因为Kafka的broker是无状态的，所以consumer必须使用partition offset来记录消费了多少数据。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果一个consumer指定了一个topic的offset，意味着该consumer已经消费了该offset之前的所有数据。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;consumer可以通过指定offset，从topic的指定位置开始消费数据。consumer的offset存储在Zookeeper中。 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;offset： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;用来保存消费进度。offset表示在当前topic，当前groupID下消费到的位置。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;offset为earliest并不代表offset=1。在不进行过期配置的情况下，kafka消息默认7天时间就会过期。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;过期后其offset也就随之发生变化，使得用数字进行配置的消费进度并不准确。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.earliest：自动重置到最早的offset。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.latest：看上去重置到最晚的offset。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.none：如果边更早的offset也没有的话，就抛出异常给consumer，告诉consumer在整个consumer group中都没有发现有这样的offset。 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;groupID： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个字符串用来指示一组consumer所在的组。相同的groupID表示在一个组里。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;相同的groupID消费记录offset时，记录的是同一个offset。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;所以，此处需要注意&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;（1）如果多个地方都使用相同的groupid，可能造成个别消费者消费不到的情况 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;（2）如果单个消费者消费能力不足的话，可以启动多个相同groupid的consumer消费，处理相同的逻辑。 &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;produce方面： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果有多个分区，发送的时候按照key值hashCode%partitionNum哈希取模分区数来决定该条信息发往哪个partition,&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这里可以自定义partition的分发策略，只要实现Partitioner接口就好，可以自定义成随机分发或者fangwang发往指定分区； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;customer方面： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;对于topic中的partition来说，一个partition只允许一个customer来消费，同一个partition上不允许customer并发； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;Partition数量 &gt; customer数量时： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个consumer会对应于多个partitions，这里主要合理分配consumer数和partition数，否则会导致partition里面的数据被取的不均匀&nbsp;。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;最好partiton数目是consumer数目的整数倍，所以partition数目很重要，比如取24，就很容易设定consumer数目&nbsp;。 &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;Partition数量 &lt; customer数量时： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;就会有剩余的customer闲置，造成浪费； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;如果一个consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同； &nbsp;&nbsp; &nbsp;kafka只保证在一个上数据是有序的（每个partition都有segment文件记录消息的顺序性），无法保证topic全局数据的顺序行； &nbsp;&nbsp; &nbsp;增减consumer，broker，partition会导致rebalance，所以rebalance后consumer对应的partition会发生变化。 4.storm消费kafka时并行度设置问题 &nbsp;&nbsp; &nbsp;1.首先明确的一点是，storm的并行度都是executor即线程级别的并行； &nbsp;&nbsp; &nbsp; &nbsp;包括work(进程)，executor(线程)的设置，具体体现在works,spout,bolt设置上，同一个executor上设置多个task还是会串行化执行，并不能提高执行效率， &nbsp;&nbsp; &nbsp; &nbsp;这也是由于并行是线程并行，一个线程的多个task肯定是有先后执行顺序的，有顺序那就不是并行； &nbsp;&nbsp; &nbsp; &nbsp;关于node，work，executor，task关系和work，spout，bolt，并行度设置网上有很多资料，挺详细； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;2.这里记录下我遇到的自己关系的另外两个问题： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个是从kafka消费消息是spout并行度设置，另一个ack响应开启的是线程还是进程以及如何设置其数量； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.第一个问题： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;其实理解了上面kafka customer和partition的关系第一个问题也就解决了，spout的并发度实例数量设置最好和partition数量一样， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样能保证一个spout消费者实例对应一个partition，即实现了一个partition中消息消费的顺序性（有时消息的顺序性要求并不是很高） &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;也能很好地提高整个topology的执行效率，至少对拓扑执行效率来说，瓶颈不会卡在spout（数据源）这里； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.第二个问题： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;通过Storm UI发现，work和spout,bolt并行度不变的情况下，多开几个acker_executors，works的数量并没有增加，反而是executors数量增加， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样就确定了acker_executors如其名一样只是线程，并不像有些网友说的ack的执行是会单独开启ack进程再在该进程里运行ack响应线程。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;他其实就是一个普通的ack线程，运行在已有的work进程里； &nbsp;&nbsp; &nbsp;3.另外通过测试发现，我设置了4个work，4个spout，4个bolt，没有设置acker_executors，Storm UI上显示Num workers是4，Num executors却是12 &nbsp;&nbsp; &nbsp; （4个spout，4个bolt 这里一共是8个executors），所以默认情况下一个work里会有一个acker_executors。 &nbsp;&nbsp; &nbsp;4.默认情况下 一个work会有一个executor，一个executor会有一个task，如果设置了他们的数量，就会按照设置的数量来生成对应实例； &nbsp;&nbsp; &nbsp; &nbsp;如开了4个work，2个spout，3个bolt，那spout和bolt的executors一共就会有5个（spout executors 2个，bolt&nbsp;executors 3个，）， &nbsp;&nbsp; &nbsp; &nbsp;相当于有2个work里的每个work都有1个spout executor和1个bolt executor，另外还有1个work里只有1个bolt executor，另外还有一个work里啥也没有； &nbsp;&nbsp; &nbsp; &nbsp;其实这种配置会导致多开一个啥活也不干的work进程，有些浪费； 5.kafka 多个消费者在同一个groupID消费者组中，只有一个消费者能收到消息，解决方案如下 &nbsp; &nbsp;&nbsp; &nbsp;1.业务需求： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;将一个业务逻辑分拆出来单独部署多个服务器上，然后与主程序之间通过kafka队列通信，每个业务实例上都有一个消费者在监听队列， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;且他们的groupid相同。我们的原意是主程序 往 队列上发送的命令参数（数据值）会被其中随机的一个消费者收到，从而实现一个负载均衡的效果， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;但是后来发现主程序发送往队列上的控制消息（数据值）始终是被其中固定的一个服务器上的消费者收到，其他的消费者从头到尾没有收到过。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样自然就达不到我们负载均衡的效果了。 &nbsp;&nbsp; &nbsp;2.后来发现，造成这个结果的原因可能有两个： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.分区数(Partition)设成了1 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.发送消息的时候指定了key值 &nbsp;&nbsp; &nbsp;3.原因一的解决方案： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.原因：分区数(Partition)设成了1 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;因为kafka为了保证了消息的一致性，同一个分区的消息只会被于此关联的同一个消费者接收到(大致就是这个意思)。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果只有一个分区的话，如果第一条消息被其中一个消费者收到后，后面的消息始终会被这个消费者收到。所以应保证分区的数目大于消费者的数目。 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.解决方案： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#查看partition数据 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;bin/kafka-topics.sh --zookeeper zk1,zk2,zk3 --describe --partitions 20 --topic topic_test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#增加partition数目 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#增加partition数目后，如果使用的是java的kafka-client的Producer的话，会由于producer内部的缓存机制， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#导致过几分钟后才被producer感知到partition数目的变化 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;bin/kafka-topics.sh --zookeeper zk1,zk2,zk3 --alter --partitions 20 --topic topic_test &nbsp;&nbsp; &nbsp;4.原因二的解决方案： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.原因：发送消息的时候指定了key值 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;我们改了partitions 的数目后，再次测试，还是始终被其中一个消费者收到。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;后来查看了producer的send方法，发现了是因为我们把key设置为了””,也就是指定了key值为空字符串。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.send方法发送 代码分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;producer.send(new ProducerRecord&lt;String, String&gt;(&quot;test&quot;,&quot;&quot;,&quot;&quot;),new Callback() { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void onCompletion(RecordMetadata metadata, Exception exception) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;}); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;4.send方法的实现 和 partition方法的调用 代码分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // first make sure the metadata for the topic is available &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; long waitedOnMetadataMs = waitOnMetadata(record.topic(), this.maxBlockTimeMs); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; long remainingWaitMs = Math.max(0, this.maxBlockTimeMs - waitedOnMetadataMs); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; byte[] serializedKey; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; serializedKey = keySerializer.serialize(record.topic(), record.key()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ClassCastException cce) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new SerializationException(&quot;Can&#39;t convert key of class &quot; + record.key().getClass().getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; specified in key.serializer&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; byte[] serializedValue; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; serializedValue = valueSerializer.serialize(record.topic(), record.value()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ClassCastException cce) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new SerializationException(&quot;Can&#39;t convert value of class &quot; + record.value().getClass().getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; specified in value.serializer&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;//可以看出 生产者往哪个partition分区 发送消息，实际是由 partition()方法得出的 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int partition = partition(record, serializedKey, serializedValue, metadata.fetch()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; ensureValidRecordSize(serializedSize); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; TopicPartition tp = new TopicPartition(record.topic(), partition); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.trace(&quot;Sending record {} with callback {} to topic {} partition {}&quot;, record, callback, record.topic(), partition); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; RecordAccumulator.RecordAppendResult result = accumulator.append(tp, serializedKey, serializedValue, callback, remainingWaitMs); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (result.batchIsFull || result.newBatchCreated) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.trace(&quot;Waking up the sender since topic {} partition {} is either full or getting a new batch&quot;, record.topic(), partition); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.sender.wakeup(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return result.future; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // handling exceptions and record the errors; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // for API exceptions return them in the future, &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // for other exceptions throw directly &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ApiException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.debug(&quot;Exception occurred during message send:&quot;, e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (callback != null) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; callback.onCompletion(null, e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return new FutureFailure(e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (InterruptedException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new InterruptException(e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (BufferExhaustedException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.metrics.sensor(&quot;buffer-exhausted-records&quot;).record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw e; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (KafkaException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw e; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;5.partition方法的实现 和 ProducerRecord对象 代码分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey , byte[] serializedValue, Cluster cluster) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;Integer partition = record.partition(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;if (partition != null) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(record.topic()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int numPartitions = partitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;// they have given us a partition, use it &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;if (partition &lt; 0 || partition &gt;= numPartitions) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;throw new IllegalArgumentException(&quot;Invalid partition given with record: &quot; + partition &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + &quot; is not in the range [0...&quot; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + numPartitions &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + &quot;].&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return partition; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return this.partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue, &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;cluster); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果发送的时候已经指定了parition的话，就会发送到指定的partition上。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;怎么指定呢？？我们发送的是一个ProducerRecoed对象。看它的构造方法，第二个参数就是partition。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public ProducerRecord(String topic, Integer partition, K key, V value) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (topic == null) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new IllegalArgumentException(&quot;Topic cannot be null&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.topic = topic; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.partition = partition; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.key = key; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.value = value; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有指定的parition的话，就调用： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;this.partitioner.partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有配置partitioner.class的话，那么partitioner默认是org.apache.kafka.clients.producer.internals.DefaultPartitioner的一个对象(参考文章末尾)， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;查看一下它的partition()方法： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int numPartitions = partitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (keyBytes == null) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int nextValue = counter.getAndIncrement(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (availablePartitions.size() &gt; 0) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int part = DefaultPartitioner.toPositive(nextValue) % availablePartitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return availablePartitions.get(part).partition(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } else { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // no partitions are available, give a non-available partition &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return DefaultPartitioner.toPositive(nextValue) % numPartitions; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } else { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // hash the keyBytes to choose a partition &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return DefaultPartitioner.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;从上面代码就看出了如果指定了key值的话，partition的值实际上是由Utils.murmur2(keyBytes)哈希计算出来，这样自然每次都是被同一个消费者接收到。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有指定的话，就会通过轮询的方式逐个发送。这里有个问题就是，如果我们每次都打印出partition的值的话， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;可能会看到peoducer发送并不一定会按照1、2、3、4、5这样的顺序发送，这个从上面代码中能看出最终返回的不是part的值， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;而是availablePartitions.get(part).partition()的值，而availablePartitions里的PartitionInfo的顺序本身就不一定是严格按照顺序排列的。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;在org.apache.kafka.clients.producer.ProducerConfig类中可以看到kafkaproducer的配置以及默认值，其中有这么一行： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;.define(PARTITIONER_CLASS_CONFIG, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Type.CLASS, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; DefaultPartitioner.class.getName(), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Importance.MEDIUM, PARTITIONER_CLASS_DOC) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;其中静态变量 PARTITIONER_CLASS_CONFIG的值是”partitioner.class”,可以看出其默认对应的类是 DefaultPartitioner.class，当然， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这里可以自定义partition的分发策略，只要实现Partitioner接口就好，例如下面代码实现了随机选择partition。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public class MyPartitioner implements Partitioner{ &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void configure(Map&lt;String, ?&gt; configs) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public int partition(String topic, Object key, byte[] keyBytes, &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;Object value, byte[] valueBytes, Cluster cluster) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int numPartitions = partitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int randomNum = new Random().nextInt(numPartitions); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return partitions.get(randomNum).partition(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void close() { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;} &nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/04/729863.html" />
<meta property="og:url" content="https://mlh.app/2019/05/04/729863.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-04T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"大数据组件使用 总文章 &nbsp; 1.使用控制台运行 &nbsp;&nbsp; &nbsp;1.创建一个topic主题 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-topics.sh &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --create --zookeeper IP:2181 --replication-factor 副本数 --partitions 分片数/分区数 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --create --zookeeper NODE1:2181 --replication-factor 1 --partitions 1 --topic order &nbsp;&nbsp; &nbsp;2.编写代码启动一个生产者，生产数据 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-console-producer.sh &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-producer.sh --broker-list IP:9092 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-producer.sh --broker-list NODE1:9092 --topic order &nbsp;&nbsp; &nbsp;3.编写代码启动给一个消费者，消费数据 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-console-consumer.sh &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-consumer.sh --zookeeper IP:2181 --from-beginning --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-consumer.sh --zookeeper NODE1:2181 --from-beginning --topic order &nbsp;&nbsp; &nbsp;4.Kafka 常用操作命令 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.查看当前服务器中的所有 topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --list --zookeeper zookeeper的IP:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --list --zookeeper NODE1:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.创建 topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --create --zookeeper zookeeper的IP:2181 --replication-factor 副本数 --partitions 分片数/分区数 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --create --zookeeper NODE1:2181 --replication-factor 1 --partitions 1 --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.删除 topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;需要在kafka集群中的每个kafka服务器中的 vim /root/kafka/config/server.properties 中设置 delete.topic.enable=true 否则只是标记删除或者直接重启 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --delete --zookeeper zookeeper的IP:2181 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --delete --zookeeper NODE1:2181 --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;4.通过 shell 命令发送消息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-producer.sh --broker-list kafka的IP:9092 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-producer.sh --broker-list NODE1:9092 --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;5.通过 shell 消费消息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-consumer.sh --zookeeper zookeeper的IP:2181 --from-beginning --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-consumer.sh --zookeeper NODE1:2181 --from-beginning --topic test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;6.查看消费位置 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper zookeeper的IP:2181 --group testGroup &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper NODE1:2181 --group testGroup &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;7.查看某个 Topic 的详情 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --topic 主题名 --describe --zookeeper zookeeper的IP:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --topic 主题名 --describe --zookeeper NODE1:2181 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;8.对分区数进行修改 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --zookeeper zookeeper的IP --alter --partitions 2 --topic 主题名 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --zookeeper NODE1 --alter --partitions 分片数/分区数 --topic 主题名 2.使用Java api运行 &nbsp;&nbsp; &nbsp;1.java工程-maven，依赖 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;dependency&gt; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;version&gt;0.11.0.1&lt;/version&gt; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;/dependency&gt; &nbsp;&nbsp; &nbsp;2.生产者（先启动消费者等待接收数据，然后再启动生产者进行发送数据） &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者：订单的生产者代码 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public class OrderProducer &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public static void main(String[] args) throws InterruptedException &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 通过Properties配置文件的方式 配置&quot;连接集群&quot;的信息 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Properties props = new Properties(); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;bootstrap.servers&quot;, &quot;NODE1:9092&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;acks&quot;, &quot;all&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;retries&quot;, 0); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;batch.size&quot;, 16384); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;linger.ms&quot;, 1); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;buffer.memory&quot;, 33554432); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;String, String&gt;(props); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (int i = 0; i &lt; 10000; i++) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:可以指定数据发往哪个partition,当ProducerRecord 的构造参数中有partition的时候，就可以发送到对应partition上 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;(&quot;topic主题名&quot;,partition,&quot;key&quot;, &quot;value&quot;) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; partition = new ProducerRecord&lt;String, String&gt;(&quot;order&quot;, 0, &quot;key&quot;, &quot;订单&quot;+i); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:如果生产者没有指定partition，但是发送消息中有key，根据key的hash值的方式发送数据到那个partition分区/分片中 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;(&quot;topic主题名&quot;,&quot;key&quot;, &quot;value&quot;) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; key = new ProducerRecord&lt;String, String&gt;(&quot;order&quot;, &quot;key&quot;, &quot;value&quot;+i); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:既没有指定partition，也没有key的情况下如何发送数据。使用轮询的方式发送数据到那个partition分区/分片中 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;(&quot;topic主题名&quot;, &quot;value&quot;) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; value = new ProducerRecord&lt;String, String&gt;(&quot;order&quot;, &quot;订单信息！&quot;+i); &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者发送数据 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kafkaProducer.send(value); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;3.消费者（先启动消费者等待接收数据，然后再启动生产者进行发送数据）&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka消费者：订单的消费者代码 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public class OrderConsumer &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public static void main(String[] args) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //1.通过Properties配置文件的方式 配置&quot;连接集群&quot;的信息 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Properties props = new Properties(); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;bootstrap.servers&quot;, &quot;NODE1:9092&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //配置group.id：多个Consumer的group.id都相同的话，表示多个Consumer都在同一个消费组group中 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;group.id&quot;, &quot;test&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;enable.auto.commit&quot;, &quot;true&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // Kafka消费者 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(props); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //2.订阅某个topic主题下的相关消息数据用于消费。可以订阅多个topic主题，封装到一个 List中，可以使用Arrays.asList进行封装 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kafkaConsumer.subscribe(Arrays.asList(&quot;order&quot;)); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; while (true) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //jdk queue的操作方法：offer插入元素、poll获取元素。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //blockingqueue的操作方法：put插入元素，take获取元素。 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(100); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (ConsumerRecord&lt;String, String&gt; record : consumerRecords) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; { &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //此处需要等待到有数据才能消费进行获取打印 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println(&quot;消费的数据为：&quot; + record.value()); &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; } &nbsp; 3.topic主体中的分区partition 和 group.id消费者组 的关系 &nbsp;&nbsp; &nbsp;Consumer Group（CG）消费者组： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这是 kafka 用来实现一个 topic(主题) 消息的广播（发给所有的 consumer消费者）和 单播（发给任意一个 consumer消费者）的手段。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个 topic(主题) 可以有多个 CG消费者组。topic(主题) 的消息会复制（不是真的复制，是概念上的）到所有的 CG消费者组， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;但每个 partion(分区) 只会把消息发给该 CG 中的一个consumer消费者。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果需要实现广播，只要每个 consumer消费者 有一个独立的 CG 就可以了。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;要实现单播只要所有的 consumer 在同一个 CG。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;用 CG消费者组 还可以将 consumer消费者 进行自由的分组，而不需要多次发送消息到不同的 topic(主题)。 &nbsp;&nbsp; &nbsp;Broker：一台 kafka 服务器就是一个 broker(中间人)。一个kafka集群(cluster) 由多个 broker(中间人) 组成。一个 broker 可以容纳多个 topic(主题)。 &nbsp;&nbsp; &nbsp;Partition(分区)： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;为了实现扩展性，一个非常大的 topic(主题) 可以分布到多个 broker（即服务器）上，一个topic(主题) 可以分为多个 partition(分区)， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;每个 partition(分区) 是一个有序的队列(Queue)。partition(分区) 中的每条消息都会被分配一个有序的 id（offset）。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;kafka 只保证按一个 partition(分区) 中的顺序 将消息发给 consumer消费者，不保证一个 topic 的整体（多个 partition分区 间）的顺序 &nbsp; &nbsp;&nbsp; &nbsp;情况一：同一个topic的一个partition只能被同一个customerGroup的一个customer消费；group里多于partition数量的customer会空闲； &nbsp;&nbsp; &nbsp;情况二：同一个topic的partition数量多于同一个customerGroup的customer数量时，会有一个customer消费多个partition，这样也就没法保证customer接收到的消息的顺序性，kafka只保证在一个partition上数据是有序的，无法保证topic全局数据的顺序性； &nbsp;&nbsp; &nbsp;情况三：一个topic 的partitions被多个customerGroup消费时，可以并行重复消费； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;kafka topic&nbsp;partition被同一个GROUPID的多个消费者消费，只有一个能收到消息的原因一般有如下： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.只有一个partition分区； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;同一个topic的同一个partition分区只允许同一个customerGroup的一个消费者消费信息，一个partition上不允许同一个消费者组的多个消费者并发， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;但同一个partition上是可以多个不同消费者组种的消费者并发消费的； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.多个partition分区，但是，消息在生产时只发往到了一个partiton上； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;key的hashCode%partitionNum相同导致，或者自定义了分区策略； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;导致这种严重的数据倾斜； &nbsp; &nbsp;&nbsp; &nbsp;1.发送到Kafka的消息会根据key，发送到对应topic的partition中，有默认的分发规则（也可以自己重写分发规则）， &nbsp;&nbsp; &nbsp; &nbsp;基本上就是相同的key发送到一个partition中，不同的key有可能发送到相同的partition中。 &nbsp;&nbsp; &nbsp;2.group是消费者中的概念，按照group（组）对消费者进行区分。 &nbsp;&nbsp; &nbsp; &nbsp;对于每个group，需要先指定订阅哪个topic的消息，然后该topic下的partition会平均分配到group下面的consumer上。 &nbsp;&nbsp; &nbsp; &nbsp;所以会出现以下这些情况： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.一个topic被多个group订阅，那么一条消息就会被不同group中的多个consumer处理。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果需要实现广播，只要每个 consumer消费者 有一个独立的 CG 就可以了。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;一个 topic(主题) 可以有多个 CG消费者组。topic(主题) 的消息会复制（不是真的复制，是概念上的）到所有的 CG消费者组， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;但每个 partion(分区) 只会把消息发给该 CG 中的一个consumer消费者。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.同一个group中，每个partition只会被一个consumer处理，这个consumer处理的消息不一定是同一个key的。所以需要在处理的地方判断。 &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;要实现单播只要所有的 consumer 在同一个 CG。 &nbsp;&nbsp; &nbsp;3.例子，现在有一个用户信息修改的回调消息扔到消息队列里，有两个业务要处理，一个是更新数据库，一个是更新es索引信息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;topic：user_update_topic &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;key：user_update_key_cid //cid标志公司的区分信息 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样子的话，同一个公司的用户更新会被分配到一个partition中，同一个公司的用户更新能保证前后顺序不变 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group1中的consumer更新数据库，group：consumer1_group &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group2中的consumer更新es索引信息，group：consumer2_group &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group1、group2这个两个group会分别消费这个topic下的数据，对于每个group，内部的consumer会平分topic下的partition， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;相当于group中的每个consumer会处理多个公司的数据，但处理的公司不会有重叠。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;以上是topic中partition多余group中的消费者的时候，如果group下面有3个消费者，但是分区partition只有一个，那么三个消费者中只有一个会消费消息。 &nbsp; &nbsp;&nbsp; &nbsp;消费者组 (Consumer Group) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;consumer group是kafka提供的可扩展且具有容错性的消费者机制。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;既然是一个组，那么组内必然可以有多个消费者或消费者实例(consumer instance)，它们共享一个公共的ID，即group ID。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;当然，每个分区只能由同一个消费组内的一个consumer来消费(当然该分区还可以同时分配给其他group中的某个consumer来消费)。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;个人认为，理解consumer group记住下面这三个特性就好了： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.consumer group下可以有一个或多个consumer instance，consumer instance可以是一个进程，也可以是一个线程 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.group.id是一个字符串，唯一标识一个consumer group &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.consumer group下订阅的topic下的每个分区只能分配给某个group下的一个consumer， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;当然该分区还可以同时分配给其他group中的某个consumer来消费。 &nbsp;&nbsp; &nbsp;kafka consumer： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;消费者可以从多个broker中读取数据。消费者可以消费多个topic中的数据。 &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;因为Kafka的broker是无状态的，所以consumer必须使用partition offset来记录消费了多少数据。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果一个consumer指定了一个topic的offset，意味着该consumer已经消费了该offset之前的所有数据。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;consumer可以通过指定offset，从topic的指定位置开始消费数据。consumer的offset存储在Zookeeper中。 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;offset： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;用来保存消费进度。offset表示在当前topic，当前groupID下消费到的位置。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;offset为earliest并不代表offset=1。在不进行过期配置的情况下，kafka消息默认7天时间就会过期。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;过期后其offset也就随之发生变化，使得用数字进行配置的消费进度并不准确。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.earliest：自动重置到最早的offset。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.latest：看上去重置到最晚的offset。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.none：如果边更早的offset也没有的话，就抛出异常给consumer，告诉consumer在整个consumer group中都没有发现有这样的offset。 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;groupID： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个字符串用来指示一组consumer所在的组。相同的groupID表示在一个组里。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;相同的groupID消费记录offset时，记录的是同一个offset。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;所以，此处需要注意&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;（1）如果多个地方都使用相同的groupid，可能造成个别消费者消费不到的情况 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;（2）如果单个消费者消费能力不足的话，可以启动多个相同groupid的consumer消费，处理相同的逻辑。 &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;produce方面： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果有多个分区，发送的时候按照key值hashCode%partitionNum哈希取模分区数来决定该条信息发往哪个partition,&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这里可以自定义partition的分发策略，只要实现Partitioner接口就好，可以自定义成随机分发或者fangwang发往指定分区； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;customer方面： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;对于topic中的partition来说，一个partition只允许一个customer来消费，同一个partition上不允许customer并发； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;Partition数量 &gt; customer数量时： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个consumer会对应于多个partitions，这里主要合理分配consumer数和partition数，否则会导致partition里面的数据被取的不均匀&nbsp;。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;最好partiton数目是consumer数目的整数倍，所以partition数目很重要，比如取24，就很容易设定consumer数目&nbsp;。 &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;Partition数量 &lt; customer数量时： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;就会有剩余的customer闲置，造成浪费； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;如果一个consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同； &nbsp;&nbsp; &nbsp;kafka只保证在一个上数据是有序的（每个partition都有segment文件记录消息的顺序性），无法保证topic全局数据的顺序行； &nbsp;&nbsp; &nbsp;增减consumer，broker，partition会导致rebalance，所以rebalance后consumer对应的partition会发生变化。 4.storm消费kafka时并行度设置问题 &nbsp;&nbsp; &nbsp;1.首先明确的一点是，storm的并行度都是executor即线程级别的并行； &nbsp;&nbsp; &nbsp; &nbsp;包括work(进程)，executor(线程)的设置，具体体现在works,spout,bolt设置上，同一个executor上设置多个task还是会串行化执行，并不能提高执行效率， &nbsp;&nbsp; &nbsp; &nbsp;这也是由于并行是线程并行，一个线程的多个task肯定是有先后执行顺序的，有顺序那就不是并行； &nbsp;&nbsp; &nbsp; &nbsp;关于node，work，executor，task关系和work，spout，bolt，并行度设置网上有很多资料，挺详细； &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;2.这里记录下我遇到的自己关系的另外两个问题： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个是从kafka消费消息是spout并行度设置，另一个ack响应开启的是线程还是进程以及如何设置其数量； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.第一个问题： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;其实理解了上面kafka customer和partition的关系第一个问题也就解决了，spout的并发度实例数量设置最好和partition数量一样， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样能保证一个spout消费者实例对应一个partition，即实现了一个partition中消息消费的顺序性（有时消息的顺序性要求并不是很高） &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;也能很好地提高整个topology的执行效率，至少对拓扑执行效率来说，瓶颈不会卡在spout（数据源）这里； &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.第二个问题： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;通过Storm UI发现，work和spout,bolt并行度不变的情况下，多开几个acker_executors，works的数量并没有增加，反而是executors数量增加， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样就确定了acker_executors如其名一样只是线程，并不像有些网友说的ack的执行是会单独开启ack进程再在该进程里运行ack响应线程。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;他其实就是一个普通的ack线程，运行在已有的work进程里； &nbsp;&nbsp; &nbsp;3.另外通过测试发现，我设置了4个work，4个spout，4个bolt，没有设置acker_executors，Storm UI上显示Num workers是4，Num executors却是12 &nbsp;&nbsp; &nbsp; （4个spout，4个bolt 这里一共是8个executors），所以默认情况下一个work里会有一个acker_executors。 &nbsp;&nbsp; &nbsp;4.默认情况下 一个work会有一个executor，一个executor会有一个task，如果设置了他们的数量，就会按照设置的数量来生成对应实例； &nbsp;&nbsp; &nbsp; &nbsp;如开了4个work，2个spout，3个bolt，那spout和bolt的executors一共就会有5个（spout executors 2个，bolt&nbsp;executors 3个，）， &nbsp;&nbsp; &nbsp; &nbsp;相当于有2个work里的每个work都有1个spout executor和1个bolt executor，另外还有1个work里只有1个bolt executor，另外还有一个work里啥也没有； &nbsp;&nbsp; &nbsp; &nbsp;其实这种配置会导致多开一个啥活也不干的work进程，有些浪费； 5.kafka 多个消费者在同一个groupID消费者组中，只有一个消费者能收到消息，解决方案如下 &nbsp; &nbsp;&nbsp; &nbsp;1.业务需求： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;将一个业务逻辑分拆出来单独部署多个服务器上，然后与主程序之间通过kafka队列通信，每个业务实例上都有一个消费者在监听队列， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;且他们的groupid相同。我们的原意是主程序 往 队列上发送的命令参数（数据值）会被其中随机的一个消费者收到，从而实现一个负载均衡的效果， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;但是后来发现主程序发送往队列上的控制消息（数据值）始终是被其中固定的一个服务器上的消费者收到，其他的消费者从头到尾没有收到过。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样自然就达不到我们负载均衡的效果了。 &nbsp;&nbsp; &nbsp;2.后来发现，造成这个结果的原因可能有两个： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.分区数(Partition)设成了1 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.发送消息的时候指定了key值 &nbsp;&nbsp; &nbsp;3.原因一的解决方案： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.原因：分区数(Partition)设成了1 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;因为kafka为了保证了消息的一致性，同一个分区的消息只会被于此关联的同一个消费者接收到(大致就是这个意思)。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果只有一个分区的话，如果第一条消息被其中一个消费者收到后，后面的消息始终会被这个消费者收到。所以应保证分区的数目大于消费者的数目。 &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.解决方案： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#查看partition数据 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;bin/kafka-topics.sh --zookeeper zk1,zk2,zk3 --describe --partitions 20 --topic topic_test &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#增加partition数目 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#增加partition数目后，如果使用的是java的kafka-client的Producer的话，会由于producer内部的缓存机制， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#导致过几分钟后才被producer感知到partition数目的变化 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;bin/kafka-topics.sh --zookeeper zk1,zk2,zk3 --alter --partitions 20 --topic topic_test &nbsp;&nbsp; &nbsp;4.原因二的解决方案： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.原因：发送消息的时候指定了key值 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;我们改了partitions 的数目后，再次测试，还是始终被其中一个消费者收到。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;后来查看了producer的send方法，发现了是因为我们把key设置为了””,也就是指定了key值为空字符串。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.send方法发送 代码分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;producer.send(new ProducerRecord&lt;String, String&gt;(&quot;test&quot;,&quot;&quot;,&quot;&quot;),new Callback() { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void onCompletion(RecordMetadata metadata, Exception exception) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;}); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;4.send方法的实现 和 partition方法的调用 代码分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // first make sure the metadata for the topic is available &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; long waitedOnMetadataMs = waitOnMetadata(record.topic(), this.maxBlockTimeMs); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; long remainingWaitMs = Math.max(0, this.maxBlockTimeMs - waitedOnMetadataMs); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; byte[] serializedKey; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; serializedKey = keySerializer.serialize(record.topic(), record.key()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ClassCastException cce) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new SerializationException(&quot;Can&#39;t convert key of class &quot; + record.key().getClass().getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; specified in key.serializer&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; byte[] serializedValue; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; serializedValue = valueSerializer.serialize(record.topic(), record.value()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ClassCastException cce) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new SerializationException(&quot;Can&#39;t convert value of class &quot; + record.value().getClass().getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &quot; specified in value.serializer&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;//可以看出 生产者往哪个partition分区 发送消息，实际是由 partition()方法得出的 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int partition = partition(record, serializedKey, serializedValue, metadata.fetch()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; ensureValidRecordSize(serializedSize); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; TopicPartition tp = new TopicPartition(record.topic(), partition); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.trace(&quot;Sending record {} with callback {} to topic {} partition {}&quot;, record, callback, record.topic(), partition); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; RecordAccumulator.RecordAppendResult result = accumulator.append(tp, serializedKey, serializedValue, callback, remainingWaitMs); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (result.batchIsFull || result.newBatchCreated) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.trace(&quot;Waking up the sender since topic {} partition {} is either full or getting a new batch&quot;, record.topic(), partition); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.sender.wakeup(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return result.future; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // handling exceptions and record the errors; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // for API exceptions return them in the future, &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // for other exceptions throw directly &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ApiException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.debug(&quot;Exception occurred during message send:&quot;, e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (callback != null) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; callback.onCompletion(null, e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return new FutureFailure(e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (InterruptedException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new InterruptException(e); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (BufferExhaustedException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.metrics.sensor(&quot;buffer-exhausted-records&quot;).record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw e; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (KafkaException e) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw e; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;5.partition方法的实现 和 ProducerRecord对象 代码分析： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey , byte[] serializedValue, Cluster cluster) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;Integer partition = record.partition(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;if (partition != null) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(record.topic()); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int numPartitions = partitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;// they have given us a partition, use it &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;if (partition &lt; 0 || partition &gt;= numPartitions) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;throw new IllegalArgumentException(&quot;Invalid partition given with record: &quot; + partition &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + &quot; is not in the range [0...&quot; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + numPartitions &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + &quot;].&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return partition; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return this.partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue, &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;cluster); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果发送的时候已经指定了parition的话，就会发送到指定的partition上。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;怎么指定呢？？我们发送的是一个ProducerRecoed对象。看它的构造方法，第二个参数就是partition。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public ProducerRecord(String topic, Integer partition, K key, V value) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (topic == null) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new IllegalArgumentException(&quot;Topic cannot be null&quot;); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.topic = topic; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.partition = partition; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.key = key; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.value = value; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有指定的parition的话，就调用： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;this.partitioner.partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有配置partitioner.class的话，那么partitioner默认是org.apache.kafka.clients.producer.internals.DefaultPartitioner的一个对象(参考文章末尾)， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;查看一下它的partition()方法： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int numPartitions = partitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (keyBytes == null) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int nextValue = counter.getAndIncrement(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (availablePartitions.size() &gt; 0) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int part = DefaultPartitioner.toPositive(nextValue) % availablePartitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return availablePartitions.get(part).partition(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } else { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // no partitions are available, give a non-available partition &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return DefaultPartitioner.toPositive(nextValue) % numPartitions; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } else { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // hash the keyBytes to choose a partition &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return DefaultPartitioner.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;从上面代码就看出了如果指定了key值的话，partition的值实际上是由Utils.murmur2(keyBytes)哈希计算出来，这样自然每次都是被同一个消费者接收到。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有指定的话，就会通过轮询的方式逐个发送。这里有个问题就是，如果我们每次都打印出partition的值的话， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;可能会看到peoducer发送并不一定会按照1、2、3、4、5这样的顺序发送，这个从上面代码中能看出最终返回的不是part的值， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;而是availablePartitions.get(part).partition()的值，而availablePartitions里的PartitionInfo的顺序本身就不一定是严格按照顺序排列的。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;在org.apache.kafka.clients.producer.ProducerConfig类中可以看到kafkaproducer的配置以及默认值，其中有这么一行： &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;.define(PARTITIONER_CLASS_CONFIG, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Type.CLASS, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; DefaultPartitioner.class.getName(), &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Importance.MEDIUM, PARTITIONER_CLASS_DOC) &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;其中静态变量 PARTITIONER_CLASS_CONFIG的值是”partitioner.class”,可以看出其默认对应的类是 DefaultPartitioner.class，当然， &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这里可以自定义partition的分发策略，只要实现Partitioner接口就好，例如下面代码实现了随机选择partition。 &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public class MyPartitioner implements Partitioner{ &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void configure(Map&lt;String, ?&gt; configs) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public int partition(String topic, Object key, byte[] keyBytes, &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;Object value, byte[] valueBytes, Cluster cluster) { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int numPartitions = partitions.size(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int randomNum = new Random().nextInt(numPartitions); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return partitions.get(randomNum).partition(); &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void close() { &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;} &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;} &nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/04/729863.html","headline":"kafka 命令、API","dateModified":"2019-05-04T00:00:00+08:00","datePublished":"2019-05-04T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/04/729863.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>kafka 命令、API</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <h1><u><strong><a href="https://blog.csdn.net/zimiao552147572/article/details/88602959" rel="nofollow">大数据组件使用 总文章</a></strong></u></h1> 
  <p>&nbsp;</p> 
  <p>1.使用控制台运行</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;1.创建一个topic主题 &nbsp;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-topics.sh
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --create --zookeeper IP:2181 --replication-factor 副本数 --partitions 分片数/分区数 --topic 主题名
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --create --zookeeper NODE1:2181 --replication-factor 1 --partitions 1 --topic order</code></pre> 
  <p><img alt="" class="has" height="335" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190504220713541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ppbWlhbzU1MjE0NzU3Mg==,size_16,color_FFFFFF,t_70" width="1089"></p> 
  <p>&nbsp;&nbsp; &nbsp;2.编写代码启动一个生产者，生产数据</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-console-producer.sh
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-producer.sh --broker-list IP:9092 --topic 主题名
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-producer.sh --broker-list NODE1:9092 --topic order</code></pre> 
  <p><img alt="" class="has" height="99" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019050422072462.png" width="747"></p> 
  <p><br> &nbsp;&nbsp; &nbsp;3.编写代码启动给一个消费者，消费数据</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka&nbsp;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;chmod 777 /root/kafka/bin/kafka-console-consumer.sh
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-consumer.sh --zookeeper IP:2181 --from-beginning --topic 主题名
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-consumer.sh --zookeeper NODE1:2181 --from-beginning --topic order</code></pre> 
  <p><img alt="" class="has" height="142" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190504220732314.png" width="1030"></p> 
  <p>&nbsp;&nbsp; &nbsp;4.Kafka 常用操作命令</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.查看当前服务器中的所有 topic
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --list --zookeeper zookeeper的IP:2181
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --list --zookeeper NODE1:2181
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.创建 topic
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --create --zookeeper zookeeper的IP:2181 --replication-factor 副本数 --partitions 分片数/分区数 --topic 主题名
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --create --zookeeper NODE1:2181 --replication-factor 1 --partitions 1 --topic test
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.删除 topic
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;需要在kafka集群中的每个kafka服务器中的 vim /root/kafka/config/server.properties 中设置 delete.topic.enable=true 否则只是标记删除或者直接重启
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --delete --zookeeper zookeeper的IP:2181 --topic 主题名
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --delete --zookeeper NODE1:2181 --topic test
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;4.通过 shell 命令发送消息
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-producer.sh --broker-list kafka的IP:9092 --topic 主题名
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-producer.sh --broker-list NODE1:9092 --topic test
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;5.通过 shell 消费消息
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-console-consumer.sh --zookeeper zookeeper的IP:2181 --from-beginning --topic 主题名
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-console-consumer.sh --zookeeper NODE1:2181 --from-beginning --topic test
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;6.查看消费位置
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper zookeeper的IP:2181 --group testGroup
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zookeeper NODE1:2181 --group testGroup
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;7.查看某个 Topic 的详情
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --topic 主题名 --describe --zookeeper zookeeper的IP:2181
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --topic 主题名 --describe --zookeeper NODE1:2181
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;8.对分区数进行修改
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;cd /root/kafka
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;格式：bin/kafka-topics.sh --zookeeper zookeeper的IP --alter --partitions 2 --topic 主题名
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;例子：bin/kafka-topics.sh --zookeeper NODE1 --alter --partitions 分片数/分区数 --topic 主题名</code></pre> 
  <p><br> 2.使用Java api运行</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;1.java工程-maven，依赖
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;dependency&gt;
&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;version&gt;0.11.0.1&lt;/version&gt;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&lt;/dependency&gt;

&nbsp;&nbsp; &nbsp;2.生产者（先启动消费者等待接收数据，然后再启动生产者进行发送数据）
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者：订单的生产者代码
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public class OrderProducer
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public static void main(String[] args) throws InterruptedException
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 通过Properties配置文件的方式 配置"连接集群"的信息
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Properties props = new Properties();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("bootstrap.servers", "NODE1:9092");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("acks", "all");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("retries", 0);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("batch.size", 16384);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("linger.ms", 1);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("buffer.memory", 33554432);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;String, String&gt;(props);
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (int i = 0; i &lt; 10000; i++)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:可以指定数据发往哪个partition,当ProducerRecord 的构造参数中有partition的时候，就可以发送到对应partition上
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;("topic主题名",partition,"key", "value")
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; partition = new ProducerRecord&lt;String, String&gt;("order", 0, "key", "订单"+i);
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:如果生产者没有指定partition，但是发送消息中有key，根据key的hash值的方式发送数据到那个partition分区/分片中
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;("topic主题名","key", "value")
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; key = new ProducerRecord&lt;String, String&gt;("order", "key", "value"+i);
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //数据分发策略:既没有指定partition，也没有key的情况下如何发送数据。使用轮询的方式发送数据到那个partition分区/分片中
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // new ProducerRecord&lt;String, String&gt;("topic主题名", "value")
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ProducerRecord&lt;String, String&gt; value = new ProducerRecord&lt;String, String&gt;("order", "订单信息！"+i);
&nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka生产者发送数据
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kafkaProducer.send(value);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }


&nbsp;&nbsp; &nbsp;3.消费者（先启动消费者等待接收数据，然后再启动生产者进行发送数据）&nbsp;&nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //kafka消费者：订单的消费者代码
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public class OrderConsumer
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; public static void main(String[] args)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //1.通过Properties配置文件的方式 配置"连接集群"的信息
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Properties props = new Properties();
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("bootstrap.servers", "NODE1:9092");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //配置group.id：多个Consumer的group.id都相同的话，表示多个Consumer都在同一个消费组group中
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("group.id", "test");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("enable.auto.commit", "true");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("auto.commit.interval.ms", "1000");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // Kafka消费者
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; KafkaConsumer&lt;String, String&gt; kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(props);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //2.订阅某个topic主题下的相关消息数据用于消费。可以订阅多个topic主题，封装到一个 List中，可以使用Arrays.asList进行封装
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; kafkaConsumer.subscribe(Arrays.asList("order"));
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; while (true)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //jdk queue的操作方法：offer插入元素、poll获取元素。
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //blockingqueue的操作方法：put插入元素，take获取元素。
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(100);
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for (ConsumerRecord&lt;String, String&gt; record : consumerRecords)
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; //此处需要等待到有数据才能消费进行获取打印
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; System.out.println("消费的数据为：" + record.value());
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; }</code></pre> 
  <p>&nbsp;<br> 3.topic主体中的分区partition 和 group.id消费者组 的关系</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;Consumer Group（CG）消费者组：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这是 kafka 用来实现一个 topic(主题) 消息的广播（发给所有的 consumer消费者）和 单播（发给任意一个 consumer消费者）的手段。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个 topic(主题) 可以有多个 CG消费者组。topic(主题) 的消息会复制（不是真的复制，是概念上的）到所有的 CG消费者组，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;但每个 partion(分区) 只会把消息发给该 CG 中的一个consumer消费者。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果需要实现广播，只要每个 consumer消费者 有一个独立的 CG 就可以了。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;要实现单播只要所有的 consumer 在同一个 CG。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;用 CG消费者组 还可以将 consumer消费者 进行自由的分组，而不需要多次发送消息到不同的 topic(主题)。
&nbsp;&nbsp; &nbsp;Broker：一台 kafka 服务器就是一个 broker(中间人)。一个kafka集群(cluster) 由多个 broker(中间人) 组成。一个 broker 可以容纳多个 topic(主题)。
&nbsp;&nbsp; &nbsp;Partition(分区)：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;为了实现扩展性，一个非常大的 topic(主题) 可以分布到多个 broker（即服务器）上，一个topic(主题) 可以分为多个 partition(分区)，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;每个 partition(分区) 是一个有序的队列(Queue)。partition(分区) 中的每条消息都会被分配一个有序的 id（offset）。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;kafka 只保证按一个 partition(分区) 中的顺序 将消息发给 consumer消费者，不保证一个 topic 的整体（多个 partition分区 间）的顺序</code></pre> 
  <p>&nbsp;</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;情况一：同一个topic的一个partition只能被同一个customerGroup的一个customer消费；group里多于partition数量的customer会空闲；</code></pre> 
  <p><br><img alt="" class="has" height="176" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190504220815262.png" width="340"></p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;情况二：同一个topic的partition数量多于同一个customerGroup的customer数量时，会有一个customer消费多个partition，这样也就没法保证customer接收到的消息的顺序性，kafka只保证在一个partition上数据是有序的，无法保证topic全局数据的顺序性；</code></pre> 
  <p><br><img alt="" class="has" height="256" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190504220818908.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ppbWlhbzU1MjE0NzU3Mg==,size_16,color_FFFFFF,t_70" width="344"></p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;情况三：一个topic 的partitions被多个customerGroup消费时，可以并行重复消费；</code></pre> 
  <p><img alt="" class="has" height="388" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190504220822638.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ppbWlhbzU1MjE0NzU3Mg==,size_16,color_FFFFFF,t_70" width="336"></p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;kafka topic&nbsp;partition被同一个GROUPID的多个消费者消费，只有一个能收到消息的原因一般有如下：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.只有一个partition分区；
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;同一个topic的同一个partition分区只允许同一个customerGroup的一个消费者消费信息，一个partition上不允许同一个消费者组的多个消费者并发，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;但同一个partition上是可以多个不同消费者组种的消费者并发消费的；
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.多个partition分区，但是，消息在生产时只发往到了一个partiton上；
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;key的hashCode%partitionNum相同导致，或者自定义了分区策略；
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;导致这种严重的数据倾斜；</code></pre> 
  <p>&nbsp;</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;1.发送到Kafka的消息会根据key，发送到对应topic的partition中，有默认的分发规则（也可以自己重写分发规则），
&nbsp;&nbsp; &nbsp; &nbsp;基本上就是相同的key发送到一个partition中，不同的key有可能发送到相同的partition中。

&nbsp;&nbsp; &nbsp;2.group是消费者中的概念，按照group（组）对消费者进行区分。
&nbsp;&nbsp; &nbsp; &nbsp;对于每个group，需要先指定订阅哪个topic的消息，然后该topic下的partition会平均分配到group下面的consumer上。
&nbsp;&nbsp; &nbsp; &nbsp;所以会出现以下这些情况：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.一个topic被多个group订阅，那么一条消息就会被不同group中的多个consumer处理。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果需要实现广播，只要每个 consumer消费者 有一个独立的 CG 就可以了。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;一个 topic(主题) 可以有多个 CG消费者组。topic(主题) 的消息会复制（不是真的复制，是概念上的）到所有的 CG消费者组，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;但每个 partion(分区) 只会把消息发给该 CG 中的一个consumer消费者。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.同一个group中，每个partition只会被一个consumer处理，这个consumer处理的消息不一定是同一个key的。所以需要在处理的地方判断。
&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;要实现单播只要所有的 consumer 在同一个 CG。

&nbsp;&nbsp; &nbsp;3.例子，现在有一个用户信息修改的回调消息扔到消息队列里，有两个业务要处理，一个是更新数据库，一个是更新es索引信息
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;topic：user_update_topic
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;key：user_update_key_cid //cid标志公司的区分信息
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样子的话，同一个公司的用户更新会被分配到一个partition中，同一个公司的用户更新能保证前后顺序不变

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group1中的consumer更新数据库，group：consumer1_group
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group2中的consumer更新es索引信息，group：consumer2_group
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;group1、group2这个两个group会分别消费这个topic下的数据，对于每个group，内部的consumer会平分topic下的partition，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;相当于group中的每个consumer会处理多个公司的数据，但处理的公司不会有重叠。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;以上是topic中partition多余group中的消费者的时候，如果group下面有3个消费者，但是分区partition只有一个，那么三个消费者中只有一个会消费消息。</code></pre> 
  <p>&nbsp;</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;消费者组 (Consumer Group)
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;consumer group是kafka提供的可扩展且具有容错性的消费者机制。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;既然是一个组，那么组内必然可以有多个消费者或消费者实例(consumer instance)，它们共享一个公共的ID，即group ID。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;当然，每个分区只能由同一个消费组内的一个consumer来消费(当然该分区还可以同时分配给其他group中的某个consumer来消费)。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;个人认为，理解consumer group记住下面这三个特性就好了：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.consumer group下可以有一个或多个consumer instance，consumer instance可以是一个进程，也可以是一个线程
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.group.id是一个字符串，唯一标识一个consumer group
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.consumer group下订阅的topic下的每个分区只能分配给某个group下的一个consumer，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;当然该分区还可以同时分配给其他group中的某个consumer来消费。
&nbsp;&nbsp; &nbsp;kafka consumer：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;消费者可以从多个broker中读取数据。消费者可以消费多个topic中的数据。
&nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;因为Kafka的broker是无状态的，所以consumer必须使用partition offset来记录消费了多少数据。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果一个consumer指定了一个topic的offset，意味着该consumer已经消费了该offset之前的所有数据。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;consumer可以通过指定offset，从topic的指定位置开始消费数据。consumer的offset存储在Zookeeper中。
&nbsp; &nbsp;&nbsp;
&nbsp;&nbsp; &nbsp;offset：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;用来保存消费进度。offset表示在当前topic，当前groupID下消费到的位置。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;offset为earliest并不代表offset=1。在不进行过期配置的情况下，kafka消息默认7天时间就会过期。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;过期后其offset也就随之发生变化，使得用数字进行配置的消费进度并不准确。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.earliest：自动重置到最早的offset。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.latest：看上去重置到最晚的offset。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.none：如果边更早的offset也没有的话，就抛出异常给consumer，告诉consumer在整个consumer group中都没有发现有这样的offset。
&nbsp; &nbsp;&nbsp;
&nbsp;&nbsp; &nbsp;groupID：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个字符串用来指示一组consumer所在的组。相同的groupID表示在一个组里。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;相同的groupID消费记录offset时，记录的是同一个offset。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;所以，此处需要注意&nbsp;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;（1）如果多个地方都使用相同的groupid，可能造成个别消费者消费不到的情况
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;（2）如果单个消费者消费能力不足的话，可以启动多个相同groupid的consumer消费，处理相同的逻辑。</code></pre> 
  <p><br> &nbsp;&nbsp;</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;produce方面：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果有多个分区，发送的时候按照key值hashCode%partitionNum哈希取模分区数来决定该条信息发往哪个partition,&nbsp;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这里可以自定义partition的分发策略，只要实现Partitioner接口就好，可以自定义成随机分发或者fangwang发往指定分区；
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;customer方面：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;对于topic中的partition来说，一个partition只允许一个customer来消费，同一个partition上不允许customer并发；
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;Partition数量 &gt; customer数量时：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个consumer会对应于多个partitions，这里主要合理分配consumer数和partition数，否则会导致partition里面的数据被取的不均匀&nbsp;。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;最好partiton数目是consumer数目的整数倍，所以partition数目很重要，比如取24，就很容易设定consumer数目&nbsp;。
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;Partition数量 &lt; customer数量时：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;就会有剩余的customer闲置，造成浪费；
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;如果一个consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同；
&nbsp;&nbsp; &nbsp;kafka只保证在一个上数据是有序的（每个partition都有segment文件记录消息的顺序性），无法保证topic全局数据的顺序行；
&nbsp;&nbsp; &nbsp;增减consumer，broker，partition会导致rebalance，所以rebalance后consumer对应的partition会发生变化。</code></pre> 
  <p><br> 4.storm消费kafka时并行度设置问题</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;1.首先明确的一点是，storm的并行度都是executor即线程级别的并行；
&nbsp;&nbsp; &nbsp; &nbsp;包括work(进程)，executor(线程)的设置，具体体现在works,spout,bolt设置上，同一个executor上设置多个task还是会串行化执行，并不能提高执行效率，
&nbsp;&nbsp; &nbsp; &nbsp;这也是由于并行是线程并行，一个线程的多个task肯定是有先后执行顺序的，有顺序那就不是并行；
&nbsp;&nbsp; &nbsp; &nbsp;关于node，work，executor，task关系和work，spout，bolt，并行度设置网上有很多资料，挺详细；
&nbsp;&nbsp; &nbsp;
&nbsp;&nbsp; &nbsp;2.这里记录下我遇到的自己关系的另外两个问题：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;一个是从kafka消费消息是spout并行度设置，另一个ack响应开启的是线程还是进程以及如何设置其数量；
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.第一个问题：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;其实理解了上面kafka customer和partition的关系第一个问题也就解决了，spout的并发度实例数量设置最好和partition数量一样，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样能保证一个spout消费者实例对应一个partition，即实现了一个partition中消息消费的顺序性（有时消息的顺序性要求并不是很高）
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp;也能很好地提高整个topology的执行效率，至少对拓扑执行效率来说，瓶颈不会卡在spout（数据源）这里；
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.第二个问题：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;通过Storm UI发现，work和spout,bolt并行度不变的情况下，多开几个acker_executors，works的数量并没有增加，反而是executors数量增加，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样就确定了acker_executors如其名一样只是线程，并不像有些网友说的ack的执行是会单独开启ack进程再在该进程里运行ack响应线程。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;他其实就是一个普通的ack线程，运行在已有的work进程里；

&nbsp;&nbsp; &nbsp;3.另外通过测试发现，我设置了4个work，4个spout，4个bolt，没有设置acker_executors，Storm UI上显示Num workers是4，Num executors却是12
&nbsp;&nbsp; &nbsp; （4个spout，4个bolt 这里一共是8个executors），所以默认情况下一个work里会有一个acker_executors。

&nbsp;&nbsp; &nbsp;4.默认情况下 一个work会有一个executor，一个executor会有一个task，如果设置了他们的数量，就会按照设置的数量来生成对应实例；
&nbsp;&nbsp; &nbsp; &nbsp;如开了4个work，2个spout，3个bolt，那spout和bolt的executors一共就会有5个（spout executors 2个，bolt&nbsp;executors 3个，），
&nbsp;&nbsp; &nbsp; &nbsp;相当于有2个work里的每个work都有1个spout executor和1个bolt executor，另外还有1个work里只有1个bolt executor，另外还有一个work里啥也没有；
&nbsp;&nbsp; &nbsp; &nbsp;其实这种配置会导致多开一个啥活也不干的work进程，有些浪费；</code></pre> 
  <p><br> 5.kafka 多个消费者在同一个groupID消费者组中，只有一个消费者能收到消息，解决方案如下<br> &nbsp;</p> 
  <pre class="has">
<code>&nbsp;&nbsp; &nbsp;1.业务需求：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;将一个业务逻辑分拆出来单独部署多个服务器上，然后与主程序之间通过kafka队列通信，每个业务实例上都有一个消费者在监听队列，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;且他们的groupid相同。我们的原意是主程序 往 队列上发送的命令参数（数据值）会被其中随机的一个消费者收到，从而实现一个负载均衡的效果，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;但是后来发现主程序发送往队列上的控制消息（数据值）始终是被其中固定的一个服务器上的消费者收到，其他的消费者从头到尾没有收到过。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这样自然就达不到我们负载均衡的效果了。

&nbsp;&nbsp; &nbsp;2.后来发现，造成这个结果的原因可能有两个：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.分区数(Partition)设成了1
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.发送消息的时候指定了key值

&nbsp;&nbsp; &nbsp;3.原因一的解决方案：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.原因：分区数(Partition)设成了1
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.分析：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;因为kafka为了保证了消息的一致性，同一个分区的消息只会被于此关联的同一个消费者接收到(大致就是这个意思)。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;如果只有一个分区的话，如果第一条消息被其中一个消费者收到后，后面的消息始终会被这个消费者收到。所以应保证分区的数目大于消费者的数目。
&nbsp;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.解决方案：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#查看partition数据
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;bin/kafka-topics.sh --zookeeper zk1,zk2,zk3 --describe --partitions 20 --topic topic_test

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#增加partition数目
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#增加partition数目后，如果使用的是java的kafka-client的Producer的话，会由于producer内部的缓存机制，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;#导致过几分钟后才被producer感知到partition数目的变化
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;bin/kafka-topics.sh --zookeeper zk1,zk2,zk3 --alter --partitions 20 --topic topic_test

&nbsp;&nbsp; &nbsp;4.原因二的解决方案：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;1.原因：发送消息的时候指定了key值
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;2.分析：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;我们改了partitions 的数目后，再次测试，还是始终被其中一个消费者收到。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;后来查看了producer的send方法，发现了是因为我们把key设置为了””,也就是指定了key值为空字符串。

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;3.send方法发送 代码分析：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;producer.send(new ProducerRecord&lt;String, String&gt;("test","",""),new Callback() {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void onCompletion(RecordMetadata metadata, Exception exception) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;});

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;4.send方法的实现 和 partition方法的调用 代码分析：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;@Override
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // first make sure the metadata for the topic is available
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; long waitedOnMetadataMs = waitOnMetadata(record.topic(), this.maxBlockTimeMs);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; long remainingWaitMs = Math.max(0, this.maxBlockTimeMs - waitedOnMetadataMs);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; byte[] serializedKey;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; serializedKey = keySerializer.serialize(record.topic(), record.key());
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ClassCastException cce) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new SerializationException("Can't convert key of class " + record.key().getClass().getName() +
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; " to class " + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; " specified in key.serializer");
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; }
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; byte[] serializedValue;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; try {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; serializedValue = valueSerializer.serialize(record.topic(), record.value());
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ClassCastException cce) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new SerializationException("Can't convert value of class " + record.value().getClass().getName() +
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; " to class " + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; " specified in value.serializer");
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; }
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;//可以看出 生产者往哪个partition分区 发送消息，实际是由 partition()方法得出的
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int partition = partition(record, serializedKey, serializedValue, metadata.fetch());
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; ensureValidRecordSize(serializedSize);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; TopicPartition tp = new TopicPartition(record.topic(), partition);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.trace("Sending record {} with callback {} to topic {} partition {}", record, callback, record.topic(), partition);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; RecordAccumulator.RecordAppendResult result = accumulator.append(tp, serializedKey, serializedValue, callback, remainingWaitMs);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (result.batchIsFull || result.newBatchCreated) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.trace("Waking up the sender since topic {} partition {} is either full or getting a new batch", record.topic(), partition);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.sender.wakeup();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; }
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return result.future;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // handling exceptions and record the errors;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // for API exceptions return them in the future,
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // for other exceptions throw directly
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (ApiException e) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; log.debug("Exception occurred during message send:", e);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (callback != null)
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; callback.onCompletion(null, e);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return new FutureFailure(e);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (InterruptedException e) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new InterruptException(e);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (BufferExhaustedException e) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.metrics.sensor("buffer-exhausted-records").record();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw e;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } catch (KafkaException e) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.errors.record();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw e;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; }
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; }

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;5.partition方法的实现 和 ProducerRecord对象 代码分析：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey , byte[] serializedValue, Cluster cluster) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;Integer partition = record.partition();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;if (partition != null) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(record.topic());
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int numPartitions = partitions.size();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;// they have given us a partition, use it
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;if (partition &lt; 0 || partition &gt;= numPartitions)
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;throw new IllegalArgumentException("Invalid partition given with record: " + partition
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + " is not in the range [0..."
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + numPartitions
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; + "].");
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return partition;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return this.partitioner.partition(record.topic(), record.key(), serializedKey, record.value(), serializedValue,
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;cluster);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;}

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果发送的时候已经指定了parition的话，就会发送到指定的partition上。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;怎么指定呢？？我们发送的是一个ProducerRecoed对象。看它的构造方法，第二个参数就是partition。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public ProducerRecord(String topic, Integer partition, K key, V value) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (topic == null)
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; throw new IllegalArgumentException("Topic cannot be null");
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.topic = topic;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.partition = partition;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.key = key;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; this.value = value;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; }

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有指定的parition的话，就调用：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;this.partitioner.partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster)

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有配置partitioner.class的话，那么partitioner默认是org.apache.kafka.clients.producer.internals.DefaultPartitioner的一个对象(参考文章末尾)，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;查看一下它的partition()方法：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int numPartitions = partitions.size();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (keyBytes == null) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int nextValue = counter.getAndIncrement();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; if (availablePartitions.size() &gt; 0) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; int part = DefaultPartitioner.toPositive(nextValue) % availablePartitions.size();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return availablePartitions.get(part).partition();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } else {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // no partitions are available, give a non-available partition
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return DefaultPartitioner.toPositive(nextValue) % numPartitions;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; }
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; } else {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; // hash the keyBytes to choose a partition
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; return DefaultPartitioner.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; }
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; }

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;从上面代码就看出了如果指定了key值的话，partition的值实际上是由Utils.murmur2(keyBytes)哈希计算出来，这样自然每次都是被同一个消费者接收到。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;如果没有指定的话，就会通过轮询的方式逐个发送。这里有个问题就是，如果我们每次都打印出partition的值的话，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;可能会看到peoducer发送并不一定会按照1、2、3、4、5这样的顺序发送，这个从上面代码中能看出最终返回的不是part的值，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;而是availablePartitions.get(part).partition()的值，而availablePartitions里的PartitionInfo的顺序本身就不一定是严格按照顺序排列的。

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;在org.apache.kafka.clients.producer.ProducerConfig类中可以看到kafkaproducer的配置以及默认值，其中有这么一行：
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;.define(PARTITIONER_CLASS_CONFIG,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Type.CLASS,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; DefaultPartitioner.class.getName(),
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Importance.MEDIUM, PARTITIONER_CLASS_DOC)

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;其中静态变量 PARTITIONER_CLASS_CONFIG的值是”partitioner.class”,可以看出其默认对应的类是 DefaultPartitioner.class，当然，
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;这里可以自定义partition的分发策略，只要实现Partitioner接口就好，例如下面代码实现了随机选择partition。
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;public class MyPartitioner implements Partitioner{
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void configure(Map&lt;String, ?&gt; configs) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;}

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public int partition(String topic, Object key, byte[] keyBytes,
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;Object value, byte[] valueBytes, Cluster cluster) {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int numPartitions = partitions.size();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;int randomNum = new Random().nextInt(numPartitions);
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;return partitions.get(randomNum).partition();
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;}

&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;@Override
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;public void close() {
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;// TODO Auto-generated method stub
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp;}
&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;}
</code></pre> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
