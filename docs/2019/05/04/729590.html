<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>认识决策树算法及随机森林 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="认识决策树算法及随机森林" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="目录 &nbsp; 认识决策树： 信息熵 信息增益 决策树是怎样对特征抽取后的结果进行计算的？ 构造的决策树，随着深度的加深，容易造成过拟合现象： 认识决策树： 例子：对是否可以贷款进行决策树构造 决策树就好比：二叉树分类中，关键的信息分类标准占重要地位且放在首位（信息增益值大的特征） 信息熵：（描述信息的不确定性，值越大，不确定性越大） &nbsp; 信息熵 信息等价于消除不确定性 信息增益 特征A对训练数据集D的信息增益g(D,A), 定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为： 简单例子进行了解： 决策树是怎样对特征抽取后的结果进行计算的？ 决策树的每一层都需要根据最大信息增益来进行分类特征的选取。实际上，信息增益最大找的是样本之间差异最大的特征 构造的决策树，随着深度的加深，容易造成过拟合现象： 解决方法：1：剪枝法 dec = DecisionTreeClassifier(min_samples_leaf=1) # min_samples_leaf=1 表示针对小于1的样本的叶子结点删除 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2：随机森林（随机抽样建立多个决策树） class&nbsp;sklearn.ensemble.RandomForestClassifier(n_estimators=10,&nbsp;criterion=’gini’,&nbsp;max_depth=None, bootstrap=True,&nbsp;random_state=None)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 随机森林分类器 n_estimators：integer，optional（default = 10） 森林里的树木数量 criteria：string，可选（default =“gini”）分割特征的测量方法 max_depth：integer或None，可选（默认=无）树的最大深度 bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样 &nbsp;" />
<meta property="og:description" content="目录 &nbsp; 认识决策树： 信息熵 信息增益 决策树是怎样对特征抽取后的结果进行计算的？ 构造的决策树，随着深度的加深，容易造成过拟合现象： 认识决策树： 例子：对是否可以贷款进行决策树构造 决策树就好比：二叉树分类中，关键的信息分类标准占重要地位且放在首位（信息增益值大的特征） 信息熵：（描述信息的不确定性，值越大，不确定性越大） &nbsp; 信息熵 信息等价于消除不确定性 信息增益 特征A对训练数据集D的信息增益g(D,A), 定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为： 简单例子进行了解： 决策树是怎样对特征抽取后的结果进行计算的？ 决策树的每一层都需要根据最大信息增益来进行分类特征的选取。实际上，信息增益最大找的是样本之间差异最大的特征 构造的决策树，随着深度的加深，容易造成过拟合现象： 解决方法：1：剪枝法 dec = DecisionTreeClassifier(min_samples_leaf=1) # min_samples_leaf=1 表示针对小于1的样本的叶子结点删除 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2：随机森林（随机抽样建立多个决策树） class&nbsp;sklearn.ensemble.RandomForestClassifier(n_estimators=10,&nbsp;criterion=’gini’,&nbsp;max_depth=None, bootstrap=True,&nbsp;random_state=None)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 随机森林分类器 n_estimators：integer，optional（default = 10） 森林里的树木数量 criteria：string，可选（default =“gini”）分割特征的测量方法 max_depth：integer或None，可选（默认=无）树的最大深度 bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样 &nbsp;" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-04T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"目录 &nbsp; 认识决策树： 信息熵 信息增益 决策树是怎样对特征抽取后的结果进行计算的？ 构造的决策树，随着深度的加深，容易造成过拟合现象： 认识决策树： 例子：对是否可以贷款进行决策树构造 决策树就好比：二叉树分类中，关键的信息分类标准占重要地位且放在首位（信息增益值大的特征） 信息熵：（描述信息的不确定性，值越大，不确定性越大） &nbsp; 信息熵 信息等价于消除不确定性 信息增益 特征A对训练数据集D的信息增益g(D,A), 定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为： 简单例子进行了解： 决策树是怎样对特征抽取后的结果进行计算的？ 决策树的每一层都需要根据最大信息增益来进行分类特征的选取。实际上，信息增益最大找的是样本之间差异最大的特征 构造的决策树，随着深度的加深，容易造成过拟合现象： 解决方法：1：剪枝法 dec = DecisionTreeClassifier(min_samples_leaf=1) # min_samples_leaf=1 表示针对小于1的样本的叶子结点删除 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2：随机森林（随机抽样建立多个决策树） class&nbsp;sklearn.ensemble.RandomForestClassifier(n_estimators=10,&nbsp;criterion=’gini’,&nbsp;max_depth=None, bootstrap=True,&nbsp;random_state=None)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 随机森林分类器 n_estimators：integer，optional（default = 10） 森林里的树木数量 criteria：string，可选（default =“gini”）分割特征的测量方法 max_depth：integer或None，可选（默认=无）树的最大深度 bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样 &nbsp;","@type":"BlogPosting","url":"/2019/05/04/729590.html","headline":"认识决策树算法及随机森林","dateModified":"2019-05-04T00:00:00+08:00","datePublished":"2019-05-04T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/05/04/729590.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>认识决策树算法及随机森林</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p id="main-toc"><strong>目录</strong></p> 
  <p id="-toc" style="margin-left:80px;">&nbsp;</p> 
  <p id="%E8%AE%A4%E8%AF%86%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E8%AE%A4%E8%AF%86%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%9A" rel="nofollow">认识决策树：</a></p> 
  <p id="%E4%BF%A1%E6%81%AF%E7%86%B5-toc" style="margin-left:80px;"><a href="#%E4%BF%A1%E6%81%AF%E7%86%B5" rel="nofollow">信息熵</a></p> 
  <p id="%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A-toc" style="margin-left:80px;"><a href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A" rel="nofollow">信息增益</a></p> 
  <p id="%E5%86%B3%E7%AD%96%E6%A0%91%E6%98%AF%E6%80%8E%E6%A0%B7%E5%AF%B9%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%E5%90%8E%E7%9A%84%E7%BB%93%E6%9E%9C%E8%BF%9B%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%9A%84%EF%BC%9F-toc" style="margin-left:80px;"><a href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%98%AF%E6%80%8E%E6%A0%B7%E5%AF%B9%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%E5%90%8E%E7%9A%84%E7%BB%93%E6%9E%9C%E8%BF%9B%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%9A%84%EF%BC%9F" rel="nofollow">决策树是怎样对特征抽取后的结果进行计算的？</a></p> 
  <p id="%E6%9E%84%E9%80%A0%E7%9A%84%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%8C%E9%9A%8F%E7%9D%80%E6%B7%B1%E5%BA%A6%E7%9A%84%E5%8A%A0%E6%B7%B1%EF%BC%8C%E5%AE%B9%E6%98%93%E9%80%A0%E6%88%90%E8%BF%87%E6%8B%9F%E5%90%88%E7%8E%B0%E8%B1%A1%EF%BC%9A-toc" style="margin-left:80px;"><a href="#%E6%9E%84%E9%80%A0%E7%9A%84%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%8C%E9%9A%8F%E7%9D%80%E6%B7%B1%E5%BA%A6%E7%9A%84%E5%8A%A0%E6%B7%B1%EF%BC%8C%E5%AE%B9%E6%98%93%E9%80%A0%E6%88%90%E8%BF%87%E6%8B%9F%E5%90%88%E7%8E%B0%E8%B1%A1%EF%BC%9A" rel="nofollow">构造的决策树，随着深度的加深，容易造成过拟合现象：</a></p> 
  <hr id="hr-toc">
  <h3 id="%E8%AE%A4%E8%AF%86%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%9A">认识决策树：</h3> 
  <p>例子：对是否可以贷款进行决策树构造</p> 
  <p><img alt="" class="has" height="315" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190504175103884.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dyb3dpbmdfaGFja2Vy,size_16,color_FFFFFF,t_70" width="318"></p> 
  <blockquote> 
   <p>决策树就好比：二叉树分类中，关键的信息分类标准占重要地位且放在首位（信息增益值大的特征）</p> 
   <p>信息熵：（描述信息的不确定性，值越大，不确定性越大）</p> 
  </blockquote> 
  <p>&nbsp;</p> 
  <h3 id="%E4%BF%A1%E6%81%AF%E7%86%B5">信息熵</h3> 
  <p><img alt="" class="has" height="64" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190504175517130.png" width="349"></p> 
  <p>信息等价于消除不确定性</p> 
  <h3 id="%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A">信息增益</h3> 
  <p style="margin-left:0in;"><span style="color:#333333;">特征</span><span style="color:#333333;">A</span><span style="color:#333333;">对训练数据集</span><span style="color:#333333;">D</span><span style="color:#333333;">的信息增益</span><span style="color:#333333;">g(D,A),</span></p> 
  <p style="margin-left:0in;"><span style="color:#333333;">定义为集合</span><span style="color:#333333;">D</span><span style="color:#333333;">的</span><span style="color:#333333;">信息熵</span><span style="color:#333333;">H(D)</span><span style="color:#333333;">与特征</span><span style="color:#333333;">A</span><span style="color:#333333;">给定条件下</span><span style="color:#333333;">D</span><span style="color:#333333;">的信息条件熵</span><span style="color:#333333;">H(D|A)</span><span style="color:#333333;">之差，即公式为：</span></p> 
  <p><img alt="" class="has" height="78" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190504175809174.png" width="408"></p> 
  <p><img alt="" class="has" height="256" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019050417594432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dyb3dpbmdfaGFja2Vy,size_16,color_FFFFFF,t_70" width="519"></p> 
  <p>简单例子进行了解：</p> 
  <p><img alt="" class="has" height="495" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190504180114165.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dyb3dpbmdfaGFja2Vy,size_16,color_FFFFFF,t_70" width="578"></p> 
  <p><img alt="" class="has" height="304" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190504180044228.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dyb3dpbmdfaGFja2Vy,size_16,color_FFFFFF,t_70" width="572"></p> 
  <h3 id="%E5%86%B3%E7%AD%96%E6%A0%91%E6%98%AF%E6%80%8E%E6%A0%B7%E5%AF%B9%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%E5%90%8E%E7%9A%84%E7%BB%93%E6%9E%9C%E8%BF%9B%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%9A%84%EF%BC%9F">决策树是怎样对特征抽取后的结果进行计算的？</h3> 
  <p>决策树的每一层都需要根据最大信息增益来进行分类特征的选取。实际上，信息增益最大找的是样本之间差异最大的特征</p> 
  <h3 id="%E6%9E%84%E9%80%A0%E7%9A%84%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%8C%E9%9A%8F%E7%9D%80%E6%B7%B1%E5%BA%A6%E7%9A%84%E5%8A%A0%E6%B7%B1%EF%BC%8C%E5%AE%B9%E6%98%93%E9%80%A0%E6%88%90%E8%BF%87%E6%8B%9F%E5%90%88%E7%8E%B0%E8%B1%A1%EF%BC%9A">构造的决策树，随着深度的加深，容易造成过拟合现象：</h3> 
  <p>解决方法：1：剪枝法</p> 
  <blockquote> 
   <pre>
<code class="language-html hljs">dec = DecisionTreeClassifier(min_samples_leaf=1)    # min_samples_leaf=1  表示针对小于1的样本的叶子结点删除</code></pre> 
  </blockquote> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2：随机森林（随机抽样建立多个决策树）</p> 
  <p><span style="color:#000000;"><em>class&nbsp;</em></span><span style="color:#000000;">sklearn.ensemble.RandomForestClassifier</span><span style="color:#000000;">(</span><span style="color:#000000;"><em>n_estimators</em></span><span style="color:#000000;"><em>=10</em></span><span style="color:#000000;">,&nbsp;</span><span style="color:#000000;"><em>criterion=’</em></span><span style="color:#000000;"><em>gini</em></span><span style="color:#000000;"><em>’</em></span><span style="color:#000000;">,</span>&nbsp;<span style="color:#000000;"><em>max_depth</em></span><span style="color:#000000;"><em>=None</em></span><span style="color:#000000;">, </span><span style="color:#000000;"><em>bootstrap=True</em></span><span style="color:#000000;">,&nbsp;</span><span style="color:#000000;"><em>random_state</em></span><span style="color:#000000;"><em>=None</em></span><span style="color:#000000;">)&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 随机森林分类器</span></p> 
  <blockquote> 
   <p><span style="color:#000000;">n_estimators</span><span style="color:#000000;">：</span><span style="color:#000000;">integer</span><span style="color:#000000;">，</span><span style="color:#000000;">optional</span><span style="color:#000000;">（</span><span style="color:#000000;">default = 10</span><span style="color:#000000;">） 森林里的树木数量</span></p> 
   <p><span style="color:#000000;">criteria</span><span style="color:#000000;">：</span><span style="color:#000000;">string</span><span style="color:#000000;">，可选（</span><span style="color:#000000;">default =“</span><span style="color:#000000;">gini</span><span style="color:#000000;">”</span><span style="color:#000000;">）分割特征的测量方法</span></p> 
   <p><span style="color:#000000;">max_depth</span><span style="color:#000000;">：</span><span style="color:#000000;">integer</span><span style="color:#000000;">或</span><span style="color:#000000;">None</span><span style="color:#000000;">，可选（默认</span><span style="color:#000000;">=</span><span style="color:#000000;">无）树的最大深度 </span></p> 
   <p><span style="color:#000000;">bootstrap</span><span style="color:#000000;">：</span><span style="color:#000000;">boolean</span><span style="color:#000000;">，</span><span style="color:#000000;">optional</span><span style="color:#000000;">（</span><span style="color:#000000;">default = True</span><span style="color:#000000;">）是否在构建树时使用放回抽样 </span></p> 
  </blockquote> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
