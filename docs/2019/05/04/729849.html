<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>AlphaGo Zero算法简介 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="AlphaGo Zero算法简介" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp; &nbsp; &nbsp; AlphaGo Zero 引起巨大社会轰动 　　只告诉机器围棋的基本规则，但是不告诉它人类摸索了上千年才总结出来的定式等围棋战术，让机器完全依靠自学，打败人类。这个题目不仅新鲜，而且热辣。 　　上周 DeepMind AlphaGo 人工智能围棋团队的一篇新论文，题目是“Mastering the Game of Go without Human Knowledge”。 　　这篇论文不仅被顶级学术期刊 Nature 发表，而且立刻被媒体反复报导，引起社会热议。 　　这篇论文让人惊艳的亮点有四， 　　只告诉机器围棋规则，但是不告诉它定式等等人类总结的围棋战术，也不让它读人类棋手比赛的棋谱，让机器完全自学成才。 　　机器完全靠自己摸索，自主总结出了定式等等围棋战术，而且还发现了人类上千年来没有发现的定式。 　　从零开始，机器自学了不到 40 天，就超越了前一版 AlphaGo（AlphaGo Master），而 AlphaGo Master 几个月前，曾以 60 : 0 的战绩，战胜了当今几乎所有人类围棋高手。 　　AlphaGo Zero 的算法，比 AlphaGo Master 简练很多。 　　不过，有些关于AlphaGo Zero 的评论，似乎渲染过度，把它的算法，说得神乎其神。本文尝试用大白话，通俗地解释一下 AlphaGo Zero 的算法。 　　AlphaGo Zero 的算法，说来并不复杂。理解清楚 Monte Carlo Tree Search、深度学习启发函数和置信上限，这三个概念就行了。 　　Monte Carlo Tree Search：不穷举所有组合，找到最优或次优位置 　　围棋棋面总共有 19 * 19 = 361 个落子位置。假如电脑有足够的计算能力，理论上来说，我们可以穷举黑白双方所有可能的落子位置，找到最优落子策略。 　　但是，如果穷举黑白双方所有可能的落子位置，各种组合的总数，大约是 250^150 数量级。这个数太大了，以至于用当今世界最强大云计算系统，算几十年也算不完。 　　有没有不穷举所有组合，就能找到最优或者次优落子策略的算法呢？有，Monte Carlo Tree Search 就是这样一种算法。 　　刚刚开始教机器下围棋的时候，机器除了规则，对围棋一无所知。让两台机器对弈，分别执黑子与白子。只要不违反规则，以均等概率，在所有合法的位置上，随意选择一个地点落子。 　　黑方先行，它有 361 个合法投子位置。黑方先随机考虑一个候选位置，譬如天元（9，9）。开局是否投子在天元呢？取决于假如投子在此，是否有可能赢得胜利。如何估算赢得胜利的可能性呢？黑方模拟对局。 　　假如黑方第一手投子天元，那么白方的第二手会投子哪里呢？根据均等概率的初步策略，白方有 360 个合法位置，在任何一处投子的概率均等。假如白方的第二手投子在棋盘的最边缘（0，0）。 　　接下去，黑方在剩余的 359 个合法位置中，随机选择一个落子位置。接下去白方投子。如此重复，直到终局。 　　完成这样一次对局模拟的过程，上限是 361 手，计算成本很低。 　　假如黑白两个机器，以黑方投子天元开局，一路乱走，最终以黑方胜利。那么根据 Monto Carlo Tree Search 算法，投子天元的开局，有可能获胜，那么第一手，就真的投子天元。 　　假如一路乱走，最终黑方失败，那么黑方就换一个候选位置，再次模拟对局。假如第二次模拟对局以黑方获胜，就投子在第二个位置。假如失败，那就再换到第三个候选位置，第三次模拟对局。如此重复。 　　这样反复乱走，收集到了第一批棋谱，当然，这些棋谱的水平，惨不忍睹。 　　水平之所以惨不忍睹，是因为 “以均等概率，在所有合法的位置上，随意选择一个地点落子” 的下棋策略。 　　如何通过自学，不断改进下棋策略？ 　　AlphaGo Zero 用深度学习神经网络来解决这个问题。 　　用深度学习网络实现启发函数 　　AlphaGo Zero 用 CNN 来改进围棋投子策略。具体到 CNN 的系统架构，AlphaGo Zero 用的是 Residual 架构 ResNet。而 Residual 架构是其时任职于微软亚洲研究院的中国人 Kaiming He、Xiangyu Zhang、Shaoqing Ren、Jian Sun，于 2015 年发明的。 　　ResNet 的输入是当前的棋面 S_{t} 。它的输出有两个， 　　当前棋面 S_{t} 的赢率，v( S_{t} )，赢率就是最终获胜的概率，是一个数值。 　　下一手投子的位置及其概率，P( a_{t+1} | S_{t} )，这是一个向量。投子的位置可能有多种，每个位置的概率不同，概率越高，说明在以往的棋谱中，经常投子在这个位置。 　　用先前收集到的棋谱，来训练 ResNet，拟合输入 S_{t}，以及输出 P( a_{t+1} | S_{t} ) 向量和当前棋面的赢率 v( S_{t} )。 　　AlphaGo Zero 只用机器自我对弈的棋谱，来训练 ResNet。 　　当然，也可以用人类棋手的棋谱来训练 ResNet。理论上来说，用人类棋手的棋谱来训练 ResNet，AlphaGo Zero 的水平，会在更短时间内，获得更快提升。 　　但是，即便不用人类棋手的棋谱，只用机器自我对弈的棋谱，来训练 ResNet，在短短 40 天内，AlphaGo Zero 就已经超越人类棋手的水平。这个速度，实在让人惊艳。 　　ResNet 训练好了以后，仍然用 Monte Carlo Tree Search，继续让机器自我对弈。只不过把投子的策略，从均等概率的随机投子，改为根据 ResNet 的指导，来决定下一手的投子位置。 论文配图：MCTS 使用神经网络模拟落子选择的过程 　　具体策略如下， 　　根据当前棋面 S_{t}，让 ResNet 估算下一手可能的投子位置，a_{t+1}，及其概率 P( a_{t+1} | S_{t} )。 　　下一手的投子位置，a_{t+1} 有多种，每一种位置的赢率 v(S_{t+1}) ，和投子概率 P( a_{t+1} | S_{t} ) 不同。赢率和投子概率越高，得分越高。 　　赢率 v(S_{t+1}) 和 投子概率 P( a_{t+1} | S_{t} ) ，是对以往棋谱的总结。而置信上限（Upper Confidence Bound，UCB ），是来鼓励探索新的投子位置，越是以往很少投子的位置，UCB( a_{t+1} ) 得分越高。 　　综合考虑下一手的棋面的赢率 v( S_{t+1} )，投子概率 P( a_{t+1} | S_{t} ) ，和置信上限 UCB( a_{t+1} )，给下一手的各个投子位置打分。取其中得分最高者，来指导 Monto Carlo Tree Search，决定下一个投子的位置。 　　用改进了投子策略的 Monte Carlo Tree Search，继续让机器自我对弈，这样得到更多棋谱。然后，用这些棋谱，再次训练 ResNet，提高赢率和投子概率的估算精度。如此循环重复，不断提高 ResNet 的精度。 　　定式（Joseki）与投子位置热力图 　　投子概率 P( a_{t+1} | S_{t} ) ，反应了下一手投子位置的热力图。各个位置被投子的概率非常不均等，其中某些位置被投子的概率，比其它位置显著地高。 　　这些位置，加上前面几手的落子位置和相应的棋面，就是围棋定式（Joseki）。 论文补充材料：训练中AlphaGo Zero偏好的投子位置热力图 　　AlphaGo Zero 在五天以内，就通过机器自我对弈，总结出了常见的定式。 　　而人类发现这些定式，花费了几百年。 　　更加令人惊艳的是，AlphaGo Zero 还发现了新的定式，而这些定式，人类迄今为止并没有发现。 　　点击查看大图：在 2 小时时间限制下，AlphaGo Zero (20 个残差模块，训练 3 天) 对战 AlphaGo Lee 的 20 局，每局展示了前 100 步棋。 　　总结一下，AlphaGo Zero 的算法非常简洁，Monte Carlo Tree Search + ResNet。 　　与传统的 A* 算法比较一下，Monte Carlo Tree Search 只是 A* 算法中的树拓展的一种特例，而 ResNet 是 A* 算法中启发函数的一种特例。" />
<meta property="og:description" content="&nbsp; &nbsp; &nbsp; AlphaGo Zero 引起巨大社会轰动 　　只告诉机器围棋的基本规则，但是不告诉它人类摸索了上千年才总结出来的定式等围棋战术，让机器完全依靠自学，打败人类。这个题目不仅新鲜，而且热辣。 　　上周 DeepMind AlphaGo 人工智能围棋团队的一篇新论文，题目是“Mastering the Game of Go without Human Knowledge”。 　　这篇论文不仅被顶级学术期刊 Nature 发表，而且立刻被媒体反复报导，引起社会热议。 　　这篇论文让人惊艳的亮点有四， 　　只告诉机器围棋规则，但是不告诉它定式等等人类总结的围棋战术，也不让它读人类棋手比赛的棋谱，让机器完全自学成才。 　　机器完全靠自己摸索，自主总结出了定式等等围棋战术，而且还发现了人类上千年来没有发现的定式。 　　从零开始，机器自学了不到 40 天，就超越了前一版 AlphaGo（AlphaGo Master），而 AlphaGo Master 几个月前，曾以 60 : 0 的战绩，战胜了当今几乎所有人类围棋高手。 　　AlphaGo Zero 的算法，比 AlphaGo Master 简练很多。 　　不过，有些关于AlphaGo Zero 的评论，似乎渲染过度，把它的算法，说得神乎其神。本文尝试用大白话，通俗地解释一下 AlphaGo Zero 的算法。 　　AlphaGo Zero 的算法，说来并不复杂。理解清楚 Monte Carlo Tree Search、深度学习启发函数和置信上限，这三个概念就行了。 　　Monte Carlo Tree Search：不穷举所有组合，找到最优或次优位置 　　围棋棋面总共有 19 * 19 = 361 个落子位置。假如电脑有足够的计算能力，理论上来说，我们可以穷举黑白双方所有可能的落子位置，找到最优落子策略。 　　但是，如果穷举黑白双方所有可能的落子位置，各种组合的总数，大约是 250^150 数量级。这个数太大了，以至于用当今世界最强大云计算系统，算几十年也算不完。 　　有没有不穷举所有组合，就能找到最优或者次优落子策略的算法呢？有，Monte Carlo Tree Search 就是这样一种算法。 　　刚刚开始教机器下围棋的时候，机器除了规则，对围棋一无所知。让两台机器对弈，分别执黑子与白子。只要不违反规则，以均等概率，在所有合法的位置上，随意选择一个地点落子。 　　黑方先行，它有 361 个合法投子位置。黑方先随机考虑一个候选位置，譬如天元（9，9）。开局是否投子在天元呢？取决于假如投子在此，是否有可能赢得胜利。如何估算赢得胜利的可能性呢？黑方模拟对局。 　　假如黑方第一手投子天元，那么白方的第二手会投子哪里呢？根据均等概率的初步策略，白方有 360 个合法位置，在任何一处投子的概率均等。假如白方的第二手投子在棋盘的最边缘（0，0）。 　　接下去，黑方在剩余的 359 个合法位置中，随机选择一个落子位置。接下去白方投子。如此重复，直到终局。 　　完成这样一次对局模拟的过程，上限是 361 手，计算成本很低。 　　假如黑白两个机器，以黑方投子天元开局，一路乱走，最终以黑方胜利。那么根据 Monto Carlo Tree Search 算法，投子天元的开局，有可能获胜，那么第一手，就真的投子天元。 　　假如一路乱走，最终黑方失败，那么黑方就换一个候选位置，再次模拟对局。假如第二次模拟对局以黑方获胜，就投子在第二个位置。假如失败，那就再换到第三个候选位置，第三次模拟对局。如此重复。 　　这样反复乱走，收集到了第一批棋谱，当然，这些棋谱的水平，惨不忍睹。 　　水平之所以惨不忍睹，是因为 “以均等概率，在所有合法的位置上，随意选择一个地点落子” 的下棋策略。 　　如何通过自学，不断改进下棋策略？ 　　AlphaGo Zero 用深度学习神经网络来解决这个问题。 　　用深度学习网络实现启发函数 　　AlphaGo Zero 用 CNN 来改进围棋投子策略。具体到 CNN 的系统架构，AlphaGo Zero 用的是 Residual 架构 ResNet。而 Residual 架构是其时任职于微软亚洲研究院的中国人 Kaiming He、Xiangyu Zhang、Shaoqing Ren、Jian Sun，于 2015 年发明的。 　　ResNet 的输入是当前的棋面 S_{t} 。它的输出有两个， 　　当前棋面 S_{t} 的赢率，v( S_{t} )，赢率就是最终获胜的概率，是一个数值。 　　下一手投子的位置及其概率，P( a_{t+1} | S_{t} )，这是一个向量。投子的位置可能有多种，每个位置的概率不同，概率越高，说明在以往的棋谱中，经常投子在这个位置。 　　用先前收集到的棋谱，来训练 ResNet，拟合输入 S_{t}，以及输出 P( a_{t+1} | S_{t} ) 向量和当前棋面的赢率 v( S_{t} )。 　　AlphaGo Zero 只用机器自我对弈的棋谱，来训练 ResNet。 　　当然，也可以用人类棋手的棋谱来训练 ResNet。理论上来说，用人类棋手的棋谱来训练 ResNet，AlphaGo Zero 的水平，会在更短时间内，获得更快提升。 　　但是，即便不用人类棋手的棋谱，只用机器自我对弈的棋谱，来训练 ResNet，在短短 40 天内，AlphaGo Zero 就已经超越人类棋手的水平。这个速度，实在让人惊艳。 　　ResNet 训练好了以后，仍然用 Monte Carlo Tree Search，继续让机器自我对弈。只不过把投子的策略，从均等概率的随机投子，改为根据 ResNet 的指导，来决定下一手的投子位置。 论文配图：MCTS 使用神经网络模拟落子选择的过程 　　具体策略如下， 　　根据当前棋面 S_{t}，让 ResNet 估算下一手可能的投子位置，a_{t+1}，及其概率 P( a_{t+1} | S_{t} )。 　　下一手的投子位置，a_{t+1} 有多种，每一种位置的赢率 v(S_{t+1}) ，和投子概率 P( a_{t+1} | S_{t} ) 不同。赢率和投子概率越高，得分越高。 　　赢率 v(S_{t+1}) 和 投子概率 P( a_{t+1} | S_{t} ) ，是对以往棋谱的总结。而置信上限（Upper Confidence Bound，UCB ），是来鼓励探索新的投子位置，越是以往很少投子的位置，UCB( a_{t+1} ) 得分越高。 　　综合考虑下一手的棋面的赢率 v( S_{t+1} )，投子概率 P( a_{t+1} | S_{t} ) ，和置信上限 UCB( a_{t+1} )，给下一手的各个投子位置打分。取其中得分最高者，来指导 Monto Carlo Tree Search，决定下一个投子的位置。 　　用改进了投子策略的 Monte Carlo Tree Search，继续让机器自我对弈，这样得到更多棋谱。然后，用这些棋谱，再次训练 ResNet，提高赢率和投子概率的估算精度。如此循环重复，不断提高 ResNet 的精度。 　　定式（Joseki）与投子位置热力图 　　投子概率 P( a_{t+1} | S_{t} ) ，反应了下一手投子位置的热力图。各个位置被投子的概率非常不均等，其中某些位置被投子的概率，比其它位置显著地高。 　　这些位置，加上前面几手的落子位置和相应的棋面，就是围棋定式（Joseki）。 论文补充材料：训练中AlphaGo Zero偏好的投子位置热力图 　　AlphaGo Zero 在五天以内，就通过机器自我对弈，总结出了常见的定式。 　　而人类发现这些定式，花费了几百年。 　　更加令人惊艳的是，AlphaGo Zero 还发现了新的定式，而这些定式，人类迄今为止并没有发现。 　　点击查看大图：在 2 小时时间限制下，AlphaGo Zero (20 个残差模块，训练 3 天) 对战 AlphaGo Lee 的 20 局，每局展示了前 100 步棋。 　　总结一下，AlphaGo Zero 的算法非常简洁，Monte Carlo Tree Search + ResNet。 　　与传统的 A* 算法比较一下，Monte Carlo Tree Search 只是 A* 算法中的树拓展的一种特例，而 ResNet 是 A* 算法中启发函数的一种特例。" />
<link rel="canonical" href="https://mlh.app/2019/05/04/729849.html" />
<meta property="og:url" content="https://mlh.app/2019/05/04/729849.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-04T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp; &nbsp; &nbsp; AlphaGo Zero 引起巨大社会轰动 　　只告诉机器围棋的基本规则，但是不告诉它人类摸索了上千年才总结出来的定式等围棋战术，让机器完全依靠自学，打败人类。这个题目不仅新鲜，而且热辣。 　　上周 DeepMind AlphaGo 人工智能围棋团队的一篇新论文，题目是“Mastering the Game of Go without Human Knowledge”。 　　这篇论文不仅被顶级学术期刊 Nature 发表，而且立刻被媒体反复报导，引起社会热议。 　　这篇论文让人惊艳的亮点有四， 　　只告诉机器围棋规则，但是不告诉它定式等等人类总结的围棋战术，也不让它读人类棋手比赛的棋谱，让机器完全自学成才。 　　机器完全靠自己摸索，自主总结出了定式等等围棋战术，而且还发现了人类上千年来没有发现的定式。 　　从零开始，机器自学了不到 40 天，就超越了前一版 AlphaGo（AlphaGo Master），而 AlphaGo Master 几个月前，曾以 60 : 0 的战绩，战胜了当今几乎所有人类围棋高手。 　　AlphaGo Zero 的算法，比 AlphaGo Master 简练很多。 　　不过，有些关于AlphaGo Zero 的评论，似乎渲染过度，把它的算法，说得神乎其神。本文尝试用大白话，通俗地解释一下 AlphaGo Zero 的算法。 　　AlphaGo Zero 的算法，说来并不复杂。理解清楚 Monte Carlo Tree Search、深度学习启发函数和置信上限，这三个概念就行了。 　　Monte Carlo Tree Search：不穷举所有组合，找到最优或次优位置 　　围棋棋面总共有 19 * 19 = 361 个落子位置。假如电脑有足够的计算能力，理论上来说，我们可以穷举黑白双方所有可能的落子位置，找到最优落子策略。 　　但是，如果穷举黑白双方所有可能的落子位置，各种组合的总数，大约是 250^150 数量级。这个数太大了，以至于用当今世界最强大云计算系统，算几十年也算不完。 　　有没有不穷举所有组合，就能找到最优或者次优落子策略的算法呢？有，Monte Carlo Tree Search 就是这样一种算法。 　　刚刚开始教机器下围棋的时候，机器除了规则，对围棋一无所知。让两台机器对弈，分别执黑子与白子。只要不违反规则，以均等概率，在所有合法的位置上，随意选择一个地点落子。 　　黑方先行，它有 361 个合法投子位置。黑方先随机考虑一个候选位置，譬如天元（9，9）。开局是否投子在天元呢？取决于假如投子在此，是否有可能赢得胜利。如何估算赢得胜利的可能性呢？黑方模拟对局。 　　假如黑方第一手投子天元，那么白方的第二手会投子哪里呢？根据均等概率的初步策略，白方有 360 个合法位置，在任何一处投子的概率均等。假如白方的第二手投子在棋盘的最边缘（0，0）。 　　接下去，黑方在剩余的 359 个合法位置中，随机选择一个落子位置。接下去白方投子。如此重复，直到终局。 　　完成这样一次对局模拟的过程，上限是 361 手，计算成本很低。 　　假如黑白两个机器，以黑方投子天元开局，一路乱走，最终以黑方胜利。那么根据 Monto Carlo Tree Search 算法，投子天元的开局，有可能获胜，那么第一手，就真的投子天元。 　　假如一路乱走，最终黑方失败，那么黑方就换一个候选位置，再次模拟对局。假如第二次模拟对局以黑方获胜，就投子在第二个位置。假如失败，那就再换到第三个候选位置，第三次模拟对局。如此重复。 　　这样反复乱走，收集到了第一批棋谱，当然，这些棋谱的水平，惨不忍睹。 　　水平之所以惨不忍睹，是因为 “以均等概率，在所有合法的位置上，随意选择一个地点落子” 的下棋策略。 　　如何通过自学，不断改进下棋策略？ 　　AlphaGo Zero 用深度学习神经网络来解决这个问题。 　　用深度学习网络实现启发函数 　　AlphaGo Zero 用 CNN 来改进围棋投子策略。具体到 CNN 的系统架构，AlphaGo Zero 用的是 Residual 架构 ResNet。而 Residual 架构是其时任职于微软亚洲研究院的中国人 Kaiming He、Xiangyu Zhang、Shaoqing Ren、Jian Sun，于 2015 年发明的。 　　ResNet 的输入是当前的棋面 S_{t} 。它的输出有两个， 　　当前棋面 S_{t} 的赢率，v( S_{t} )，赢率就是最终获胜的概率，是一个数值。 　　下一手投子的位置及其概率，P( a_{t+1} | S_{t} )，这是一个向量。投子的位置可能有多种，每个位置的概率不同，概率越高，说明在以往的棋谱中，经常投子在这个位置。 　　用先前收集到的棋谱，来训练 ResNet，拟合输入 S_{t}，以及输出 P( a_{t+1} | S_{t} ) 向量和当前棋面的赢率 v( S_{t} )。 　　AlphaGo Zero 只用机器自我对弈的棋谱，来训练 ResNet。 　　当然，也可以用人类棋手的棋谱来训练 ResNet。理论上来说，用人类棋手的棋谱来训练 ResNet，AlphaGo Zero 的水平，会在更短时间内，获得更快提升。 　　但是，即便不用人类棋手的棋谱，只用机器自我对弈的棋谱，来训练 ResNet，在短短 40 天内，AlphaGo Zero 就已经超越人类棋手的水平。这个速度，实在让人惊艳。 　　ResNet 训练好了以后，仍然用 Monte Carlo Tree Search，继续让机器自我对弈。只不过把投子的策略，从均等概率的随机投子，改为根据 ResNet 的指导，来决定下一手的投子位置。 论文配图：MCTS 使用神经网络模拟落子选择的过程 　　具体策略如下， 　　根据当前棋面 S_{t}，让 ResNet 估算下一手可能的投子位置，a_{t+1}，及其概率 P( a_{t+1} | S_{t} )。 　　下一手的投子位置，a_{t+1} 有多种，每一种位置的赢率 v(S_{t+1}) ，和投子概率 P( a_{t+1} | S_{t} ) 不同。赢率和投子概率越高，得分越高。 　　赢率 v(S_{t+1}) 和 投子概率 P( a_{t+1} | S_{t} ) ，是对以往棋谱的总结。而置信上限（Upper Confidence Bound，UCB ），是来鼓励探索新的投子位置，越是以往很少投子的位置，UCB( a_{t+1} ) 得分越高。 　　综合考虑下一手的棋面的赢率 v( S_{t+1} )，投子概率 P( a_{t+1} | S_{t} ) ，和置信上限 UCB( a_{t+1} )，给下一手的各个投子位置打分。取其中得分最高者，来指导 Monto Carlo Tree Search，决定下一个投子的位置。 　　用改进了投子策略的 Monte Carlo Tree Search，继续让机器自我对弈，这样得到更多棋谱。然后，用这些棋谱，再次训练 ResNet，提高赢率和投子概率的估算精度。如此循环重复，不断提高 ResNet 的精度。 　　定式（Joseki）与投子位置热力图 　　投子概率 P( a_{t+1} | S_{t} ) ，反应了下一手投子位置的热力图。各个位置被投子的概率非常不均等，其中某些位置被投子的概率，比其它位置显著地高。 　　这些位置，加上前面几手的落子位置和相应的棋面，就是围棋定式（Joseki）。 论文补充材料：训练中AlphaGo Zero偏好的投子位置热力图 　　AlphaGo Zero 在五天以内，就通过机器自我对弈，总结出了常见的定式。 　　而人类发现这些定式，花费了几百年。 　　更加令人惊艳的是，AlphaGo Zero 还发现了新的定式，而这些定式，人类迄今为止并没有发现。 　　点击查看大图：在 2 小时时间限制下，AlphaGo Zero (20 个残差模块，训练 3 天) 对战 AlphaGo Lee 的 20 局，每局展示了前 100 步棋。 　　总结一下，AlphaGo Zero 的算法非常简洁，Monte Carlo Tree Search + ResNet。 　　与传统的 A* 算法比较一下，Monte Carlo Tree Search 只是 A* 算法中的树拓展的一种特例，而 ResNet 是 A* 算法中启发函数的一种特例。","@type":"BlogPosting","url":"https://mlh.app/2019/05/04/729849.html","headline":"AlphaGo Zero算法简介","dateModified":"2019-05-04T00:00:00+08:00","datePublished":"2019-05-04T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/04/729849.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>AlphaGo Zero算法简介</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div id="article_content" class="article_content clearfix csdn-tracking-statistics"> 
   <div class="htmledit_views" id="content_views"> 
   </div>
  </div>
  <p> <span><span>&nbsp; &nbsp; &nbsp; AlphaGo Zero 引起巨大社会轰动</span></span></p> 
  <p> </p> 
  <p> 　　只告诉机器围棋的基本规则，但是不告诉它人类摸索了上千年才总结出来的定式等围棋战术，让机器完全依靠自学，打败人类。这个题目不仅新鲜，而且热辣。</p> 
  <p> 　　上周 DeepMind AlphaGo 人工智能围棋团队的一篇新论文，题目是“Mastering the Game of Go without Human Knowledge”。</p> 
  <p> 　　这篇论文不仅被顶级学术期刊 Nature 发表，而且立刻被媒体反复报导，引起社会热议。</p> 
  <p> 　　这篇论文让人惊艳的亮点有四，</p> 
  <p> 　　只告诉机器围棋规则，但是不告诉它定式等等人类总结的围棋战术，也不让它读人类棋手比赛的棋谱，让机器完全自学成才。</p> 
  <p> 　　机器完全靠自己摸索，自主总结出了定式等等围棋战术，而且还发现了人类上千年来没有发现的定式。</p> 
  <p> 　　从零开始，机器自学了不到 40 天，就超越了前一版 AlphaGo（AlphaGo Master），而 AlphaGo Master 几个月前，曾以 60 : 0 的战绩，战胜了当今几乎所有人类围棋高手。</p> 
  <p> 　　AlphaGo Zero 的算法，比 AlphaGo Master 简练很多。</p> 
  <p> 　　不过，有些关于AlphaGo Zero 的评论，似乎渲染过度，把它的算法，说得神乎其神。本文尝试用大白话，通俗地解释一下 AlphaGo Zero 的算法。</p> 
  <p> 　　AlphaGo Zero 的算法，说来并不复杂。理解清楚 Monte Carlo Tree Search、深度学习启发函数和置信上限，这三个概念就行了。</p> 
  <p> 　　Monte Carlo Tree Search：不穷举所有组合，找到最优或次优位置</p> 
  <p> 　　围棋棋面总共有 19 * 19 = 361 个落子位置。假如电脑有足够的计算能力，理论上来说，我们可以穷举黑白双方所有可能的落子位置，找到最优落子策略。</p> 
  <p> 　　但是，如果穷举黑白双方所有可能的落子位置，各种组合的总数，大约是 250^150 数量级。这个数太大了，以至于用当今世界最强大云计算系统，算几十年也算不完。</p> 
  <p> 　　有没有不穷举所有组合，就能找到最优或者次优落子策略的算法呢？有，Monte Carlo Tree Search 就是这样一种算法。</p> 
  <p> 　　刚刚开始教机器下围棋的时候，机器除了规则，对围棋一无所知。让两台机器对弈，分别执黑子与白子。只要不违反规则，以均等概率，在所有合法的位置上，随意选择一个地点落子。</p> 
  <p> </p> 
  <div class="table-box">
   <table align="center">
    <tbody>
     <tr>
      <td> 
       <div align="center">
        <img alt="黑方先行，它有 361 个合法投子位置。黑方先随机考虑一个候选位置，譬如天元（9，9）。开局是否投子在天元呢？取决于假如投子在此，是否有可能赢得胜利。如何估算赢得胜利的可能性呢？黑方模拟对局。" src="http://i6.hexun.com/2017-10-25/191358767.jpg">
       </div> </td> 
     </tr>
    </tbody>
   </table>
  </div>
  <span>　　黑方先行，它有 361 个合法投子位置。黑方先随机考虑一个候选位置，譬如天元（9，9）。开局是否投子在天元呢？取决于假如投子在此，是否有可能赢得胜利。如何估算赢得胜利的可能性呢？黑方模拟对局。</span> 
  <p> </p> 
  <p> 　　假如黑方第一手投子天元，那么白方的第二手会投子哪里呢？根据均等概率的初步策略，白方有 360 个合法位置，在任何一处投子的概率均等。假如白方的第二手投子在棋盘的最边缘（0，0）。</p> 
  <p> 　　接下去，黑方在剩余的 359 个合法位置中，随机选择一个落子位置。接下去白方投子。如此重复，直到终局。</p> 
  <p> 　　完成这样一次对局模拟的过程，上限是 361 手，计算成本很低。</p> 
  <p> 　　假如黑白两个机器，以黑方投子天元开局，一路乱走，最终以黑方胜利。那么根据 Monto Carlo Tree Search 算法，投子天元的开局，有可能获胜，那么第一手，就真的投子天元。</p> 
  <p> 　　假如一路乱走，最终黑方失败，那么黑方就换一个候选位置，再次模拟对局。假如第二次模拟对局以黑方获胜，就投子在第二个位置。假如失败，那就再换到第三个候选位置，第三次模拟对局。如此重复。</p> 
  <p> 　　这样反复乱走，收集到了第一批棋谱，当然，这些棋谱的水平，惨不忍睹。</p> 
  <p> 　　水平之所以惨不忍睹，是因为 “以均等概率，在所有合法的位置上，随意选择一个地点落子” 的下棋策略。</p> 
  <p> 　　如何通过自学，不断改进下棋策略？</p> 
  <p> 　　AlphaGo Zero 用深度学习神经网络来解决这个问题。</p> 
  <p> 　　用深度学习网络实现启发函数</p> 
  <p> 　　AlphaGo Zero 用 CNN 来改进围棋投子策略。具体到 CNN 的系统架构，AlphaGo Zero 用的是 Residual 架构 ResNet。而 Residual 架构是其时任职于微软亚洲研究院的中国人 Kaiming He、Xiangyu Zhang、Shaoqing Ren、Jian Sun，于 2015 年发明的。</p> 
  <p> 　　ResNet 的输入是当前的棋面 S_{t} 。它的输出有两个，</p> 
  <p> 　　当前棋面 S_{t} 的赢率，v( S_{t} )，赢率就是最终获胜的概率，是一个数值。</p> 
  <p> 　　下一手投子的位置及其概率，P( a_{t+1} | S_{t} )，这是一个向量。投子的位置可能有多种，每个位置的概率不同，概率越高，说明在以往的棋谱中，经常投子在这个位置。</p> 
  <p> 　　用先前收集到的棋谱，来训练 ResNet，拟合输入 S_{t}，以及输出 P( a_{t+1} | S_{t} ) 向量和当前棋面的赢率 v( S_{t} )。</p> 
  <p> 　　AlphaGo Zero 只用机器自我对弈的棋谱，来训练 ResNet。</p> 
  <p> 　　当然，也可以用人类棋手的棋谱来训练 ResNet。理论上来说，用人类棋手的棋谱来训练 ResNet，AlphaGo Zero 的水平，会在更短时间内，获得更快提升。</p> 
  <p> 　　但是，即便不用人类棋手的棋谱，只用机器自我对弈的棋谱，来训练 ResNet，在短短 40 天内，AlphaGo Zero 就已经超越人类棋手的水平。这个速度，实在让人惊艳。</p> 
  <p> 　　ResNet 训练好了以后，仍然用 Monte Carlo Tree Search，继续让机器自我对弈。只不过把投子的策略，从均等概率的随机投子，改为根据 ResNet 的指导，来决定下一手的投子位置。</p> 
  <p> </p> 
  <div class="table-box">
   <table align="center">
    <tbody>
     <tr>
      <td> 
       <div align="center">
        <img alt="论文配图：MCTS 使用神经网络模拟落子选择的过程" src="http://i4.hexun.com/2017-10-25/191358769.jpg" width="556">
       </div> </td> 
     </tr>
    </tbody>
   </table>
  </div>
  <div align="center"> 
   <span>论文配图：MCTS 使用神经网络模拟落子选择的过程</span>
  </div> 
  <p> </p> 
  <p> 　　具体策略如下，</p> 
  <p> 　　根据当前棋面 S_{t}，让 ResNet 估算下一手可能的投子位置，a_{t+1}，及其概率 P( a_{t+1} | S_{t} )。</p> 
  <p> 　　下一手的投子位置，a_{t+1} 有多种，每一种位置的赢率 v(S_{t+1}) ，和投子概率 P( a_{t+1} | S_{t} ) 不同。赢率和投子概率越高，得分越高。</p> 
  <p> 　　赢率 v(S_{t+1}) 和 投子概率 P( a_{t+1} | S_{t} ) ，是对以往棋谱的总结。而置信上限（Upper Confidence Bound，UCB ），是来鼓励探索新的投子位置，越是以往很少投子的位置，UCB( a_{t+1} ) 得分越高。</p> 
  <p> 　　综合考虑下一手的棋面的赢率 v( S_{t+1} )，投子概率 P( a_{t+1} | S_{t} ) ，和置信上限 UCB( a_{t+1} )，给下一手的各个投子位置打分。取其中得分最高者，来指导 Monto Carlo Tree Search，决定下一个投子的位置。</p> 
  <p> 　　用改进了投子策略的 Monte Carlo Tree Search，继续让机器自我对弈，这样得到更多棋谱。然后，用这些棋谱，再次训练 ResNet，提高赢率和投子概率的估算精度。如此循环重复，不断提高 ResNet 的精度。</p> 
  <p> 　　定式（Joseki）与投子位置热力图</p> 
  <p> 　　投子概率 P( a_{t+1} | S_{t} ) ，反应了下一手投子位置的热力图。各个位置被投子的概率非常不均等，其中某些位置被投子的概率，比其它位置显著地高。</p> 
  <p> 　　这些位置，加上前面几手的落子位置和相应的棋面，就是围棋定式（Joseki）。</p> 
  <p> </p> 
  <div class="table-box">
   <table align="center">
    <tbody>
     <tr>
      <td> 
       <div align="center">
        <img alt="论文补充材料：训练中AlphaGo Zero偏好的投子位置热力图" src="http://i9.hexun.com/2017-10-25/191358770.jpg">
       </div> </td> 
     </tr>
    </tbody>
   </table>
  </div>
  <div align="center"> 
   <span>论文补充材料：训练中AlphaGo Zero偏好的投子位置热力图</span>
  </div> 
  <p> </p> 
  <p> 　　AlphaGo Zero 在五天以内，就通过机器自我对弈，总结出了常见的定式。</p> 
  <p> 　　而人类发现这些定式，花费了几百年。</p> 
  <p> 　　更加令人惊艳的是，AlphaGo Zero 还发现了新的定式，而这些定式，人类迄今为止并没有发现。</p> 
  <p> </p> 
  <div class="table-box">
   <table align="center">
    <tbody>
     <tr>
      <td> 
       <div align="center">
        <img alt="点击查看大图：在 2 小时时间限制下，AlphaGo Zero (20 个残差模块，训练 3 天) 对战 AlphaGo Lee 的 20 局，每局展示了前 100 步棋。" src="http://i7.hexun.com/2017-10-25/191358772.jpg">
       </div> </td> 
     </tr>
    </tbody>
   </table>
  </div>
  <span>　　点击查看大图：在 2 小时时间限制下，AlphaGo Zero (20 个残差模块，训练 3 天) 对战 AlphaGo Lee 的 20 局，每局展示了前 100 步棋。</span> 
  <p> </p> 
  <p> 　　总结一下，AlphaGo Zero 的算法非常简洁，Monte Carlo Tree Search + ResNet。</p> 
  <p> 　　与传统的 A* 算法比较一下，Monte Carlo Tree Search 只是 A* 算法中的树拓展的一种特例，而 ResNet 是 A* 算法中启发函数的一种特例。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?aee162ba0c05d8e682055e28ff964fa2";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
