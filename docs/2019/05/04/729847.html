<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>使用Pytorch构建MLP模型实现MNIST手写数字识别 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="使用Pytorch构建MLP模型实现MNIST手写数字识别" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="基本流程 1、加载数据集 2、预处理数据（标准化并转换为张量） 3、查阅资料，看看是否已经有人做了这个问题，使用的是什么模型架构，并定义模型 4、确定损失函数和优化函数，并开始训练模型 5、使用模型从未见过的数据测试模型 本文在谷歌的Colab上实现 from torchvision import datasets import torchvision.transforms as transforms import torch #非并行加载就填0 num_workers = 0 #决定每次读取多少图片 batch_size = 20 #转换成张量 transform = transforms.ToTensor() #下载数据 train_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) test_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) #创建加载器 train_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, num_workers = num_workers) test_loader = torch.utils.data.DataLoader(test_data,batch_size = batch_size, num_workers = num_workers) 接下来的可视化部分可以省略 #可视化 import numpy as np import matplotlib.pyplot as plt %matplotlib inline dataiter = iter(train_loader) images,labels = next(dataiter) images = images.numpy() fig = plt.figure(figsize = (25,4)) for idx in np.arange(20):#前面是读20张，所以这里就是20 ax = fig.add_subplot(2,20/2,idx + 1,xticks = [],yticks = []) ax.imshow(np.squeeze(images[idx]),cmap = &#39;gray&#39;) ax.set_title(str(labels[idx].item())) 下面是输出，证明数据已经加载进来了 接下来定义我们的模型 # 定义MLP模型 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net,self).__init__() #两个全连接的隐藏层，一个输出层 #因为图片是28*28的，需要全部展开，最终我们要输出数字，一共10个数字。 #10个数字实际上是10个类别，输出是概率分布，最后选取概率最大的作为预测值输出 hidden_1 = 512 hidden_2 = 512 self.fc1 = nn.Linear(28 * 28,hidden_1) self.fc2 = nn.Linear(hidden_1,hidden_2) self.fc3 = nn.Linear(hidden_2,10) #使用dropout防止过拟合 self.dropout = nn.Dropout(0.2) def forward(self,x): x = x.view(-1,28 * 28) x = F.relu(self.fc1(x)) x = self.dropout(x) x = F.relu(self.fc2(x)) x = self.dropout(x) x = self.fc3(x) # x = F.log_softmax(x,dim = 1) return x model = Net() #打印出来看是否正确 print(model) #定义损失函数和优化器 # criterion = nn.NLLLoss() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params = model.parameters(),lr = 0.01) #训练 n_epochs = 50 for epoch in range(n_epochs): train_loss = 0.0 for data,target in train_loader: optimizer.zero_grad() output = model(data)#得到预测值 loss = criterion(output,target) loss.backward() optimizer.step() train_loss += loss.item()*data.size(0) train_loss = train_loss / len(train_loader.dataset) print(&#39;Epoch: {} \tTraining Loss: {:.6f}&#39;.format( epoch + 1, train_loss)) 这里是测试了，我们用之前的测试数据来测试训练好的模型，然后统计正确的数目，最后计算每个数字的正确率与总正确率 # initialize lists to monitor test loss and accuracy test_loss = 0.0 class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) model.eval() # prep model for *evaluation* for data, target in test_loader: # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the loss loss = criterion(output, target) # update test loss test_loss += loss.item()*data.size(0) # convert output probabilities to predicted class _, pred = torch.max(output, 1) # compare predictions to true label correct = np.squeeze(pred.eq(target.data.view_as(pred))) # calculate test accuracy for each object class for i in range(batch_size): label = target.data[i] class_correct[label] += correct[i].item() class_total[label] += 1 # calculate and print avg test loss test_loss = test_loss/len(test_loader.dataset) print(&#39;Test Loss: {:.6f}\n&#39;.format(test_loss)) for i in range(10): if class_total[i] &gt; 0: print(&#39;Test Accuracy of %5s: %2d%% (%2d/%2d)&#39; % ( str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i]))) else: print(&#39;Test Accuracy of %5s: N/A (no training examples)&#39; % (classes[i])) print(&#39;\nTest Accuracy (Overall): %2d%% (%2d/%2d)&#39; % ( 100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total))) 以下是我测试的结果 Out： Test Loss: 0.003499 Test Accuracy of 0: 100% (5923/5923) Test Accuracy of 1: 99% (6740/6742) Test Accuracy of 2: 99% (5955/5958) Test Accuracy of 3: 99% (6125/6131) Test Accuracy of 4: 99% (5841/5842) Test Accuracy of 5: 100% (5421/5421) Test Accuracy of 6: 100% (5918/5918) Test Accuracy of 7: 99% (6264/6265) Test Accuracy of 8: 99% (5850/5851) Test Accuracy of 9: 99% (5947/5949) Test Accuracy (Overall): 99% (59984/60000) 防止过拟合优化 为了防止过拟合，我们还可以采取另一种办法：利用训练集训练模型，利用检验集检验当前模型的效果（当本来是好的，变得不再那么好，可能就是出现了过拟合现象了）利用测试集做测试。 而之所以一定要加入检验集做检验，不使用测试集做检验，是因为在检验集做检验的时候，结果会带有一定的倾向性，即对检验集有利的模型和参数会被保留。如果用测试集做检验，最后的结果肯定会对结果有利，不利于模型的泛化。 下面是加入检验集的代码： from torchvision import datasets import torchvision.transforms as transforms import torch #拆分数据集 from torch.utils.data.sampler import SubsetRandomSampler num_workers = 0 batch_size = 20 #添加验证集，让模型自动判断是否过拟合 valid_size = 0.2 transform = transforms.ToTensor() train_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) test_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) num_train = len(train_data) indices = list(range(num_train)) np.random.shuffle(indices) split = int(np.floor(valid_size * num_train)) train_idx,valid_idx = indices[split:],indices[:split] train_sampler = SubsetRandomSampler(train_idx) valid_sampler = SubsetRandomSampler(valid_idx) train_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, sampler = train_sampler,num_workers = num_workers) valid_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, sampler = valid_sampler) test_loader = torch.utils.data.DataLoader(test_data,batch_size = batch_size, num_workers = num_workers) # 定义MLP模型 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net,self).__init__() hidden_1 = 512 hidden_2 = 512 self.fc1 = nn.Linear(28 * 28,hidden_1) self.fc2 = nn.Linear(hidden_1,hidden_2) self.fc3 = nn.Linear(hidden_2,10) self.dropout = nn.Dropout(0.2) def forward(self,x): x = x.view(-1,28 * 28) x = F.relu(self.fc1(x)) x = self.dropout(x) x = F.relu(self.fc2(x)) x = self.dropout(x) x = self.fc3(x) # x = F.log_softmax(x,dim = 1) return x model = Net() print(model) #定义损失函数和优化器 # criterion = nn.NLLLoss() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params = model.parameters(),lr = 0.01) 这里如果训练集的损失还在减少，但是检验集的损失开始上升了，说明可能是过拟合了。 当两者都还在减少的时候，则保存模型，用于后期的使用 n_epochs = 50 valid_loss_min = np.Inf for epoch in range(n_epochs): train_loss = 0.0 valid_loss = 0.0 for data,target in train_loader: optimizer.zero_grad() output = model(data)#得到预测值 loss = criterion(output,target) loss.backward() optimizer.step() train_loss += loss.item()*data.size(0) #计算检验集的损失，这里不需要反向传播 for data,target in valid_loader: output = model(data) loss = criterion(output,target) valid_loss += loss.item() * data.size(0) train_loss = train_loss / len(train_loader.dataset) valid_loss = valid_loss / len(valid_loader.dataset) print(&#39;Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}&#39;.format( epoch + 1, train_loss, valid_loss)) if valid_loss &lt;= valid_loss_min:#保存模型 print(&#39;Validation loss decreased ({:.6f} --&gt; {:.6f}). Saving model...&#39;.format( valid_loss_min, valid_loss)) torch.save(model.state_dict(),&#39;model.pt&#39;) valid_loss_min = valid_loss 这一步直接加载之前保存的模型 model.load_state_dict(torch.load(&#39;model.pt&#39;)) # initialize lists to monitor test loss and accuracy test_loss = 0.0 class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) model.eval() # prep model for *evaluation* for data, target in test_loader: # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the loss loss = criterion(output, target) # update test loss test_loss += loss.item()*data.size(0) # convert output probabilities to predicted class _, pred = torch.max(output, 1) # compare predictions to true label correct = np.squeeze(pred.eq(target.data.view_as(pred))) # calculate test accuracy for each object class for i in range(batch_size): label = target.data[i] class_correct[label] += correct[i].item() class_total[label] += 1 # calculate and print avg test loss test_loss = test_loss/len(test_loader.dataset) print(&#39;Test Loss: {:.6f}\n&#39;.format(test_loss)) for i in range(10): if class_total[i] &gt; 0: print(&#39;Test Accuracy of %5s: %2d%% (%2d/%2d)&#39; % ( str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i]))) else: print(&#39;Test Accuracy of %5s: N/A (no training examples)&#39; % (classes[i])) print(&#39;\nTest Accuracy (Overall): %2d%% (%2d/%2d)&#39; % ( 100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total))) Out: Test Loss: 0.019485 Test Accuracy of 0: 99% (5914/5923) Test Accuracy of 1: 99% (6716/6742) Test Accuracy of 2: 99% (5929/5958) Test Accuracy of 3: 99% (6086/6131) Test Accuracy of 4: 99% (5822/5842) Test Accuracy of 5: 99% (5393/5421) Test Accuracy of 6: 99% (5902/5918) Test Accuracy of 7: 99% (6231/6265) Test Accuracy of 8: 99% (5822/5851) Test Accuracy of 9: 99% (5914/5949) Test Accuracy (Overall): 99% (59729/60000) 这里发现，其实结果不如之前的那个模型，但是没关系，前面其实也使用了dropout来防止过拟合，而且这个案例特征实际上不是很多，模型并不复杂，但特征变得多起来的时候，检验集的威力就能得到显现了。" />
<meta property="og:description" content="基本流程 1、加载数据集 2、预处理数据（标准化并转换为张量） 3、查阅资料，看看是否已经有人做了这个问题，使用的是什么模型架构，并定义模型 4、确定损失函数和优化函数，并开始训练模型 5、使用模型从未见过的数据测试模型 本文在谷歌的Colab上实现 from torchvision import datasets import torchvision.transforms as transforms import torch #非并行加载就填0 num_workers = 0 #决定每次读取多少图片 batch_size = 20 #转换成张量 transform = transforms.ToTensor() #下载数据 train_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) test_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) #创建加载器 train_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, num_workers = num_workers) test_loader = torch.utils.data.DataLoader(test_data,batch_size = batch_size, num_workers = num_workers) 接下来的可视化部分可以省略 #可视化 import numpy as np import matplotlib.pyplot as plt %matplotlib inline dataiter = iter(train_loader) images,labels = next(dataiter) images = images.numpy() fig = plt.figure(figsize = (25,4)) for idx in np.arange(20):#前面是读20张，所以这里就是20 ax = fig.add_subplot(2,20/2,idx + 1,xticks = [],yticks = []) ax.imshow(np.squeeze(images[idx]),cmap = &#39;gray&#39;) ax.set_title(str(labels[idx].item())) 下面是输出，证明数据已经加载进来了 接下来定义我们的模型 # 定义MLP模型 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net,self).__init__() #两个全连接的隐藏层，一个输出层 #因为图片是28*28的，需要全部展开，最终我们要输出数字，一共10个数字。 #10个数字实际上是10个类别，输出是概率分布，最后选取概率最大的作为预测值输出 hidden_1 = 512 hidden_2 = 512 self.fc1 = nn.Linear(28 * 28,hidden_1) self.fc2 = nn.Linear(hidden_1,hidden_2) self.fc3 = nn.Linear(hidden_2,10) #使用dropout防止过拟合 self.dropout = nn.Dropout(0.2) def forward(self,x): x = x.view(-1,28 * 28) x = F.relu(self.fc1(x)) x = self.dropout(x) x = F.relu(self.fc2(x)) x = self.dropout(x) x = self.fc3(x) # x = F.log_softmax(x,dim = 1) return x model = Net() #打印出来看是否正确 print(model) #定义损失函数和优化器 # criterion = nn.NLLLoss() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params = model.parameters(),lr = 0.01) #训练 n_epochs = 50 for epoch in range(n_epochs): train_loss = 0.0 for data,target in train_loader: optimizer.zero_grad() output = model(data)#得到预测值 loss = criterion(output,target) loss.backward() optimizer.step() train_loss += loss.item()*data.size(0) train_loss = train_loss / len(train_loader.dataset) print(&#39;Epoch: {} \tTraining Loss: {:.6f}&#39;.format( epoch + 1, train_loss)) 这里是测试了，我们用之前的测试数据来测试训练好的模型，然后统计正确的数目，最后计算每个数字的正确率与总正确率 # initialize lists to monitor test loss and accuracy test_loss = 0.0 class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) model.eval() # prep model for *evaluation* for data, target in test_loader: # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the loss loss = criterion(output, target) # update test loss test_loss += loss.item()*data.size(0) # convert output probabilities to predicted class _, pred = torch.max(output, 1) # compare predictions to true label correct = np.squeeze(pred.eq(target.data.view_as(pred))) # calculate test accuracy for each object class for i in range(batch_size): label = target.data[i] class_correct[label] += correct[i].item() class_total[label] += 1 # calculate and print avg test loss test_loss = test_loss/len(test_loader.dataset) print(&#39;Test Loss: {:.6f}\n&#39;.format(test_loss)) for i in range(10): if class_total[i] &gt; 0: print(&#39;Test Accuracy of %5s: %2d%% (%2d/%2d)&#39; % ( str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i]))) else: print(&#39;Test Accuracy of %5s: N/A (no training examples)&#39; % (classes[i])) print(&#39;\nTest Accuracy (Overall): %2d%% (%2d/%2d)&#39; % ( 100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total))) 以下是我测试的结果 Out： Test Loss: 0.003499 Test Accuracy of 0: 100% (5923/5923) Test Accuracy of 1: 99% (6740/6742) Test Accuracy of 2: 99% (5955/5958) Test Accuracy of 3: 99% (6125/6131) Test Accuracy of 4: 99% (5841/5842) Test Accuracy of 5: 100% (5421/5421) Test Accuracy of 6: 100% (5918/5918) Test Accuracy of 7: 99% (6264/6265) Test Accuracy of 8: 99% (5850/5851) Test Accuracy of 9: 99% (5947/5949) Test Accuracy (Overall): 99% (59984/60000) 防止过拟合优化 为了防止过拟合，我们还可以采取另一种办法：利用训练集训练模型，利用检验集检验当前模型的效果（当本来是好的，变得不再那么好，可能就是出现了过拟合现象了）利用测试集做测试。 而之所以一定要加入检验集做检验，不使用测试集做检验，是因为在检验集做检验的时候，结果会带有一定的倾向性，即对检验集有利的模型和参数会被保留。如果用测试集做检验，最后的结果肯定会对结果有利，不利于模型的泛化。 下面是加入检验集的代码： from torchvision import datasets import torchvision.transforms as transforms import torch #拆分数据集 from torch.utils.data.sampler import SubsetRandomSampler num_workers = 0 batch_size = 20 #添加验证集，让模型自动判断是否过拟合 valid_size = 0.2 transform = transforms.ToTensor() train_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) test_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) num_train = len(train_data) indices = list(range(num_train)) np.random.shuffle(indices) split = int(np.floor(valid_size * num_train)) train_idx,valid_idx = indices[split:],indices[:split] train_sampler = SubsetRandomSampler(train_idx) valid_sampler = SubsetRandomSampler(valid_idx) train_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, sampler = train_sampler,num_workers = num_workers) valid_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, sampler = valid_sampler) test_loader = torch.utils.data.DataLoader(test_data,batch_size = batch_size, num_workers = num_workers) # 定义MLP模型 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net,self).__init__() hidden_1 = 512 hidden_2 = 512 self.fc1 = nn.Linear(28 * 28,hidden_1) self.fc2 = nn.Linear(hidden_1,hidden_2) self.fc3 = nn.Linear(hidden_2,10) self.dropout = nn.Dropout(0.2) def forward(self,x): x = x.view(-1,28 * 28) x = F.relu(self.fc1(x)) x = self.dropout(x) x = F.relu(self.fc2(x)) x = self.dropout(x) x = self.fc3(x) # x = F.log_softmax(x,dim = 1) return x model = Net() print(model) #定义损失函数和优化器 # criterion = nn.NLLLoss() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params = model.parameters(),lr = 0.01) 这里如果训练集的损失还在减少，但是检验集的损失开始上升了，说明可能是过拟合了。 当两者都还在减少的时候，则保存模型，用于后期的使用 n_epochs = 50 valid_loss_min = np.Inf for epoch in range(n_epochs): train_loss = 0.0 valid_loss = 0.0 for data,target in train_loader: optimizer.zero_grad() output = model(data)#得到预测值 loss = criterion(output,target) loss.backward() optimizer.step() train_loss += loss.item()*data.size(0) #计算检验集的损失，这里不需要反向传播 for data,target in valid_loader: output = model(data) loss = criterion(output,target) valid_loss += loss.item() * data.size(0) train_loss = train_loss / len(train_loader.dataset) valid_loss = valid_loss / len(valid_loader.dataset) print(&#39;Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}&#39;.format( epoch + 1, train_loss, valid_loss)) if valid_loss &lt;= valid_loss_min:#保存模型 print(&#39;Validation loss decreased ({:.6f} --&gt; {:.6f}). Saving model...&#39;.format( valid_loss_min, valid_loss)) torch.save(model.state_dict(),&#39;model.pt&#39;) valid_loss_min = valid_loss 这一步直接加载之前保存的模型 model.load_state_dict(torch.load(&#39;model.pt&#39;)) # initialize lists to monitor test loss and accuracy test_loss = 0.0 class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) model.eval() # prep model for *evaluation* for data, target in test_loader: # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the loss loss = criterion(output, target) # update test loss test_loss += loss.item()*data.size(0) # convert output probabilities to predicted class _, pred = torch.max(output, 1) # compare predictions to true label correct = np.squeeze(pred.eq(target.data.view_as(pred))) # calculate test accuracy for each object class for i in range(batch_size): label = target.data[i] class_correct[label] += correct[i].item() class_total[label] += 1 # calculate and print avg test loss test_loss = test_loss/len(test_loader.dataset) print(&#39;Test Loss: {:.6f}\n&#39;.format(test_loss)) for i in range(10): if class_total[i] &gt; 0: print(&#39;Test Accuracy of %5s: %2d%% (%2d/%2d)&#39; % ( str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i]))) else: print(&#39;Test Accuracy of %5s: N/A (no training examples)&#39; % (classes[i])) print(&#39;\nTest Accuracy (Overall): %2d%% (%2d/%2d)&#39; % ( 100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total))) Out: Test Loss: 0.019485 Test Accuracy of 0: 99% (5914/5923) Test Accuracy of 1: 99% (6716/6742) Test Accuracy of 2: 99% (5929/5958) Test Accuracy of 3: 99% (6086/6131) Test Accuracy of 4: 99% (5822/5842) Test Accuracy of 5: 99% (5393/5421) Test Accuracy of 6: 99% (5902/5918) Test Accuracy of 7: 99% (6231/6265) Test Accuracy of 8: 99% (5822/5851) Test Accuracy of 9: 99% (5914/5949) Test Accuracy (Overall): 99% (59729/60000) 这里发现，其实结果不如之前的那个模型，但是没关系，前面其实也使用了dropout来防止过拟合，而且这个案例特征实际上不是很多，模型并不复杂，但特征变得多起来的时候，检验集的威力就能得到显现了。" />
<link rel="canonical" href="https://mlh.app/2019/05/04/729847.html" />
<meta property="og:url" content="https://mlh.app/2019/05/04/729847.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-04T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"基本流程 1、加载数据集 2、预处理数据（标准化并转换为张量） 3、查阅资料，看看是否已经有人做了这个问题，使用的是什么模型架构，并定义模型 4、确定损失函数和优化函数，并开始训练模型 5、使用模型从未见过的数据测试模型 本文在谷歌的Colab上实现 from torchvision import datasets import torchvision.transforms as transforms import torch #非并行加载就填0 num_workers = 0 #决定每次读取多少图片 batch_size = 20 #转换成张量 transform = transforms.ToTensor() #下载数据 train_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) test_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) #创建加载器 train_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, num_workers = num_workers) test_loader = torch.utils.data.DataLoader(test_data,batch_size = batch_size, num_workers = num_workers) 接下来的可视化部分可以省略 #可视化 import numpy as np import matplotlib.pyplot as plt %matplotlib inline dataiter = iter(train_loader) images,labels = next(dataiter) images = images.numpy() fig = plt.figure(figsize = (25,4)) for idx in np.arange(20):#前面是读20张，所以这里就是20 ax = fig.add_subplot(2,20/2,idx + 1,xticks = [],yticks = []) ax.imshow(np.squeeze(images[idx]),cmap = &#39;gray&#39;) ax.set_title(str(labels[idx].item())) 下面是输出，证明数据已经加载进来了 接下来定义我们的模型 # 定义MLP模型 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net,self).__init__() #两个全连接的隐藏层，一个输出层 #因为图片是28*28的，需要全部展开，最终我们要输出数字，一共10个数字。 #10个数字实际上是10个类别，输出是概率分布，最后选取概率最大的作为预测值输出 hidden_1 = 512 hidden_2 = 512 self.fc1 = nn.Linear(28 * 28,hidden_1) self.fc2 = nn.Linear(hidden_1,hidden_2) self.fc3 = nn.Linear(hidden_2,10) #使用dropout防止过拟合 self.dropout = nn.Dropout(0.2) def forward(self,x): x = x.view(-1,28 * 28) x = F.relu(self.fc1(x)) x = self.dropout(x) x = F.relu(self.fc2(x)) x = self.dropout(x) x = self.fc3(x) # x = F.log_softmax(x,dim = 1) return x model = Net() #打印出来看是否正确 print(model) #定义损失函数和优化器 # criterion = nn.NLLLoss() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params = model.parameters(),lr = 0.01) #训练 n_epochs = 50 for epoch in range(n_epochs): train_loss = 0.0 for data,target in train_loader: optimizer.zero_grad() output = model(data)#得到预测值 loss = criterion(output,target) loss.backward() optimizer.step() train_loss += loss.item()*data.size(0) train_loss = train_loss / len(train_loader.dataset) print(&#39;Epoch: {} \\tTraining Loss: {:.6f}&#39;.format( epoch + 1, train_loss)) 这里是测试了，我们用之前的测试数据来测试训练好的模型，然后统计正确的数目，最后计算每个数字的正确率与总正确率 # initialize lists to monitor test loss and accuracy test_loss = 0.0 class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) model.eval() # prep model for *evaluation* for data, target in test_loader: # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the loss loss = criterion(output, target) # update test loss test_loss += loss.item()*data.size(0) # convert output probabilities to predicted class _, pred = torch.max(output, 1) # compare predictions to true label correct = np.squeeze(pred.eq(target.data.view_as(pred))) # calculate test accuracy for each object class for i in range(batch_size): label = target.data[i] class_correct[label] += correct[i].item() class_total[label] += 1 # calculate and print avg test loss test_loss = test_loss/len(test_loader.dataset) print(&#39;Test Loss: {:.6f}\\n&#39;.format(test_loss)) for i in range(10): if class_total[i] &gt; 0: print(&#39;Test Accuracy of %5s: %2d%% (%2d/%2d)&#39; % ( str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i]))) else: print(&#39;Test Accuracy of %5s: N/A (no training examples)&#39; % (classes[i])) print(&#39;\\nTest Accuracy (Overall): %2d%% (%2d/%2d)&#39; % ( 100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total))) 以下是我测试的结果 Out： Test Loss: 0.003499 Test Accuracy of 0: 100% (5923/5923) Test Accuracy of 1: 99% (6740/6742) Test Accuracy of 2: 99% (5955/5958) Test Accuracy of 3: 99% (6125/6131) Test Accuracy of 4: 99% (5841/5842) Test Accuracy of 5: 100% (5421/5421) Test Accuracy of 6: 100% (5918/5918) Test Accuracy of 7: 99% (6264/6265) Test Accuracy of 8: 99% (5850/5851) Test Accuracy of 9: 99% (5947/5949) Test Accuracy (Overall): 99% (59984/60000) 防止过拟合优化 为了防止过拟合，我们还可以采取另一种办法：利用训练集训练模型，利用检验集检验当前模型的效果（当本来是好的，变得不再那么好，可能就是出现了过拟合现象了）利用测试集做测试。 而之所以一定要加入检验集做检验，不使用测试集做检验，是因为在检验集做检验的时候，结果会带有一定的倾向性，即对检验集有利的模型和参数会被保留。如果用测试集做检验，最后的结果肯定会对结果有利，不利于模型的泛化。 下面是加入检验集的代码： from torchvision import datasets import torchvision.transforms as transforms import torch #拆分数据集 from torch.utils.data.sampler import SubsetRandomSampler num_workers = 0 batch_size = 20 #添加验证集，让模型自动判断是否过拟合 valid_size = 0.2 transform = transforms.ToTensor() train_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) test_data = datasets.MNIST(root = &#39;./drive/data&#39;,train = True, download = True,transform = transform) num_train = len(train_data) indices = list(range(num_train)) np.random.shuffle(indices) split = int(np.floor(valid_size * num_train)) train_idx,valid_idx = indices[split:],indices[:split] train_sampler = SubsetRandomSampler(train_idx) valid_sampler = SubsetRandomSampler(valid_idx) train_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, sampler = train_sampler,num_workers = num_workers) valid_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size, sampler = valid_sampler) test_loader = torch.utils.data.DataLoader(test_data,batch_size = batch_size, num_workers = num_workers) # 定义MLP模型 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net,self).__init__() hidden_1 = 512 hidden_2 = 512 self.fc1 = nn.Linear(28 * 28,hidden_1) self.fc2 = nn.Linear(hidden_1,hidden_2) self.fc3 = nn.Linear(hidden_2,10) self.dropout = nn.Dropout(0.2) def forward(self,x): x = x.view(-1,28 * 28) x = F.relu(self.fc1(x)) x = self.dropout(x) x = F.relu(self.fc2(x)) x = self.dropout(x) x = self.fc3(x) # x = F.log_softmax(x,dim = 1) return x model = Net() print(model) #定义损失函数和优化器 # criterion = nn.NLLLoss() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params = model.parameters(),lr = 0.01) 这里如果训练集的损失还在减少，但是检验集的损失开始上升了，说明可能是过拟合了。 当两者都还在减少的时候，则保存模型，用于后期的使用 n_epochs = 50 valid_loss_min = np.Inf for epoch in range(n_epochs): train_loss = 0.0 valid_loss = 0.0 for data,target in train_loader: optimizer.zero_grad() output = model(data)#得到预测值 loss = criterion(output,target) loss.backward() optimizer.step() train_loss += loss.item()*data.size(0) #计算检验集的损失，这里不需要反向传播 for data,target in valid_loader: output = model(data) loss = criterion(output,target) valid_loss += loss.item() * data.size(0) train_loss = train_loss / len(train_loader.dataset) valid_loss = valid_loss / len(valid_loader.dataset) print(&#39;Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}&#39;.format( epoch + 1, train_loss, valid_loss)) if valid_loss &lt;= valid_loss_min:#保存模型 print(&#39;Validation loss decreased ({:.6f} --&gt; {:.6f}). Saving model...&#39;.format( valid_loss_min, valid_loss)) torch.save(model.state_dict(),&#39;model.pt&#39;) valid_loss_min = valid_loss 这一步直接加载之前保存的模型 model.load_state_dict(torch.load(&#39;model.pt&#39;)) # initialize lists to monitor test loss and accuracy test_loss = 0.0 class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) model.eval() # prep model for *evaluation* for data, target in test_loader: # forward pass: compute predicted outputs by passing inputs to the model output = model(data) # calculate the loss loss = criterion(output, target) # update test loss test_loss += loss.item()*data.size(0) # convert output probabilities to predicted class _, pred = torch.max(output, 1) # compare predictions to true label correct = np.squeeze(pred.eq(target.data.view_as(pred))) # calculate test accuracy for each object class for i in range(batch_size): label = target.data[i] class_correct[label] += correct[i].item() class_total[label] += 1 # calculate and print avg test loss test_loss = test_loss/len(test_loader.dataset) print(&#39;Test Loss: {:.6f}\\n&#39;.format(test_loss)) for i in range(10): if class_total[i] &gt; 0: print(&#39;Test Accuracy of %5s: %2d%% (%2d/%2d)&#39; % ( str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i]))) else: print(&#39;Test Accuracy of %5s: N/A (no training examples)&#39; % (classes[i])) print(&#39;\\nTest Accuracy (Overall): %2d%% (%2d/%2d)&#39; % ( 100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total))) Out: Test Loss: 0.019485 Test Accuracy of 0: 99% (5914/5923) Test Accuracy of 1: 99% (6716/6742) Test Accuracy of 2: 99% (5929/5958) Test Accuracy of 3: 99% (6086/6131) Test Accuracy of 4: 99% (5822/5842) Test Accuracy of 5: 99% (5393/5421) Test Accuracy of 6: 99% (5902/5918) Test Accuracy of 7: 99% (6231/6265) Test Accuracy of 8: 99% (5822/5851) Test Accuracy of 9: 99% (5914/5949) Test Accuracy (Overall): 99% (59729/60000) 这里发现，其实结果不如之前的那个模型，但是没关系，前面其实也使用了dropout来防止过拟合，而且这个案例特征实际上不是很多，模型并不复杂，但特征变得多起来的时候，检验集的威力就能得到显现了。","@type":"BlogPosting","url":"https://mlh.app/2019/05/04/729847.html","headline":"使用Pytorch构建MLP模型实现MNIST手写数字识别","dateModified":"2019-05-04T00:00:00+08:00","datePublished":"2019-05-04T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/04/729847.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>使用Pytorch构建MLP模型实现MNIST手写数字识别</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <h1><a id="_0"></a>基本流程</h1> 
  <h2><a id="1_1"></a>1、加载数据集</h2> 
  <h2><a id="2_3"></a>2、预处理数据（标准化并转换为张量）</h2> 
  <h2><a id="3_5"></a>3、查阅资料，看看是否已经有人做了这个问题，使用的是什么模型架构，并定义模型</h2> 
  <h2><a id="4_7"></a>4、确定损失函数和优化函数，并开始训练模型</h2> 
  <h2><a id="5_9"></a>5、使用模型从未见过的数据测试模型</h2> 
  <p>本文在谷歌的Colab上实现</p> 
  <pre><code>from torchvision import datasets
import torchvision.transforms as transforms
import torch

#非并行加载就填0
num_workers = 0
#决定每次读取多少图片
batch_size = 20

#转换成张量
transform = transforms.ToTensor()


#下载数据
train_data = datasets.MNIST(root = './drive/data',train = True,
                           download = True,transform = transform)
test_data = datasets.MNIST(root = './drive/data',train = True,
                          download = True,transform = transform)

#创建加载器
train_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size,
                                           num_workers = num_workers)
test_loader = torch.utils.data.DataLoader(test_data,batch_size = batch_size,
                                         num_workers = num_workers)

</code></pre> 
  <p>接下来的可视化部分可以省略</p> 
  <pre><code>#可视化
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

dataiter = iter(train_loader)
images,labels = next(dataiter)
images = images.numpy()

fig = plt.figure(figsize = (25,4))
for idx in np.arange(20):#前面是读20张，所以这里就是20
  ax = fig.add_subplot(2,20/2,idx + 1,xticks = [],yticks = [])
  ax.imshow(np.squeeze(images[idx]),cmap = 'gray')
  
  ax.set_title(str(labels[idx].item()))
</code></pre> 
  <p>下面是输出，证明数据已经加载进来了<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190504212702984.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2wxNjA2NDY4MTU1,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <p>接下来定义我们的模型</p> 
  <pre><code># 定义MLP模型

import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
  def __init__(self):
    super(Net,self).__init__()
    
	#两个全连接的隐藏层，一个输出层
 	#因为图片是28*28的，需要全部展开，最终我们要输出数字，一共10个数字。
 	#10个数字实际上是10个类别，输出是概率分布，最后选取概率最大的作为预测值输出
    hidden_1 = 512
    hidden_2 = 512
    self.fc1 = nn.Linear(28 * 28,hidden_1)
    self.fc2 = nn.Linear(hidden_1,hidden_2)
    self.fc3 = nn.Linear(hidden_2,10)
    #使用dropout防止过拟合
    self.dropout = nn.Dropout(0.2)
  def forward(self,x):
    x = x.view(-1,28 * 28)
    x = F.relu(self.fc1(x))
    x = self.dropout(x)
    
    x = F.relu(self.fc2(x))
    
    x = self.dropout(x)
    x = self.fc3(x)
#     x = F.log_softmax(x,dim = 1)
    
    return x
  
model = Net()
#打印出来看是否正确
print(model)
</code></pre> 
  <pre><code>#定义损失函数和优化器

# criterion = nn.NLLLoss()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params = model.parameters(),lr = 0.01)
</code></pre> 
  <pre><code>#训练
n_epochs = 50

for epoch in range(n_epochs):
  train_loss = 0.0
  
  
  for data,target in train_loader:
    optimizer.zero_grad()
    output = model(data)#得到预测值
    
    loss = criterion(output,target)
    loss.backward()
    
    optimizer.step()
    train_loss += loss.item()*data.size(0)
  train_loss = train_loss / len(train_loader.dataset)
  print('Epoch:  {}  \tTraining Loss: {:.6f}'.format(
    epoch + 1,
    train_loss))
</code></pre> 
  <p>这里是测试了，我们用之前的测试数据来测试训练好的模型，然后统计正确的数目，最后计算每个数字的正确率与总正确率</p> 
  <pre><code># initialize lists to monitor test loss and accuracy
test_loss = 0.0
class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))

model.eval() # prep model for *evaluation*

for data, target in test_loader:
    # forward pass: compute predicted outputs by passing inputs to the model
    output = model(data)
    # calculate the loss
    loss = criterion(output, target)
    # update test loss 
    test_loss += loss.item()*data.size(0)
    # convert output probabilities to predicted class
    _, pred = torch.max(output, 1)
    # compare predictions to true label
    correct = np.squeeze(pred.eq(target.data.view_as(pred)))
    # calculate test accuracy for each object class
    for i in range(batch_size):
        label = target.data[i]
        class_correct[label] += correct[i].item()
        class_total[label] += 1

# calculate and print avg test loss
test_loss = test_loss/len(test_loader.dataset)
print('Test Loss: {:.6f}\n'.format(test_loss))

for i in range(10):
    if class_total[i] &gt; 0:
        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
            str(i), 100 * class_correct[i] / class_total[i],
            np.sum(class_correct[i]), np.sum(class_total[i])))
    else:
        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
    100. * np.sum(class_correct) / np.sum(class_total),
    np.sum(class_correct), np.sum(class_total)))
</code></pre> 
  <p>以下是我测试的结果</p> 
  <pre><code>Out：
Test Loss: 0.003499

Test Accuracy of     0: 100% (5923/5923)
Test Accuracy of     1: 99% (6740/6742)
Test Accuracy of     2: 99% (5955/5958)
Test Accuracy of     3: 99% (6125/6131)
Test Accuracy of     4: 99% (5841/5842)
Test Accuracy of     5: 100% (5421/5421)
Test Accuracy of     6: 100% (5918/5918)
Test Accuracy of     7: 99% (6264/6265)
Test Accuracy of     8: 99% (5850/5851)
Test Accuracy of     9: 99% (5947/5949)

Test Accuracy (Overall): 99% (59984/60000)
</code></pre> 
  <h1><a id="_198"></a>防止过拟合优化</h1> 
  <p>为了防止过拟合，我们还可以采取另一种办法：利用训练集训练模型，利用检验集检验当前模型的效果（当本来是好的，变得不再那么好，可能就是出现了过拟合现象了）利用测试集做测试。<br> 而之所以一定要加入检验集做检验，不使用测试集做检验，是因为在检验集做检验的时候，结果会带有一定的倾向性，即对检验集有利的模型和参数会被保留。如果用测试集做检验，最后的结果肯定会对结果有利，不利于模型的泛化。</p> 
  <p>下面是加入检验集的代码：</p> 
  <pre><code>from torchvision import datasets
import torchvision.transforms as transforms
import torch

#拆分数据集
from torch.utils.data.sampler import SubsetRandomSampler

num_workers = 0

batch_size = 20

#添加验证集，让模型自动判断是否过拟合
valid_size = 0.2


transform = transforms.ToTensor()

train_data = datasets.MNIST(root = './drive/data',train = True,
                           download = True,transform = transform)
test_data = datasets.MNIST(root = './drive/data',train = True,
                          download = True,transform = transform)


num_train = len(train_data)
indices = list(range(num_train))
np.random.shuffle(indices)
split = int(np.floor(valid_size * num_train))
train_idx,valid_idx = indices[split:],indices[:split]

train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)

train_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size,
                            sampler = train_sampler,num_workers = num_workers)
valid_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size,
                            sampler = valid_sampler)
test_loader = torch.utils.data.DataLoader(test_data,batch_size = batch_size,
                                         num_workers = num_workers)

</code></pre> 
  <pre><code># 定义MLP模型

import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
  def __init__(self):
    super(Net,self).__init__()
    
    hidden_1 = 512
    hidden_2 = 512
    self.fc1 = nn.Linear(28 * 28,hidden_1)
    self.fc2 = nn.Linear(hidden_1,hidden_2)
    self.fc3 = nn.Linear(hidden_2,10)
    
    self.dropout = nn.Dropout(0.2)
  def forward(self,x):
    x = x.view(-1,28 * 28)
    x = F.relu(self.fc1(x))
    x = self.dropout(x)
    
    x = F.relu(self.fc2(x))
    
    x = self.dropout(x)
    
    x = self.fc3(x)
#     x = F.log_softmax(x,dim = 1)
    
    return x
  
model = Net()
print(model)
</code></pre> 
  <pre><code>#定义损失函数和优化器

# criterion = nn.NLLLoss()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params = model.parameters(),lr = 0.01)
</code></pre> 
  <p>这里如果训练集的损失还在减少，但是检验集的损失开始上升了，说明可能是过拟合了。<br> 当两者都还在减少的时候，则保存模型，用于后期的使用</p> 
  <pre><code>n_epochs = 50

valid_loss_min = np.Inf

for epoch in range(n_epochs):
  train_loss = 0.0
  valid_loss = 0.0
  
  for data,target in train_loader:
    optimizer.zero_grad()
    output = model(data)#得到预测值
    
    loss = criterion(output,target)
    loss.backward()
    
    optimizer.step()
    train_loss += loss.item()*data.size(0)
  
  #计算检验集的损失，这里不需要反向传播
  for data,target in valid_loader:
    output = model(data)
    loss = criterion(output,target)
    valid_loss += loss.item() * data.size(0)
  
  train_loss = train_loss / len(train_loader.dataset)
  valid_loss = valid_loss / len(valid_loader.dataset)
  print('Epoch:  {}  \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
    epoch + 1,
    train_loss,
    valid_loss))
  if valid_loss &lt;= valid_loss_min:#保存模型
    print('Validation loss decreased ({:.6f} --&gt; {:.6f}). Saving model...'.format(
    valid_loss_min,
    valid_loss))
    torch.save(model.state_dict(),'model.pt')
    valid_loss_min = valid_loss
</code></pre> 
  <p>这一步直接加载之前保存的模型</p> 
  <pre><code>model.load_state_dict(torch.load('model.pt'))
</code></pre> 
  <pre><code># initialize lists to monitor test loss and accuracy
test_loss = 0.0
class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))

model.eval() # prep model for *evaluation*

for data, target in test_loader:
    # forward pass: compute predicted outputs by passing inputs to the model
    output = model(data)
    # calculate the loss
    loss = criterion(output, target)
    # update test loss 
    test_loss += loss.item()*data.size(0)
    # convert output probabilities to predicted class
    _, pred = torch.max(output, 1)
    # compare predictions to true label
    correct = np.squeeze(pred.eq(target.data.view_as(pred)))
    # calculate test accuracy for each object class
    for i in range(batch_size):
        label = target.data[i]
        class_correct[label] += correct[i].item()
        class_total[label] += 1

# calculate and print avg test loss
test_loss = test_loss/len(test_loader.dataset)
print('Test Loss: {:.6f}\n'.format(test_loss))

for i in range(10):
    if class_total[i] &gt; 0:
        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
            str(i), 100 * class_correct[i] / class_total[i],
            np.sum(class_correct[i]), np.sum(class_total[i])))
    else:
        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
    100. * np.sum(class_correct) / np.sum(class_total),
    np.sum(class_correct), np.sum(class_total)))
</code></pre> 
  <pre><code>Out:
Test Loss: 0.019485

Test Accuracy of     0: 99% (5914/5923)
Test Accuracy of     1: 99% (6716/6742)
Test Accuracy of     2: 99% (5929/5958)
Test Accuracy of     3: 99% (6086/6131)
Test Accuracy of     4: 99% (5822/5842)
Test Accuracy of     5: 99% (5393/5421)
Test Accuracy of     6: 99% (5902/5918)
Test Accuracy of     7: 99% (6231/6265)
Test Accuracy of     8: 99% (5822/5851)
Test Accuracy of     9: 99% (5914/5949)

Test Accuracy (Overall): 99% (59729/60000)
</code></pre> 
  <p>这里发现，其实结果不如之前的那个模型，但是没关系，前面其实也使用了dropout来防止过拟合，而且这个案例特征实际上不是很多，模型并不复杂，但特征变得多起来的时候，检验集的威力就能得到显现了。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
