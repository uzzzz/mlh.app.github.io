<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>MapReduce编程之自定义序列化类及自定义排序 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="MapReduce编程之自定义序列化类及自定义排序" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="数据： //姓名 收入 支出 时间 zhangsan 6000 0 2016-05-01 lisi 2000 0 2016-05-01 lisi 0 100 2016-05-01 zhangsan 3000 0 2016-05-01 wangwu 9000 0 2016-05-01 wangwu 0 200 2016-05-01 zhangsan 200 400 2016-05-01 需求： 计算每个用户的收入、支出及利润情况，并优先显示利润最大的用户（按利润降序、如果利润相同则按收入降序） 分析： 实现WritableComparable接口，Writable接口是一个实现了序列化协议的序列化对象。在Hadoop中定义一个结构化对象都要实现Writable接口，使得该结构化对象可以序列化为字节流，字节流也可以反序列化为结构化对象。那WritableComparable接口是可序列化并且可比较的接口。 MapReduce中所有的key值类型都必须实现这个接口，既然是可序列化的那就必须得实现readFields（）和write（）这两个序列化和反序列化函数；既然是可比较的就必须实现compareTo（）函数，该函数即是比较和排序规则的实现。这样MR中的key值就既能可序列化又是可比较的。 代码： TradeBean类： package com.wqs.myWritableComparable; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import org.apache.hadoop.io.WritableComparable; public class TradeBean implements WritableComparable&lt;TradeBean&gt;{ private String name; private int income; private int pay; private int profit; public TradeBean() { super(); // TODO 自动生成的构造函数存根 } public TradeBean(String name, int income, int pay, int profit) { super(); this.name = name; this.income = income; this.pay = pay; this.profit = profit; } @Override public void readFields(DataInput in) throws IOException { name = in.readUTF(); income = in.readInt(); pay = in.readInt(); profit = in.readInt(); } @Override public void write(DataOutput out) throws IOException { out.writeUTF(name); out.writeInt(income); out.writeInt(pay); out.writeInt(profit); } @Override public int compareTo(TradeBean tradeBean) { if(this.profit &gt; tradeBean.getProfit()) return -1; else if(this.profit &lt; tradeBean.getProfit()) return 1; else if(this.income &gt; tradeBean.getIncome()) return -1; else if(this.income &lt; tradeBean.getIncome()) return -1; else return 0; } @Override public String toString() { return name + &quot; &quot; + income + &quot; &quot; + pay + &quot; &quot; + profit; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getIncome() { return income; } public void setIncome(int income) { this.income = income; } public int getPay() { return pay; } public void setPay(int pay) { this.pay = pay; } public int getProfit() { return profit; } public void setProfit(int profit) { this.profit = profit; } } Map类： package com.wqs.myWritableComparable; import java.io.IOException; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class Map extends Mapper&lt;Object, Text, Text, TradeBean&gt;{ private TradeBean bean = new TradeBean(); private Text name = new Text(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException{ String line = value.toString(); String[] temp = line.split(&quot; &quot;); name.set(temp[0]); bean.setName(temp[0]); bean.setIncome(Integer.valueOf(temp[1])); bean.setPay(Integer.valueOf(temp[2])); bean.setProfit(0); context.write(name, bean); } } Reduce类： package com.wqs.myWritableComparable; import java.io.IOException; import java.util.ArrayList; import java.util.Collections; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; public class Reduce extends Reducer&lt;Text, TradeBean, TradeBean, NullWritable&gt;{ ArrayList&lt;TradeBean&gt; tradeBeans = new ArrayList&lt;&gt;(); @Override protected void reduce(Text k2, Iterable&lt;TradeBean&gt; vs2, Context context) throws IOException, InterruptedException { String name = null; int income = 0; int pay = 0; int profit = 0; for (TradeBean tradeBean : vs2) { income += tradeBean.getIncome(); pay += tradeBean.getPay(); } name = k2.toString(); profit = income - pay; tradeBeans.add(new TradeBean(name, income, pay, profit)); } /** * 在所有reduce执行结束之后对tradeBeans进行排序 * cleanup方法的作用：在所有reduce执行结束之后调用 * 目的：使结果按照利润进行排序。前面map阶段为了reduce阶段容易统计每个人的数据，将K1设置为了name * 那么此时我们发现结果排序是按照name进行排序的，而不是需求所要求的按照利润进行排序，故把最终的结果集 * sort一下就可以了 */ @Override protected void cleanup(Context context) throws IOException, InterruptedException { Collections.sort(tradeBeans); for (TradeBean tradeBean : tradeBeans) { context.write(tradeBean, NullWritable.get()); } } } Main： package com.wqs.myWritableComparable; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; public class Main { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); System.setProperty(&quot;hadoop.home.dir&quot;, &quot;E:/hadoop-2.7.7&quot;); args = new String[] { &quot;/demo03/in/&quot;, &quot;/demo03/out&quot; }; String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs(); if(otherArgs.length != 2){ System.err.println(&quot;Usage:InvertedIndex&quot;); System.exit(2); } Job job = Job.getInstance(); job.setJarByClass(Main.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TradeBean.class); job.setOutputKeyClass(TradeBean.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.addInputPath(job, new Path(&quot;hdfs://192.168.222.128:9000&quot; + args[0])); FileOutputFormat.setOutputPath(job, new Path(&quot;hdfs://192.168.222.128:9000&quot; + args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } } &nbsp;" />
<meta property="og:description" content="数据： //姓名 收入 支出 时间 zhangsan 6000 0 2016-05-01 lisi 2000 0 2016-05-01 lisi 0 100 2016-05-01 zhangsan 3000 0 2016-05-01 wangwu 9000 0 2016-05-01 wangwu 0 200 2016-05-01 zhangsan 200 400 2016-05-01 需求： 计算每个用户的收入、支出及利润情况，并优先显示利润最大的用户（按利润降序、如果利润相同则按收入降序） 分析： 实现WritableComparable接口，Writable接口是一个实现了序列化协议的序列化对象。在Hadoop中定义一个结构化对象都要实现Writable接口，使得该结构化对象可以序列化为字节流，字节流也可以反序列化为结构化对象。那WritableComparable接口是可序列化并且可比较的接口。 MapReduce中所有的key值类型都必须实现这个接口，既然是可序列化的那就必须得实现readFields（）和write（）这两个序列化和反序列化函数；既然是可比较的就必须实现compareTo（）函数，该函数即是比较和排序规则的实现。这样MR中的key值就既能可序列化又是可比较的。 代码： TradeBean类： package com.wqs.myWritableComparable; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import org.apache.hadoop.io.WritableComparable; public class TradeBean implements WritableComparable&lt;TradeBean&gt;{ private String name; private int income; private int pay; private int profit; public TradeBean() { super(); // TODO 自动生成的构造函数存根 } public TradeBean(String name, int income, int pay, int profit) { super(); this.name = name; this.income = income; this.pay = pay; this.profit = profit; } @Override public void readFields(DataInput in) throws IOException { name = in.readUTF(); income = in.readInt(); pay = in.readInt(); profit = in.readInt(); } @Override public void write(DataOutput out) throws IOException { out.writeUTF(name); out.writeInt(income); out.writeInt(pay); out.writeInt(profit); } @Override public int compareTo(TradeBean tradeBean) { if(this.profit &gt; tradeBean.getProfit()) return -1; else if(this.profit &lt; tradeBean.getProfit()) return 1; else if(this.income &gt; tradeBean.getIncome()) return -1; else if(this.income &lt; tradeBean.getIncome()) return -1; else return 0; } @Override public String toString() { return name + &quot; &quot; + income + &quot; &quot; + pay + &quot; &quot; + profit; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getIncome() { return income; } public void setIncome(int income) { this.income = income; } public int getPay() { return pay; } public void setPay(int pay) { this.pay = pay; } public int getProfit() { return profit; } public void setProfit(int profit) { this.profit = profit; } } Map类： package com.wqs.myWritableComparable; import java.io.IOException; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class Map extends Mapper&lt;Object, Text, Text, TradeBean&gt;{ private TradeBean bean = new TradeBean(); private Text name = new Text(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException{ String line = value.toString(); String[] temp = line.split(&quot; &quot;); name.set(temp[0]); bean.setName(temp[0]); bean.setIncome(Integer.valueOf(temp[1])); bean.setPay(Integer.valueOf(temp[2])); bean.setProfit(0); context.write(name, bean); } } Reduce类： package com.wqs.myWritableComparable; import java.io.IOException; import java.util.ArrayList; import java.util.Collections; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; public class Reduce extends Reducer&lt;Text, TradeBean, TradeBean, NullWritable&gt;{ ArrayList&lt;TradeBean&gt; tradeBeans = new ArrayList&lt;&gt;(); @Override protected void reduce(Text k2, Iterable&lt;TradeBean&gt; vs2, Context context) throws IOException, InterruptedException { String name = null; int income = 0; int pay = 0; int profit = 0; for (TradeBean tradeBean : vs2) { income += tradeBean.getIncome(); pay += tradeBean.getPay(); } name = k2.toString(); profit = income - pay; tradeBeans.add(new TradeBean(name, income, pay, profit)); } /** * 在所有reduce执行结束之后对tradeBeans进行排序 * cleanup方法的作用：在所有reduce执行结束之后调用 * 目的：使结果按照利润进行排序。前面map阶段为了reduce阶段容易统计每个人的数据，将K1设置为了name * 那么此时我们发现结果排序是按照name进行排序的，而不是需求所要求的按照利润进行排序，故把最终的结果集 * sort一下就可以了 */ @Override protected void cleanup(Context context) throws IOException, InterruptedException { Collections.sort(tradeBeans); for (TradeBean tradeBean : tradeBeans) { context.write(tradeBean, NullWritable.get()); } } } Main： package com.wqs.myWritableComparable; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; public class Main { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); System.setProperty(&quot;hadoop.home.dir&quot;, &quot;E:/hadoop-2.7.7&quot;); args = new String[] { &quot;/demo03/in/&quot;, &quot;/demo03/out&quot; }; String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs(); if(otherArgs.length != 2){ System.err.println(&quot;Usage:InvertedIndex&quot;); System.exit(2); } Job job = Job.getInstance(); job.setJarByClass(Main.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TradeBean.class); job.setOutputKeyClass(TradeBean.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.addInputPath(job, new Path(&quot;hdfs://192.168.222.128:9000&quot; + args[0])); FileOutputFormat.setOutputPath(job, new Path(&quot;hdfs://192.168.222.128:9000&quot; + args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } } &nbsp;" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-04T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"数据： //姓名 收入 支出 时间 zhangsan 6000 0 2016-05-01 lisi 2000 0 2016-05-01 lisi 0 100 2016-05-01 zhangsan 3000 0 2016-05-01 wangwu 9000 0 2016-05-01 wangwu 0 200 2016-05-01 zhangsan 200 400 2016-05-01 需求： 计算每个用户的收入、支出及利润情况，并优先显示利润最大的用户（按利润降序、如果利润相同则按收入降序） 分析： 实现WritableComparable接口，Writable接口是一个实现了序列化协议的序列化对象。在Hadoop中定义一个结构化对象都要实现Writable接口，使得该结构化对象可以序列化为字节流，字节流也可以反序列化为结构化对象。那WritableComparable接口是可序列化并且可比较的接口。 MapReduce中所有的key值类型都必须实现这个接口，既然是可序列化的那就必须得实现readFields（）和write（）这两个序列化和反序列化函数；既然是可比较的就必须实现compareTo（）函数，该函数即是比较和排序规则的实现。这样MR中的key值就既能可序列化又是可比较的。 代码： TradeBean类： package com.wqs.myWritableComparable; import java.io.DataInput; import java.io.DataOutput; import java.io.IOException; import org.apache.hadoop.io.WritableComparable; public class TradeBean implements WritableComparable&lt;TradeBean&gt;{ private String name; private int income; private int pay; private int profit; public TradeBean() { super(); // TODO 自动生成的构造函数存根 } public TradeBean(String name, int income, int pay, int profit) { super(); this.name = name; this.income = income; this.pay = pay; this.profit = profit; } @Override public void readFields(DataInput in) throws IOException { name = in.readUTF(); income = in.readInt(); pay = in.readInt(); profit = in.readInt(); } @Override public void write(DataOutput out) throws IOException { out.writeUTF(name); out.writeInt(income); out.writeInt(pay); out.writeInt(profit); } @Override public int compareTo(TradeBean tradeBean) { if(this.profit &gt; tradeBean.getProfit()) return -1; else if(this.profit &lt; tradeBean.getProfit()) return 1; else if(this.income &gt; tradeBean.getIncome()) return -1; else if(this.income &lt; tradeBean.getIncome()) return -1; else return 0; } @Override public String toString() { return name + &quot; &quot; + income + &quot; &quot; + pay + &quot; &quot; + profit; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getIncome() { return income; } public void setIncome(int income) { this.income = income; } public int getPay() { return pay; } public void setPay(int pay) { this.pay = pay; } public int getProfit() { return profit; } public void setProfit(int profit) { this.profit = profit; } } Map类： package com.wqs.myWritableComparable; import java.io.IOException; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Mapper; public class Map extends Mapper&lt;Object, Text, Text, TradeBean&gt;{ private TradeBean bean = new TradeBean(); private Text name = new Text(); @Override protected void map(Object key, Text value, Context context) throws IOException, InterruptedException{ String line = value.toString(); String[] temp = line.split(&quot; &quot;); name.set(temp[0]); bean.setName(temp[0]); bean.setIncome(Integer.valueOf(temp[1])); bean.setPay(Integer.valueOf(temp[2])); bean.setProfit(0); context.write(name, bean); } } Reduce类： package com.wqs.myWritableComparable; import java.io.IOException; import java.util.ArrayList; import java.util.Collections; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Reducer; public class Reduce extends Reducer&lt;Text, TradeBean, TradeBean, NullWritable&gt;{ ArrayList&lt;TradeBean&gt; tradeBeans = new ArrayList&lt;&gt;(); @Override protected void reduce(Text k2, Iterable&lt;TradeBean&gt; vs2, Context context) throws IOException, InterruptedException { String name = null; int income = 0; int pay = 0; int profit = 0; for (TradeBean tradeBean : vs2) { income += tradeBean.getIncome(); pay += tradeBean.getPay(); } name = k2.toString(); profit = income - pay; tradeBeans.add(new TradeBean(name, income, pay, profit)); } /** * 在所有reduce执行结束之后对tradeBeans进行排序 * cleanup方法的作用：在所有reduce执行结束之后调用 * 目的：使结果按照利润进行排序。前面map阶段为了reduce阶段容易统计每个人的数据，将K1设置为了name * 那么此时我们发现结果排序是按照name进行排序的，而不是需求所要求的按照利润进行排序，故把最终的结果集 * sort一下就可以了 */ @Override protected void cleanup(Context context) throws IOException, InterruptedException { Collections.sort(tradeBeans); for (TradeBean tradeBean : tradeBeans) { context.write(tradeBean, NullWritable.get()); } } } Main： package com.wqs.myWritableComparable; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.NullWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.GenericOptionsParser; public class Main { public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); System.setProperty(&quot;hadoop.home.dir&quot;, &quot;E:/hadoop-2.7.7&quot;); args = new String[] { &quot;/demo03/in/&quot;, &quot;/demo03/out&quot; }; String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs(); if(otherArgs.length != 2){ System.err.println(&quot;Usage:InvertedIndex&quot;); System.exit(2); } Job job = Job.getInstance(); job.setJarByClass(Main.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TradeBean.class); job.setOutputKeyClass(TradeBean.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.addInputPath(job, new Path(&quot;hdfs://192.168.222.128:9000&quot; + args[0])); FileOutputFormat.setOutputPath(job, new Path(&quot;hdfs://192.168.222.128:9000&quot; + args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } } &nbsp;","@type":"BlogPosting","url":"/2019/05/04/729593.html","headline":"MapReduce编程之自定义序列化类及自定义排序","dateModified":"2019-05-04T00:00:00+08:00","datePublished":"2019-05-04T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2019/05/04/729593.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>MapReduce编程之自定义序列化类及自定义排序</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p><strong>数据：</strong></p> 
  <pre class="has">
<code>//姓名 收入 支出 时间
zhangsan 6000 0 2016-05-01
lisi 2000 0 2016-05-01
lisi 0 100 2016-05-01
zhangsan 3000 0 2016-05-01
wangwu 9000 0 2016-05-01
wangwu 0 200 2016-05-01
zhangsan 200 400 2016-05-01
</code></pre> 
  <p><strong>需求：</strong></p> 
  <p>计算每个用户的收入、支出及利润情况，并优先显示利润最大的用户（按利润降序、如果利润相同则按收入降序）</p> 
  <p><strong>分析：</strong></p> 
  <p><span style="color:#f33b45;">实现WritableComparable接口，</span>Writable接口是一个实现了序列化协议的序列化对象。在Hadoop中定义一个结构化对象都要实现Writable接口，使得该结构化对象可以序列化为字节流，字节流也可以反序列化为结构化对象。那WritableComparable接口是可序列化并且可比较的接口。 MapReduce中所有的key值类型都必须实现这个接口，既然是可序列化的那就必须得实现readFields（）和write（）这两个序列化和反序列化函数；既然是可比较的就必须实现compareTo（）函数，该函数即是比较和排序规则的实现。这样MR中的key值就既能可序列化又是可比较的。</p> 
  <p><strong>代码：</strong></p> 
  <p><span style="color:#3399ea;"><strong>TradeBean类：</strong></span></p> 
  <pre class="has">
<code class="language-java">package com.wqs.myWritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

import org.apache.hadoop.io.WritableComparable;

public class TradeBean implements WritableComparable&lt;TradeBean&gt;{
	private String name;
	private int income;
	private int pay;
	private int profit;
	
	public TradeBean() {
		super();
		// TODO 自动生成的构造函数存根
	}

	public TradeBean(String name, int income, int pay, int profit) {
		super();
		this.name = name;
		this.income = income;
		this.pay = pay;
		this.profit = profit;
	}

	@Override
	public void readFields(DataInput in) throws IOException {
		name = in.readUTF();
		income = in.readInt();
		pay = in.readInt();
		profit = in.readInt();
	}

	@Override
	public void write(DataOutput out) throws IOException {
		out.writeUTF(name);
		out.writeInt(income);
		out.writeInt(pay);
		out.writeInt(profit);
	}

	@Override
	public int compareTo(TradeBean tradeBean) {
		if(this.profit &gt; tradeBean.getProfit()) return -1;
		else if(this.profit &lt; tradeBean.getProfit()) return 1;
		else if(this.income &gt; tradeBean.getIncome()) return -1;
		else if(this.income &lt; tradeBean.getIncome()) return -1;
		else return 0;
	}
	
	@Override
	public String toString() {
		return name + " " + income + " " + pay + " " + profit;
	}

	public String getName() {
		return name;
	}

	public void setName(String name) {
		this.name = name;
	}

	public int getIncome() {
		return income;
	}

	public void setIncome(int income) {
		this.income = income;
	}

	public int getPay() {
		return pay;
	}

	public void setPay(int pay) {
		this.pay = pay;
	}

	public int getProfit() {
		return profit;
	}

	public void setProfit(int profit) {
		this.profit = profit;
	}

}
</code></pre> 
  <p><span style="color:#3399ea;"><strong>Map类：</strong></span></p> 
  <pre class="has">
<code class="language-java">package com.wqs.myWritableComparable;

import java.io.IOException;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class Map extends Mapper&lt;Object, Text, Text, TradeBean&gt;{
	private TradeBean bean = new TradeBean();
	private Text name = new Text();
	@Override
	protected void map(Object key, Text value, Context context) throws IOException, InterruptedException{
		String line = value.toString();
		String[] temp = line.split(" ");
		name.set(temp[0]);
		bean.setName(temp[0]);
		bean.setIncome(Integer.valueOf(temp[1]));
		bean.setPay(Integer.valueOf(temp[2]));
		bean.setProfit(0);
		context.write(name, bean);
	}
}
</code></pre> 
  <p><span style="color:#3399ea;"><strong>Reduce类：</strong></span></p> 
  <pre class="has">
<code class="language-java">package com.wqs.myWritableComparable;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class Reduce extends Reducer&lt;Text, TradeBean, TradeBean, NullWritable&gt;{
	ArrayList&lt;TradeBean&gt; tradeBeans = new ArrayList&lt;&gt;();
	@Override
	protected void reduce(Text k2, Iterable&lt;TradeBean&gt; vs2, Context context) 
			throws IOException, InterruptedException {
		String name = null;
		int income = 0;
		int pay = 0;
		int profit = 0;
		for (TradeBean tradeBean : vs2) {
			income += tradeBean.getIncome();
			pay += tradeBean.getPay();
		}
		name = k2.toString();
		profit = income - pay;
		tradeBeans.add(new TradeBean(name, income, pay, profit));
	}
	
	/**
	 * 在所有reduce执行结束之后对tradeBeans进行排序
	 * cleanup方法的作用：在所有reduce执行结束之后调用
	 * 目的：使结果按照利润进行排序。前面map阶段为了reduce阶段容易统计每个人的数据，将K1设置为了name
	 * 那么此时我们发现结果排序是按照name进行排序的，而不是需求所要求的按照利润进行排序，故把最终的结果集
	 * sort一下就可以了
	 */
	@Override
	protected void cleanup(Context context) throws IOException, InterruptedException {
		Collections.sort(tradeBeans);
		for (TradeBean tradeBean : tradeBeans) {
			context.write(tradeBean, NullWritable.get());
		}
	}
}
</code></pre> 
  <p><span style="color:#3399ea;"><strong>Main：</strong></span></p> 
  <pre class="has">
<code class="language-java">package com.wqs.myWritableComparable;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class Main {
	
	public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        System.setProperty("hadoop.home.dir", "E:/hadoop-2.7.7");
		args = new String[] { "/demo03/in/", "/demo03/out" };
        String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs();
        if(otherArgs.length != 2){
            System.err.println("Usage:InvertedIndex");
            System.exit(2);
        }
        Job job = Job.getInstance();
        job.setJarByClass(Main.class);
        job.setMapperClass(Map.class);
        job.setReducerClass(Reduce.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(TradeBean.class);
        job.setOutputKeyClass(TradeBean.class);
        job.setOutputValueClass(NullWritable.class);
         
        FileInputFormat.addInputPath(job, new Path("hdfs://192.168.222.128:9000" + args[0]));
        FileOutputFormat.setOutputPath(job, new Path("hdfs://192.168.222.128:9000" + args[1]));
         
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}

</code></pre> 
  <p>&nbsp;</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
