<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Word2Vec、fastText、Glove训练词向量 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Word2Vec、fastText、Glove训练词向量" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Word2Vec 环境： gensim 3.5.0 python 3.6.1 训练 import logging import os.path import sys import multiprocessing from gensim.corpora import WikiCorpus from gensim.models import Word2Vec from gensim.models.word2vec import LineSentence program = os.path.basename(&#39;train_word2vec_model&#39;) logger = logging.getLogger(program) logging.basicConfig(format=&#39;%(asctime)s: %(levelname)s: %(message)s&#39;) logging.root.setLevel(level=logging.INFO) logger.info(&quot;running %s&quot; % &#39; &#39;.join(sys.argv)) inp=&#39;./data/cleaned_tweets_text_160W.csv&#39; outp=&#39;Word2vec_model/word2vec_model_160w_200d&#39; n_dim=200 model = Word2Vec(LineSentence(inp), size=n_dim, window=5, min_count=5, workers=multiprocessing.cpu_count()) model.save(outp) 使用 import gensim outp=&#39;Word2vec_model/word2vec_model_160w_200d&#39; model = gensim.models.Word2Vec.load(outp) model[&#39;happy&#39;] array([ -7.10430890e-02, -5.29273868e-01, 4.72357810e-01, 6.88659430e-01, -2.42118329e-01, -9.04537499e-01, 3.61087114e-01, -3.96869183e-01, -1.67573178e+00, ... 1.18314767e+00, -1.24723041e+00, 1.19374382e+00, 3.74429256e-01, 1.68333733e+00], dtype=float32) result = model.most_similar(u&quot;sad&quot;,topn=10) for e in result: print(e[0],e[1]) sad&quot; 0.760986328125 upset 0.7119562029838562 sadd 0.7007699012756348 depressed 0.6737781763076782 bummed 0.6687890291213989 saad 0.6555933952331543 upsetting 0.6473105549812317 devastated 0.6356861591339111 disappointed 0.6215260624885559 heartbroken 0.5960309505462646 # man - woman = king - queen # man - woman = husband - wife # man - woman = boy - girl model.most_similar([&#39;woman&#39;, &#39;boy&#39;], [&#39;man&#39;], topn=1) [(‘girl’, 0.6189800500869751)] print (model.doesnt_match(u&quot;happy sad like desk&quot;.split())) desk fastText 环境： gensim 3.5.0 python 3.6.1 安装fasttext 可见：https://github.com/facebookresearch/fastText/tree/master/python $ git clone https://github.com/facebookresearch/fastText.git $ cd fastText $ pip install . 训练 from gensim.models import FastText from gensim.models.word2vec import LineSentence import logging import os.path import sys program = os.path.basename(&#39;fast-text&#39;) logger = logging.getLogger(program) logging.basicConfig(format=&#39;%(asctime)s: %(levelname)s: %(message)s&#39;) logging.root.setLevel(level=logging.INFO) logger.info(&quot;running %s&quot; % &#39; &#39;.join(sys.argv)) inp=&#39;./data/cleaned_tweets_text_160W.csv&#39; outp=&#39;fasttext_model/fasttext_model_160w_200d&#39; n_dim=200 model = FastText(LineSentence(inp), size=n_dim, window=5, min_count=5) model.save(outp) 使用 from gensim.models import FastText # 使用：模型得保存与加载 outp=&#39;../word-Vectorization/fasttext_model/fasttext_model_160w_200d&#39; model = FastText.load(outp) # 指定词条词向量 model[&#39;happy&#39;] array([ 8.10116410e-01, 5.25403082e-01, -3.36705118e-01, -4.42356855e-01, -2.04296446e+00, -9.93114054e-01, … 1.20443761e+00, -3.73263896e-01, 1.15821147e+00, -1.95465660e+00, 2.11104417e+00], dtype=float32) result = model.most_similar(u&quot;queeen&quot;) for e in result: print(e[0],e[1]) queen&quot; 0.8662827014923096 mcqueen 0.8575066924095154 queenie 0.8428698182106018 queens 0.7900538444519043 queer 0.7410856485366821 queezy 0.7156778573989868 queenstown 0.7068071365356445 quen 0.7057324647903442 queensland 0.7055833339691162 Glove 环境： python 3.6.1 官方glove:https://github.com/stanfordnlp/GloVe 笔者使用：https://github.com/maciejkula/glove-python 安装： pip install glove_python 训练 from __future__ import print_function import argparse import pprint import gensim from glove import Glove from glove import Corpus from gensim.models.word2vec import LineSentence inp=&#39;./data/processed_160Msemeval-2016-2017-task3-QAText&#39; outp=&#39;glove_model/QAText_200d&#39; n_dim=200 corpus_model = Corpus() corpus_model.fit(LineSentence(inp), window=5) #corpus_model.save(&#39;corpus.model&#39;) print(&#39;Dict size: %s&#39; % len(corpus_model.dictionary)) print(&#39;Collocations: %s&#39; % corpus_model.matrix.nnz) glove = Glove(no_components=n_dim, learning_rate=0.05) glove.fit(corpus_model.matrix, epochs=10, no_threads=1, verbose=True) glove.add_dictionary(corpus_model.dictionary) glove.save(outp) 使用 from glove import Glove outp=&#39;../word-Vectorization/glove_model/glove_model_160w_200d&#39; glove = Glove.load(outp) glove.most_similar(&#39;happiness&#39;, number=10) [(‘optimism’, 0.88474224538009472), (‘technology’, 0.87399374238355365), (‘photography’, 0.87293168487727013), (‘jquery’, 0.87160586331482937), (‘excitement’, 0.87015550639464156), (‘mailplane’, 0.86709197956363315), (‘secrets’, 0.86616698912523094), (‘laughter’, 0.86593359775060097), (‘records’, 0.86521431383969472)] # 指定词条词向量 glove.word_vectors[glove.dictionary[&#39;happy&#39;]] array([ 0.1824374 , -0.15493986, -0.23131742, -0.20251903, -0.25899053, 0.16043589, -0.11017494, -0.15413852, 0.12485044, 0.28871841, … 0.14323183, -0.21197602, -0.18841062, 0.32888953, -0.32943953, 0.15334943, -0.09995708, 0.10678763, 0.12507708, -0.26995188, 0.17373759, -0.17477675, 0.16042781, -0.3823496 , -0.21795925]) 参考： https://www.zybuluo.com/hanxiaoyang/note/472184?tdsourcetag=s_pctim_aiomsg https://blog.csdn.net/sinat_26917383/article/details/83041424 https://blog.csdn.net/sinat_26917383/article/details/83029140" />
<meta property="og:description" content="Word2Vec 环境： gensim 3.5.0 python 3.6.1 训练 import logging import os.path import sys import multiprocessing from gensim.corpora import WikiCorpus from gensim.models import Word2Vec from gensim.models.word2vec import LineSentence program = os.path.basename(&#39;train_word2vec_model&#39;) logger = logging.getLogger(program) logging.basicConfig(format=&#39;%(asctime)s: %(levelname)s: %(message)s&#39;) logging.root.setLevel(level=logging.INFO) logger.info(&quot;running %s&quot; % &#39; &#39;.join(sys.argv)) inp=&#39;./data/cleaned_tweets_text_160W.csv&#39; outp=&#39;Word2vec_model/word2vec_model_160w_200d&#39; n_dim=200 model = Word2Vec(LineSentence(inp), size=n_dim, window=5, min_count=5, workers=multiprocessing.cpu_count()) model.save(outp) 使用 import gensim outp=&#39;Word2vec_model/word2vec_model_160w_200d&#39; model = gensim.models.Word2Vec.load(outp) model[&#39;happy&#39;] array([ -7.10430890e-02, -5.29273868e-01, 4.72357810e-01, 6.88659430e-01, -2.42118329e-01, -9.04537499e-01, 3.61087114e-01, -3.96869183e-01, -1.67573178e+00, ... 1.18314767e+00, -1.24723041e+00, 1.19374382e+00, 3.74429256e-01, 1.68333733e+00], dtype=float32) result = model.most_similar(u&quot;sad&quot;,topn=10) for e in result: print(e[0],e[1]) sad&quot; 0.760986328125 upset 0.7119562029838562 sadd 0.7007699012756348 depressed 0.6737781763076782 bummed 0.6687890291213989 saad 0.6555933952331543 upsetting 0.6473105549812317 devastated 0.6356861591339111 disappointed 0.6215260624885559 heartbroken 0.5960309505462646 # man - woman = king - queen # man - woman = husband - wife # man - woman = boy - girl model.most_similar([&#39;woman&#39;, &#39;boy&#39;], [&#39;man&#39;], topn=1) [(‘girl’, 0.6189800500869751)] print (model.doesnt_match(u&quot;happy sad like desk&quot;.split())) desk fastText 环境： gensim 3.5.0 python 3.6.1 安装fasttext 可见：https://github.com/facebookresearch/fastText/tree/master/python $ git clone https://github.com/facebookresearch/fastText.git $ cd fastText $ pip install . 训练 from gensim.models import FastText from gensim.models.word2vec import LineSentence import logging import os.path import sys program = os.path.basename(&#39;fast-text&#39;) logger = logging.getLogger(program) logging.basicConfig(format=&#39;%(asctime)s: %(levelname)s: %(message)s&#39;) logging.root.setLevel(level=logging.INFO) logger.info(&quot;running %s&quot; % &#39; &#39;.join(sys.argv)) inp=&#39;./data/cleaned_tweets_text_160W.csv&#39; outp=&#39;fasttext_model/fasttext_model_160w_200d&#39; n_dim=200 model = FastText(LineSentence(inp), size=n_dim, window=5, min_count=5) model.save(outp) 使用 from gensim.models import FastText # 使用：模型得保存与加载 outp=&#39;../word-Vectorization/fasttext_model/fasttext_model_160w_200d&#39; model = FastText.load(outp) # 指定词条词向量 model[&#39;happy&#39;] array([ 8.10116410e-01, 5.25403082e-01, -3.36705118e-01, -4.42356855e-01, -2.04296446e+00, -9.93114054e-01, … 1.20443761e+00, -3.73263896e-01, 1.15821147e+00, -1.95465660e+00, 2.11104417e+00], dtype=float32) result = model.most_similar(u&quot;queeen&quot;) for e in result: print(e[0],e[1]) queen&quot; 0.8662827014923096 mcqueen 0.8575066924095154 queenie 0.8428698182106018 queens 0.7900538444519043 queer 0.7410856485366821 queezy 0.7156778573989868 queenstown 0.7068071365356445 quen 0.7057324647903442 queensland 0.7055833339691162 Glove 环境： python 3.6.1 官方glove:https://github.com/stanfordnlp/GloVe 笔者使用：https://github.com/maciejkula/glove-python 安装： pip install glove_python 训练 from __future__ import print_function import argparse import pprint import gensim from glove import Glove from glove import Corpus from gensim.models.word2vec import LineSentence inp=&#39;./data/processed_160Msemeval-2016-2017-task3-QAText&#39; outp=&#39;glove_model/QAText_200d&#39; n_dim=200 corpus_model = Corpus() corpus_model.fit(LineSentence(inp), window=5) #corpus_model.save(&#39;corpus.model&#39;) print(&#39;Dict size: %s&#39; % len(corpus_model.dictionary)) print(&#39;Collocations: %s&#39; % corpus_model.matrix.nnz) glove = Glove(no_components=n_dim, learning_rate=0.05) glove.fit(corpus_model.matrix, epochs=10, no_threads=1, verbose=True) glove.add_dictionary(corpus_model.dictionary) glove.save(outp) 使用 from glove import Glove outp=&#39;../word-Vectorization/glove_model/glove_model_160w_200d&#39; glove = Glove.load(outp) glove.most_similar(&#39;happiness&#39;, number=10) [(‘optimism’, 0.88474224538009472), (‘technology’, 0.87399374238355365), (‘photography’, 0.87293168487727013), (‘jquery’, 0.87160586331482937), (‘excitement’, 0.87015550639464156), (‘mailplane’, 0.86709197956363315), (‘secrets’, 0.86616698912523094), (‘laughter’, 0.86593359775060097), (‘records’, 0.86521431383969472)] # 指定词条词向量 glove.word_vectors[glove.dictionary[&#39;happy&#39;]] array([ 0.1824374 , -0.15493986, -0.23131742, -0.20251903, -0.25899053, 0.16043589, -0.11017494, -0.15413852, 0.12485044, 0.28871841, … 0.14323183, -0.21197602, -0.18841062, 0.32888953, -0.32943953, 0.15334943, -0.09995708, 0.10678763, 0.12507708, -0.26995188, 0.17373759, -0.17477675, 0.16042781, -0.3823496 , -0.21795925]) 参考： https://www.zybuluo.com/hanxiaoyang/note/472184?tdsourcetag=s_pctim_aiomsg https://blog.csdn.net/sinat_26917383/article/details/83041424 https://blog.csdn.net/sinat_26917383/article/details/83029140" />
<link rel="canonical" href="https://mlh.app/2019/05/18/787133.html" />
<meta property="og:url" content="https://mlh.app/2019/05/18/787133.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-18T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Word2Vec 环境： gensim 3.5.0 python 3.6.1 训练 import logging import os.path import sys import multiprocessing from gensim.corpora import WikiCorpus from gensim.models import Word2Vec from gensim.models.word2vec import LineSentence program = os.path.basename(&#39;train_word2vec_model&#39;) logger = logging.getLogger(program) logging.basicConfig(format=&#39;%(asctime)s: %(levelname)s: %(message)s&#39;) logging.root.setLevel(level=logging.INFO) logger.info(&quot;running %s&quot; % &#39; &#39;.join(sys.argv)) inp=&#39;./data/cleaned_tweets_text_160W.csv&#39; outp=&#39;Word2vec_model/word2vec_model_160w_200d&#39; n_dim=200 model = Word2Vec(LineSentence(inp), size=n_dim, window=5, min_count=5, workers=multiprocessing.cpu_count()) model.save(outp) 使用 import gensim outp=&#39;Word2vec_model/word2vec_model_160w_200d&#39; model = gensim.models.Word2Vec.load(outp) model[&#39;happy&#39;] array([ -7.10430890e-02, -5.29273868e-01, 4.72357810e-01, 6.88659430e-01, -2.42118329e-01, -9.04537499e-01, 3.61087114e-01, -3.96869183e-01, -1.67573178e+00, ... 1.18314767e+00, -1.24723041e+00, 1.19374382e+00, 3.74429256e-01, 1.68333733e+00], dtype=float32) result = model.most_similar(u&quot;sad&quot;,topn=10) for e in result: print(e[0],e[1]) sad&quot; 0.760986328125 upset 0.7119562029838562 sadd 0.7007699012756348 depressed 0.6737781763076782 bummed 0.6687890291213989 saad 0.6555933952331543 upsetting 0.6473105549812317 devastated 0.6356861591339111 disappointed 0.6215260624885559 heartbroken 0.5960309505462646 # man - woman = king - queen # man - woman = husband - wife # man - woman = boy - girl model.most_similar([&#39;woman&#39;, &#39;boy&#39;], [&#39;man&#39;], topn=1) [(‘girl’, 0.6189800500869751)] print (model.doesnt_match(u&quot;happy sad like desk&quot;.split())) desk fastText 环境： gensim 3.5.0 python 3.6.1 安装fasttext 可见：https://github.com/facebookresearch/fastText/tree/master/python $ git clone https://github.com/facebookresearch/fastText.git $ cd fastText $ pip install . 训练 from gensim.models import FastText from gensim.models.word2vec import LineSentence import logging import os.path import sys program = os.path.basename(&#39;fast-text&#39;) logger = logging.getLogger(program) logging.basicConfig(format=&#39;%(asctime)s: %(levelname)s: %(message)s&#39;) logging.root.setLevel(level=logging.INFO) logger.info(&quot;running %s&quot; % &#39; &#39;.join(sys.argv)) inp=&#39;./data/cleaned_tweets_text_160W.csv&#39; outp=&#39;fasttext_model/fasttext_model_160w_200d&#39; n_dim=200 model = FastText(LineSentence(inp), size=n_dim, window=5, min_count=5) model.save(outp) 使用 from gensim.models import FastText # 使用：模型得保存与加载 outp=&#39;../word-Vectorization/fasttext_model/fasttext_model_160w_200d&#39; model = FastText.load(outp) # 指定词条词向量 model[&#39;happy&#39;] array([ 8.10116410e-01, 5.25403082e-01, -3.36705118e-01, -4.42356855e-01, -2.04296446e+00, -9.93114054e-01, … 1.20443761e+00, -3.73263896e-01, 1.15821147e+00, -1.95465660e+00, 2.11104417e+00], dtype=float32) result = model.most_similar(u&quot;queeen&quot;) for e in result: print(e[0],e[1]) queen&quot; 0.8662827014923096 mcqueen 0.8575066924095154 queenie 0.8428698182106018 queens 0.7900538444519043 queer 0.7410856485366821 queezy 0.7156778573989868 queenstown 0.7068071365356445 quen 0.7057324647903442 queensland 0.7055833339691162 Glove 环境： python 3.6.1 官方glove:https://github.com/stanfordnlp/GloVe 笔者使用：https://github.com/maciejkula/glove-python 安装： pip install glove_python 训练 from __future__ import print_function import argparse import pprint import gensim from glove import Glove from glove import Corpus from gensim.models.word2vec import LineSentence inp=&#39;./data/processed_160Msemeval-2016-2017-task3-QAText&#39; outp=&#39;glove_model/QAText_200d&#39; n_dim=200 corpus_model = Corpus() corpus_model.fit(LineSentence(inp), window=5) #corpus_model.save(&#39;corpus.model&#39;) print(&#39;Dict size: %s&#39; % len(corpus_model.dictionary)) print(&#39;Collocations: %s&#39; % corpus_model.matrix.nnz) glove = Glove(no_components=n_dim, learning_rate=0.05) glove.fit(corpus_model.matrix, epochs=10, no_threads=1, verbose=True) glove.add_dictionary(corpus_model.dictionary) glove.save(outp) 使用 from glove import Glove outp=&#39;../word-Vectorization/glove_model/glove_model_160w_200d&#39; glove = Glove.load(outp) glove.most_similar(&#39;happiness&#39;, number=10) [(‘optimism’, 0.88474224538009472), (‘technology’, 0.87399374238355365), (‘photography’, 0.87293168487727013), (‘jquery’, 0.87160586331482937), (‘excitement’, 0.87015550639464156), (‘mailplane’, 0.86709197956363315), (‘secrets’, 0.86616698912523094), (‘laughter’, 0.86593359775060097), (‘records’, 0.86521431383969472)] # 指定词条词向量 glove.word_vectors[glove.dictionary[&#39;happy&#39;]] array([ 0.1824374 , -0.15493986, -0.23131742, -0.20251903, -0.25899053, 0.16043589, -0.11017494, -0.15413852, 0.12485044, 0.28871841, … 0.14323183, -0.21197602, -0.18841062, 0.32888953, -0.32943953, 0.15334943, -0.09995708, 0.10678763, 0.12507708, -0.26995188, 0.17373759, -0.17477675, 0.16042781, -0.3823496 , -0.21795925]) 参考： https://www.zybuluo.com/hanxiaoyang/note/472184?tdsourcetag=s_pctim_aiomsg https://blog.csdn.net/sinat_26917383/article/details/83041424 https://blog.csdn.net/sinat_26917383/article/details/83029140","@type":"BlogPosting","url":"https://mlh.app/2019/05/18/787133.html","headline":"Word2Vec、fastText、Glove训练词向量","dateModified":"2019-05-18T00:00:00+08:00","datePublished":"2019-05-18T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/18/787133.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Word2Vec、fastText、Glove训练词向量</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-github-gist"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h2><a id="Word2Vec_0"></a>Word2Vec</h2> 
  <p>环境：<br> gensim 3.5.0<br> python 3.6.1</p> 
  <h3><a id="_5"></a>训练</h3> 
  <pre><code>import logging
import os.path
import sys
import multiprocessing
from gensim.corpora import WikiCorpus
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence

program = os.path.basename('train_word2vec_model')
logger = logging.getLogger(program)
logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
logging.root.setLevel(level=logging.INFO)
logger.info("running %s" % ' '.join(sys.argv))

 inp='./data/cleaned_tweets_text_160W.csv'
 outp='Word2vec_model/word2vec_model_160w_200d'

n_dim=200
model = Word2Vec(LineSentence(inp), size=n_dim, window=5, min_count=5,
            workers=multiprocessing.cpu_count())
model.save(outp)
</code></pre> 
  <h3><a id="_31"></a>使用</h3> 
  <pre><code>import gensim
outp='Word2vec_model/word2vec_model_160w_200d'
model = gensim.models.Word2Vec.load(outp)
</code></pre> 
  <pre><code>model['happy']
</code></pre> 
  <pre>array([ -7.10430890e-02,  -5.29273868e-01,   4.72357810e-01,
         6.88659430e-01,  -2.42118329e-01,  -9.04537499e-01,
         3.61087114e-01,  -3.96869183e-01,  -1.67573178e+00,
         ...
         1.18314767e+00,  -1.24723041e+00,   1.19374382e+00,
         3.74429256e-01,   1.68333733e+00], dtype=float32)</pre> 
  <pre><code>result = model.most_similar(u"sad",topn=10)
for e in result:
    print(e[0],e[1])
</code></pre> 
  <p>sad" 0.760986328125<br> upset 0.7119562029838562<br> sadd 0.7007699012756348<br> depressed 0.6737781763076782<br> bummed 0.6687890291213989<br> saad 0.6555933952331543<br> upsetting 0.6473105549812317<br> devastated 0.6356861591339111<br> disappointed 0.6215260624885559<br> heartbroken 0.5960309505462646</p> 
  <pre><code># man - woman = king - queen 
# man - woman = husband - wife
# man - woman = boy - girl
model.most_similar(['woman', 'boy'], ['man'], topn=1)
</code></pre> 
  <p>[(‘girl’, 0.6189800500869751)]</p> 
  <pre><code>print (model.doesnt_match(u"happy sad like desk".split()))
</code></pre> 
  <p>desk</p> 
  <h2><a id="fastText_80"></a>fastText</h2> 
  <p>环境：<br> gensim 3.5.0<br> python 3.6.1</p> 
  <p><strong>安装fasttext</strong><br> 可见：<a href="https://github.com/facebookresearch/fastText/tree/master/python" rel="nofollow">https://github.com/facebookresearch/fastText/tree/master/python</a></p> 
  <pre><code>$ git clone https://github.com/facebookresearch/fastText.git
$ cd fastText
$ pip install .
</code></pre> 
  <h3><a id="_94"></a>训练</h3> 
  <pre><code>from gensim.models import FastText
from gensim.models.word2vec import LineSentence
import logging
import os.path
import sys

program = os.path.basename('fast-text')
logger = logging.getLogger(program)
logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')
logging.root.setLevel(level=logging.INFO)
logger.info("running %s" % ' '.join(sys.argv))

inp='./data/cleaned_tweets_text_160W.csv'
outp='fasttext_model/fasttext_model_160w_200d'

n_dim=200
model = FastText(LineSentence(inp), size=n_dim, window=5, min_count=5)
model.save(outp)
</code></pre> 
  <h3><a id="_119"></a>使用</h3> 
  <pre><code>from gensim.models import FastText
# 使用：模型得保存与加载
outp='../word-Vectorization/fasttext_model/fasttext_model_160w_200d'
model = FastText.load(outp)
</code></pre> 
  <pre><code># 指定词条词向量
model['happy']
</code></pre> 
  <p>array([ 8.10116410e-01, 5.25403082e-01, -3.36705118e-01,<br> -4.42356855e-01, -2.04296446e+00, -9.93114054e-01,<br> …<br> 1.20443761e+00, -3.73263896e-01, 1.15821147e+00,<br> -1.95465660e+00, 2.11104417e+00], dtype=float32)</p> 
  <pre><code>result = model.most_similar(u"queeen")
for e in result:
    print(e[0],e[1])
</code></pre> 
  <p>queen" 0.8662827014923096<br> mcqueen 0.8575066924095154<br> queenie 0.8428698182106018<br> queens 0.7900538444519043<br> queer 0.7410856485366821<br> queezy 0.7156778573989868<br> queenstown 0.7068071365356445<br> quen 0.7057324647903442<br> queensland 0.7055833339691162</p> 
  <h2><a id="Glove_155"></a>Glove</h2> 
  <p>环境：</p> 
  <p>python 3.6.1</p> 
  <p>官方glove:<a href="https://github.com/stanfordnlp/GloVe" rel="nofollow">https://github.com/stanfordnlp/GloVe</a><br> 笔者使用：<a href="https://github.com/maciejkula/glove-python" rel="nofollow">https://github.com/maciejkula/glove-python</a></p> 
  <p><strong>安装：</strong><br> pip install glove_python</p> 
  <h3><a id="_166"></a>训练</h3> 
  <pre><code>from __future__ import print_function
import argparse
import pprint
import gensim
from glove import Glove
from glove import Corpus
from gensim.models.word2vec import LineSentence

inp='./data/processed_160Msemeval-2016-2017-task3-QAText'
outp='glove_model/QAText_200d'

n_dim=200

corpus_model = Corpus()
corpus_model.fit(LineSentence(inp), window=5)
#corpus_model.save('corpus.model')
print('Dict size: %s' % len(corpus_model.dictionary))
print('Collocations: %s' % corpus_model.matrix.nnz)

glove = Glove(no_components=n_dim, learning_rate=0.05)
glove.fit(corpus_model.matrix, epochs=10,
          no_threads=1, verbose=True)
glove.add_dictionary(corpus_model.dictionary)

glove.save(outp)
</code></pre> 
  <h3><a id="_196"></a>使用</h3> 
  <pre><code>from glove import Glove

outp='../word-Vectorization/glove_model/glove_model_160w_200d'
glove = Glove.load(outp)
</code></pre> 
  <pre><code>glove.most_similar('happiness', number=10)
</code></pre> 
  <p>[(‘optimism’, 0.88474224538009472),<br> (‘technology’, 0.87399374238355365),<br> (‘photography’, 0.87293168487727013),<br> (‘jquery’, 0.87160586331482937),<br> (‘excitement’, 0.87015550639464156),<br> (‘mailplane’, 0.86709197956363315),<br> (‘secrets’, 0.86616698912523094),<br> (‘laughter’, 0.86593359775060097),<br> (‘records’, 0.86521431383969472)]</p> 
  <pre><code># 指定词条词向量
glove.word_vectors[glove.dictionary['happy']]
</code></pre> 
  <p>array([ 0.1824374 , -0.15493986, -0.23131742, -0.20251903, -0.25899053,<br> 0.16043589, -0.11017494, -0.15413852, 0.12485044, 0.28871841,<br> …<br> 0.14323183, -0.21197602, -0.18841062, 0.32888953, -0.32943953,<br> 0.15334943, -0.09995708, 0.10678763, 0.12507708, -0.26995188,<br> 0.17373759, -0.17477675, 0.16042781, -0.3823496 , -0.21795925])</p> 
  <p>参考：<br> <a href="https://www.zybuluo.com/hanxiaoyang/note/472184?tdsourcetag=s_pctim_aiomsg" rel="nofollow">https://www.zybuluo.com/hanxiaoyang/note/472184?tdsourcetag=s_pctim_aiomsg</a><br> <a href="https://blog.csdn.net/sinat_26917383/article/details/83041424" rel="nofollow">https://blog.csdn.net/sinat_26917383/article/details/83041424</a><br> <a href="https://blog.csdn.net/sinat_26917383/article/details/83029140" rel="nofollow">https://blog.csdn.net/sinat_26917383/article/details/83029140</a></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
