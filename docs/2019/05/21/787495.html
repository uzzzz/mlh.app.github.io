<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>PyTorch系列4 — 自动求导 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="PyTorch系列4 — 自动求导" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="参考： pytorch的自动求导机制 - 计算图的建立 （推荐）PyTorch学习总结(七)——自动求导机制 （推荐）pytorch学习笔记（三）：自动求导 ##我们将介绍PyTorch中的自动求导机制，自动求导是PyTorch中非常重要的特性，能够让我们避免手动去计算非常复杂的导数， # 这能够极大地减少我们构建模型的时间，，这也是其前身Torch这个框架不具备的特性 import torch from torch.autograd import Variable ##简单情况下的自动求导。“简单”体现在计算的结果都是标量，也就是一个数，我们对这个标量进行自动求导。 x = Variable(torch.Tensor([2]),requires_grad = True) print(&quot;x的值：&quot;,x.data) y = x + 2 z = y**2 + 3 print(&quot;z = y**2 + 3的值：&quot;,z) #使用自动求导 z.backward() print(&quot;x.grad：&quot;,x.grad) #表示z 对x 求导的结果，其中z = (x+2)^2 + 3 所以 z&#39; = 2(x + 2) 运行结果： ##在看一个复杂点的例子 x = Variable(torch.randn(10,20),requires_grad = True) y = Variable(torch.randn(10,5),requires_grad = True) w = Variable(torch.randn(20,5),requires_grad = True) out = torch.mean(y - torch.matmul(x,w)) # torch.matmul 是做矩阵乘法 out.backward() #得到 x 的梯度 print(&quot;x.grad：&quot;,x.grad) #得到y的梯度 print(&quot;y.grad：&quot;,y.grad) #得到 w 的梯度 print(&quot;w.grad：&quot;,w.grad) #上面的数学公式就更加复杂，，矩阵乘法之后对两个矩阵对应元素相乘，然后所有元素求平均。。使用PyTorch能够非常容易的得到x, y 和 w 的导数， # 因为深度学习中充满大量的矩阵运算，所以我们没有办法手动去求这些导数，，有了自动求导可以非常方便的解决网络更新的问题 运行结果： #复杂情况的自动求导 ##对多维数组自动求导机制 m = Variable(torch.FloatTensor([[2,3]]),requires_grad = True) #构建一个1 X 2 的矩阵 n = Variable(torch.zeros(1,2)) #构建一个相同大小的 0 矩阵 print(&quot;m：&quot;,m) print(&quot;n：&quot;,n) # 通过 m 中的值计算新的 n 中的值 n[0,0] = m[0,0]**2 n[0,1] = m[0,1]**3 print(&quot;通过 m 中的值计算新的 n 中的值后n 的值：&quot;,n) # 将上面的式子写成数学公式，可以得到：n = (n0,n1) = (m0^2, m1^3) = (2^2, 3^3) #下面我们直接对n进行反向求导，，也就是n 对 m 求导 #在PyTorch中，如果要调用自动求导，需要往backward()中传入一个参数，这个参数的形状和n 一样大，比如是（w0, w1），，其实也就是对不同变量分别求导，然后拼成一个向量 n.backward(torch.ones_like(n)) #将（w0, w1） 取成(1,1) print(&quot;m.grad：&quot;,m.grad) 运行结果： #多次自动求导 #通过调用backward我们可以进行一次自动求导，如果我们再调用一次backward，会发现程序报错，没有办法再做一次。 # 这是因为P有Torch默认做完一次自动求导之后，计算图就被丢弃了，所以两次自动求导需要手动设置一个东西，例子如下： x = Variable(torch.FloatTensor([3]),requires_grad = True) print(&quot;x：&quot;,x) y = x * 2 + x ** 2 +3 print(&quot;y = x * 2 + x ** 2 +3：&quot;,y) #设置 retain_graph 为True 来保留计算图 y.backward(retain_graph = True) print(&quot;第一次自动求导x.grad：&quot;,x.grad) #第一次自动求导x.grad： tensor([8.]) y.backward() #再做一次自动求导，这次不保留计算图 print(&quot;第二次自动求导x.grad：&quot;,x.grad) #第二次自动求导x.grad： tensor([16.]) #可以发现x 的梯度变成了16，因为这里做了两次自动求导，，所以第一次的梯度8 和 第二次的梯度8 加起来得到了16的结果 运行结果： #小练习 x = Variable(torch.FloatTensor([2,3]),requires_grad = True) k = Variable(torch.zeros(2)) k[0] = x[0]**2 + 3*x[1] k[1] = 2*x[0] + x[1]**2 print(&quot;k：&quot;,k) j = torch.zeros(2,2) k.backward(torch.FloatTensor([1,0]),retain_graph = True) j[0] = x.grad.data print(&quot;j[0]：&quot;,j[0]) x.grad.data.zero_() #归零之前求得的梯度 k.backward(torch.FloatTensor([0,1])) j[1] = x.grad.data print(&quot;j[1]：&quot;,j[1]) print(&quot;j：&quot;,j) 运行结果：" />
<meta property="og:description" content="参考： pytorch的自动求导机制 - 计算图的建立 （推荐）PyTorch学习总结(七)——自动求导机制 （推荐）pytorch学习笔记（三）：自动求导 ##我们将介绍PyTorch中的自动求导机制，自动求导是PyTorch中非常重要的特性，能够让我们避免手动去计算非常复杂的导数， # 这能够极大地减少我们构建模型的时间，，这也是其前身Torch这个框架不具备的特性 import torch from torch.autograd import Variable ##简单情况下的自动求导。“简单”体现在计算的结果都是标量，也就是一个数，我们对这个标量进行自动求导。 x = Variable(torch.Tensor([2]),requires_grad = True) print(&quot;x的值：&quot;,x.data) y = x + 2 z = y**2 + 3 print(&quot;z = y**2 + 3的值：&quot;,z) #使用自动求导 z.backward() print(&quot;x.grad：&quot;,x.grad) #表示z 对x 求导的结果，其中z = (x+2)^2 + 3 所以 z&#39; = 2(x + 2) 运行结果： ##在看一个复杂点的例子 x = Variable(torch.randn(10,20),requires_grad = True) y = Variable(torch.randn(10,5),requires_grad = True) w = Variable(torch.randn(20,5),requires_grad = True) out = torch.mean(y - torch.matmul(x,w)) # torch.matmul 是做矩阵乘法 out.backward() #得到 x 的梯度 print(&quot;x.grad：&quot;,x.grad) #得到y的梯度 print(&quot;y.grad：&quot;,y.grad) #得到 w 的梯度 print(&quot;w.grad：&quot;,w.grad) #上面的数学公式就更加复杂，，矩阵乘法之后对两个矩阵对应元素相乘，然后所有元素求平均。。使用PyTorch能够非常容易的得到x, y 和 w 的导数， # 因为深度学习中充满大量的矩阵运算，所以我们没有办法手动去求这些导数，，有了自动求导可以非常方便的解决网络更新的问题 运行结果： #复杂情况的自动求导 ##对多维数组自动求导机制 m = Variable(torch.FloatTensor([[2,3]]),requires_grad = True) #构建一个1 X 2 的矩阵 n = Variable(torch.zeros(1,2)) #构建一个相同大小的 0 矩阵 print(&quot;m：&quot;,m) print(&quot;n：&quot;,n) # 通过 m 中的值计算新的 n 中的值 n[0,0] = m[0,0]**2 n[0,1] = m[0,1]**3 print(&quot;通过 m 中的值计算新的 n 中的值后n 的值：&quot;,n) # 将上面的式子写成数学公式，可以得到：n = (n0,n1) = (m0^2, m1^3) = (2^2, 3^3) #下面我们直接对n进行反向求导，，也就是n 对 m 求导 #在PyTorch中，如果要调用自动求导，需要往backward()中传入一个参数，这个参数的形状和n 一样大，比如是（w0, w1），，其实也就是对不同变量分别求导，然后拼成一个向量 n.backward(torch.ones_like(n)) #将（w0, w1） 取成(1,1) print(&quot;m.grad：&quot;,m.grad) 运行结果： #多次自动求导 #通过调用backward我们可以进行一次自动求导，如果我们再调用一次backward，会发现程序报错，没有办法再做一次。 # 这是因为P有Torch默认做完一次自动求导之后，计算图就被丢弃了，所以两次自动求导需要手动设置一个东西，例子如下： x = Variable(torch.FloatTensor([3]),requires_grad = True) print(&quot;x：&quot;,x) y = x * 2 + x ** 2 +3 print(&quot;y = x * 2 + x ** 2 +3：&quot;,y) #设置 retain_graph 为True 来保留计算图 y.backward(retain_graph = True) print(&quot;第一次自动求导x.grad：&quot;,x.grad) #第一次自动求导x.grad： tensor([8.]) y.backward() #再做一次自动求导，这次不保留计算图 print(&quot;第二次自动求导x.grad：&quot;,x.grad) #第二次自动求导x.grad： tensor([16.]) #可以发现x 的梯度变成了16，因为这里做了两次自动求导，，所以第一次的梯度8 和 第二次的梯度8 加起来得到了16的结果 运行结果： #小练习 x = Variable(torch.FloatTensor([2,3]),requires_grad = True) k = Variable(torch.zeros(2)) k[0] = x[0]**2 + 3*x[1] k[1] = 2*x[0] + x[1]**2 print(&quot;k：&quot;,k) j = torch.zeros(2,2) k.backward(torch.FloatTensor([1,0]),retain_graph = True) j[0] = x.grad.data print(&quot;j[0]：&quot;,j[0]) x.grad.data.zero_() #归零之前求得的梯度 k.backward(torch.FloatTensor([0,1])) j[1] = x.grad.data print(&quot;j[1]：&quot;,j[1]) print(&quot;j：&quot;,j) 运行结果：" />
<link rel="canonical" href="https://mlh.app/2019/05/21/787495.html" />
<meta property="og:url" content="https://mlh.app/2019/05/21/787495.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-21T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"参考： pytorch的自动求导机制 - 计算图的建立 （推荐）PyTorch学习总结(七)——自动求导机制 （推荐）pytorch学习笔记（三）：自动求导 ##我们将介绍PyTorch中的自动求导机制，自动求导是PyTorch中非常重要的特性，能够让我们避免手动去计算非常复杂的导数， # 这能够极大地减少我们构建模型的时间，，这也是其前身Torch这个框架不具备的特性 import torch from torch.autograd import Variable ##简单情况下的自动求导。“简单”体现在计算的结果都是标量，也就是一个数，我们对这个标量进行自动求导。 x = Variable(torch.Tensor([2]),requires_grad = True) print(&quot;x的值：&quot;,x.data) y = x + 2 z = y**2 + 3 print(&quot;z = y**2 + 3的值：&quot;,z) #使用自动求导 z.backward() print(&quot;x.grad：&quot;,x.grad) #表示z 对x 求导的结果，其中z = (x+2)^2 + 3 所以 z&#39; = 2(x + 2) 运行结果： ##在看一个复杂点的例子 x = Variable(torch.randn(10,20),requires_grad = True) y = Variable(torch.randn(10,5),requires_grad = True) w = Variable(torch.randn(20,5),requires_grad = True) out = torch.mean(y - torch.matmul(x,w)) # torch.matmul 是做矩阵乘法 out.backward() #得到 x 的梯度 print(&quot;x.grad：&quot;,x.grad) #得到y的梯度 print(&quot;y.grad：&quot;,y.grad) #得到 w 的梯度 print(&quot;w.grad：&quot;,w.grad) #上面的数学公式就更加复杂，，矩阵乘法之后对两个矩阵对应元素相乘，然后所有元素求平均。。使用PyTorch能够非常容易的得到x, y 和 w 的导数， # 因为深度学习中充满大量的矩阵运算，所以我们没有办法手动去求这些导数，，有了自动求导可以非常方便的解决网络更新的问题 运行结果： #复杂情况的自动求导 ##对多维数组自动求导机制 m = Variable(torch.FloatTensor([[2,3]]),requires_grad = True) #构建一个1 X 2 的矩阵 n = Variable(torch.zeros(1,2)) #构建一个相同大小的 0 矩阵 print(&quot;m：&quot;,m) print(&quot;n：&quot;,n) # 通过 m 中的值计算新的 n 中的值 n[0,0] = m[0,0]**2 n[0,1] = m[0,1]**3 print(&quot;通过 m 中的值计算新的 n 中的值后n 的值：&quot;,n) # 将上面的式子写成数学公式，可以得到：n = (n0,n1) = (m0^2, m1^3) = (2^2, 3^3) #下面我们直接对n进行反向求导，，也就是n 对 m 求导 #在PyTorch中，如果要调用自动求导，需要往backward()中传入一个参数，这个参数的形状和n 一样大，比如是（w0, w1），，其实也就是对不同变量分别求导，然后拼成一个向量 n.backward(torch.ones_like(n)) #将（w0, w1） 取成(1,1) print(&quot;m.grad：&quot;,m.grad) 运行结果： #多次自动求导 #通过调用backward我们可以进行一次自动求导，如果我们再调用一次backward，会发现程序报错，没有办法再做一次。 # 这是因为P有Torch默认做完一次自动求导之后，计算图就被丢弃了，所以两次自动求导需要手动设置一个东西，例子如下： x = Variable(torch.FloatTensor([3]),requires_grad = True) print(&quot;x：&quot;,x) y = x * 2 + x ** 2 +3 print(&quot;y = x * 2 + x ** 2 +3：&quot;,y) #设置 retain_graph 为True 来保留计算图 y.backward(retain_graph = True) print(&quot;第一次自动求导x.grad：&quot;,x.grad) #第一次自动求导x.grad： tensor([8.]) y.backward() #再做一次自动求导，这次不保留计算图 print(&quot;第二次自动求导x.grad：&quot;,x.grad) #第二次自动求导x.grad： tensor([16.]) #可以发现x 的梯度变成了16，因为这里做了两次自动求导，，所以第一次的梯度8 和 第二次的梯度8 加起来得到了16的结果 运行结果： #小练习 x = Variable(torch.FloatTensor([2,3]),requires_grad = True) k = Variable(torch.zeros(2)) k[0] = x[0]**2 + 3*x[1] k[1] = 2*x[0] + x[1]**2 print(&quot;k：&quot;,k) j = torch.zeros(2,2) k.backward(torch.FloatTensor([1,0]),retain_graph = True) j[0] = x.grad.data print(&quot;j[0]：&quot;,j[0]) x.grad.data.zero_() #归零之前求得的梯度 k.backward(torch.FloatTensor([0,1])) j[1] = x.grad.data print(&quot;j[1]：&quot;,j[1]) print(&quot;j：&quot;,j) 运行结果：","@type":"BlogPosting","url":"https://mlh.app/2019/05/21/787495.html","headline":"PyTorch系列4 — 自动求导","dateModified":"2019-05-21T00:00:00+08:00","datePublished":"2019-05-21T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/21/787495.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>PyTorch系列4 --- 自动求导</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p>参考：</p> 
  <ul> 
   <li><a href="https://www.cnblogs.com/catnip/p/8760780.html" rel="nofollow">pytorch的自动求导机制 - 计算图的建立</a></li> 
   <li><a href="https://blog.csdn.net/manong_wxd/article/details/78734358" rel="nofollow">（推荐）PyTorch学习总结(七)——自动求导机制</a></li> 
   <li><a href="https://blog.csdn.net/u012436149/article/details/66971822" rel="nofollow">（推荐）pytorch学习笔记（三）：自动求导</a></li> 
  </ul> 
  <pre><code>##我们将介绍PyTorch中的自动求导机制，自动求导是PyTorch中非常重要的特性，能够让我们避免手动去计算非常复杂的导数，
# 这能够极大地减少我们构建模型的时间，，这也是其前身Torch这个框架不具备的特性
import torch
from torch.autograd import Variable
##简单情况下的自动求导。“简单”体现在计算的结果都是标量，也就是一个数，我们对这个标量进行自动求导。
x = Variable(torch.Tensor([2]),requires_grad = True)
print("x的值：",x.data)
y = x + 2
z = y**2 + 3
print("z = y**2 + 3的值：",z)
#使用自动求导
z.backward()
print("x.grad：",x.grad) #表示z 对x 求导的结果，其中z = (x+2)^2 + 3  所以 z' = 2(x + 2)
</code></pre> 
  <p>运行结果：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190521174702276." alt="在这里插入图片描述"></p> 
  <pre><code>##在看一个复杂点的例子
x = Variable(torch.randn(10,20),requires_grad = True)
y = Variable(torch.randn(10,5),requires_grad = True)
w = Variable(torch.randn(20,5),requires_grad = True)
out = torch.mean(y - torch.matmul(x,w)) # torch.matmul 是做矩阵乘法
out.backward()
#得到 x 的梯度
print("x.grad：",x.grad)
#得到y的梯度
print("y.grad：",y.grad)
#得到 w 的梯度
print("w.grad：",w.grad)
#上面的数学公式就更加复杂，，矩阵乘法之后对两个矩阵对应元素相乘，然后所有元素求平均。。使用PyTorch能够非常容易的得到x, y 和 w 的导数，
# 因为深度学习中充满大量的矩阵运算，所以我们没有办法手动去求这些导数，，有了自动求导可以非常方便的解决网络更新的问题

</code></pre> 
  <p>运行结果：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019052118280222.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1MDMzNTg3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190521182848543.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1MDMzNTg3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190521182919863.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1MDMzNTg3,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p> 
  <pre><code>#复杂情况的自动求导
##对多维数组自动求导机制
m = Variable(torch.FloatTensor([[2,3]]),requires_grad = True) #构建一个1 X 2 的矩阵
n = Variable(torch.zeros(1,2)) #构建一个相同大小的 0 矩阵
print("m：",m)
print("n：",n)
# 通过 m 中的值计算新的 n 中的值
n[0,0] = m[0,0]**2
n[0,1] = m[0,1]**3
print("通过 m 中的值计算新的 n 中的值后n 的值：",n)
# 将上面的式子写成数学公式，可以得到：n = (n0,n1) = (m0^2, m1^3) = (2^2, 3^3)
#下面我们直接对n进行反向求导，，也就是n 对 m 求导
#在PyTorch中，如果要调用自动求导，需要往backward()中传入一个参数，这个参数的形状和n 一样大，比如是（w0, w1），，其实也就是对不同变量分别求导，然后拼成一个向量
n.backward(torch.ones_like(n)) #将（w0, w1） 取成(1,1)
print("m.grad：",m.grad)
</code></pre> 
  <p>运行结果：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190521185314869." alt="在这里插入图片描述"></p> 
  <pre><code>#多次自动求导
#通过调用backward我们可以进行一次自动求导，如果我们再调用一次backward，会发现程序报错，没有办法再做一次。
# 这是因为P有Torch默认做完一次自动求导之后，计算图就被丢弃了，所以两次自动求导需要手动设置一个东西，例子如下：
x = Variable(torch.FloatTensor([3]),requires_grad = True)
print("x：",x)
y = x * 2 + x ** 2 +3
print("y = x * 2 + x ** 2 +3：",y)
#设置 retain_graph 为True 来保留计算图
y.backward(retain_graph = True)
print("第一次自动求导x.grad：",x.grad) #第一次自动求导x.grad： tensor([8.])
y.backward() #再做一次自动求导，这次不保留计算图
print("第二次自动求导x.grad：",x.grad) #第二次自动求导x.grad： tensor([16.])
#可以发现x 的梯度变成了16，因为这里做了两次自动求导，，所以第一次的梯度8 和 第二次的梯度8 加起来得到了16的结果
</code></pre> 
  <p>运行结果：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190521191003724." alt="在这里插入图片描述"></p> 
  <pre><code>#小练习
x = Variable(torch.FloatTensor([2,3]),requires_grad = True)
k = Variable(torch.zeros(2))
k[0] = x[0]**2 + 3*x[1]
k[1] = 2*x[0] + x[1]**2
print("k：",k)

j = torch.zeros(2,2)
k.backward(torch.FloatTensor([1,0]),retain_graph = True)
j[0] = x.grad.data
print("j[0]：",j[0])
x.grad.data.zero_() #归零之前求得的梯度
k.backward(torch.FloatTensor([0,1]))
j[1] = x.grad.data
print("j[1]：",j[1])
print("j：",j)
</code></pre> 
  <p>运行结果：<br> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190521192245668." alt="在这里插入图片描述"></p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
