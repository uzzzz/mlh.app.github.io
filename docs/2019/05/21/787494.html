<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>深度学习 神经网络中的前向传播和反向传播算法推导 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="深度学习 神经网络中的前向传播和反向传播算法推导" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. 神经网络 这是一个常见的神经网络的图： 这是一个常见的三层神经网络的基本构成，Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，当我们输入x1,x2,x3等数据时，通过隐含层的计算、转换，输出你的期望，当你的输入和输出是一样的时候，成为自编码模型（Auto-Encoder）,而当你输入和输出是不一致的时候，也就是我们常说的人工神经网络。 2. &nbsp;如何计算传播 首先我们先构建一个简单的网络层作为例子： 在这个网络层中有 第一层输入层：里面包含神经元i1,i2，截距：b1，权重：w1,w2,w3,w4 第二层是隐含层：里面包含h1,h2，截距：b2，权重：w5,w6,w7,w8 第三层是输出层：里面包含o1,o2 我们使用sigmoid作为激活函数 PS:如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 假定我们输入数据 i1: 0.02 i2: 0.04&nbsp;截距 b1:0.4 b2:0.7 期望的输出数据 o1:0.5 &nbsp;o2:0.9 未知的是权重w1,w2,w3,w4,w5,w6,w7,w8 我们的目的 是为了能的到o1:0.5 o2:0.9的期望的值，计算出w1,w2,w3....w8的权重值 先假如构造一个权重w1,w2,w3.....w8的值，通过计算获取到最佳的w1,w2,w3....w8的权重 权重的初使值： w1=0.25 w2=0.25 w3=0.15 w4=0.20 w5=0.30 w6=0.35 w7=0.40 w8=0.35 1 2.1 前向传播 2.1.1 输入层到隐含层 NET(h1)=w1*i1+w2*i2+b1=0.25*0.02+0.25*0.04+0.4=0.005+0.01+0.4=0.415 神经元h1到输出h1的激活函数是sigmoid OUT(h1)=1/(1+e^(-NET(h1)))=1/(1+0.660340281)=0.602286177 同理我们也可以获取OUT(h2)的值 NET(h2)=w3*i1+w4*i2+b1=0.15*0.02+0.20*0.04+0.4=0.003+0.008+0.4=0.411 OUT(h2)=1/(1+e^(-NET(h2)))=1/(1+0.662986932)=0.601327636 2.1.2 从隐含层到输出层 计算输出层的神经元o1, o2的值，计算方法和输出层到隐含层类似 NET(o1)=w5*h1+w6*h2+b2=0.3*0.602286177+0.35*0.601327636+0.7=0.180685853+0.210464672+0.7=1.091150525 OUT(o1)=1/(1+e^(-NET(o1)))=1/(1+0.335829891)=0.748598311 同理 NET(o2)=w7*h1+w8*h2+b2=0.4*0.602286177+0.35*0.601327636+0.7=0.240914471+0.210464672+0.7=1.151379143 OUT(o2)=1/(1+e^(-NET(o2)))=1/1.316200383=0.759762733 o1:0.748598311 &nbsp; o2:0.759762733 距离我们期望的 o1:0.5 &nbsp;o2:0.9还是有很大的距离 2.2 计算总误差 公式： 也就是我们需要计算每个期望误差的和 E(total)= E(o0)+E(o1)=(1/2)*(0.748598311-0.5)^2+(1/2)*(0.759762733-0.9)^2=0.01545028+0.009833246=0.025283526 2.3 反向传播 每一个权重对误差的影响，我们可以通过下图更直观的看清楚误差的反向传播 2.3.1 隐含层到输出层的权值更新 隐含层到输出层的权值，在上面的例子里是W5,W6,W7,W8 我们以W6参数为例子，计算W6对整体误差的影响有多大，可以使用整体误差对W6参数求偏导： 很明显并没有W6对Etotal的计算公式，我们只有W6对Net(o1)的计算公式 但根据偏导数的链式法则，我们可以将我们存在的推导公式进行链式乘法 我们来计算每一个公式的偏导： 计算： 这是一个复合函数的导数 设{\displaystyle f}和{\displaystyle g}为两个关于{\displaystyle x}可导函数，则复合函数{\displaystyle (f\circ g)(x)}的导数{\displaystyle (f\circ g)&#39;(x)}为： {\displaystyle (f\circ g)&#39;(x)=f&#39;(g(x))g&#39;(x).} 这里g(x)=target(o1)-out(o1) &nbsp;g&#39;(x)=-1 =-(0.5-0.748598311)=0.248598311 计算 已知 我们来推导一下 ： 还是复合函数的推导 最后推导的结果： =0.748598311*(1-0.748598311)=0.251401689*0.748598311=0.18819888 计算 也就是 net(o1)&#39;=out(h2)=0.601327636 最后我们的公式 = *out(h2) = 0.248598311*0.18819888*0.601327636=0.028133669 2.3.1.1 跟新W6的权重 W6=W6-x* 其中 x 就是我们常说的学习速率，设置x学习速率为0.1 那么新的w6的权重就是 0.35-0.1* 0.028133669=0.347186633 相同的道理，我们也可以计算新的W5,W6,W7,W8的权重 可是如何计算和跟新W1,W2,W3,W4的权重呢？ 2.3.2 隐含层的权值跟新 大概的算法还是和前面类似，如下图所展示： 计算公式： 2.3.2.1 计算 对Out(h1)来说Etotal并不依赖于Out(h1)计算，需要将Total分拆成两个Eo1和Eo2来计算 公式如下： 接着推导公式： 计算 同理也可以计算 2.3.2.2 计算 2.3.2.3 计算 最后三者相乘： 2.3.2.4 整体公式 根据前面的公式，我们可以推导出最后的公式 2.3.2.4 跟新W1的权重 和计算W6的权重一样： 设置学习速率，计算的到w1的权重值 3. 计算获取最佳的权重 我们将获取的新的权重不停的迭代，迭代一定的次数后直到接近期望值 o1:0.5 o2:0.9后，所的到权重w1...w8，就是所需要的权重。 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. 神经网络 这是一个常见的神经网络的图： 这是一个常见的三层神经网络的基本构成，Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，当我们输入x1,x2,x3等数据时，通过隐含层的计算、转换，输出你的期望，当你的输入和输出是一样的时候，成为自编码模型（Auto-Encoder）,而当你输入和输出是不一致的时候，也就是我们常说的人工神经网络。 2. &nbsp;如何计算传播 首先我们先构建一个简单的网络层作为例子： 在这个网络层中有 第一层输入层：里面包含神经元i1,i2，截距：b1，权重：w1,w2,w3,w4 第二层是隐含层：里面包含h1,h2，截距：b2，权重：w5,w6,w7,w8 第三层是输出层：里面包含o1,o2 我们使用sigmoid作为激活函数 PS:如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 假定我们输入数据 i1: 0.02 i2: 0.04&nbsp;截距 b1:0.4 b2:0.7 期望的输出数据 o1:0.5 &nbsp;o2:0.9 未知的是权重w1,w2,w3,w4,w5,w6,w7,w8 我们的目的 是为了能的到o1:0.5 o2:0.9的期望的值，计算出w1,w2,w3....w8的权重值 先假如构造一个权重w1,w2,w3.....w8的值，通过计算获取到最佳的w1,w2,w3....w8的权重 权重的初使值： w1=0.25 w2=0.25 w3=0.15 w4=0.20 w5=0.30 w6=0.35 w7=0.40 w8=0.35 1 2.1 前向传播 2.1.1 输入层到隐含层 NET(h1)=w1*i1+w2*i2+b1=0.25*0.02+0.25*0.04+0.4=0.005+0.01+0.4=0.415 神经元h1到输出h1的激活函数是sigmoid OUT(h1)=1/(1+e^(-NET(h1)))=1/(1+0.660340281)=0.602286177 同理我们也可以获取OUT(h2)的值 NET(h2)=w3*i1+w4*i2+b1=0.15*0.02+0.20*0.04+0.4=0.003+0.008+0.4=0.411 OUT(h2)=1/(1+e^(-NET(h2)))=1/(1+0.662986932)=0.601327636 2.1.2 从隐含层到输出层 计算输出层的神经元o1, o2的值，计算方法和输出层到隐含层类似 NET(o1)=w5*h1+w6*h2+b2=0.3*0.602286177+0.35*0.601327636+0.7=0.180685853+0.210464672+0.7=1.091150525 OUT(o1)=1/(1+e^(-NET(o1)))=1/(1+0.335829891)=0.748598311 同理 NET(o2)=w7*h1+w8*h2+b2=0.4*0.602286177+0.35*0.601327636+0.7=0.240914471+0.210464672+0.7=1.151379143 OUT(o2)=1/(1+e^(-NET(o2)))=1/1.316200383=0.759762733 o1:0.748598311 &nbsp; o2:0.759762733 距离我们期望的 o1:0.5 &nbsp;o2:0.9还是有很大的距离 2.2 计算总误差 公式： 也就是我们需要计算每个期望误差的和 E(total)= E(o0)+E(o1)=(1/2)*(0.748598311-0.5)^2+(1/2)*(0.759762733-0.9)^2=0.01545028+0.009833246=0.025283526 2.3 反向传播 每一个权重对误差的影响，我们可以通过下图更直观的看清楚误差的反向传播 2.3.1 隐含层到输出层的权值更新 隐含层到输出层的权值，在上面的例子里是W5,W6,W7,W8 我们以W6参数为例子，计算W6对整体误差的影响有多大，可以使用整体误差对W6参数求偏导： 很明显并没有W6对Etotal的计算公式，我们只有W6对Net(o1)的计算公式 但根据偏导数的链式法则，我们可以将我们存在的推导公式进行链式乘法 我们来计算每一个公式的偏导： 计算： 这是一个复合函数的导数 设{\displaystyle f}和{\displaystyle g}为两个关于{\displaystyle x}可导函数，则复合函数{\displaystyle (f\circ g)(x)}的导数{\displaystyle (f\circ g)&#39;(x)}为： {\displaystyle (f\circ g)&#39;(x)=f&#39;(g(x))g&#39;(x).} 这里g(x)=target(o1)-out(o1) &nbsp;g&#39;(x)=-1 =-(0.5-0.748598311)=0.248598311 计算 已知 我们来推导一下 ： 还是复合函数的推导 最后推导的结果： =0.748598311*(1-0.748598311)=0.251401689*0.748598311=0.18819888 计算 也就是 net(o1)&#39;=out(h2)=0.601327636 最后我们的公式 = *out(h2) = 0.248598311*0.18819888*0.601327636=0.028133669 2.3.1.1 跟新W6的权重 W6=W6-x* 其中 x 就是我们常说的学习速率，设置x学习速率为0.1 那么新的w6的权重就是 0.35-0.1* 0.028133669=0.347186633 相同的道理，我们也可以计算新的W5,W6,W7,W8的权重 可是如何计算和跟新W1,W2,W3,W4的权重呢？ 2.3.2 隐含层的权值跟新 大概的算法还是和前面类似，如下图所展示： 计算公式： 2.3.2.1 计算 对Out(h1)来说Etotal并不依赖于Out(h1)计算，需要将Total分拆成两个Eo1和Eo2来计算 公式如下： 接着推导公式： 计算 同理也可以计算 2.3.2.2 计算 2.3.2.3 计算 最后三者相乘： 2.3.2.4 整体公式 根据前面的公式，我们可以推导出最后的公式 2.3.2.4 跟新W1的权重 和计算W6的权重一样： 设置学习速率，计算的到w1的权重值 3. 计算获取最佳的权重 我们将获取的新的权重不停的迭代，迭代一定的次数后直到接近期望值 o1:0.5 o2:0.9后，所的到权重w1...w8，就是所需要的权重。 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/21/787494.html" />
<meta property="og:url" content="https://mlh.app/2019/05/21/787494.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-21T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1. 神经网络 这是一个常见的神经网络的图： 这是一个常见的三层神经网络的基本构成，Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，当我们输入x1,x2,x3等数据时，通过隐含层的计算、转换，输出你的期望，当你的输入和输出是一样的时候，成为自编码模型（Auto-Encoder）,而当你输入和输出是不一致的时候，也就是我们常说的人工神经网络。 2. &nbsp;如何计算传播 首先我们先构建一个简单的网络层作为例子： 在这个网络层中有 第一层输入层：里面包含神经元i1,i2，截距：b1，权重：w1,w2,w3,w4 第二层是隐含层：里面包含h1,h2，截距：b2，权重：w5,w6,w7,w8 第三层是输出层：里面包含o1,o2 我们使用sigmoid作为激活函数 PS:如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 假定我们输入数据 i1: 0.02 i2: 0.04&nbsp;截距 b1:0.4 b2:0.7 期望的输出数据 o1:0.5 &nbsp;o2:0.9 未知的是权重w1,w2,w3,w4,w5,w6,w7,w8 我们的目的 是为了能的到o1:0.5 o2:0.9的期望的值，计算出w1,w2,w3....w8的权重值 先假如构造一个权重w1,w2,w3.....w8的值，通过计算获取到最佳的w1,w2,w3....w8的权重 权重的初使值： w1=0.25 w2=0.25 w3=0.15 w4=0.20 w5=0.30 w6=0.35 w7=0.40 w8=0.35 1 2.1 前向传播 2.1.1 输入层到隐含层 NET(h1)=w1*i1+w2*i2+b1=0.25*0.02+0.25*0.04+0.4=0.005+0.01+0.4=0.415 神经元h1到输出h1的激活函数是sigmoid OUT(h1)=1/(1+e^(-NET(h1)))=1/(1+0.660340281)=0.602286177 同理我们也可以获取OUT(h2)的值 NET(h2)=w3*i1+w4*i2+b1=0.15*0.02+0.20*0.04+0.4=0.003+0.008+0.4=0.411 OUT(h2)=1/(1+e^(-NET(h2)))=1/(1+0.662986932)=0.601327636 2.1.2 从隐含层到输出层 计算输出层的神经元o1, o2的值，计算方法和输出层到隐含层类似 NET(o1)=w5*h1+w6*h2+b2=0.3*0.602286177+0.35*0.601327636+0.7=0.180685853+0.210464672+0.7=1.091150525 OUT(o1)=1/(1+e^(-NET(o1)))=1/(1+0.335829891)=0.748598311 同理 NET(o2)=w7*h1+w8*h2+b2=0.4*0.602286177+0.35*0.601327636+0.7=0.240914471+0.210464672+0.7=1.151379143 OUT(o2)=1/(1+e^(-NET(o2)))=1/1.316200383=0.759762733 o1:0.748598311 &nbsp; o2:0.759762733 距离我们期望的 o1:0.5 &nbsp;o2:0.9还是有很大的距离 2.2 计算总误差 公式： 也就是我们需要计算每个期望误差的和 E(total)= E(o0)+E(o1)=(1/2)*(0.748598311-0.5)^2+(1/2)*(0.759762733-0.9)^2=0.01545028+0.009833246=0.025283526 2.3 反向传播 每一个权重对误差的影响，我们可以通过下图更直观的看清楚误差的反向传播 2.3.1 隐含层到输出层的权值更新 隐含层到输出层的权值，在上面的例子里是W5,W6,W7,W8 我们以W6参数为例子，计算W6对整体误差的影响有多大，可以使用整体误差对W6参数求偏导： 很明显并没有W6对Etotal的计算公式，我们只有W6对Net(o1)的计算公式 但根据偏导数的链式法则，我们可以将我们存在的推导公式进行链式乘法 我们来计算每一个公式的偏导： 计算： 这是一个复合函数的导数 设{\\displaystyle f}和{\\displaystyle g}为两个关于{\\displaystyle x}可导函数，则复合函数{\\displaystyle (f\\circ g)(x)}的导数{\\displaystyle (f\\circ g)&#39;(x)}为： {\\displaystyle (f\\circ g)&#39;(x)=f&#39;(g(x))g&#39;(x).} 这里g(x)=target(o1)-out(o1) &nbsp;g&#39;(x)=-1 =-(0.5-0.748598311)=0.248598311 计算 已知 我们来推导一下 ： 还是复合函数的推导 最后推导的结果： =0.748598311*(1-0.748598311)=0.251401689*0.748598311=0.18819888 计算 也就是 net(o1)&#39;=out(h2)=0.601327636 最后我们的公式 = *out(h2) = 0.248598311*0.18819888*0.601327636=0.028133669 2.3.1.1 跟新W6的权重 W6=W6-x* 其中 x 就是我们常说的学习速率，设置x学习速率为0.1 那么新的w6的权重就是 0.35-0.1* 0.028133669=0.347186633 相同的道理，我们也可以计算新的W5,W6,W7,W8的权重 可是如何计算和跟新W1,W2,W3,W4的权重呢？ 2.3.2 隐含层的权值跟新 大概的算法还是和前面类似，如下图所展示： 计算公式： 2.3.2.1 计算 对Out(h1)来说Etotal并不依赖于Out(h1)计算，需要将Total分拆成两个Eo1和Eo2来计算 公式如下： 接着推导公式： 计算 同理也可以计算 2.3.2.2 计算 2.3.2.3 计算 最后三者相乘： 2.3.2.4 整体公式 根据前面的公式，我们可以推导出最后的公式 2.3.2.4 跟新W1的权重 和计算W6的权重一样： 设置学习速率，计算的到w1的权重值 3. 计算获取最佳的权重 我们将获取的新的权重不停的迭代，迭代一定的次数后直到接近期望值 o1:0.5 o2:0.9后，所的到权重w1...w8，就是所需要的权重。 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/21/787494.html","headline":"深度学习 神经网络中的前向传播和反向传播算法推导","dateModified":"2019-05-21T00:00:00+08:00","datePublished":"2019-05-21T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/21/787494.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>深度学习 神经网络中的前向传播和反向传播算法推导</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <div class="markdown_views prism-tomorrow-night" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
   <div class="htmledit_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <h1><a></a><a target="_blank"></a>1. 神经网络</h1>
    <div>
     这是一个常见的神经网络的图：
    </div>
    <div>
     <img width="500" height="300" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170802162359968?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
     <br>
    </div>
    <div>
     这是一个常见的三层神经网络的基本构成，Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，当我们输入x1,x2,x3等数据时，通过隐含层的计算、转换，输出你的期望，当你的输入和输出是一样的时候，成为自编码模型（Auto-Encoder）,而当你输入和输出是不一致的时候，也就是我们常说的人工神经网络。
    </div>
    <p></p>
   </div>
   <h1><a></a><a target="_blank"></a>2. &nbsp;如何计算传播</h1>
   <div>
    首先我们先构建一个简单的网络层作为例子：
   </div>
   <div>
    <img width="500" height="300" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170807103200184?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <div>
    在这个网络层中有
   </div>
   <div>
    <ul>
     <li>第一层输入层：里面包含神经元i1,i2，截距：b1，权重：w1,w2,w3,w4</li>
     <li>第二层是隐含层：里面包含h1,h2，截距：b2，权重：w5,w6,w7,w8</li>
     <li>第三层是输出层：里面包含o1,o2</li>
    </ul>
    <div>
     我们使用sigmoid作为激活函数
    </div>
   </div>
   <div>
    <br>
   </div>
   <p>PS:如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <div>
    假定我们输入数据
    <strong><span>i1: 0.02 i2: 0.04</span></strong>&nbsp;截距
    <strong><span>b1:0.4 b2:0.7</span></strong> 期望的输出数据
    <strong><span>o1:0.5 &nbsp;o2:0.9</span></strong>
   </div>
   <div>
    未知的是权重w1,w2,w3,w4,w5,w6,w7,w8
   </div>
   <div>
    我们的目的
    <strong>是为了能的到o1:0.5 o2:0.9的期望的值</strong>，计算出w1,w2,w3....w8的权重值
   </div>
   <div>
    <br>
   </div>
   <div>
    先假如构造一个权重w1,w2,w3.....w8的值，通过计算获取到最佳的w1,w2,w3....w8的权重
   </div>
   <div>
    <br>
   </div>
   <div>
    权重的初使值：
   </div>
   <div>
    <pre class="prettyprint"><code class="language-html hljs xml has-numbering"></code>
     <ol class="hljs-ln">
      <li>
       <div class="hljs-ln-numbers">
        <div class="hljs-ln-line hljs-ln-n"></div>
       </div>
       <div class="hljs-ln-code">
        <div class="hljs-ln-line">
         w1=0.25
        </div>
       </div></li>
      <li>
       <div class="hljs-ln-numbers">
        <div class="hljs-ln-line hljs-ln-n"></div>
       </div>
       <div class="hljs-ln-code">
        <div class="hljs-ln-line">
         w2=0.25
        </div>
       </div></li>
      <li>
       <div class="hljs-ln-numbers">
        <div class="hljs-ln-line hljs-ln-n"></div>
       </div>
       <div class="hljs-ln-code">
        <div class="hljs-ln-line">
         w3=0.15
        </div>
       </div></li>
      <li>
       <div class="hljs-ln-numbers">
        <div class="hljs-ln-line hljs-ln-n"></div>
       </div>
       <div class="hljs-ln-code">
        <div class="hljs-ln-line">
         w4=0.20
        </div>
       </div></li>
      <li>
       <div class="hljs-ln-numbers">
        <div class="hljs-ln-line hljs-ln-n"></div>
       </div>
       <div class="hljs-ln-code">
        <div class="hljs-ln-line">
         w5=0.30
        </div>
       </div></li>
      <li>
       <div class="hljs-ln-numbers">
        <div class="hljs-ln-line hljs-ln-n"></div>
       </div>
       <div class="hljs-ln-code">
        <div class="hljs-ln-line">
         w6=0.35
        </div>
       </div></li>
      <li>
       <div class="hljs-ln-numbers">
        <div class="hljs-ln-line hljs-ln-n"></div>
       </div>
       <div class="hljs-ln-code">
        <div class="hljs-ln-line">
         w7=0.40
        </div>
       </div></li>
      <li>
       <div class="hljs-ln-numbers">
        <div class="hljs-ln-line hljs-ln-n"></div>
       </div>
       <div class="hljs-ln-code">
        <div class="hljs-ln-line">
         w8=0.35
        </div>
       </div></li>
     </ol>
     <div class="hljs-button {2}"></div>
     <ul class="pre-numbering">
      <li>1</li>
     </ul></pre>
    <br>
   </div>
   <h2><a></a><a target="_blank"></a>2.1 前向传播</h2>
   <h3><a></a><a target="_blank"></a>2.1.1 输入层到隐含层</h3>
   <div>
    NET(h1)=w1*i1+w2*i2+b1=0.25*0.02+0.25*0.04+0.4=0.005+0.01+0.4=0.415
   </div>
   <div>
    <br>
   </div>
   <div>
    神经元h1到输出h1的激活函数是sigmoid
   </div>
   <div>
    <br>
   </div>
   <div>
    OUT(h1)=1/(1+e^(-NET(h1)))=1/(1+0.660340281)=0.602286177
   </div>
   <div>
    <br>
   </div>
   <div>
    同理我们也可以获取OUT(h2)的值
   </div>
   <div>
    NET(h2)=w3*i1+w4*i2+b1=0.15*0.02+0.20*0.04+0.4=0.003+0.008+0.4=0.411
    <br>
   </div>
   <div>
    OUT(h2)=1/(1+e^(-NET(h2)))=1/(1+0.662986932)=0.601327636
    <br>
   </div>
   <div>
    <br>
   </div>
   <h3><a></a><a target="_blank"></a>2.1.2 从隐含层到输出层</h3>
   <div>
    计算输出层的神经元o1, o2的值，计算方法和输出层到隐含层类似
   </div>
   <div>
    NET(o1)=w5*h1+w6*h2+b2=0.3*0.602286177+0.35*0.601327636+0.7=0.180685853+0.210464672+0.7=1.091150525
    <br>
   </div>
   <div>
    OUT(o1)=1/(1+e^(-NET(o1)))=1/(1+0.335829891)=0.748598311
   </div>
   <div>
    <br>
   </div>
   <div>
    同理
   </div>
   <div>
    <div>
     NET(o2)=w7*h1+w8*h2+b2=0.4*0.602286177+0.35*0.601327636+0.7=0.240914471+0.210464672+0.7=1.151379143
     <br>
    </div>
    <div>
     OUT(o2)=1/(1+e^(-NET(o2)))=1/1.316200383=0.759762733
    </div>
    <br>
   </div>
   <div>
    o1:0.748598311 &nbsp; o2:0.759762733 距离我们期望的
    <strong><span>o1:0.5 &nbsp;o2:0.9</span></strong>还是有很大的距离
   </div>
   <div>
    <br>
   </div>
   <div>
    <br>
   </div>
   <h2><a></a><a target="_blank"></a>2.2 计算总误差</h2>
   <div>
    公式：
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170807165058850?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <div>
    也就是我们需要计算每个期望误差的和
   </div>
   <div>
    E(total)= E(o0)+E(o1)=(1/2)*(0.748598311-0.5)^2+(1/2)*(0.759762733-0.9)^2=0.01545028+0.009833246=0.025283526
   </div>
   <div>
    <br>
   </div>
   <h2><a></a><a target="_blank"></a>2.3 反向传播</h2>
   <div>
    每一个权重对误差的影响，我们可以通过下图更直观的看清楚误差的反向传播
   </div>
   <div>
    <img width="600" height="300" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808094702276?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <h3><a></a><a target="_blank"></a>2.3.1 隐含层到输出层的权值更新</h3>
   <div>
    隐含层到输出层的权值，在上面的例子里是W5,W6,W7,W8
   </div>
   <div>
    <br>
   </div>
   <div>
    我们以W6参数为例子，计算W6对整体误差的影响有多大，可以使用整体误差对W6参数求偏导：
   </div>
   <div>
    <br>
   </div>
   <div>
    <p><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808092237178?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"><sub></sub></p>
    <p><br></p>
    <div>
     很明显并没有W6对Etotal的计算公式，我们只有W6对Net(o1)的计算公式
    </div>
    <div>
     但根据偏导数的链式法则，我们可以将我们存在的推导公式进行链式乘法
    </div>
    <div>
     <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808094353212?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
     <br>
    </div>
    <div>
     我们来计算每一个公式的偏导：
    </div>
    <div>
     <ul>
      <li>计算<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808102451195?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">：</li>
     </ul>
    </div>
    <div>
     <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808101501929?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
     <br>
    </div>
    <div>
     <br>
    </div>
    <div>
     这是一个复合函数的导数
    </div>
   </div>
   <div>
    <p>设<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y">{\displaystyle f}</span><img class="mwe-math-fallback-image-inline" alt="f" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61"></span>和<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y">{\displaystyle g}</span><img class="mwe-math-fallback-image-inline" alt="g" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d3556280e66fe2c0d0140df20935a6f057381d77"></span>为两个关于<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y">{\displaystyle x}</span><img class="mwe-math-fallback-image-inline" alt="x" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4"></span>可导函数，则复合函数<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y">{\displaystyle (f\circ g)(x)}</span><img class="mwe-math-fallback-image-inline" alt="(f\circ g)(x)" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/05fc249be3a7aa490b2e638f26b2f28cec7361f7"></span>的导数<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y">{\displaystyle (f\circ g)'(x)}</span><img class="mwe-math-fallback-image-inline" alt="(f\circ g)'(x)" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6f35d55ae5d082e26b902978bc9cf2f8fcea7906"></span>为：</p>
    <dl>
     <dd>
      <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y">{\displaystyle (f\circ g)'(x)=f'(g(x))g'(x).}</span><img class="mwe-math-fallback-image-inline" alt="(f\circ g)'(x)=f'(g(x))g'(x)." src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a82fffce04d3a2d25421ce718886bd0497f80132"></span>
     </dd>
    </dl>
    <br>
   </div>
   <div>
    这里g(x)=target(o1)-out(o1) &nbsp;g'(x)=-1
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808102638799?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <div>
    <strong><span>=-(0.5-0.748598311)=0.248598311</span></strong>
    <br>
   </div>
   <div>
    <br>
   </div>
   <div>
    <ul>
     <li>计算<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808103122110?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"></li>
    </ul>
    <div>
     已知
     <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808103243144?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
     <br>
    </div>
   </div>
   <div>
    <br>
   </div>
   <div>
    我们来推导一下
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808103122110?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">：
   </div>
   <div>
    还是复合函数的推导
   </div>
   <div></div>
   <img width="500" height="300" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808104349186?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808103428101?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <div>
    <br>
   </div>
   <div>
    最后推导的结果：
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808103746919?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
   </div>
   <div>
    <strong><span>=0.748598311*(1-0.748598311)=0.251401689*0.748598311=0.18819888</span></strong>
   </div>
   <div>
    <br>
   </div>
   <div>
    <ul>
     <li>计算<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808105122464?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"></li>
    </ul>
    <div>
     <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808105223748?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
     <br>
    </div>
   </div>
   <div>
    也就是
    <strong><span>net(o1)'=out(h2)=0.601327636</span></strong>
   </div>
   <div>
    <br>
   </div>
   <div>
    最后我们的公式
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808092237178?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">=
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808110154056?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">*out(h2)
    <br>
   </div>
   <div>
    <br>
   </div>
   <div>
    <br>
   </div>
   <div>
    =
    <span><span>0.248598311*<strong><span>0.18819888*<strong><span>0.601327636=0.028133669</span></strong></span></strong></span></span>
   </div>
   <div>
    <br>
   </div>
   <h4>2.3.1.1 跟新W6的权重</h4>
   <div>
    W6=W6-x*
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808092237178?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
   </div>
   <div>
    其中 x 就是我们常说的学习速率，设置x学习速率为0.1 那么新的w6的权重就是
   </div>
   <div>
    <br>
   </div>
   <div>
    <strong><span>0.35-0.1*</span></strong>
    <span><strong>0.028133669=0.347186633</strong></span>
   </div>
   <div>
    <br>
   </div>
   <div>
    相同的道理，我们也可以计算新的W5,W6,W7,W8的权重
   </div>
   <div>
    <br>
   </div>
   <div>
    可是如何计算和跟新W1,W2,W3,W4的权重呢？
   </div>
   <div>
    <br>
   </div>
   <h3><a></a><a target="_blank"></a>2.3.2 隐含层的权值跟新</h3>
   <div>
    大概的算法还是和前面类似，如下图所展示：
   </div>
   <div>
    <br>
   </div>
   <div>
    <img width="400" height="300" alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808155805193?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <div>
    <br>
   </div>
   <div>
    计算公式：
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808162155864?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <h4>2.3.2.1 计算<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808162325719?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"></h4>
   <div>
    对Out(h1)来说Etotal并不依赖于Out(h1)计算，需要将Total分拆成两个Eo1和Eo2来计算
   </div>
   <div>
    公式如下：
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808160222005?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <div>
    接着推导公式：
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808161143876?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
   </div>
   <div>
    <ul>
     <li>计算<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808161704983?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"></li>
    </ul>
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808161302455?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808161344562?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808161425234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
   </div>
   <div>
    <ul>
     <li>同理也可以计算<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808161833204?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"></li>
    </ul>
   </div>
   <div>
    <br>
   </div>
   <h4>2.3.2.2 计算<img width="62" height="43" alt="" src="https://uzshare.com/_p?http://images2015.cnblogs.com/blog/853467/201606/853467-20160630155555562-1422254830.png"></h4>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808162434838?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <div>
    <br>
   </div>
   <h4>2.3.2.3 计算<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808162616981?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"></h4>
   <div>
    <br>
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808162646055?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808162711159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <div>
    <br>
   </div>
   <div>
    最后三者相乘：
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808162155864?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <h4>2.3.2.4 整体公式</h4>
   <div>
    根据前面的公式，我们可以推导出最后的公式
   </div>
   <div>
    <br>
   </div>
   <div>
    <br>
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808164256798?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <div>
    <br>
   </div>
   <h4>2.3.2.4 跟新W1的权重</h4>
   <div>
    和计算W6的权重一样：
   </div>
   <div>
    <img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170808164446640?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcmFpbnR1bmdsaQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
    <br>
   </div>
   <div>
    设置学习速率，计算的到w1的权重值
   </div>
   <div>
    <br>
   </div>
   <div>
    <br>
   </div>
   <h1><a></a><a target="_blank"></a>3. 计算获取最佳的权重</h1>
   <div>
    我们将获取的新的权重不停的迭代，迭代一定的次数后直到接近期望值
    <span>o1:0.5 o2:0.9</span>后，所的到权重w1...w8，就是所需要的权重。
   </div>
   <div>
    <br>
   </div>
   <div>
    &nbsp;
   </div>
   <div>
    <br>
   </div>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
