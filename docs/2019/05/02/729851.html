<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>医生再添新助手！深度学习诊断传染病 完整代码+实操 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="医生再添新助手！深度学习诊断传染病 完整代码+实操" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="作者 | Dipanjan (DJ) Sarkar 译者 | Monanfei 编辑 | Rachel、Jane 出品 | AI科技大本营（id：rgznai100） 【导读】文本基于深度学习和迁移学习方法，对疟疾等传染病检测问题进行了研究。作者对疟疾的检测原理以及迁移学习理论进行了介绍，并使用VGG-19预训练模型，进行了基于特征提取和基于参数微调的迁移学习实践。 前言 &quot;健康就是财富&quot;，这是一个老生常谈的话题，但不得不说这是一个真理。在这篇文章中，我们将研究如何利用AI技术来检测一种致命的疾病——疟疾。本文将提出一个低成本、高效率和高准确率的开源解决方案。本文有两个目的：1.了解疟疾的传染原因和其致命性；2、介绍如何运用深度学习有效检测疟疾。本章的主要内容如下： 开展本项目的动机 疟疾检测的方法 用深度学习检测疟疾 从头开始训练卷积神经网络（CNN） 利用预训练模型进行迁移学习 本文不是为了宣扬 AI 将要取代人类的工作，或者接管世界等论调，而是仅仅展示 AI 是如何用一种低成本、高效率和高准确率的方案，来帮助人类去检测和诊断疟疾，并尽量减少人工操作。 Python and TensorFlow — A great combo to build open-source deep learning solutions 在本文中，我们将使用 Python 和 tensorflow ，来构建一个强大的、可扩展的、有效的深度学习解决方案。这些工具都是免费并且开源的，这使得我们能够构建一个真正低成本、高效精准的解决方案，而且可以让每个人都可以轻松使用。让我们开始吧！ 动机 疟疾是经疟蚊叮咬而感染疟原虫所引起的虫媒传染病，疟疾最常通过受感染的雌性疟蚊来传播。虽然我们不必详细了解这种疾病，但是我们需要知道疟疾有五种常见的类型。下图展示了这种疾病的致死性在全球的分布情况。 Malaria Estimated Risk Heath Map (Source: treated.com) 从上图中可以明显看到，疟疾遍布全球，尤其是在热带区域分布密集。本项目就是基于这种疾病的特性和致命性来开展的，下面我们举个例子来说明。起初，如果你被一只受感染的蚊子叮咬了，那么蚊子所携带的寄生虫就会进入你的血液，并且开始摧毁你体内的携氧红细胞。通常来讲，你会在被疟蚊叮咬后的几天或几周内感到不适，一般会首先出现类似流感或者病毒感染的症状。然而，这些致命的寄生虫可以在你身体里完好地存活超过一年的时间，并且不产生任何其他症状！延迟接受正确的治疗，可能会导致并发症甚至死亡。因此，早期并有效的疟疾检测和排查可以挽救这些生命。 世界卫生组织（WHO）发布了几个关于疟疾的重要事实，详情见此。简而言之，世界上将近一半的人口面临疟疾风险，每年有超过2亿的疟疾病例，以及有大约40万人死于疟疾。这些事实让我们认识到，快速简单高效的疟疾检查是多么重要，这也是本文的动机所在。 疟疾检查的方法 文章《 Pre-trained convolutional neural networks as feature extractors toward improved Malaria parasite detection in thin blood smear images》（本文的数据和分析也是基于这篇文章）简要介绍了疟疾检测的几种方法，这些方法包括但是不限于厚薄血涂片检查、聚合酶链式反应（PCR）和快速诊断测试（RDT）。在本文中，我们没有对这些方法进行详细介绍，但是需要注意的一点是，后两种方法常常作为替代方案使用，尤其是在缺乏高质量显微镜服务的情况下。 我们将简要讨论基于血液涂片检测流程的标准疟疾诊断方法，首先感谢 Carlos Ariza 的博文，以及 Adrian Rosebrock 关于疟疾检查的文章，这两篇文章让我们对疟疾检查领域有了更为深入的了解。 A blood smear workflow for Malaria detection (Source) 根据上图所示的 WHO 的血液涂片检测流程，该工作包括在100倍放大倍数下对血涂片进行深入检查，其中人们需要从5000个细胞中，手动检测出含有寄生虫的红细胞。Rajaraman 等人的论文中更加详细的给出了相关的描述，如下所示： 厚血涂片有助于检测寄生虫的存在，而薄血涂片有助于识别引起感染的寄生虫种类（Centers for Disease Control and Prevention, 2012）。诊断准确性在很大程度上取决于人类的专业知识，并且可能受到观察者间的差异和观察者的可靠性所带来的不利影响，以及受到在疾病流行或资源受限的区域内的大规模诊断造成的负担所带来的不利影响（Mitiku，Mengistu＆Gelaw，2003）。替代技术，例如聚合酶链式反应（PCR）和快速诊断测试（RDT），也会被使用；但是PCR分析受到其性能的限制（Hommelsheim等，2014），而RDT在疾病流行地区的成本效益较低（Hawkes，Katsuva＆Masumbuko，2009）。 因此，传统的疟疾检测绝对是一个密集的手工过程，或许深度学习技术可以帮助它完成自动化。上文提到的这些内容为后文打下了基础。 用深度学习检测疟疾 手工诊断血液涂片，是一项重复且规律的工作，而且需要一定的专业知识来区分和统计被寄生的和未感染的细胞。如果某些地区的工作人员没有正确的专业知识，那么这种方法就不能很好地推广，并且会导致一些问题。现有工作已经取得了一些进展，包括利用最先进的图像处理和分析技术来提取手工设计的特征，并利用这些特性构建基于机器学习的分类模型。但是，由于手工设计的部分需要花费大量的时间，当有更多的数据可供训练时，模型却无法及时的进行扩展。 深度学习模型，或更具体地说，卷积神经网络（CNN）在各种计算机视觉任务中获得了非常好的效果。本文假设您已经对 CNN 有一定的了解，但是如果您并不了解 CNN ，可以通过这篇文章进行深入了解。简单来讲，CNN 最关键的层主要包括卷积层和池化层，如下图所示。 A typical CNN architeture (Source: deeplearning.net) 卷积层从数据中学习空间层级模式，这些模式具有平移不变性，因此卷积层能够学习图像的不同方面。例如，第一卷积层将学习诸如边缘和角落的微型局部模式，第二卷积层将基于第一层所提取的特征，来学习更大的图像模式，如此循序渐进。这使得 CNN 能够自动进行特征工程，并且学习有效的特征，这些特征对新的数据具有很好的泛化能力。池化层常用于下采样和降维。 因此，CNN 能够帮助我们实现自动化的和可扩展的特征工程。此外，在模型的末端接入密集层，能够使我们执行图像分类等任务。使用像CNN这样的深度学习模型，进行自动化的疟疾检测，可能是一个高效、低成本、可扩展的方案。特别是随着迁移学习的发展和预训练模型的共享，在数据量较少等限制条件下，深度学习模型也能取得很好的效果。 Rajaraman 等人的论文&nbsp;《Pre-trained convolutional neural networks as feature extractors toward improved parasite detection in thin blood smear images》利用 6 个预训练模型，在进行疟疾检测时取得了 95.9% 的准确率。本文的重点是从头开始尝试一些简单的 CNN 模型和一些预先训练的模型，并利用迁移学习来检验我们在同一数据集下得到的结果。本文将使用 Python 和 TensorFlow 框架来构建模型。 数据集的详情 首先感谢 Lister Hill 国家生物医学通信中心（LHNCBC）的研究人员（国家医学图书馆（NLM）的部门），他们仔细收集并注释了这个血涂片图像的数据集，数据中包含健康和感染这两种类型的血涂片图像。您可以从官方网站上下载这些图像。 实际上，他们开发了一款可以运行在标准安卓智能手机上的应用程序，该程序可以连接传统的光学显微镜 (Poostchi et al., 2018) 。他们从孟加拉国吉大港医学院附属医院进行拍照记录了样本集，其中包括150个恶性疟原虫感染的样本和 50 个健康的样本，每个样本都是经过 Giemsa 染色的薄血涂片。智能手机的内置摄像头可以捕获样本的每一个局部微观视图。来自泰国曼谷的玛希隆-牛津热带医学研究所的专业人员为这些图像进行了手动注释。让我们简要地看一下数据集结构。首先根据本文所使用的操作系统，我们需要安装一些基本的依赖项。 本文所使用的系统是云上的 Debian 系统，该系统配置有 GPU ，这能够加速我们模型的训练。首先安装依赖树，这能够方便我们查看目录结构。（sudo apt install tree） 从上图所示的目录结构中可以看到，我们的文件里包含两个文件夹，分别包含受感染的和健康的细胞图像。利用以下代码，我们可以进一步了解图像的总数是多少。 import osimport globbase_dir = os.path.join(&#39;./cell_images&#39;)infected_dir = os.path.join(base_dir,&#39;Parasitized&#39;)healthy_dir = os.path.join(base_dir,&#39;Uninfected&#39;)infected_files = glob.glob(infected_dir+&#39;/*.png&#39;)healthy_files = glob.glob(healthy_dir+&#39;/*.png&#39;)len(infected_files), len(healthy_files)# Output(13779, 13779) 从上述结果可以看到， 疟疾和非疟疾（未感染）的细胞图像的数据集均包含13779张图片，两个数据集的大小是相对平衡的。接下来我们将利用这些数据构建一个基于pandas的dataframe类型的数据，这对我们后续构建数据集很有帮助。 import numpy as npimport pandas as pdnp.random.seed(42)files_df = pd.DataFrame({ &#39;filename&#39;: infected_files + healthy_files, &#39;label&#39;: [&#39;malaria&#39;] * len(infected_files) + [&#39;healthy&#39;] * len(healthy_files)}).sample(frac=1, random_state=42).reset_index(drop=True)files_df.head() 构建和探索图像数据集 在构建深度学习模型之前，我们不仅需要训练数据，还需要未用于训练的数据来验证和测试模型的性能。本文采用 60:10:30 的比例来划分训练集、验证集和测试集。我们将使用训练集和验证集来训练模型，并利用测试集来检验模型的性能。 from sklearn.model_selection import train_test_splitfrom collections import Countertrain_files, test_files, train_labels, test_labels = train_test_split(files_df[&#39;filename&#39;].values, files_df[&#39;label&#39;].values, test_size=0.3, random_state=42) train_files, val_files, train_labels, val_labels = train_test_split(train_files, train_labels, test_size=0.1, random_state=42)print(train_files.shape, val_files.shape, test_files.shape)print(&#39;Train:&#39;, Counter(train_labels), &#39;\nVal:&#39;, Counter(val_labels), &#39;\nTest:&#39;, Counter(test_labels))# Output(17361,) (1929,) (8268,)Train: Counter({&#39;healthy&#39;: 8734, &#39;malaria&#39;: 8627}) Val: Counter({&#39;healthy&#39;: 970, &#39;malaria&#39;: 959}) Test: Counter({&#39;malaria&#39;: 4193, &#39;healthy&#39;: 4075}) 可以发现，由于血液来源、测试方法以及图像拍摄的方向不同，血液涂片和细胞的图像尺寸不尽相同。我们需要获取一些训练数据的统计信息，从而确定最优的图像尺寸（请注意，在这里我们完全没用到测试集！）。 import cv2from concurrent import futuresimport threadingdef get_img_shape_parallel(idx, img, total_imgs): if idx % 5000 == 0 or idx == (total_imgs - 1): print(&#39;{}: working on img num:{}&#39;.format(threading.current_thread().name,idx)) return cv2.imread(img).shape ex = futures.ThreadPoolExecutor(max_workers=None)data_inp = [(idx, img, len(train_files)) for idx, img in enumerate(train_files)]print(&#39;Starting Img shape computation:&#39;)train_img_dims_map = ex.map(get_img_shape_parallel, [record[0] for record in data_inp], [record[1] for record in data_inp], [record[2] for record in data_inp])train_img_dims = list(train_img_dims_map)print(&#39;Min Dimensions:&#39;, np.min(train_img_dims, axis=0)) print(&#39;Avg Dimensions:&#39;, np.mean(train_img_dims, axis=0))print(&#39;Median Dimensions:&#39;, np.median(train_img_dims, axis=0))print(&#39;Max Dimensions:&#39;, np.max(train_img_dims, axis=0))# OutputStarting Img shape computation:ThreadPoolExecutor-0_0: working on img num: 0ThreadPoolExecutor-0_17: working on img num: 5000ThreadPoolExecutor-0_15: working on img num: 10000ThreadPoolExecutor-0_1: working on img num: 15000ThreadPoolExecutor-0_7: working on img num: 17360Min Dimensions: [46 46 3]Avg Dimensions: [132.77311215 132.45757733 3.]Median Dimensions: [130. 130. 3.]Max Dimensions: [385 394 3] 我们采用了并行处理的策略来加速图像读取操作。基于汇总的统计信息，我们决定将每张图像的大小调整为125x125。现在让我们加载所有的图像，并把他们的大小都调整为上述固定的尺寸。 IMG_DIMS = (125, 125)def get_img_data_parallel(idx, img, total_imgs): if idx % 5000 == 0 or idx == (total_imgs - 1): print(&#39;{}: working on img num: {}&#39;.format(threading.current_thread().name,idx)) img = cv2.imread(img) img = cv2.resize(img, dsize=IMG_DIMS, interpolation=cv2.INTER_CUBIC) img = np.array(img, dtype=np.float32) return imgex = futures.ThreadPoolExecutor(max_workers=None)train_data_inp = [(idx, img, len(train_files)) for idx, img in enumerate(train_files)]val_data_inp = [(idx, img, len(val_files)) for idx, img in enumerate(val_files)]test_data_inp = [(idx, img, len(test_files)) for idx, img in enumerate(test_files)]print(&#39;Loading Train Images:&#39;)train_data_map = ex.map(get_img_data_parallel, [record[0] for record in train_data_inp], [record[1] for record in train_data_inp], [record[2] for record in train_data_inp])train_data = np.array(list(train_data_map))print(&#39;\nLoading Validation Images:&#39;)val_data_map = ex.map(get_img_data_parallel, [record[0] for record in val_data_inp], [record[1] for record in val_data_inp], [record[2] for record in val_data_inp])val_data = np.array(list(val_data_map))print(&#39;\nLoading Test Images:&#39;)test_data_map = ex.map(get_img_data_parallel, [record[0] for record in test_data_inp], [record[1] for record in test_data_inp], [record[2] for record in test_data_inp])test_data = np.array(list(test_data_map))train_data.shape, val_data.shape, test_data.shape # OutputLoading Train Images:ThreadPoolExecutor-1_0: working on img num: 0ThreadPoolExecutor-1_12: working on img num: 5000ThreadPoolExecutor-1_6: working on img num: 10000ThreadPoolExecutor-1_10: working on img num: 15000ThreadPoolExecutor-1_3: working on img num: 17360Loading Validation Images:ThreadPoolExecutor-1_13: working on img num: 0ThreadPoolExecutor-1_18: working on img num: 1928Loading Test Images:ThreadPoolExecutor-1_5: working on img num: 0ThreadPoolExecutor-1_19: working on img num: 5000ThreadPoolExecutor-1_8: working on img num: 8267((17361, 125, 125, 3), (1929, 125, 125, 3), (8268, 125, 125, 3)) 我们再次运用了并行处理策略来加速图像加载和尺寸调整的计算，如上面输出结果中展示的，我们最终得到了所需尺寸的图像张量。现在我们可以查看一些样本的细胞图像，从而从直观上认识一下我们的数据的情况。 import matplotlib.pyplot as plt%matplotlib inlineplt.figure(1 , figsize = (8 , 8))n = 0 for i in range(16): n += 1 r = np.random.randint(0 , train_data.shape[0] , 1) plt.subplot(4 , 4 , n) plt.subplots_adjust(hspace = 0.5 , wspace = 0.5) plt.imshow(train_data[r[0]]/255.) plt.title(&#39;{}&#39;.format(train_labels[r[0]])) plt.xticks([]) , plt.yticks([]) 从上面的样本图像可以看出，疟疾和健康细胞图像之间存在一些细微差别。我们将构建深度学习模型，通过不断训练来使模型尝试学习这些模式。在开始训练模型之前，我们先对模型的参数进行一些基本的设置。 BATCH_SIZE = 64NUM_CLASSES = 2EPOCHS = 25INPUT_SHAPE = (125, 125, 3)train_imgs_scaled = train_data / 255.val_imgs_scaled = val_data / 255.# encode text category labelsfrom sklearn.preprocessing import LabelEncoderle = LabelEncoder()le.fit(train_labels)train_labels_enc = le.transform(train_labels)val_labels_enc = le.transform(val_labels)print(train_labels[:6], train_labels_enc[:6])# Output[&#39;malaria&#39; &#39;malaria&#39; &#39;malaria&#39; &#39;healthy&#39; &#39;healthy&#39; &#39;malaria&#39;][1 1 1 0 0 1] 上面的代码设定了图像的维度，批尺寸，epoch 的次数，并且对我们的类别标签进行了编码。TensorFLow 2.0 alpha 版本在2019年3月发布，它为我们项目的实施提供了一个完美的接口。 import tensorflow as tf# Load the TensorBoard notebook extension (optional)%load_ext tensorboard.notebooktf.random.set_seed(42)tf.__version__# Output&#39;2.0.0-alpha0&#39; 深度学习模型的训练阶段 在模型训练阶段，我们将构建几个深度学习模型，利用前面构建的训练集进行训练，并在验证集上比较它们的性能。然后，我们将保存这些模型，并在模型评估阶段再次使用它们。 模型1：从头开始训练CNN 对于本文的第一个疟疾检测模型，我们将构建并从头开始训练一个基本的卷积神经网络（CNN）。首先，我们需要定义模型的结构。 inp = tf.keras.layers.Input(shape=INPUT_SHAPE)conv1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(inp)pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)conv2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(pool1)pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)conv3 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(pool2)pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)flat = tf.keras.layers.Flatten()(pool3)hidden1 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(flat)drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)hidden2 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(drop1)drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)out = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(drop2)model = tf.keras.Model(inputs=inp, outputs=out)model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])model.summary()# OutputModel: &quot;model&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_1 (InputLayer) [(None, 125, 125, 3)] 0 _________________________________________________________________conv2d (Conv2D) (None, 125, 125, 32) 896 _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 62, 62, 32) 0 _________________________________________________________________conv2d_1 (Conv2D) (None, 62, 62, 64) 18496 _________________________________________________________________......_________________________________________________________________dense_1 (Dense) (None, 512) 262656 _________________________________________________________________dropout_1 (Dropout) (None, 512) 0 _________________________________________________________________dense_2 (Dense) (None, 1) 513 =================================================================Total params: 15,102,529Trainable params: 15,102,529Non-trainable params: 0_________________________________________________________________ 上述代码所构建的 CNN 模型，包含3个卷积层、1个池化层以及2个全连接层，并对全连接层设置 dropout 参数用于正则化。现在让我们开始训练模型吧！ import datetimelogdir = os.path.join(&#39;/home/dipanzan_sarkar/projects/tensorboard_logs&#39;, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=&#39;val_loss&#39;,factor=0.5,patience=2, min_lr=0.000001)callbacks = [reduce_lr, tensorboard_callback]history = model.fit(x=train_imgs_scaled, y=train_labels_enc, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(val_imgs_scaled, val_labels_enc), callbacks=callbacks, verbose=1) # OutputTrain on 17361 samples, validate on 1929 samplesEpoch 1/2517361/17361 [====] - 32s 2ms/sample - loss: 0.4373 - accuracy: 0.7814 - val_loss: 0.1834 - val_accuracy: 0.9393Epoch 2/2517361/17361 [====] - 30s 2ms/sample - loss: 0.1725 - accuracy: 0.9434 - val_loss: 0.1567 - val_accuracy: 0.9513......Epoch 24/2517361/17361 [====] - 30s 2ms/sample - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.3693 - val_accuracy: 0.9565Epoch 25/2517361/17361 [====] - 30s 2ms/sample - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.3699 - val_accuracy: 0.9559 &nbsp; 从上面的结果可以看到，我们的模型在验证集上的准确率为 95.6% ，这是非常好的。我们注意到模型在训练集上的准确率为 99.9% ，这看起来有一些过拟合。为了更加清晰地查看这个问题，我们可以分别绘制在训练和验证阶段的准确度曲线和损失曲线。 f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))t = f.suptitle(&#39;Basic CNN Performance&#39;, fontsize=12)f.subplots_adjust(top=0.85, wspace=0.3)max_epoch = len(history.history[&#39;accuracy&#39;])+1epoch_list = list(range(1,max_epoch))ax1.plot(epoch_list, history.history[&#39;accuracy&#39;], label=&#39;Train Accuracy&#39;)ax1.plot(epoch_list, history.history[&#39;val_accuracy&#39;], label=&#39;Validation Accuracy&#39;)ax1.set_xticks(np.arange(1, max_epoch, 5))ax1.set_ylabel(&#39;Accuracy Value&#39;)ax1.set_xlabel(&#39;Epoch&#39;)ax1.set_title(&#39;Accuracy&#39;)l1 = ax1.legend(loc=&quot;best&quot;)ax2.plot(epoch_list, history.history[&#39;loss&#39;], label=&#39;Train Loss&#39;)ax2.plot(epoch_list, history.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39;)ax2.set_xticks(np.arange(1, max_epoch, 5))ax2.set_ylabel(&#39;Loss Value&#39;)ax2.set_xlabel(&#39;Epoch&#39;)ax2.set_title(&#39;Loss&#39;)l2 = ax2.legend(loc=&quot;best&quot;) Learning Curves for Basic&nbsp;CNN 从图中可以看出，在第5个 epoch 之后，在验证集上的精度似乎不再提高。我们先将这个模型保存，在后面我们会再次用到它。 model.save(&#39;basic_cnn.h5&#39;) 深度迁移学习 就像人类能够运用知识完成跨任务工作一样，迁移学习使得我们能够利用在先前任务中学习到的知识，来处理新的任务，在机器学习和深度学习的环境下也是如此。这些文章涵盖了迁移学习的详细介绍和讨论，有兴趣的读者可以参考学习。 Ideas for deep transfer&nbsp;learning 我们能否采用迁移学习的思想，将预训练的深度学习模型（已在大型数据集上进行过训练的模型——例如 ImageNet）的知识应用到我们的问题——进行疟疾检测上呢？我们将采用两种目前最主流的迁移学习策略。 将预训练模型作为特征提取器 对预训练模型进行微调 我们将使用由牛津大学视觉几何组（VGG）所开发的预训练模型 VGG-19 进行实验。像 VGG-19 这样的预训练模型，一般已经在大型数据集上进行过训练，这些数据集涵盖多种类别的图像。基于此，这些预训练模型应该已经使用CNN模型学习到了一个具有高度鲁棒性的特征的层次结构，并且其应具有尺度、旋转和平移不变性。因此，这个已经学习了超过一百万个图像的具有良好特征表示的模型，可以作为一个很棒的图像特征提取器，为包括疟疾检测问题在内的其他计算机视觉问题服务。在引入强大的迁移学习之前，我们先简要讨论一下 VGG-19 的结构。 理解VGG-19模型 VGG-19 是一个具有 19 个层（包括卷积层和全连接层）的深度学习网络，该模型基于 ImageNet 数据集进行训练，该数据集是专门为图像识别和分类所构建的。VGG-19 是由 Karen Simonyan 和 Andrew Zisserman 提出的，该模型在他们的论文《Very Deep Convolutional Networks for Large-Scale Image Recognition》中有详细介绍，建议有兴趣的读者可以去读一读这篇优秀的论文。VGG-19 模型的结构如下图所示。 VGG-19 Model Architecture 从上图可以清楚地看到，该模型具有 16 个使用 3x3 卷积核的卷积层，其中部分卷积层后面接了一个最大池化层，用于下采样；随后依次连接了两个具有 4096 个隐层神经元的全连接层，接着连接了一个具有 1000 个隐层神经元的全连接层， 最后一个全连接层的每个神经元都代表 ImageNet 数据集中的一个图像类别。由于我们需要使用新的全连接层来分类疟疾，因此我们不需要最后的三个全连接层。我们更关心的是前五个块，以便我们可以利用 VGG 模型作为有效的特征提取器。 前文提到有两种迁移学习的策略，对于第一种策略，我们将把 VGG 模型当做一个特征提取器，这可以通过冻结前五个卷积块，使得它们的权重参数不会随着新的训练过程而更新来实现。对于第二种策略，我们将会解冻最后的两个卷积块（模块4和模块5），从而使得它们的参数会随着新的训练过程而不断更新。 模型2：将预训练模型作为特征提取机 为了构建这个模型，我们将利用 TensorFlow 加载 VGG-19 模型，并冻结它的卷积块，以便我们可以将其用作图像特征提取器。我们将在该模型的末尾插入自己的全连接层，用于执行本文的分类任务。 vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights=&#39;imagenet&#39;,input_shape=INPUT_SHAPE)vgg.trainable = False# Freeze the layersfor layer in vgg.layers: layer.trainable = False base_vgg = vggbase_out = base_vgg.outputpool_out = tf.keras.layers.Flatten()(base_out)hidden1 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(pool_out)drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)hidden2 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(drop1)drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)out = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(drop2)model = tf.keras.Model(inputs=base_vgg.input, outputs=out)model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])model.summary()# OutputModel: &quot;model_1&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_2 (InputLayer) [(None, 125, 125, 3)] 0 _________________________________________________________________block1_conv1 (Conv2D) (None, 125, 125, 64) 1792 _________________________________________________________________block1_conv2 (Conv2D) (None, 125, 125, 64) 36928 _________________________________________________________________......_________________________________________________________________block5_pool (MaxPooling2D) (None, 3, 3, 512) 0 _________________________________________________________________flatten_1 (Flatten) (None, 4608) 0 _________________________________________________________________dense_3 (Dense) (None, 512) 2359808 _________________________________________________________________dropout_2 (Dropout) (None, 512) 0 _________________________________________________________________dense_4 (Dense) (None, 512) 262656 _________________________________________________________________dropout_3 (Dropout) (None, 512) 0 _________________________________________________________________dense_5 (Dense) (None, 1) 513 =================================================================Total params: 22,647,361Trainable params: 2,622,977Non-trainable params: 20,024,384 从上面代码的输出可以看到，我们的模型有很多层，并且我们仅仅只利用了 VGG-19 的冻结层来提取特征。下面的代码可以验证本模型中有多少层用于训练，以及检验本模型中一共有多少层。 print(&quot;Total Layers:&quot;, len(model.layers))print(&quot;Total trainable layers:&quot;,sum([1 for l in model.layers if l.trainable]))# OutputTotal Layers: 28Total trainable layers: 6 现在我们将训练该模型，在训练过程中所用到的配置和回调函数与模型1中的类似，完整的代码可以参考github链接。下图展示了在训练过程中，模型的准确度曲线和损失曲线。 Learning Curves for frozen pre-trained CNN 从上图可以看出，该模型不像模型1中基本的 CNN 模型那样存在过拟合的现象，但是性能并不是很好。事实上，它的性能还没有基本的 CNN 模型好。现在我们将模型保存，用于后续的评估。 model.save（ &#39;vgg_frozen.h5&#39;） 模型3：具有图像增广的微调的预训练模型 在这个模型中，我们将微调预训练 VGG-19 模型的最后两个区块中层的权重。除此之外，我们还将介绍图像增广的概念。图像增广背后的原理与它的名称听起来完全一样。我们首先从训练数据集中加载现有的图像，然后对它们进行一些图像变换的操作，例如旋转，剪切，平移，缩放等，从而生成现有图像的新的、变化的版本。由于这些随机变换的操作，我们每次都会得到不同的图像。我们将使用 tf.keras 中的 ImageDataGenerator 工具，它能够帮助我们实现图像增广。 train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, zoom_range=0.05, rotation_range=25, width_shift_range=0.05, height_shift_range=0.05, shear_range=0.05, horizontal_flip=True, fill_mode=&#39;nearest&#39;)val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)# build image augmentation generatorstrain_generator = train_datagen.flow(train_data, train_labels_enc, batch_size=BATCH_SIZE, shuffle=True)val_generator = val_datagen.flow(val_data, val_labels_enc, batch_size=BATCH_SIZE, shuffle=False) 在验证集上，我们只会对图像进行缩放操作，而不进行其他的转换，这是因为我们需要在每个训练的 epoch 结束后，用验证集来评估我们的模型。有关图像增广的详细说明，可以参考这篇文章。让我们来看看进行图像增广变换后的一些样本结果。 img_id = 0sample_generator = train_datagen.flow(train_data[img_id:img_id+1], train_labels[img_id:img_id+1],batch_size=1)sample = [next(sample_generator) for i in range(0,5)]fig, ax = plt.subplots(1,5, figsize=(16, 6))print(&#39;Labels:&#39;, [item[1][0] for item in sample])l = [ax[i].imshow(sample[i][0][0]) for i in range(0,5)] Sample Augmented Images 从上图可以清楚的看到图像发生了轻微的变化。现在我们将构建新的深度模型，该模型需要确保 VGG-19 模型的最后两个块可以进行训练。 vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights=&#39;imagenet&#39;,input_shape=INPUT_SHAPE)# Freeze the layersvgg.trainable = Trueset_trainable = Falsefor layer in vgg.layers: if layer.name in [&#39;block5_conv1&#39;, &#39;block4_conv1&#39;]: set_trainable = True if set_trainable: layer.trainable = True else: layer.trainable = False base_vgg = vggbase_out = base_vgg.outputpool_out = tf.keras.layers.Flatten()(base_out)hidden1 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(pool_out)drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)hidden2 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(drop1)drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)out = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(drop2)model = tf.keras.Model(inputs=base_vgg.input, outputs=out)model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-5),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])print(&quot;Total Layers:&quot;, len(model.layers))print(&quot;Total trainable layers:&quot;, sum([1 for l in model.layers if l.trainable]))# OutputTotal Layers: 28Total trainable layers: 16 由于我们不希望在微调过程中，对预训练的层进行较大的权重更新，我们降低了模型的学习率。由于我们使用数据生成器来加载数据，本模型的训练过程会和之前稍稍不同，在这里，我们需要用到函数 fit_generator(…) 。 tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.5,patience=2, min_lr=0.000001)callbacks = [reduce_lr, tensorboard_callback]train_steps_per_epoch = train_generator.n //train_generator.batch_sizeval_steps_per_epoch = val_generator.n //val_generator.batch_sizehistory = model.fit_generator(train_generator, steps_per_epoch=train_steps_per_epoch,epochs=EPOCHS,validation_data=val_generator,validation_steps=val_steps_per_epoch,verbose=1)# OutputEpoch 1/25271/271 [====] - 133s 489ms/step - loss: 0.2267 - accuracy: 0.9117 - val_loss: 0.1414 - val_accuracy: 0.9531Epoch 2/25271/271 [====] - 129s 475ms/step - loss: 0.1399 - accuracy: 0.9552 - val_loss: 0.1292 - val_accuracy: 0.9589......Epoch 24/25271/271 [====] - 128s 473ms/step - loss: 0.0815 - accuracy: 0.9727 - val_loss: 0.1466 - val_accuracy: 0.9682Epoch 25/25271/271 [====] - 128s 473ms/step - loss: 0.0792 - accuracy: 0.9729 - val_loss: 0.1127 - val_accuracy: 0.9641 下图展示了该模型的训练曲线，可以看出该模型是这三个模型中最好的模型，其验证准确度几乎达到了 96.5% ，而且从训练准确度上看，我们的模型也没有像第一个模型那样出现过拟合。 Learning Curves for fine-tuned pre-trained CNN 现在让我们保存这个模型，很快我们将在测试集上用到它进行性能评估。 model.save（ &#39;vgg_finetuned.h5&#39;） 至此，模型训练阶段告一段落，我们即将在真实的测试集上去测试这些模型的性能。 深度学习模型的性能评估阶段 现在，我们将对之前训练好的三个模型进行评估。仅仅使用验证集来评估模型的好坏是不够的， 因此，我们将使用测试集来进一步评估模型的性能。我们构建了一个实用的模块 model_evaluation_utils，该模块采用相关的分类指标，用于评估深度学习模型的性能。首先我们需要将测试数据进行缩放。 test_imgs_scaled = test_data / 255.test_imgs_scaled.shape, test_labels.shape# Output((8268, 125, 125, 3), (8268,)) 第二步是加载之前所保存的深度学习模型，然后在测试集上进行预测。 # Load Saved Deep Learning Modelsbasic_cnn = tf.keras.models.load_model(&#39;./basic_cnn.h5&#39;)vgg_frz = tf.keras.models.load_model(&#39;./vgg_frozen.h5&#39;)vgg_ft = tf.keras.models.load_model(&#39;./vgg_finetuned.h5&#39;)# Make Predictions on Test Databasic_cnn_preds = basic_cnn.predict(test_imgs_scaled, batch_size=512)vgg_frz_preds = vgg_frz.predict(test_imgs_scaled, batch_size=512)vgg_ft_preds = vgg_ft.predict(test_imgs_scaled, batch_size=512)basic_cnn_pred_labels = le.inverse_transform([1 if pred &gt; 0.5 else 0 for pred in basic_cnn_preds.ravel()])vgg_frz_pred_labels = le.inverse_transform([1 if pred &gt; 0.5 else 0 for pred in vgg_frz_preds.ravel()])vgg_ft_pred_labels&nbsp;=&nbsp;le.inverse_transform([1&nbsp;if&nbsp;pred&nbsp;&gt;&nbsp;0.5&nbsp;else&nbsp;0&nbsp;for&nbsp;pred&nbsp;in&nbsp;vgg_ft_preds.ravel()]) 最后一步是利用 model_evaluation_utils 模块，根据不同的分类评价指标，来评估每个模型的性能。 import model_evaluation_utils as meuimport pandas as pdbasic_cnn_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=basic_cnn_pred_labels)vgg_frz_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=vgg_frz_pred_labels)vgg_ft_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=vgg_ft_pred_labels)pd.DataFrame([basic_cnn_metrics, vgg_frz_metrics, vgg_ft_metrics], index=[&#39;Basic CNN&#39;, &#39;VGG-19 Frozen&#39;, &#39;VGG-19 Fine-tuned&#39;]) 从图中可以看到，第三个模型在测试集上的性能是最好的，其准确度和 f1-score 都达到了96%，这是一个非常好的结果，而且这个结果和论文中提到的更为复杂的模型所得到的结果具有相当的 可比性！ 结论 本文研究了一个有趣的医学影像案例——疟疾检测。疟疾检测是一个复杂的过程，而且能够进行正确操作的医疗人员也很少，这是一个很严重的问题。本文利用 AI 技术构建了一个开源的项目，该项目在疟疾检测问题上具有最高的准确率，并使AI技术为社会带来了效益。 相关链接：https://towardsdatascience.com/detecting-malaria-with-deep-learning-9e45c1e34b60 （本文为AI科技大本营编译文章，转载请微信联系 1092722531） ◆ CTA核心技术及应用峰会 ◆ 5月25-27日，由中国IT社区CSDN与数字经济人才发展中心联合主办的第一届CTA核心技术及应用峰会将在杭州国际博览中心隆重召开，峰会将围绕人工智能领域，邀请技术领航者，与开发者共同探讨机器学习和知识图谱的前沿研究及应用。 更多重磅嘉宾请识别海报二维码查看，目前会议早鸟票发售中（原票价1099元），点击阅读原文即刻抢购。添加小助手微信15101014297，备注“CTA”，了解票务以及会务详情。 推荐阅读 他25岁进贝尔实验室，32岁提信息论，40岁办达特茅斯会议，晚年患上阿兹海默 | 人物志 微软沈向洋，百度李彦宏、王海峰，阿里王坚均候选中国工程院院士 小样，加张图你就不认识我了？“补丁”模型骗你没商量！| 技术头条 东大漆桂林、清华李涓子、复旦肖仰华等大牛确认出席CTA峰会！5月一起打卡杭州 京东 60 天哗变！CTO 成优化第一人 | 畅言 异构计算=未来？一文带你秒懂3大主流异构 《互联网人叹气图鉴》 回报率29%! 大神用情感分析创建一个比特币交易算法, 原来交易玩的是心理战 她说：为啥程序员都特想要机械键盘？这答案我服！" />
<meta property="og:description" content="作者 | Dipanjan (DJ) Sarkar 译者 | Monanfei 编辑 | Rachel、Jane 出品 | AI科技大本营（id：rgznai100） 【导读】文本基于深度学习和迁移学习方法，对疟疾等传染病检测问题进行了研究。作者对疟疾的检测原理以及迁移学习理论进行了介绍，并使用VGG-19预训练模型，进行了基于特征提取和基于参数微调的迁移学习实践。 前言 &quot;健康就是财富&quot;，这是一个老生常谈的话题，但不得不说这是一个真理。在这篇文章中，我们将研究如何利用AI技术来检测一种致命的疾病——疟疾。本文将提出一个低成本、高效率和高准确率的开源解决方案。本文有两个目的：1.了解疟疾的传染原因和其致命性；2、介绍如何运用深度学习有效检测疟疾。本章的主要内容如下： 开展本项目的动机 疟疾检测的方法 用深度学习检测疟疾 从头开始训练卷积神经网络（CNN） 利用预训练模型进行迁移学习 本文不是为了宣扬 AI 将要取代人类的工作，或者接管世界等论调，而是仅仅展示 AI 是如何用一种低成本、高效率和高准确率的方案，来帮助人类去检测和诊断疟疾，并尽量减少人工操作。 Python and TensorFlow — A great combo to build open-source deep learning solutions 在本文中，我们将使用 Python 和 tensorflow ，来构建一个强大的、可扩展的、有效的深度学习解决方案。这些工具都是免费并且开源的，这使得我们能够构建一个真正低成本、高效精准的解决方案，而且可以让每个人都可以轻松使用。让我们开始吧！ 动机 疟疾是经疟蚊叮咬而感染疟原虫所引起的虫媒传染病，疟疾最常通过受感染的雌性疟蚊来传播。虽然我们不必详细了解这种疾病，但是我们需要知道疟疾有五种常见的类型。下图展示了这种疾病的致死性在全球的分布情况。 Malaria Estimated Risk Heath Map (Source: treated.com) 从上图中可以明显看到，疟疾遍布全球，尤其是在热带区域分布密集。本项目就是基于这种疾病的特性和致命性来开展的，下面我们举个例子来说明。起初，如果你被一只受感染的蚊子叮咬了，那么蚊子所携带的寄生虫就会进入你的血液，并且开始摧毁你体内的携氧红细胞。通常来讲，你会在被疟蚊叮咬后的几天或几周内感到不适，一般会首先出现类似流感或者病毒感染的症状。然而，这些致命的寄生虫可以在你身体里完好地存活超过一年的时间，并且不产生任何其他症状！延迟接受正确的治疗，可能会导致并发症甚至死亡。因此，早期并有效的疟疾检测和排查可以挽救这些生命。 世界卫生组织（WHO）发布了几个关于疟疾的重要事实，详情见此。简而言之，世界上将近一半的人口面临疟疾风险，每年有超过2亿的疟疾病例，以及有大约40万人死于疟疾。这些事实让我们认识到，快速简单高效的疟疾检查是多么重要，这也是本文的动机所在。 疟疾检查的方法 文章《 Pre-trained convolutional neural networks as feature extractors toward improved Malaria parasite detection in thin blood smear images》（本文的数据和分析也是基于这篇文章）简要介绍了疟疾检测的几种方法，这些方法包括但是不限于厚薄血涂片检查、聚合酶链式反应（PCR）和快速诊断测试（RDT）。在本文中，我们没有对这些方法进行详细介绍，但是需要注意的一点是，后两种方法常常作为替代方案使用，尤其是在缺乏高质量显微镜服务的情况下。 我们将简要讨论基于血液涂片检测流程的标准疟疾诊断方法，首先感谢 Carlos Ariza 的博文，以及 Adrian Rosebrock 关于疟疾检查的文章，这两篇文章让我们对疟疾检查领域有了更为深入的了解。 A blood smear workflow for Malaria detection (Source) 根据上图所示的 WHO 的血液涂片检测流程，该工作包括在100倍放大倍数下对血涂片进行深入检查，其中人们需要从5000个细胞中，手动检测出含有寄生虫的红细胞。Rajaraman 等人的论文中更加详细的给出了相关的描述，如下所示： 厚血涂片有助于检测寄生虫的存在，而薄血涂片有助于识别引起感染的寄生虫种类（Centers for Disease Control and Prevention, 2012）。诊断准确性在很大程度上取决于人类的专业知识，并且可能受到观察者间的差异和观察者的可靠性所带来的不利影响，以及受到在疾病流行或资源受限的区域内的大规模诊断造成的负担所带来的不利影响（Mitiku，Mengistu＆Gelaw，2003）。替代技术，例如聚合酶链式反应（PCR）和快速诊断测试（RDT），也会被使用；但是PCR分析受到其性能的限制（Hommelsheim等，2014），而RDT在疾病流行地区的成本效益较低（Hawkes，Katsuva＆Masumbuko，2009）。 因此，传统的疟疾检测绝对是一个密集的手工过程，或许深度学习技术可以帮助它完成自动化。上文提到的这些内容为后文打下了基础。 用深度学习检测疟疾 手工诊断血液涂片，是一项重复且规律的工作，而且需要一定的专业知识来区分和统计被寄生的和未感染的细胞。如果某些地区的工作人员没有正确的专业知识，那么这种方法就不能很好地推广，并且会导致一些问题。现有工作已经取得了一些进展，包括利用最先进的图像处理和分析技术来提取手工设计的特征，并利用这些特性构建基于机器学习的分类模型。但是，由于手工设计的部分需要花费大量的时间，当有更多的数据可供训练时，模型却无法及时的进行扩展。 深度学习模型，或更具体地说，卷积神经网络（CNN）在各种计算机视觉任务中获得了非常好的效果。本文假设您已经对 CNN 有一定的了解，但是如果您并不了解 CNN ，可以通过这篇文章进行深入了解。简单来讲，CNN 最关键的层主要包括卷积层和池化层，如下图所示。 A typical CNN architeture (Source: deeplearning.net) 卷积层从数据中学习空间层级模式，这些模式具有平移不变性，因此卷积层能够学习图像的不同方面。例如，第一卷积层将学习诸如边缘和角落的微型局部模式，第二卷积层将基于第一层所提取的特征，来学习更大的图像模式，如此循序渐进。这使得 CNN 能够自动进行特征工程，并且学习有效的特征，这些特征对新的数据具有很好的泛化能力。池化层常用于下采样和降维。 因此，CNN 能够帮助我们实现自动化的和可扩展的特征工程。此外，在模型的末端接入密集层，能够使我们执行图像分类等任务。使用像CNN这样的深度学习模型，进行自动化的疟疾检测，可能是一个高效、低成本、可扩展的方案。特别是随着迁移学习的发展和预训练模型的共享，在数据量较少等限制条件下，深度学习模型也能取得很好的效果。 Rajaraman 等人的论文&nbsp;《Pre-trained convolutional neural networks as feature extractors toward improved parasite detection in thin blood smear images》利用 6 个预训练模型，在进行疟疾检测时取得了 95.9% 的准确率。本文的重点是从头开始尝试一些简单的 CNN 模型和一些预先训练的模型，并利用迁移学习来检验我们在同一数据集下得到的结果。本文将使用 Python 和 TensorFlow 框架来构建模型。 数据集的详情 首先感谢 Lister Hill 国家生物医学通信中心（LHNCBC）的研究人员（国家医学图书馆（NLM）的部门），他们仔细收集并注释了这个血涂片图像的数据集，数据中包含健康和感染这两种类型的血涂片图像。您可以从官方网站上下载这些图像。 实际上，他们开发了一款可以运行在标准安卓智能手机上的应用程序，该程序可以连接传统的光学显微镜 (Poostchi et al., 2018) 。他们从孟加拉国吉大港医学院附属医院进行拍照记录了样本集，其中包括150个恶性疟原虫感染的样本和 50 个健康的样本，每个样本都是经过 Giemsa 染色的薄血涂片。智能手机的内置摄像头可以捕获样本的每一个局部微观视图。来自泰国曼谷的玛希隆-牛津热带医学研究所的专业人员为这些图像进行了手动注释。让我们简要地看一下数据集结构。首先根据本文所使用的操作系统，我们需要安装一些基本的依赖项。 本文所使用的系统是云上的 Debian 系统，该系统配置有 GPU ，这能够加速我们模型的训练。首先安装依赖树，这能够方便我们查看目录结构。（sudo apt install tree） 从上图所示的目录结构中可以看到，我们的文件里包含两个文件夹，分别包含受感染的和健康的细胞图像。利用以下代码，我们可以进一步了解图像的总数是多少。 import osimport globbase_dir = os.path.join(&#39;./cell_images&#39;)infected_dir = os.path.join(base_dir,&#39;Parasitized&#39;)healthy_dir = os.path.join(base_dir,&#39;Uninfected&#39;)infected_files = glob.glob(infected_dir+&#39;/*.png&#39;)healthy_files = glob.glob(healthy_dir+&#39;/*.png&#39;)len(infected_files), len(healthy_files)# Output(13779, 13779) 从上述结果可以看到， 疟疾和非疟疾（未感染）的细胞图像的数据集均包含13779张图片，两个数据集的大小是相对平衡的。接下来我们将利用这些数据构建一个基于pandas的dataframe类型的数据，这对我们后续构建数据集很有帮助。 import numpy as npimport pandas as pdnp.random.seed(42)files_df = pd.DataFrame({ &#39;filename&#39;: infected_files + healthy_files, &#39;label&#39;: [&#39;malaria&#39;] * len(infected_files) + [&#39;healthy&#39;] * len(healthy_files)}).sample(frac=1, random_state=42).reset_index(drop=True)files_df.head() 构建和探索图像数据集 在构建深度学习模型之前，我们不仅需要训练数据，还需要未用于训练的数据来验证和测试模型的性能。本文采用 60:10:30 的比例来划分训练集、验证集和测试集。我们将使用训练集和验证集来训练模型，并利用测试集来检验模型的性能。 from sklearn.model_selection import train_test_splitfrom collections import Countertrain_files, test_files, train_labels, test_labels = train_test_split(files_df[&#39;filename&#39;].values, files_df[&#39;label&#39;].values, test_size=0.3, random_state=42) train_files, val_files, train_labels, val_labels = train_test_split(train_files, train_labels, test_size=0.1, random_state=42)print(train_files.shape, val_files.shape, test_files.shape)print(&#39;Train:&#39;, Counter(train_labels), &#39;\nVal:&#39;, Counter(val_labels), &#39;\nTest:&#39;, Counter(test_labels))# Output(17361,) (1929,) (8268,)Train: Counter({&#39;healthy&#39;: 8734, &#39;malaria&#39;: 8627}) Val: Counter({&#39;healthy&#39;: 970, &#39;malaria&#39;: 959}) Test: Counter({&#39;malaria&#39;: 4193, &#39;healthy&#39;: 4075}) 可以发现，由于血液来源、测试方法以及图像拍摄的方向不同，血液涂片和细胞的图像尺寸不尽相同。我们需要获取一些训练数据的统计信息，从而确定最优的图像尺寸（请注意，在这里我们完全没用到测试集！）。 import cv2from concurrent import futuresimport threadingdef get_img_shape_parallel(idx, img, total_imgs): if idx % 5000 == 0 or idx == (total_imgs - 1): print(&#39;{}: working on img num:{}&#39;.format(threading.current_thread().name,idx)) return cv2.imread(img).shape ex = futures.ThreadPoolExecutor(max_workers=None)data_inp = [(idx, img, len(train_files)) for idx, img in enumerate(train_files)]print(&#39;Starting Img shape computation:&#39;)train_img_dims_map = ex.map(get_img_shape_parallel, [record[0] for record in data_inp], [record[1] for record in data_inp], [record[2] for record in data_inp])train_img_dims = list(train_img_dims_map)print(&#39;Min Dimensions:&#39;, np.min(train_img_dims, axis=0)) print(&#39;Avg Dimensions:&#39;, np.mean(train_img_dims, axis=0))print(&#39;Median Dimensions:&#39;, np.median(train_img_dims, axis=0))print(&#39;Max Dimensions:&#39;, np.max(train_img_dims, axis=0))# OutputStarting Img shape computation:ThreadPoolExecutor-0_0: working on img num: 0ThreadPoolExecutor-0_17: working on img num: 5000ThreadPoolExecutor-0_15: working on img num: 10000ThreadPoolExecutor-0_1: working on img num: 15000ThreadPoolExecutor-0_7: working on img num: 17360Min Dimensions: [46 46 3]Avg Dimensions: [132.77311215 132.45757733 3.]Median Dimensions: [130. 130. 3.]Max Dimensions: [385 394 3] 我们采用了并行处理的策略来加速图像读取操作。基于汇总的统计信息，我们决定将每张图像的大小调整为125x125。现在让我们加载所有的图像，并把他们的大小都调整为上述固定的尺寸。 IMG_DIMS = (125, 125)def get_img_data_parallel(idx, img, total_imgs): if idx % 5000 == 0 or idx == (total_imgs - 1): print(&#39;{}: working on img num: {}&#39;.format(threading.current_thread().name,idx)) img = cv2.imread(img) img = cv2.resize(img, dsize=IMG_DIMS, interpolation=cv2.INTER_CUBIC) img = np.array(img, dtype=np.float32) return imgex = futures.ThreadPoolExecutor(max_workers=None)train_data_inp = [(idx, img, len(train_files)) for idx, img in enumerate(train_files)]val_data_inp = [(idx, img, len(val_files)) for idx, img in enumerate(val_files)]test_data_inp = [(idx, img, len(test_files)) for idx, img in enumerate(test_files)]print(&#39;Loading Train Images:&#39;)train_data_map = ex.map(get_img_data_parallel, [record[0] for record in train_data_inp], [record[1] for record in train_data_inp], [record[2] for record in train_data_inp])train_data = np.array(list(train_data_map))print(&#39;\nLoading Validation Images:&#39;)val_data_map = ex.map(get_img_data_parallel, [record[0] for record in val_data_inp], [record[1] for record in val_data_inp], [record[2] for record in val_data_inp])val_data = np.array(list(val_data_map))print(&#39;\nLoading Test Images:&#39;)test_data_map = ex.map(get_img_data_parallel, [record[0] for record in test_data_inp], [record[1] for record in test_data_inp], [record[2] for record in test_data_inp])test_data = np.array(list(test_data_map))train_data.shape, val_data.shape, test_data.shape # OutputLoading Train Images:ThreadPoolExecutor-1_0: working on img num: 0ThreadPoolExecutor-1_12: working on img num: 5000ThreadPoolExecutor-1_6: working on img num: 10000ThreadPoolExecutor-1_10: working on img num: 15000ThreadPoolExecutor-1_3: working on img num: 17360Loading Validation Images:ThreadPoolExecutor-1_13: working on img num: 0ThreadPoolExecutor-1_18: working on img num: 1928Loading Test Images:ThreadPoolExecutor-1_5: working on img num: 0ThreadPoolExecutor-1_19: working on img num: 5000ThreadPoolExecutor-1_8: working on img num: 8267((17361, 125, 125, 3), (1929, 125, 125, 3), (8268, 125, 125, 3)) 我们再次运用了并行处理策略来加速图像加载和尺寸调整的计算，如上面输出结果中展示的，我们最终得到了所需尺寸的图像张量。现在我们可以查看一些样本的细胞图像，从而从直观上认识一下我们的数据的情况。 import matplotlib.pyplot as plt%matplotlib inlineplt.figure(1 , figsize = (8 , 8))n = 0 for i in range(16): n += 1 r = np.random.randint(0 , train_data.shape[0] , 1) plt.subplot(4 , 4 , n) plt.subplots_adjust(hspace = 0.5 , wspace = 0.5) plt.imshow(train_data[r[0]]/255.) plt.title(&#39;{}&#39;.format(train_labels[r[0]])) plt.xticks([]) , plt.yticks([]) 从上面的样本图像可以看出，疟疾和健康细胞图像之间存在一些细微差别。我们将构建深度学习模型，通过不断训练来使模型尝试学习这些模式。在开始训练模型之前，我们先对模型的参数进行一些基本的设置。 BATCH_SIZE = 64NUM_CLASSES = 2EPOCHS = 25INPUT_SHAPE = (125, 125, 3)train_imgs_scaled = train_data / 255.val_imgs_scaled = val_data / 255.# encode text category labelsfrom sklearn.preprocessing import LabelEncoderle = LabelEncoder()le.fit(train_labels)train_labels_enc = le.transform(train_labels)val_labels_enc = le.transform(val_labels)print(train_labels[:6], train_labels_enc[:6])# Output[&#39;malaria&#39; &#39;malaria&#39; &#39;malaria&#39; &#39;healthy&#39; &#39;healthy&#39; &#39;malaria&#39;][1 1 1 0 0 1] 上面的代码设定了图像的维度，批尺寸，epoch 的次数，并且对我们的类别标签进行了编码。TensorFLow 2.0 alpha 版本在2019年3月发布，它为我们项目的实施提供了一个完美的接口。 import tensorflow as tf# Load the TensorBoard notebook extension (optional)%load_ext tensorboard.notebooktf.random.set_seed(42)tf.__version__# Output&#39;2.0.0-alpha0&#39; 深度学习模型的训练阶段 在模型训练阶段，我们将构建几个深度学习模型，利用前面构建的训练集进行训练，并在验证集上比较它们的性能。然后，我们将保存这些模型，并在模型评估阶段再次使用它们。 模型1：从头开始训练CNN 对于本文的第一个疟疾检测模型，我们将构建并从头开始训练一个基本的卷积神经网络（CNN）。首先，我们需要定义模型的结构。 inp = tf.keras.layers.Input(shape=INPUT_SHAPE)conv1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(inp)pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)conv2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(pool1)pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)conv3 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(pool2)pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)flat = tf.keras.layers.Flatten()(pool3)hidden1 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(flat)drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)hidden2 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(drop1)drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)out = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(drop2)model = tf.keras.Model(inputs=inp, outputs=out)model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])model.summary()# OutputModel: &quot;model&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_1 (InputLayer) [(None, 125, 125, 3)] 0 _________________________________________________________________conv2d (Conv2D) (None, 125, 125, 32) 896 _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 62, 62, 32) 0 _________________________________________________________________conv2d_1 (Conv2D) (None, 62, 62, 64) 18496 _________________________________________________________________......_________________________________________________________________dense_1 (Dense) (None, 512) 262656 _________________________________________________________________dropout_1 (Dropout) (None, 512) 0 _________________________________________________________________dense_2 (Dense) (None, 1) 513 =================================================================Total params: 15,102,529Trainable params: 15,102,529Non-trainable params: 0_________________________________________________________________ 上述代码所构建的 CNN 模型，包含3个卷积层、1个池化层以及2个全连接层，并对全连接层设置 dropout 参数用于正则化。现在让我们开始训练模型吧！ import datetimelogdir = os.path.join(&#39;/home/dipanzan_sarkar/projects/tensorboard_logs&#39;, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=&#39;val_loss&#39;,factor=0.5,patience=2, min_lr=0.000001)callbacks = [reduce_lr, tensorboard_callback]history = model.fit(x=train_imgs_scaled, y=train_labels_enc, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(val_imgs_scaled, val_labels_enc), callbacks=callbacks, verbose=1) # OutputTrain on 17361 samples, validate on 1929 samplesEpoch 1/2517361/17361 [====] - 32s 2ms/sample - loss: 0.4373 - accuracy: 0.7814 - val_loss: 0.1834 - val_accuracy: 0.9393Epoch 2/2517361/17361 [====] - 30s 2ms/sample - loss: 0.1725 - accuracy: 0.9434 - val_loss: 0.1567 - val_accuracy: 0.9513......Epoch 24/2517361/17361 [====] - 30s 2ms/sample - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.3693 - val_accuracy: 0.9565Epoch 25/2517361/17361 [====] - 30s 2ms/sample - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.3699 - val_accuracy: 0.9559 &nbsp; 从上面的结果可以看到，我们的模型在验证集上的准确率为 95.6% ，这是非常好的。我们注意到模型在训练集上的准确率为 99.9% ，这看起来有一些过拟合。为了更加清晰地查看这个问题，我们可以分别绘制在训练和验证阶段的准确度曲线和损失曲线。 f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))t = f.suptitle(&#39;Basic CNN Performance&#39;, fontsize=12)f.subplots_adjust(top=0.85, wspace=0.3)max_epoch = len(history.history[&#39;accuracy&#39;])+1epoch_list = list(range(1,max_epoch))ax1.plot(epoch_list, history.history[&#39;accuracy&#39;], label=&#39;Train Accuracy&#39;)ax1.plot(epoch_list, history.history[&#39;val_accuracy&#39;], label=&#39;Validation Accuracy&#39;)ax1.set_xticks(np.arange(1, max_epoch, 5))ax1.set_ylabel(&#39;Accuracy Value&#39;)ax1.set_xlabel(&#39;Epoch&#39;)ax1.set_title(&#39;Accuracy&#39;)l1 = ax1.legend(loc=&quot;best&quot;)ax2.plot(epoch_list, history.history[&#39;loss&#39;], label=&#39;Train Loss&#39;)ax2.plot(epoch_list, history.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39;)ax2.set_xticks(np.arange(1, max_epoch, 5))ax2.set_ylabel(&#39;Loss Value&#39;)ax2.set_xlabel(&#39;Epoch&#39;)ax2.set_title(&#39;Loss&#39;)l2 = ax2.legend(loc=&quot;best&quot;) Learning Curves for Basic&nbsp;CNN 从图中可以看出，在第5个 epoch 之后，在验证集上的精度似乎不再提高。我们先将这个模型保存，在后面我们会再次用到它。 model.save(&#39;basic_cnn.h5&#39;) 深度迁移学习 就像人类能够运用知识完成跨任务工作一样，迁移学习使得我们能够利用在先前任务中学习到的知识，来处理新的任务，在机器学习和深度学习的环境下也是如此。这些文章涵盖了迁移学习的详细介绍和讨论，有兴趣的读者可以参考学习。 Ideas for deep transfer&nbsp;learning 我们能否采用迁移学习的思想，将预训练的深度学习模型（已在大型数据集上进行过训练的模型——例如 ImageNet）的知识应用到我们的问题——进行疟疾检测上呢？我们将采用两种目前最主流的迁移学习策略。 将预训练模型作为特征提取器 对预训练模型进行微调 我们将使用由牛津大学视觉几何组（VGG）所开发的预训练模型 VGG-19 进行实验。像 VGG-19 这样的预训练模型，一般已经在大型数据集上进行过训练，这些数据集涵盖多种类别的图像。基于此，这些预训练模型应该已经使用CNN模型学习到了一个具有高度鲁棒性的特征的层次结构，并且其应具有尺度、旋转和平移不变性。因此，这个已经学习了超过一百万个图像的具有良好特征表示的模型，可以作为一个很棒的图像特征提取器，为包括疟疾检测问题在内的其他计算机视觉问题服务。在引入强大的迁移学习之前，我们先简要讨论一下 VGG-19 的结构。 理解VGG-19模型 VGG-19 是一个具有 19 个层（包括卷积层和全连接层）的深度学习网络，该模型基于 ImageNet 数据集进行训练，该数据集是专门为图像识别和分类所构建的。VGG-19 是由 Karen Simonyan 和 Andrew Zisserman 提出的，该模型在他们的论文《Very Deep Convolutional Networks for Large-Scale Image Recognition》中有详细介绍，建议有兴趣的读者可以去读一读这篇优秀的论文。VGG-19 模型的结构如下图所示。 VGG-19 Model Architecture 从上图可以清楚地看到，该模型具有 16 个使用 3x3 卷积核的卷积层，其中部分卷积层后面接了一个最大池化层，用于下采样；随后依次连接了两个具有 4096 个隐层神经元的全连接层，接着连接了一个具有 1000 个隐层神经元的全连接层， 最后一个全连接层的每个神经元都代表 ImageNet 数据集中的一个图像类别。由于我们需要使用新的全连接层来分类疟疾，因此我们不需要最后的三个全连接层。我们更关心的是前五个块，以便我们可以利用 VGG 模型作为有效的特征提取器。 前文提到有两种迁移学习的策略，对于第一种策略，我们将把 VGG 模型当做一个特征提取器，这可以通过冻结前五个卷积块，使得它们的权重参数不会随着新的训练过程而更新来实现。对于第二种策略，我们将会解冻最后的两个卷积块（模块4和模块5），从而使得它们的参数会随着新的训练过程而不断更新。 模型2：将预训练模型作为特征提取机 为了构建这个模型，我们将利用 TensorFlow 加载 VGG-19 模型，并冻结它的卷积块，以便我们可以将其用作图像特征提取器。我们将在该模型的末尾插入自己的全连接层，用于执行本文的分类任务。 vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights=&#39;imagenet&#39;,input_shape=INPUT_SHAPE)vgg.trainable = False# Freeze the layersfor layer in vgg.layers: layer.trainable = False base_vgg = vggbase_out = base_vgg.outputpool_out = tf.keras.layers.Flatten()(base_out)hidden1 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(pool_out)drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)hidden2 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(drop1)drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)out = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(drop2)model = tf.keras.Model(inputs=base_vgg.input, outputs=out)model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])model.summary()# OutputModel: &quot;model_1&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_2 (InputLayer) [(None, 125, 125, 3)] 0 _________________________________________________________________block1_conv1 (Conv2D) (None, 125, 125, 64) 1792 _________________________________________________________________block1_conv2 (Conv2D) (None, 125, 125, 64) 36928 _________________________________________________________________......_________________________________________________________________block5_pool (MaxPooling2D) (None, 3, 3, 512) 0 _________________________________________________________________flatten_1 (Flatten) (None, 4608) 0 _________________________________________________________________dense_3 (Dense) (None, 512) 2359808 _________________________________________________________________dropout_2 (Dropout) (None, 512) 0 _________________________________________________________________dense_4 (Dense) (None, 512) 262656 _________________________________________________________________dropout_3 (Dropout) (None, 512) 0 _________________________________________________________________dense_5 (Dense) (None, 1) 513 =================================================================Total params: 22,647,361Trainable params: 2,622,977Non-trainable params: 20,024,384 从上面代码的输出可以看到，我们的模型有很多层，并且我们仅仅只利用了 VGG-19 的冻结层来提取特征。下面的代码可以验证本模型中有多少层用于训练，以及检验本模型中一共有多少层。 print(&quot;Total Layers:&quot;, len(model.layers))print(&quot;Total trainable layers:&quot;,sum([1 for l in model.layers if l.trainable]))# OutputTotal Layers: 28Total trainable layers: 6 现在我们将训练该模型，在训练过程中所用到的配置和回调函数与模型1中的类似，完整的代码可以参考github链接。下图展示了在训练过程中，模型的准确度曲线和损失曲线。 Learning Curves for frozen pre-trained CNN 从上图可以看出，该模型不像模型1中基本的 CNN 模型那样存在过拟合的现象，但是性能并不是很好。事实上，它的性能还没有基本的 CNN 模型好。现在我们将模型保存，用于后续的评估。 model.save（ &#39;vgg_frozen.h5&#39;） 模型3：具有图像增广的微调的预训练模型 在这个模型中，我们将微调预训练 VGG-19 模型的最后两个区块中层的权重。除此之外，我们还将介绍图像增广的概念。图像增广背后的原理与它的名称听起来完全一样。我们首先从训练数据集中加载现有的图像，然后对它们进行一些图像变换的操作，例如旋转，剪切，平移，缩放等，从而生成现有图像的新的、变化的版本。由于这些随机变换的操作，我们每次都会得到不同的图像。我们将使用 tf.keras 中的 ImageDataGenerator 工具，它能够帮助我们实现图像增广。 train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, zoom_range=0.05, rotation_range=25, width_shift_range=0.05, height_shift_range=0.05, shear_range=0.05, horizontal_flip=True, fill_mode=&#39;nearest&#39;)val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)# build image augmentation generatorstrain_generator = train_datagen.flow(train_data, train_labels_enc, batch_size=BATCH_SIZE, shuffle=True)val_generator = val_datagen.flow(val_data, val_labels_enc, batch_size=BATCH_SIZE, shuffle=False) 在验证集上，我们只会对图像进行缩放操作，而不进行其他的转换，这是因为我们需要在每个训练的 epoch 结束后，用验证集来评估我们的模型。有关图像增广的详细说明，可以参考这篇文章。让我们来看看进行图像增广变换后的一些样本结果。 img_id = 0sample_generator = train_datagen.flow(train_data[img_id:img_id+1], train_labels[img_id:img_id+1],batch_size=1)sample = [next(sample_generator) for i in range(0,5)]fig, ax = plt.subplots(1,5, figsize=(16, 6))print(&#39;Labels:&#39;, [item[1][0] for item in sample])l = [ax[i].imshow(sample[i][0][0]) for i in range(0,5)] Sample Augmented Images 从上图可以清楚的看到图像发生了轻微的变化。现在我们将构建新的深度模型，该模型需要确保 VGG-19 模型的最后两个块可以进行训练。 vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights=&#39;imagenet&#39;,input_shape=INPUT_SHAPE)# Freeze the layersvgg.trainable = Trueset_trainable = Falsefor layer in vgg.layers: if layer.name in [&#39;block5_conv1&#39;, &#39;block4_conv1&#39;]: set_trainable = True if set_trainable: layer.trainable = True else: layer.trainable = False base_vgg = vggbase_out = base_vgg.outputpool_out = tf.keras.layers.Flatten()(base_out)hidden1 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(pool_out)drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)hidden2 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(drop1)drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)out = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(drop2)model = tf.keras.Model(inputs=base_vgg.input, outputs=out)model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-5),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])print(&quot;Total Layers:&quot;, len(model.layers))print(&quot;Total trainable layers:&quot;, sum([1 for l in model.layers if l.trainable]))# OutputTotal Layers: 28Total trainable layers: 16 由于我们不希望在微调过程中，对预训练的层进行较大的权重更新，我们降低了模型的学习率。由于我们使用数据生成器来加载数据，本模型的训练过程会和之前稍稍不同，在这里，我们需要用到函数 fit_generator(…) 。 tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.5,patience=2, min_lr=0.000001)callbacks = [reduce_lr, tensorboard_callback]train_steps_per_epoch = train_generator.n //train_generator.batch_sizeval_steps_per_epoch = val_generator.n //val_generator.batch_sizehistory = model.fit_generator(train_generator, steps_per_epoch=train_steps_per_epoch,epochs=EPOCHS,validation_data=val_generator,validation_steps=val_steps_per_epoch,verbose=1)# OutputEpoch 1/25271/271 [====] - 133s 489ms/step - loss: 0.2267 - accuracy: 0.9117 - val_loss: 0.1414 - val_accuracy: 0.9531Epoch 2/25271/271 [====] - 129s 475ms/step - loss: 0.1399 - accuracy: 0.9552 - val_loss: 0.1292 - val_accuracy: 0.9589......Epoch 24/25271/271 [====] - 128s 473ms/step - loss: 0.0815 - accuracy: 0.9727 - val_loss: 0.1466 - val_accuracy: 0.9682Epoch 25/25271/271 [====] - 128s 473ms/step - loss: 0.0792 - accuracy: 0.9729 - val_loss: 0.1127 - val_accuracy: 0.9641 下图展示了该模型的训练曲线，可以看出该模型是这三个模型中最好的模型，其验证准确度几乎达到了 96.5% ，而且从训练准确度上看，我们的模型也没有像第一个模型那样出现过拟合。 Learning Curves for fine-tuned pre-trained CNN 现在让我们保存这个模型，很快我们将在测试集上用到它进行性能评估。 model.save（ &#39;vgg_finetuned.h5&#39;） 至此，模型训练阶段告一段落，我们即将在真实的测试集上去测试这些模型的性能。 深度学习模型的性能评估阶段 现在，我们将对之前训练好的三个模型进行评估。仅仅使用验证集来评估模型的好坏是不够的， 因此，我们将使用测试集来进一步评估模型的性能。我们构建了一个实用的模块 model_evaluation_utils，该模块采用相关的分类指标，用于评估深度学习模型的性能。首先我们需要将测试数据进行缩放。 test_imgs_scaled = test_data / 255.test_imgs_scaled.shape, test_labels.shape# Output((8268, 125, 125, 3), (8268,)) 第二步是加载之前所保存的深度学习模型，然后在测试集上进行预测。 # Load Saved Deep Learning Modelsbasic_cnn = tf.keras.models.load_model(&#39;./basic_cnn.h5&#39;)vgg_frz = tf.keras.models.load_model(&#39;./vgg_frozen.h5&#39;)vgg_ft = tf.keras.models.load_model(&#39;./vgg_finetuned.h5&#39;)# Make Predictions on Test Databasic_cnn_preds = basic_cnn.predict(test_imgs_scaled, batch_size=512)vgg_frz_preds = vgg_frz.predict(test_imgs_scaled, batch_size=512)vgg_ft_preds = vgg_ft.predict(test_imgs_scaled, batch_size=512)basic_cnn_pred_labels = le.inverse_transform([1 if pred &gt; 0.5 else 0 for pred in basic_cnn_preds.ravel()])vgg_frz_pred_labels = le.inverse_transform([1 if pred &gt; 0.5 else 0 for pred in vgg_frz_preds.ravel()])vgg_ft_pred_labels&nbsp;=&nbsp;le.inverse_transform([1&nbsp;if&nbsp;pred&nbsp;&gt;&nbsp;0.5&nbsp;else&nbsp;0&nbsp;for&nbsp;pred&nbsp;in&nbsp;vgg_ft_preds.ravel()]) 最后一步是利用 model_evaluation_utils 模块，根据不同的分类评价指标，来评估每个模型的性能。 import model_evaluation_utils as meuimport pandas as pdbasic_cnn_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=basic_cnn_pred_labels)vgg_frz_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=vgg_frz_pred_labels)vgg_ft_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=vgg_ft_pred_labels)pd.DataFrame([basic_cnn_metrics, vgg_frz_metrics, vgg_ft_metrics], index=[&#39;Basic CNN&#39;, &#39;VGG-19 Frozen&#39;, &#39;VGG-19 Fine-tuned&#39;]) 从图中可以看到，第三个模型在测试集上的性能是最好的，其准确度和 f1-score 都达到了96%，这是一个非常好的结果，而且这个结果和论文中提到的更为复杂的模型所得到的结果具有相当的 可比性！ 结论 本文研究了一个有趣的医学影像案例——疟疾检测。疟疾检测是一个复杂的过程，而且能够进行正确操作的医疗人员也很少，这是一个很严重的问题。本文利用 AI 技术构建了一个开源的项目，该项目在疟疾检测问题上具有最高的准确率，并使AI技术为社会带来了效益。 相关链接：https://towardsdatascience.com/detecting-malaria-with-deep-learning-9e45c1e34b60 （本文为AI科技大本营编译文章，转载请微信联系 1092722531） ◆ CTA核心技术及应用峰会 ◆ 5月25-27日，由中国IT社区CSDN与数字经济人才发展中心联合主办的第一届CTA核心技术及应用峰会将在杭州国际博览中心隆重召开，峰会将围绕人工智能领域，邀请技术领航者，与开发者共同探讨机器学习和知识图谱的前沿研究及应用。 更多重磅嘉宾请识别海报二维码查看，目前会议早鸟票发售中（原票价1099元），点击阅读原文即刻抢购。添加小助手微信15101014297，备注“CTA”，了解票务以及会务详情。 推荐阅读 他25岁进贝尔实验室，32岁提信息论，40岁办达特茅斯会议，晚年患上阿兹海默 | 人物志 微软沈向洋，百度李彦宏、王海峰，阿里王坚均候选中国工程院院士 小样，加张图你就不认识我了？“补丁”模型骗你没商量！| 技术头条 东大漆桂林、清华李涓子、复旦肖仰华等大牛确认出席CTA峰会！5月一起打卡杭州 京东 60 天哗变！CTO 成优化第一人 | 畅言 异构计算=未来？一文带你秒懂3大主流异构 《互联网人叹气图鉴》 回报率29%! 大神用情感分析创建一个比特币交易算法, 原来交易玩的是心理战 她说：为啥程序员都特想要机械键盘？这答案我服！" />
<link rel="canonical" href="https://mlh.app/2019/05/02/729851.html" />
<meta property="og:url" content="https://mlh.app/2019/05/02/729851.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-02T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"作者 | Dipanjan (DJ) Sarkar 译者 | Monanfei 编辑 | Rachel、Jane 出品 | AI科技大本营（id：rgznai100） 【导读】文本基于深度学习和迁移学习方法，对疟疾等传染病检测问题进行了研究。作者对疟疾的检测原理以及迁移学习理论进行了介绍，并使用VGG-19预训练模型，进行了基于特征提取和基于参数微调的迁移学习实践。 前言 &quot;健康就是财富&quot;，这是一个老生常谈的话题，但不得不说这是一个真理。在这篇文章中，我们将研究如何利用AI技术来检测一种致命的疾病——疟疾。本文将提出一个低成本、高效率和高准确率的开源解决方案。本文有两个目的：1.了解疟疾的传染原因和其致命性；2、介绍如何运用深度学习有效检测疟疾。本章的主要内容如下： 开展本项目的动机 疟疾检测的方法 用深度学习检测疟疾 从头开始训练卷积神经网络（CNN） 利用预训练模型进行迁移学习 本文不是为了宣扬 AI 将要取代人类的工作，或者接管世界等论调，而是仅仅展示 AI 是如何用一种低成本、高效率和高准确率的方案，来帮助人类去检测和诊断疟疾，并尽量减少人工操作。 Python and TensorFlow — A great combo to build open-source deep learning solutions 在本文中，我们将使用 Python 和 tensorflow ，来构建一个强大的、可扩展的、有效的深度学习解决方案。这些工具都是免费并且开源的，这使得我们能够构建一个真正低成本、高效精准的解决方案，而且可以让每个人都可以轻松使用。让我们开始吧！ 动机 疟疾是经疟蚊叮咬而感染疟原虫所引起的虫媒传染病，疟疾最常通过受感染的雌性疟蚊来传播。虽然我们不必详细了解这种疾病，但是我们需要知道疟疾有五种常见的类型。下图展示了这种疾病的致死性在全球的分布情况。 Malaria Estimated Risk Heath Map (Source: treated.com) 从上图中可以明显看到，疟疾遍布全球，尤其是在热带区域分布密集。本项目就是基于这种疾病的特性和致命性来开展的，下面我们举个例子来说明。起初，如果你被一只受感染的蚊子叮咬了，那么蚊子所携带的寄生虫就会进入你的血液，并且开始摧毁你体内的携氧红细胞。通常来讲，你会在被疟蚊叮咬后的几天或几周内感到不适，一般会首先出现类似流感或者病毒感染的症状。然而，这些致命的寄生虫可以在你身体里完好地存活超过一年的时间，并且不产生任何其他症状！延迟接受正确的治疗，可能会导致并发症甚至死亡。因此，早期并有效的疟疾检测和排查可以挽救这些生命。 世界卫生组织（WHO）发布了几个关于疟疾的重要事实，详情见此。简而言之，世界上将近一半的人口面临疟疾风险，每年有超过2亿的疟疾病例，以及有大约40万人死于疟疾。这些事实让我们认识到，快速简单高效的疟疾检查是多么重要，这也是本文的动机所在。 疟疾检查的方法 文章《 Pre-trained convolutional neural networks as feature extractors toward improved Malaria parasite detection in thin blood smear images》（本文的数据和分析也是基于这篇文章）简要介绍了疟疾检测的几种方法，这些方法包括但是不限于厚薄血涂片检查、聚合酶链式反应（PCR）和快速诊断测试（RDT）。在本文中，我们没有对这些方法进行详细介绍，但是需要注意的一点是，后两种方法常常作为替代方案使用，尤其是在缺乏高质量显微镜服务的情况下。 我们将简要讨论基于血液涂片检测流程的标准疟疾诊断方法，首先感谢 Carlos Ariza 的博文，以及 Adrian Rosebrock 关于疟疾检查的文章，这两篇文章让我们对疟疾检查领域有了更为深入的了解。 A blood smear workflow for Malaria detection (Source) 根据上图所示的 WHO 的血液涂片检测流程，该工作包括在100倍放大倍数下对血涂片进行深入检查，其中人们需要从5000个细胞中，手动检测出含有寄生虫的红细胞。Rajaraman 等人的论文中更加详细的给出了相关的描述，如下所示： 厚血涂片有助于检测寄生虫的存在，而薄血涂片有助于识别引起感染的寄生虫种类（Centers for Disease Control and Prevention, 2012）。诊断准确性在很大程度上取决于人类的专业知识，并且可能受到观察者间的差异和观察者的可靠性所带来的不利影响，以及受到在疾病流行或资源受限的区域内的大规模诊断造成的负担所带来的不利影响（Mitiku，Mengistu＆Gelaw，2003）。替代技术，例如聚合酶链式反应（PCR）和快速诊断测试（RDT），也会被使用；但是PCR分析受到其性能的限制（Hommelsheim等，2014），而RDT在疾病流行地区的成本效益较低（Hawkes，Katsuva＆Masumbuko，2009）。 因此，传统的疟疾检测绝对是一个密集的手工过程，或许深度学习技术可以帮助它完成自动化。上文提到的这些内容为后文打下了基础。 用深度学习检测疟疾 手工诊断血液涂片，是一项重复且规律的工作，而且需要一定的专业知识来区分和统计被寄生的和未感染的细胞。如果某些地区的工作人员没有正确的专业知识，那么这种方法就不能很好地推广，并且会导致一些问题。现有工作已经取得了一些进展，包括利用最先进的图像处理和分析技术来提取手工设计的特征，并利用这些特性构建基于机器学习的分类模型。但是，由于手工设计的部分需要花费大量的时间，当有更多的数据可供训练时，模型却无法及时的进行扩展。 深度学习模型，或更具体地说，卷积神经网络（CNN）在各种计算机视觉任务中获得了非常好的效果。本文假设您已经对 CNN 有一定的了解，但是如果您并不了解 CNN ，可以通过这篇文章进行深入了解。简单来讲，CNN 最关键的层主要包括卷积层和池化层，如下图所示。 A typical CNN architeture (Source: deeplearning.net) 卷积层从数据中学习空间层级模式，这些模式具有平移不变性，因此卷积层能够学习图像的不同方面。例如，第一卷积层将学习诸如边缘和角落的微型局部模式，第二卷积层将基于第一层所提取的特征，来学习更大的图像模式，如此循序渐进。这使得 CNN 能够自动进行特征工程，并且学习有效的特征，这些特征对新的数据具有很好的泛化能力。池化层常用于下采样和降维。 因此，CNN 能够帮助我们实现自动化的和可扩展的特征工程。此外，在模型的末端接入密集层，能够使我们执行图像分类等任务。使用像CNN这样的深度学习模型，进行自动化的疟疾检测，可能是一个高效、低成本、可扩展的方案。特别是随着迁移学习的发展和预训练模型的共享，在数据量较少等限制条件下，深度学习模型也能取得很好的效果。 Rajaraman 等人的论文&nbsp;《Pre-trained convolutional neural networks as feature extractors toward improved parasite detection in thin blood smear images》利用 6 个预训练模型，在进行疟疾检测时取得了 95.9% 的准确率。本文的重点是从头开始尝试一些简单的 CNN 模型和一些预先训练的模型，并利用迁移学习来检验我们在同一数据集下得到的结果。本文将使用 Python 和 TensorFlow 框架来构建模型。 数据集的详情 首先感谢 Lister Hill 国家生物医学通信中心（LHNCBC）的研究人员（国家医学图书馆（NLM）的部门），他们仔细收集并注释了这个血涂片图像的数据集，数据中包含健康和感染这两种类型的血涂片图像。您可以从官方网站上下载这些图像。 实际上，他们开发了一款可以运行在标准安卓智能手机上的应用程序，该程序可以连接传统的光学显微镜 (Poostchi et al., 2018) 。他们从孟加拉国吉大港医学院附属医院进行拍照记录了样本集，其中包括150个恶性疟原虫感染的样本和 50 个健康的样本，每个样本都是经过 Giemsa 染色的薄血涂片。智能手机的内置摄像头可以捕获样本的每一个局部微观视图。来自泰国曼谷的玛希隆-牛津热带医学研究所的专业人员为这些图像进行了手动注释。让我们简要地看一下数据集结构。首先根据本文所使用的操作系统，我们需要安装一些基本的依赖项。 本文所使用的系统是云上的 Debian 系统，该系统配置有 GPU ，这能够加速我们模型的训练。首先安装依赖树，这能够方便我们查看目录结构。（sudo apt install tree） 从上图所示的目录结构中可以看到，我们的文件里包含两个文件夹，分别包含受感染的和健康的细胞图像。利用以下代码，我们可以进一步了解图像的总数是多少。 import osimport globbase_dir = os.path.join(&#39;./cell_images&#39;)infected_dir = os.path.join(base_dir,&#39;Parasitized&#39;)healthy_dir = os.path.join(base_dir,&#39;Uninfected&#39;)infected_files = glob.glob(infected_dir+&#39;/*.png&#39;)healthy_files = glob.glob(healthy_dir+&#39;/*.png&#39;)len(infected_files), len(healthy_files)# Output(13779, 13779) 从上述结果可以看到， 疟疾和非疟疾（未感染）的细胞图像的数据集均包含13779张图片，两个数据集的大小是相对平衡的。接下来我们将利用这些数据构建一个基于pandas的dataframe类型的数据，这对我们后续构建数据集很有帮助。 import numpy as npimport pandas as pdnp.random.seed(42)files_df = pd.DataFrame({ &#39;filename&#39;: infected_files + healthy_files, &#39;label&#39;: [&#39;malaria&#39;] * len(infected_files) + [&#39;healthy&#39;] * len(healthy_files)}).sample(frac=1, random_state=42).reset_index(drop=True)files_df.head() 构建和探索图像数据集 在构建深度学习模型之前，我们不仅需要训练数据，还需要未用于训练的数据来验证和测试模型的性能。本文采用 60:10:30 的比例来划分训练集、验证集和测试集。我们将使用训练集和验证集来训练模型，并利用测试集来检验模型的性能。 from sklearn.model_selection import train_test_splitfrom collections import Countertrain_files, test_files, train_labels, test_labels = train_test_split(files_df[&#39;filename&#39;].values, files_df[&#39;label&#39;].values, test_size=0.3, random_state=42) train_files, val_files, train_labels, val_labels = train_test_split(train_files, train_labels, test_size=0.1, random_state=42)print(train_files.shape, val_files.shape, test_files.shape)print(&#39;Train:&#39;, Counter(train_labels), &#39;\\nVal:&#39;, Counter(val_labels), &#39;\\nTest:&#39;, Counter(test_labels))# Output(17361,) (1929,) (8268,)Train: Counter({&#39;healthy&#39;: 8734, &#39;malaria&#39;: 8627}) Val: Counter({&#39;healthy&#39;: 970, &#39;malaria&#39;: 959}) Test: Counter({&#39;malaria&#39;: 4193, &#39;healthy&#39;: 4075}) 可以发现，由于血液来源、测试方法以及图像拍摄的方向不同，血液涂片和细胞的图像尺寸不尽相同。我们需要获取一些训练数据的统计信息，从而确定最优的图像尺寸（请注意，在这里我们完全没用到测试集！）。 import cv2from concurrent import futuresimport threadingdef get_img_shape_parallel(idx, img, total_imgs): if idx % 5000 == 0 or idx == (total_imgs - 1): print(&#39;{}: working on img num:{}&#39;.format(threading.current_thread().name,idx)) return cv2.imread(img).shape ex = futures.ThreadPoolExecutor(max_workers=None)data_inp = [(idx, img, len(train_files)) for idx, img in enumerate(train_files)]print(&#39;Starting Img shape computation:&#39;)train_img_dims_map = ex.map(get_img_shape_parallel, [record[0] for record in data_inp], [record[1] for record in data_inp], [record[2] for record in data_inp])train_img_dims = list(train_img_dims_map)print(&#39;Min Dimensions:&#39;, np.min(train_img_dims, axis=0)) print(&#39;Avg Dimensions:&#39;, np.mean(train_img_dims, axis=0))print(&#39;Median Dimensions:&#39;, np.median(train_img_dims, axis=0))print(&#39;Max Dimensions:&#39;, np.max(train_img_dims, axis=0))# OutputStarting Img shape computation:ThreadPoolExecutor-0_0: working on img num: 0ThreadPoolExecutor-0_17: working on img num: 5000ThreadPoolExecutor-0_15: working on img num: 10000ThreadPoolExecutor-0_1: working on img num: 15000ThreadPoolExecutor-0_7: working on img num: 17360Min Dimensions: [46 46 3]Avg Dimensions: [132.77311215 132.45757733 3.]Median Dimensions: [130. 130. 3.]Max Dimensions: [385 394 3] 我们采用了并行处理的策略来加速图像读取操作。基于汇总的统计信息，我们决定将每张图像的大小调整为125x125。现在让我们加载所有的图像，并把他们的大小都调整为上述固定的尺寸。 IMG_DIMS = (125, 125)def get_img_data_parallel(idx, img, total_imgs): if idx % 5000 == 0 or idx == (total_imgs - 1): print(&#39;{}: working on img num: {}&#39;.format(threading.current_thread().name,idx)) img = cv2.imread(img) img = cv2.resize(img, dsize=IMG_DIMS, interpolation=cv2.INTER_CUBIC) img = np.array(img, dtype=np.float32) return imgex = futures.ThreadPoolExecutor(max_workers=None)train_data_inp = [(idx, img, len(train_files)) for idx, img in enumerate(train_files)]val_data_inp = [(idx, img, len(val_files)) for idx, img in enumerate(val_files)]test_data_inp = [(idx, img, len(test_files)) for idx, img in enumerate(test_files)]print(&#39;Loading Train Images:&#39;)train_data_map = ex.map(get_img_data_parallel, [record[0] for record in train_data_inp], [record[1] for record in train_data_inp], [record[2] for record in train_data_inp])train_data = np.array(list(train_data_map))print(&#39;\\nLoading Validation Images:&#39;)val_data_map = ex.map(get_img_data_parallel, [record[0] for record in val_data_inp], [record[1] for record in val_data_inp], [record[2] for record in val_data_inp])val_data = np.array(list(val_data_map))print(&#39;\\nLoading Test Images:&#39;)test_data_map = ex.map(get_img_data_parallel, [record[0] for record in test_data_inp], [record[1] for record in test_data_inp], [record[2] for record in test_data_inp])test_data = np.array(list(test_data_map))train_data.shape, val_data.shape, test_data.shape # OutputLoading Train Images:ThreadPoolExecutor-1_0: working on img num: 0ThreadPoolExecutor-1_12: working on img num: 5000ThreadPoolExecutor-1_6: working on img num: 10000ThreadPoolExecutor-1_10: working on img num: 15000ThreadPoolExecutor-1_3: working on img num: 17360Loading Validation Images:ThreadPoolExecutor-1_13: working on img num: 0ThreadPoolExecutor-1_18: working on img num: 1928Loading Test Images:ThreadPoolExecutor-1_5: working on img num: 0ThreadPoolExecutor-1_19: working on img num: 5000ThreadPoolExecutor-1_8: working on img num: 8267((17361, 125, 125, 3), (1929, 125, 125, 3), (8268, 125, 125, 3)) 我们再次运用了并行处理策略来加速图像加载和尺寸调整的计算，如上面输出结果中展示的，我们最终得到了所需尺寸的图像张量。现在我们可以查看一些样本的细胞图像，从而从直观上认识一下我们的数据的情况。 import matplotlib.pyplot as plt%matplotlib inlineplt.figure(1 , figsize = (8 , 8))n = 0 for i in range(16): n += 1 r = np.random.randint(0 , train_data.shape[0] , 1) plt.subplot(4 , 4 , n) plt.subplots_adjust(hspace = 0.5 , wspace = 0.5) plt.imshow(train_data[r[0]]/255.) plt.title(&#39;{}&#39;.format(train_labels[r[0]])) plt.xticks([]) , plt.yticks([]) 从上面的样本图像可以看出，疟疾和健康细胞图像之间存在一些细微差别。我们将构建深度学习模型，通过不断训练来使模型尝试学习这些模式。在开始训练模型之前，我们先对模型的参数进行一些基本的设置。 BATCH_SIZE = 64NUM_CLASSES = 2EPOCHS = 25INPUT_SHAPE = (125, 125, 3)train_imgs_scaled = train_data / 255.val_imgs_scaled = val_data / 255.# encode text category labelsfrom sklearn.preprocessing import LabelEncoderle = LabelEncoder()le.fit(train_labels)train_labels_enc = le.transform(train_labels)val_labels_enc = le.transform(val_labels)print(train_labels[:6], train_labels_enc[:6])# Output[&#39;malaria&#39; &#39;malaria&#39; &#39;malaria&#39; &#39;healthy&#39; &#39;healthy&#39; &#39;malaria&#39;][1 1 1 0 0 1] 上面的代码设定了图像的维度，批尺寸，epoch 的次数，并且对我们的类别标签进行了编码。TensorFLow 2.0 alpha 版本在2019年3月发布，它为我们项目的实施提供了一个完美的接口。 import tensorflow as tf# Load the TensorBoard notebook extension (optional)%load_ext tensorboard.notebooktf.random.set_seed(42)tf.__version__# Output&#39;2.0.0-alpha0&#39; 深度学习模型的训练阶段 在模型训练阶段，我们将构建几个深度学习模型，利用前面构建的训练集进行训练，并在验证集上比较它们的性能。然后，我们将保存这些模型，并在模型评估阶段再次使用它们。 模型1：从头开始训练CNN 对于本文的第一个疟疾检测模型，我们将构建并从头开始训练一个基本的卷积神经网络（CNN）。首先，我们需要定义模型的结构。 inp = tf.keras.layers.Input(shape=INPUT_SHAPE)conv1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(inp)pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)conv2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(pool1)pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)conv3 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)(pool2)pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)flat = tf.keras.layers.Flatten()(pool3)hidden1 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(flat)drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)hidden2 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(drop1)drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)out = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(drop2)model = tf.keras.Model(inputs=inp, outputs=out)model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])model.summary()# OutputModel: &quot;model&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_1 (InputLayer) [(None, 125, 125, 3)] 0 _________________________________________________________________conv2d (Conv2D) (None, 125, 125, 32) 896 _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 62, 62, 32) 0 _________________________________________________________________conv2d_1 (Conv2D) (None, 62, 62, 64) 18496 _________________________________________________________________......_________________________________________________________________dense_1 (Dense) (None, 512) 262656 _________________________________________________________________dropout_1 (Dropout) (None, 512) 0 _________________________________________________________________dense_2 (Dense) (None, 1) 513 =================================================================Total params: 15,102,529Trainable params: 15,102,529Non-trainable params: 0_________________________________________________________________ 上述代码所构建的 CNN 模型，包含3个卷积层、1个池化层以及2个全连接层，并对全连接层设置 dropout 参数用于正则化。现在让我们开始训练模型吧！ import datetimelogdir = os.path.join(&#39;/home/dipanzan_sarkar/projects/tensorboard_logs&#39;, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=&#39;val_loss&#39;,factor=0.5,patience=2, min_lr=0.000001)callbacks = [reduce_lr, tensorboard_callback]history = model.fit(x=train_imgs_scaled, y=train_labels_enc, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(val_imgs_scaled, val_labels_enc), callbacks=callbacks, verbose=1) # OutputTrain on 17361 samples, validate on 1929 samplesEpoch 1/2517361/17361 [====] - 32s 2ms/sample - loss: 0.4373 - accuracy: 0.7814 - val_loss: 0.1834 - val_accuracy: 0.9393Epoch 2/2517361/17361 [====] - 30s 2ms/sample - loss: 0.1725 - accuracy: 0.9434 - val_loss: 0.1567 - val_accuracy: 0.9513......Epoch 24/2517361/17361 [====] - 30s 2ms/sample - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.3693 - val_accuracy: 0.9565Epoch 25/2517361/17361 [====] - 30s 2ms/sample - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.3699 - val_accuracy: 0.9559 &nbsp; 从上面的结果可以看到，我们的模型在验证集上的准确率为 95.6% ，这是非常好的。我们注意到模型在训练集上的准确率为 99.9% ，这看起来有一些过拟合。为了更加清晰地查看这个问题，我们可以分别绘制在训练和验证阶段的准确度曲线和损失曲线。 f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))t = f.suptitle(&#39;Basic CNN Performance&#39;, fontsize=12)f.subplots_adjust(top=0.85, wspace=0.3)max_epoch = len(history.history[&#39;accuracy&#39;])+1epoch_list = list(range(1,max_epoch))ax1.plot(epoch_list, history.history[&#39;accuracy&#39;], label=&#39;Train Accuracy&#39;)ax1.plot(epoch_list, history.history[&#39;val_accuracy&#39;], label=&#39;Validation Accuracy&#39;)ax1.set_xticks(np.arange(1, max_epoch, 5))ax1.set_ylabel(&#39;Accuracy Value&#39;)ax1.set_xlabel(&#39;Epoch&#39;)ax1.set_title(&#39;Accuracy&#39;)l1 = ax1.legend(loc=&quot;best&quot;)ax2.plot(epoch_list, history.history[&#39;loss&#39;], label=&#39;Train Loss&#39;)ax2.plot(epoch_list, history.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39;)ax2.set_xticks(np.arange(1, max_epoch, 5))ax2.set_ylabel(&#39;Loss Value&#39;)ax2.set_xlabel(&#39;Epoch&#39;)ax2.set_title(&#39;Loss&#39;)l2 = ax2.legend(loc=&quot;best&quot;) Learning Curves for Basic&nbsp;CNN 从图中可以看出，在第5个 epoch 之后，在验证集上的精度似乎不再提高。我们先将这个模型保存，在后面我们会再次用到它。 model.save(&#39;basic_cnn.h5&#39;) 深度迁移学习 就像人类能够运用知识完成跨任务工作一样，迁移学习使得我们能够利用在先前任务中学习到的知识，来处理新的任务，在机器学习和深度学习的环境下也是如此。这些文章涵盖了迁移学习的详细介绍和讨论，有兴趣的读者可以参考学习。 Ideas for deep transfer&nbsp;learning 我们能否采用迁移学习的思想，将预训练的深度学习模型（已在大型数据集上进行过训练的模型——例如 ImageNet）的知识应用到我们的问题——进行疟疾检测上呢？我们将采用两种目前最主流的迁移学习策略。 将预训练模型作为特征提取器 对预训练模型进行微调 我们将使用由牛津大学视觉几何组（VGG）所开发的预训练模型 VGG-19 进行实验。像 VGG-19 这样的预训练模型，一般已经在大型数据集上进行过训练，这些数据集涵盖多种类别的图像。基于此，这些预训练模型应该已经使用CNN模型学习到了一个具有高度鲁棒性的特征的层次结构，并且其应具有尺度、旋转和平移不变性。因此，这个已经学习了超过一百万个图像的具有良好特征表示的模型，可以作为一个很棒的图像特征提取器，为包括疟疾检测问题在内的其他计算机视觉问题服务。在引入强大的迁移学习之前，我们先简要讨论一下 VGG-19 的结构。 理解VGG-19模型 VGG-19 是一个具有 19 个层（包括卷积层和全连接层）的深度学习网络，该模型基于 ImageNet 数据集进行训练，该数据集是专门为图像识别和分类所构建的。VGG-19 是由 Karen Simonyan 和 Andrew Zisserman 提出的，该模型在他们的论文《Very Deep Convolutional Networks for Large-Scale Image Recognition》中有详细介绍，建议有兴趣的读者可以去读一读这篇优秀的论文。VGG-19 模型的结构如下图所示。 VGG-19 Model Architecture 从上图可以清楚地看到，该模型具有 16 个使用 3x3 卷积核的卷积层，其中部分卷积层后面接了一个最大池化层，用于下采样；随后依次连接了两个具有 4096 个隐层神经元的全连接层，接着连接了一个具有 1000 个隐层神经元的全连接层， 最后一个全连接层的每个神经元都代表 ImageNet 数据集中的一个图像类别。由于我们需要使用新的全连接层来分类疟疾，因此我们不需要最后的三个全连接层。我们更关心的是前五个块，以便我们可以利用 VGG 模型作为有效的特征提取器。 前文提到有两种迁移学习的策略，对于第一种策略，我们将把 VGG 模型当做一个特征提取器，这可以通过冻结前五个卷积块，使得它们的权重参数不会随着新的训练过程而更新来实现。对于第二种策略，我们将会解冻最后的两个卷积块（模块4和模块5），从而使得它们的参数会随着新的训练过程而不断更新。 模型2：将预训练模型作为特征提取机 为了构建这个模型，我们将利用 TensorFlow 加载 VGG-19 模型，并冻结它的卷积块，以便我们可以将其用作图像特征提取器。我们将在该模型的末尾插入自己的全连接层，用于执行本文的分类任务。 vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights=&#39;imagenet&#39;,input_shape=INPUT_SHAPE)vgg.trainable = False# Freeze the layersfor layer in vgg.layers: layer.trainable = False base_vgg = vggbase_out = base_vgg.outputpool_out = tf.keras.layers.Flatten()(base_out)hidden1 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(pool_out)drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)hidden2 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(drop1)drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)out = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(drop2)model = tf.keras.Model(inputs=base_vgg.input, outputs=out)model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])model.summary()# OutputModel: &quot;model_1&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_2 (InputLayer) [(None, 125, 125, 3)] 0 _________________________________________________________________block1_conv1 (Conv2D) (None, 125, 125, 64) 1792 _________________________________________________________________block1_conv2 (Conv2D) (None, 125, 125, 64) 36928 _________________________________________________________________......_________________________________________________________________block5_pool (MaxPooling2D) (None, 3, 3, 512) 0 _________________________________________________________________flatten_1 (Flatten) (None, 4608) 0 _________________________________________________________________dense_3 (Dense) (None, 512) 2359808 _________________________________________________________________dropout_2 (Dropout) (None, 512) 0 _________________________________________________________________dense_4 (Dense) (None, 512) 262656 _________________________________________________________________dropout_3 (Dropout) (None, 512) 0 _________________________________________________________________dense_5 (Dense) (None, 1) 513 =================================================================Total params: 22,647,361Trainable params: 2,622,977Non-trainable params: 20,024,384 从上面代码的输出可以看到，我们的模型有很多层，并且我们仅仅只利用了 VGG-19 的冻结层来提取特征。下面的代码可以验证本模型中有多少层用于训练，以及检验本模型中一共有多少层。 print(&quot;Total Layers:&quot;, len(model.layers))print(&quot;Total trainable layers:&quot;,sum([1 for l in model.layers if l.trainable]))# OutputTotal Layers: 28Total trainable layers: 6 现在我们将训练该模型，在训练过程中所用到的配置和回调函数与模型1中的类似，完整的代码可以参考github链接。下图展示了在训练过程中，模型的准确度曲线和损失曲线。 Learning Curves for frozen pre-trained CNN 从上图可以看出，该模型不像模型1中基本的 CNN 模型那样存在过拟合的现象，但是性能并不是很好。事实上，它的性能还没有基本的 CNN 模型好。现在我们将模型保存，用于后续的评估。 model.save（ &#39;vgg_frozen.h5&#39;） 模型3：具有图像增广的微调的预训练模型 在这个模型中，我们将微调预训练 VGG-19 模型的最后两个区块中层的权重。除此之外，我们还将介绍图像增广的概念。图像增广背后的原理与它的名称听起来完全一样。我们首先从训练数据集中加载现有的图像，然后对它们进行一些图像变换的操作，例如旋转，剪切，平移，缩放等，从而生成现有图像的新的、变化的版本。由于这些随机变换的操作，我们每次都会得到不同的图像。我们将使用 tf.keras 中的 ImageDataGenerator 工具，它能够帮助我们实现图像增广。 train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, zoom_range=0.05, rotation_range=25, width_shift_range=0.05, height_shift_range=0.05, shear_range=0.05, horizontal_flip=True, fill_mode=&#39;nearest&#39;)val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)# build image augmentation generatorstrain_generator = train_datagen.flow(train_data, train_labels_enc, batch_size=BATCH_SIZE, shuffle=True)val_generator = val_datagen.flow(val_data, val_labels_enc, batch_size=BATCH_SIZE, shuffle=False) 在验证集上，我们只会对图像进行缩放操作，而不进行其他的转换，这是因为我们需要在每个训练的 epoch 结束后，用验证集来评估我们的模型。有关图像增广的详细说明，可以参考这篇文章。让我们来看看进行图像增广变换后的一些样本结果。 img_id = 0sample_generator = train_datagen.flow(train_data[img_id:img_id+1], train_labels[img_id:img_id+1],batch_size=1)sample = [next(sample_generator) for i in range(0,5)]fig, ax = plt.subplots(1,5, figsize=(16, 6))print(&#39;Labels:&#39;, [item[1][0] for item in sample])l = [ax[i].imshow(sample[i][0][0]) for i in range(0,5)] Sample Augmented Images 从上图可以清楚的看到图像发生了轻微的变化。现在我们将构建新的深度模型，该模型需要确保 VGG-19 模型的最后两个块可以进行训练。 vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights=&#39;imagenet&#39;,input_shape=INPUT_SHAPE)# Freeze the layersvgg.trainable = Trueset_trainable = Falsefor layer in vgg.layers: if layer.name in [&#39;block5_conv1&#39;, &#39;block4_conv1&#39;]: set_trainable = True if set_trainable: layer.trainable = True else: layer.trainable = False base_vgg = vggbase_out = base_vgg.outputpool_out = tf.keras.layers.Flatten()(base_out)hidden1 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(pool_out)drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)hidden2 = tf.keras.layers.Dense(512, activation=&#39;relu&#39;)(drop1)drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)out = tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)(drop2)model = tf.keras.Model(inputs=base_vgg.input, outputs=out)model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-5),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])print(&quot;Total Layers:&quot;, len(model.layers))print(&quot;Total trainable layers:&quot;, sum([1 for l in model.layers if l.trainable]))# OutputTotal Layers: 28Total trainable layers: 16 由于我们不希望在微调过程中，对预训练的层进行较大的权重更新，我们降低了模型的学习率。由于我们使用数据生成器来加载数据，本模型的训练过程会和之前稍稍不同，在这里，我们需要用到函数 fit_generator(…) 。 tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.5,patience=2, min_lr=0.000001)callbacks = [reduce_lr, tensorboard_callback]train_steps_per_epoch = train_generator.n //train_generator.batch_sizeval_steps_per_epoch = val_generator.n //val_generator.batch_sizehistory = model.fit_generator(train_generator, steps_per_epoch=train_steps_per_epoch,epochs=EPOCHS,validation_data=val_generator,validation_steps=val_steps_per_epoch,verbose=1)# OutputEpoch 1/25271/271 [====] - 133s 489ms/step - loss: 0.2267 - accuracy: 0.9117 - val_loss: 0.1414 - val_accuracy: 0.9531Epoch 2/25271/271 [====] - 129s 475ms/step - loss: 0.1399 - accuracy: 0.9552 - val_loss: 0.1292 - val_accuracy: 0.9589......Epoch 24/25271/271 [====] - 128s 473ms/step - loss: 0.0815 - accuracy: 0.9727 - val_loss: 0.1466 - val_accuracy: 0.9682Epoch 25/25271/271 [====] - 128s 473ms/step - loss: 0.0792 - accuracy: 0.9729 - val_loss: 0.1127 - val_accuracy: 0.9641 下图展示了该模型的训练曲线，可以看出该模型是这三个模型中最好的模型，其验证准确度几乎达到了 96.5% ，而且从训练准确度上看，我们的模型也没有像第一个模型那样出现过拟合。 Learning Curves for fine-tuned pre-trained CNN 现在让我们保存这个模型，很快我们将在测试集上用到它进行性能评估。 model.save（ &#39;vgg_finetuned.h5&#39;） 至此，模型训练阶段告一段落，我们即将在真实的测试集上去测试这些模型的性能。 深度学习模型的性能评估阶段 现在，我们将对之前训练好的三个模型进行评估。仅仅使用验证集来评估模型的好坏是不够的， 因此，我们将使用测试集来进一步评估模型的性能。我们构建了一个实用的模块 model_evaluation_utils，该模块采用相关的分类指标，用于评估深度学习模型的性能。首先我们需要将测试数据进行缩放。 test_imgs_scaled = test_data / 255.test_imgs_scaled.shape, test_labels.shape# Output((8268, 125, 125, 3), (8268,)) 第二步是加载之前所保存的深度学习模型，然后在测试集上进行预测。 # Load Saved Deep Learning Modelsbasic_cnn = tf.keras.models.load_model(&#39;./basic_cnn.h5&#39;)vgg_frz = tf.keras.models.load_model(&#39;./vgg_frozen.h5&#39;)vgg_ft = tf.keras.models.load_model(&#39;./vgg_finetuned.h5&#39;)# Make Predictions on Test Databasic_cnn_preds = basic_cnn.predict(test_imgs_scaled, batch_size=512)vgg_frz_preds = vgg_frz.predict(test_imgs_scaled, batch_size=512)vgg_ft_preds = vgg_ft.predict(test_imgs_scaled, batch_size=512)basic_cnn_pred_labels = le.inverse_transform([1 if pred &gt; 0.5 else 0 for pred in basic_cnn_preds.ravel()])vgg_frz_pred_labels = le.inverse_transform([1 if pred &gt; 0.5 else 0 for pred in vgg_frz_preds.ravel()])vgg_ft_pred_labels&nbsp;=&nbsp;le.inverse_transform([1&nbsp;if&nbsp;pred&nbsp;&gt;&nbsp;0.5&nbsp;else&nbsp;0&nbsp;for&nbsp;pred&nbsp;in&nbsp;vgg_ft_preds.ravel()]) 最后一步是利用 model_evaluation_utils 模块，根据不同的分类评价指标，来评估每个模型的性能。 import model_evaluation_utils as meuimport pandas as pdbasic_cnn_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=basic_cnn_pred_labels)vgg_frz_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=vgg_frz_pred_labels)vgg_ft_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=vgg_ft_pred_labels)pd.DataFrame([basic_cnn_metrics, vgg_frz_metrics, vgg_ft_metrics], index=[&#39;Basic CNN&#39;, &#39;VGG-19 Frozen&#39;, &#39;VGG-19 Fine-tuned&#39;]) 从图中可以看到，第三个模型在测试集上的性能是最好的，其准确度和 f1-score 都达到了96%，这是一个非常好的结果，而且这个结果和论文中提到的更为复杂的模型所得到的结果具有相当的 可比性！ 结论 本文研究了一个有趣的医学影像案例——疟疾检测。疟疾检测是一个复杂的过程，而且能够进行正确操作的医疗人员也很少，这是一个很严重的问题。本文利用 AI 技术构建了一个开源的项目，该项目在疟疾检测问题上具有最高的准确率，并使AI技术为社会带来了效益。 相关链接：https://towardsdatascience.com/detecting-malaria-with-deep-learning-9e45c1e34b60 （本文为AI科技大本营编译文章，转载请微信联系 1092722531） ◆ CTA核心技术及应用峰会 ◆ 5月25-27日，由中国IT社区CSDN与数字经济人才发展中心联合主办的第一届CTA核心技术及应用峰会将在杭州国际博览中心隆重召开，峰会将围绕人工智能领域，邀请技术领航者，与开发者共同探讨机器学习和知识图谱的前沿研究及应用。 更多重磅嘉宾请识别海报二维码查看，目前会议早鸟票发售中（原票价1099元），点击阅读原文即刻抢购。添加小助手微信15101014297，备注“CTA”，了解票务以及会务详情。 推荐阅读 他25岁进贝尔实验室，32岁提信息论，40岁办达特茅斯会议，晚年患上阿兹海默 | 人物志 微软沈向洋，百度李彦宏、王海峰，阿里王坚均候选中国工程院院士 小样，加张图你就不认识我了？“补丁”模型骗你没商量！| 技术头条 东大漆桂林、清华李涓子、复旦肖仰华等大牛确认出席CTA峰会！5月一起打卡杭州 京东 60 天哗变！CTO 成优化第一人 | 畅言 异构计算=未来？一文带你秒懂3大主流异构 《互联网人叹气图鉴》 回报率29%! 大神用情感分析创建一个比特币交易算法, 原来交易玩的是心理战 她说：为啥程序员都特想要机械键盘？这答案我服！","@type":"BlogPosting","url":"https://mlh.app/2019/05/02/729851.html","headline":"医生再添新助手！深度学习诊断传染病 完整代码+实操","dateModified":"2019-05-02T00:00:00+08:00","datePublished":"2019-05-02T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/02/729851.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>医生再添新助手！深度学习诊断传染病 | 完整代码+实操</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="rich_media_content" id="js_content"> 
   <p style="margin-left:8px;min-height:1em;letter-spacing:.544px;text-align:center;"><img class="rich_pages" style="color:rgb(136,136,136);font-size:15px;letter-spacing:1px;text-align:center;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/BnSNEaficFAaEvfrVTwTOJmJOdd48GLj5h1RyAH4y6ib8lZENjZq9y9IjJGT5scoiaetlqgA8yoKrqr2yugAHs58w/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></p>
   <p style="min-height:1em;letter-spacing:.544px;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"></span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(136,136,136);font-size:14px;letter-spacing:.5px;">作者 | Dipanjan (DJ) Sarkar</span><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(136,136,136);font-size:14px;letter-spacing:.5px;">译者 | Monanfei</span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(136,136,136);font-size:14px;letter-spacing:.5px;">编辑 | Rachel、Jane</span></p>
   <p style="line-height:1.75em;"><span style="color:rgb(136,136,136);font-size:14px;letter-spacing:.5px;">出品 | AI科技大本营（id：</span><span style="color:rgb(136,136,136);font-size:14px;letter-spacing:.5px;">rgznai100）</span></p>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><span style="color:rgb(136,136,136);font-family:'-webkit-standard';font-size:15px;letter-spacing:1px;">【导读】文本基于深度学习和迁移学习方法，对疟疾等传染病检测问题进行了研究。作者对疟疾的检测原理以及迁移学习理论进行了介绍，并使用VGG-19预训练模型，进行了基于特征提取和基于参数微调的迁移学习实践。</span></p>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">前言</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">"健康就是财富"，这是一个老生常谈的话题，但不得不说这是一个真理。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">在这篇文章中，我们将研究如何利用AI技术来检测一种致命的疾病——疟疾。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本文将提出一个低成本、高效率和高准确率的开源解决方案。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本文有两个目的：</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">1.了解疟疾的传染原因和其致命性；</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">2、介绍如何运用深度学习有效检测疟疾。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本章的主要内容如下：</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul type="disc" class="undefined list-paddingleft-2" style="margin-left:8px;">
    <li><p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">开展本项目的动机</span></p></li>
   </ul>
   <ul type="disc" class="undefined list-paddingleft-2" style="margin-left:8px;">
    <li><p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">疟疾检测的方法</span></p></li>
   </ul>
   <ul type="disc" class="undefined list-paddingleft-2" style="margin-left:8px;">
    <li><p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">用深度学习检测疟疾</span></p></li>
   </ul>
   <ul type="disc" class="undefined list-paddingleft-2" style="margin-left:8px;">
    <li><p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从头开始训练卷积神经网络（CNN）</span></p></li>
   </ul>
   <ul type="disc" class="undefined list-paddingleft-2" style="margin-left:8px;">
    <li><p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">利用预训练模型进行迁移学习</span></p></li>
   </ul>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本文不是为了宣扬 AI 将要取代人类的工作，或者接管世界等论调，而是仅仅展示 AI 是如何用一种低成本、高效率和高准确率的方案，来帮助人类去检测和诊断疟疾，并尽量减少人工操作。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img" style="width:547px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4Gnu1sJtxYicsI01k2VEWlU0613OghHGfoibteYzUbncUicmm4icTcqUPiaykbQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;text-align:center;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">Python and TensorFlow — A great combo to build open-source deep learning solutions</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">在本文中，我们将使用 Python 和 tensorflow ，来构建一个强大的、可扩展的、有效的深度学习解决方案。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">这些工具都是免费并且开源的，这使得我们能够构建一个真正低成本、高效精准的解决方案，而且可以让每个人都可以轻松使用。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">让我们开始吧！</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">动机</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">疟疾是经疟蚊叮咬而感染疟原虫所引起的虫媒传染病，疟疾最常通过受感染的雌性疟蚊来传播。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">虽然我们不必详细了解这种疾病，但是我们需要知道疟疾有五种常见的类型。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">下图展示了这种疾病的致死性在全球的分布情况。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img" style="width:547px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4Gnua9RRjph72RLzWU73AXUQsAOfZibMbTtMjficsznPibAK2vtYlfaBU9vaQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;text-align:center;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">Malaria Estimated Risk Heath Map (Source: treated.com)</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从上图中可以明显看到，疟疾遍布全球，尤其是在热带区域分布密集。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本项目就是基于这种疾病的特性和致命性来开展的，下面我们举个例子来说明。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">起初，如果你被一只受感染的蚊子叮咬了，那么蚊子所携带的寄生虫就会进入你的血液，并且开始摧毁你体内的携氧红细胞。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">通常来讲，你会在被疟蚊叮咬后的几天或几周内感到不适，一般会首先出现类似流感或者病毒感染的症状。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">然而，这些致命的寄生虫可以在你身体里完好地存活超过一年的时间，并且不产生任何其他症状！</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">延迟接受正确的治疗，可能会导致并发症甚至死亡。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">因此，早期并有效的疟疾检测和排查可以挽救这些生命。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">世界卫生组织（WHO）发布了几个关于疟疾的重要事实，</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">详情见此</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">简而言之，世界上将近一半的人口面临疟疾风险，每年有超过2亿的疟疾病例，以及有大约40万人死于疟疾。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">这些事实让我们认识到，快速简单高效的疟疾检查是多么重要，这也是本文的动机所在。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">疟疾检查的方法</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">文章《</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"> Pre-trained convolutional neural networks as feature extractors toward improved Malaria parasite detection in thin blood smear images</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">》（本文的数据和分析也是基于这篇文章）简要介绍了疟疾检测的几种方法，这些方法包括但是不限于厚薄血涂片检查、聚合酶链式反应（PCR）和快速诊断测试（RDT）。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">在本文中，我们没有对这些方法进行详细介绍，但是需要注意的一点是，后两种方法常常作为替代方案使用，尤其是在缺乏高质量显微镜服务的情况下。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们将简要讨论基于血液涂片检测流程的标准疟疾诊断方法，首先感谢 Carlos Ariza 的</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">博文</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">，以及 Adrian Rosebrock 关于疟疾检查的</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">文章</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">，这两篇文章让我们对疟疾检查领域有了更为深入的了解。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img" style="width:547px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4GnuLOBk6bbCIibUjLflSseImStyjqs7icUj0S838WlzFPNVFnbQFFHoXvZg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;text-align:center;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">A blood smear workflow for Malaria detection (Source)</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">根据上图所示的 WHO 的血液涂片检测流程，该工作包括在100倍放大倍数下对血涂片进行深入检查，其中人们需要从5000个细胞中，手动检测出含有寄生虫的红细胞。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">Rajaraman 等人的论文中更加详细的给出了相关的描述，如下所示：</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">厚血涂片有助于检测寄生虫的存在，而薄血涂片有助于识别引起感染的寄生虫种类（Centers for Disease Control and Prevention, 2012）。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">诊断准确性在很大程度上取决于人类的专业知识，并且可能受到观察者间的差异和观察者的可靠性所带来的不利影响，以及受到在疾病流行或资源受限的区域内的大规模诊断造成的负担所带来的不利影响（Mitiku，Mengistu＆Gelaw，2003）。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">替代技术，例如聚合酶链式反应（PCR）和快速诊断测试（RDT），也会被使用；</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">但是PCR分析受到其性能的限制（Hommelsheim等，2014），而RDT在疾病流行地区的成本效益较低（Hawkes，Katsuva＆Masumbuko，2009）。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">因此，传统的疟疾检测绝对是一个密集的手工过程，或许深度学习技术可以帮助它完成自动化。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">上文提到的这些内容为后文打下了基础。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">用深度学习检测疟疾</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">手工诊断血液涂片，是一项重复且规律的工作，而且需要一定的专业知识来区分和统计被寄生的和未感染的细胞。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">如果某些地区的工作人员没有正确的专业知识，那么这种方法就不能很好地推广，并且会导致一些问题。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">现有工作已经取得了一些进展，包括利用最先进的图像处理和分析技术来提取手工设计的特征，并利用这些特性构建基于机器学习的分类模型。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">但是，由于手工设计的部分需要花费大量的时间，当有更多的数据可供训练时，模型却无法及时的进行扩展。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">深度学习模型，或更具体地说，卷积神经网络（CNN）在各种计算机视觉任务中获得了非常好的效果。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本文假设您已经对 CNN 有一定的了解，但是如果您并不了解 CNN ，可以通过这篇</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">文章</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">进行深入了解。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">简单来讲，CNN 最关键的层主要包括卷积层和池化层，如下图所示。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img" style="width:547px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4GnuPN4mIETLuEsdAVUrvribFgvnrM7SRgSgt9FeqicX2LIgavicjnv47ibrFw/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;text-align:center;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">A typical CNN architeture (Source: deeplearning.net)</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">卷积层从数据中学习空间层级模式，这些模式具有平移不变性，因此卷积层能够学习图像的不同方面。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">例如，第一卷积层将学习诸如边缘和角落的微型局部模式，第二卷积层将基于第一层所提取的特征，来学习更大的图像模式，如此循序渐进。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">这使得 CNN 能够自动进行特征工程，并且学习有效的特征，这些特征对新的数据具有很好的泛化能力。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">池化层常用于下采样和降维。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">因此，CNN 能够帮助我们实现自动化的和可扩展的特征工程。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">此外，在模型的末端接入密集层，能够使我们执行图像分类等任务。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">使用像CNN这样的深度学习模型，进行自动化的疟疾检测，可能是一个高效、低成本、可扩展的方案。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">特别是随着迁移学习的发展和预训练模型的共享，在数据量较少等限制条件下，深度学习模型也能取得很好的效果。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">Rajaraman 等人的论文&nbsp;《</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">Pre-trained convolutional neural networks as feature extractors toward improved parasite detection in thin blood smear images</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">》利用 6 个预训练模型，在进行疟疾检测时取得了 95.9% 的准确率。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本文的重点是从头开始尝试一些简单的 CNN 模型和一些预先训练的模型，并利用迁移学习来检验我们在同一数据集下得到的结果。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本文将使用 Python 和 TensorFlow 框架来构建模型。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">数据集的详情</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">首先感谢 Lister Hill 国家生物医学通信中心（LHNCBC）的研究人员（国家医学图书馆（NLM）的部门），他们仔细收集并注释了这个血涂片图像的数据集，数据中包含健康和感染这两种类型的血涂片图像。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">您可以从</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">官方网站</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">上下载这些图像。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">实际上，他们开发了一款可以运行在标准安卓智能手机上的应用程序，该程序可以连接传统的光学显微镜 (Poostchi et al., 2018) 。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">他们从孟加拉国吉大港医学院附属医院进行拍照记录了样本集，其中包括150个恶性疟原虫感染的样本和 50 个健康的样本，每个样本都是经过 Giemsa 染色的薄血涂片。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">智能手机的内置摄像头可以捕获样本的每一个局部微观视图。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">来自泰国曼谷的玛希隆-牛津热带医学研究所的专业人员为这些图像进行了手动注释。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">让我们简要地看一下数据集结构。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">首先根据本文所使用的操作系统，我们需要安装一些基本的依赖项。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img" style="width:358px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4Gnu2kx5c9fUEeGReC6vwKMxLiajnHH4uYicFT0PwzvbUpib48raVQcn8EBiaw/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本文所使用的系统是云上的 Debian 系统，该系统配置有 GPU ，这能够加速我们模型的训练。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">首先安装依赖树，这能够方便我们查看目录结构。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">（sudo apt install tree）</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img" style="width:507px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4Gnu8QawQvJrBz8lbAF13QQJqicVhsVR4g75aZeZPJVH8ub5uZgrmpYxEoA/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从上图所示的目录结构中可以看到，我们的文件里包含两个文件夹，分别包含受感染的和健康的细胞图像。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">利用以下代码，我们可以进一步了解图像的总数是多少。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">import os</span></code><code><span class="code-snippet_outer">import glob</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">base_dir = os.path.join('./cell_images')</span></code><code><span class="code-snippet_outer">infected_dir = os.path.join(base_dir,'Parasitized')</span></code><code><span class="code-snippet_outer">healthy_dir = os.path.join(base_dir,'Uninfected')</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">infected_files = glob.glob(infected_dir+'/*.png')</span></code><code><span class="code-snippet_outer">healthy_files = glob.glob(healthy_dir+'/*.png')</span></code><code><span class="code-snippet_outer">len(infected_files), len(healthy_files)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Output</span></span></code><code><span class="code-snippet_outer">(13779, 13779)</span></code></pre>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从上述结果可以看到， 疟疾和非疟疾（未感染）的细胞图像的数据集均包含13779张图片，两个数据集的大小是相对平衡的。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">接下来我们将利用这些数据构建一个基于pandas的dataframe类型的数据，这对我们后续构建数据集很有帮助。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> numpy <span class="code-snippet__keyword">as</span> np</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> pandas <span class="code-snippet__keyword">as</span> pd</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">np.random.seed(<span class="code-snippet__number">42</span>)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">files_df = pd.DataFrame({</span></code><code><span class="code-snippet_outer"> <span class="code-snippet__string">'filename'</span>: infected_files + healthy_files,</span></code><code><span class="code-snippet_outer"> <span class="code-snippet__string">'label'</span>: [<span class="code-snippet__string">'malaria'</span>] * len(infected_files) + [<span class="code-snippet__string">'healthy'</span>] * len(healthy_files)</span></code><code><span class="code-snippet_outer">}).sample(frac=<span class="code-snippet__number">1</span>, random_state=<span class="code-snippet__number">42</span>).reset_index(drop=<span class="code-snippet__keyword">True</span>)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">files_df.head()</span></code></pre>
   <p style="line-height:1.75em;"><img class="inline-img" style="width:380px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4Gnudk2Via9L7dbnPcJicdz2U1ypOKVvUATWc5b2ChABp4PMxdeZNOOle4mg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">构建和探索图像数据集</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">在构建深度学习模型之前，我们不仅需要训练数据，还需要未用于训练的数据来验证和测试模型的性能。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本文采用 60:10:30 的比例来划分训练集、验证集和测试集。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们将使用训练集和验证集来训练模型，并利用测试集来检验模型的性能。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> sklearn.model_selection <span class="code-snippet__keyword">import</span> train_test_split</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> collections <span class="code-snippet__keyword">import</span> Counter</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">train_files, test_files, train_labels, test_labels = train_test_split(files_df[<span class="code-snippet__string">'filename'</span>].values, files_df[<span class="code-snippet__string">'label'</span>].values, test_size=<span class="code-snippet__number">0.3</span>, random_state=<span class="code-snippet__number">42</span>)</span></code><code><span class="code-snippet_outer"> </span></code><code><span class="code-snippet_outer">train_files, val_files, train_labels, val_labels = train_test_split(train_files, train_labels, test_size=<span class="code-snippet__number">0.1</span>, random_state=<span class="code-snippet__number">42</span>)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">print(train_files.shape, val_files.shape, test_files.shape)</span></code><code><span class="code-snippet_outer">print(<span class="code-snippet__string">'Train:'</span>, Counter(train_labels), <span class="code-snippet__string">'\nVal:'</span>, Counter(val_labels), <span class="code-snippet__string">'\nTest:'</span>, Counter(test_labels))</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Output</span></span></code><code><span class="code-snippet_outer">(<span class="code-snippet__number">17361</span>,) (<span class="code-snippet__number">1929</span>,) (<span class="code-snippet__number">8268</span>,)</span></code><code><span class="code-snippet_outer">Train: Counter({<span class="code-snippet__string">'healthy'</span>: <span class="code-snippet__number">8734</span>, <span class="code-snippet__string">'malaria'</span>: <span class="code-snippet__number">8627</span>}) </span></code><code><span class="code-snippet_outer">Val: Counter({<span class="code-snippet__string">'healthy'</span>: <span class="code-snippet__number">970</span>, <span class="code-snippet__string">'malaria'</span>: <span class="code-snippet__number">959</span>}) </span></code><code><span class="code-snippet_outer">Test: Counter({<span class="code-snippet__string">'malaria'</span>: <span class="code-snippet__number">4193</span>, <span class="code-snippet__string">'healthy'</span>: <span class="code-snippet__number">4075</span>})</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">可以发现，由于血液来源、测试方法以及图像拍摄的方向不同，血液涂片和细胞的图像尺寸不尽相同。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们需要获取一些训练数据的统计信息，从而确定最优的图像尺寸（请注意，在这里我们完全没用到测试集！</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">）。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> cv2</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> concurrent <span class="code-snippet__keyword">import</span> futures</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> threading</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__function"><span class="code-snippet__keyword">def</span> <span class="code-snippet__title">get_img_shape_parallel</span><span class="code-snippet__params">(idx, img, total_imgs)</span>:</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__keyword">if</span> idx % <span class="code-snippet__number">5000</span> == <span class="code-snippet__number">0</span> <span class="code-snippet__keyword">or</span> idx == (total_imgs - <span class="code-snippet__number">1</span>):</span></code><code><span class="code-snippet_outer"> print(<span class="code-snippet__string">'{}: working on img num:{}'</span>.format(threading.current_thread().name,idx))</span></code><code><span class="code-snippet_outer"> <span class="code-snippet__keyword">return</span> cv2.imread(img).shape</span></code><code><span class="code-snippet_outer"> </span></code><code><span class="code-snippet_outer">ex = futures.ThreadPoolExecutor(max_workers=<span class="code-snippet__keyword">None</span>)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">data_inp = [(idx, img, len(train_files)) <span class="code-snippet__keyword">for</span> idx, img <span class="code-snippet__keyword">in</span> enumerate(train_files)]</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">print(<span class="code-snippet__string">'Starting Img shape computation:'</span>)</span></code><code><span class="code-snippet_outer">train_img_dims_map = ex.map(get_img_shape_parallel, </span></code><code><span class="code-snippet_outer"> [record[<span class="code-snippet__number">0</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> data_inp],</span></code><code><span class="code-snippet_outer"> [record[<span class="code-snippet__number">1</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> data_inp],</span></code><code><span class="code-snippet_outer"> [record[<span class="code-snippet__number">2</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> data_inp])</span></code><code><span class="code-snippet_outer">train_img_dims = list(train_img_dims_map)</span></code><code><span class="code-snippet_outer">print(<span class="code-snippet__string">'Min Dimensions:'</span>, np.min(train_img_dims, axis=<span class="code-snippet__number">0</span>)) </span></code><code><span class="code-snippet_outer">print(<span class="code-snippet__string">'Avg Dimensions:'</span>, np.mean(train_img_dims, axis=<span class="code-snippet__number">0</span>))</span></code><code><span class="code-snippet_outer">print(<span class="code-snippet__string">'Median Dimensions:'</span>, np.median(train_img_dims, axis=<span class="code-snippet__number">0</span>))</span></code><code><span class="code-snippet_outer">print(<span class="code-snippet__string">'Max Dimensions:'</span>, np.max(train_img_dims, axis=<span class="code-snippet__number">0</span>))</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Output</span></span></code><code><span class="code-snippet_outer">Starting Img shape computation:</span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-0</span>_0: working on img num: <span class="code-snippet__number">0</span></span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-0</span>_17: working on img num: <span class="code-snippet__number">5000</span></span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-0</span>_15: working on img num: <span class="code-snippet__number">10000</span></span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-0</span>_1: working on img num: <span class="code-snippet__number">15000</span></span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-0</span>_7: working on img num: <span class="code-snippet__number">17360</span></span></code><code><span class="code-snippet_outer">Min Dimensions: [<span class="code-snippet__number">46</span> <span class="code-snippet__number">46</span> <span class="code-snippet__number">3</span>]</span></code><code><span class="code-snippet_outer">Avg Dimensions: [<span class="code-snippet__number">132.77311215</span> <span class="code-snippet__number">132.45757733</span> <span class="code-snippet__number">3.</span>]</span></code><code><span class="code-snippet_outer">Median Dimensions: [<span class="code-snippet__number">130.</span> <span class="code-snippet__number">130.</span> <span class="code-snippet__number">3.</span>]</span></code><code><span class="code-snippet_outer">Max Dimensions: [<span class="code-snippet__number">385</span> <span class="code-snippet__number">394</span> <span class="code-snippet__number">3</span>]</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们采用了并行处理的策略来加速图像读取操作。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">基于汇总的统计信息，我们决定将每张图像的大小调整为125x125。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">现在让我们加载所有的图像，并把他们的大小都调整为上述固定的尺寸。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">IMG_DIMS = (<span class="code-snippet__number">125</span>, <span class="code-snippet__number">125</span>)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__function">def <span class="code-snippet__title">get_img_data_parallel</span>(<span class="code-snippet__params">idx, img, total_imgs</span>):</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet_outer"> <span class="code-snippet__keyword">if</span> idx % 5000</span> == <span class="code-snippet__number">0</span> or idx == (total_imgs - <span class="code-snippet__number">1</span>):</span></code><code><span class="code-snippet_outer"> print(<span class="code-snippet__string">'{}: working on img num: {}'</span>.format(threading.current_thread().name,idx))</span></code><code><span class="code-snippet_outer"> </span></code><code><span class="code-snippet_outer"> img = cv2.imread(img)</span></code><code><span class="code-snippet_outer"> img = cv2.resize(img, dsize=IMG_DIMS, </span></code><code><span class="code-snippet_outer"> interpolation=cv2.INTER_CUBIC)</span></code><code><span class="code-snippet_outer"> img = np.array(img, dtype=np.float32)</span></code><code><span class="code-snippet_outer"> <span class="code-snippet__keyword">return</span> img</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">ex = futures.ThreadPoolExecutor(max_workers=None)</span></code><code><span class="code-snippet_outer">train_data_inp = [(idx, img, len(train_files)) <span class="code-snippet__keyword">for</span> idx, <span class="code-snippet__function">img <span class="code-snippet__keyword">in</span> <span class="code-snippet__title">enumerate</span>(<span class="code-snippet__params">train_files</span>)]</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet_outer">val_data_inp</span> = [(idx, img, len(val_files)) <span class="code-snippet__keyword">for</span> idx, <span class="code-snippet__function">img <span class="code-snippet__keyword">in</span> <span class="code-snippet__title">enumerate</span>(<span class="code-snippet__params">val_files</span>)]</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet_outer">test_data_inp</span> = [(idx, img, len(test_files)) <span class="code-snippet__keyword">for</span> idx, <span class="code-snippet__function">img <span class="code-snippet__keyword">in</span> <span class="code-snippet__title">enumerate</span>(<span class="code-snippet__params">test_files</span>)]</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__title">print</span>(<span class="code-snippet__params"><span class="code-snippet__string">'Loading Train Images:'</span></span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet_outer">train_data_map</span> = ex.map(get_img_data_parallel, </span></code><code><span class="code-snippet_outer"> [<span class="code-snippet__meta">record[0</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> train_data_inp],</span></code><code><span class="code-snippet_outer"> [<span class="code-snippet__meta">record[1</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> train_data_inp],</span></code><code><span class="code-snippet_outer"> [<span class="code-snippet__meta">record[2</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> train_data_inp])</span></code><code><span class="code-snippet_outer">train_data = np.array(list(train_data_map))</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">print(<span class="code-snippet__string">'\nLoading Validation Images:'</span>)</span></code><code><span class="code-snippet_outer">val_data_map = ex.map(get_img_data_parallel, </span></code><code><span class="code-snippet_outer"> [<span class="code-snippet__meta">record[0</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> val_data_inp],</span></code><code><span class="code-snippet_outer"> [<span class="code-snippet__meta">record[1</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> val_data_inp],</span></code><code><span class="code-snippet_outer"> [<span class="code-snippet__meta">record[2</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> val_data_inp])</span></code><code><span class="code-snippet_outer">val_data = np.array(list(val_data_map))</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">print(<span class="code-snippet__string">'\nLoading Test Images:'</span>)</span></code><code><span class="code-snippet_outer">test_data_map = ex.map(get_img_data_parallel, </span></code><code><span class="code-snippet_outer"> [<span class="code-snippet__meta">record[0</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> test_data_inp],</span></code><code><span class="code-snippet_outer"> [<span class="code-snippet__meta">record[1</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> test_data_inp],</span></code><code><span class="code-snippet_outer"> [<span class="code-snippet__meta">record[2</span>] <span class="code-snippet__keyword">for</span> record <span class="code-snippet__keyword">in</span> test_data_inp])</span></code><code><span class="code-snippet_outer">test_data = np.array(list(test_data_map))</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">train_data.shape, val_data.shape, test_data.shape </span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__meta"># Output</span></span></code><code><span class="code-snippet_outer">Loading Train Images:</span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-1</span>_0: working <span class="code-snippet__keyword">on</span> img num: <span class="code-snippet__number">0</span></span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-1</span>_12: working <span class="code-snippet__keyword">on</span> img num: <span class="code-snippet__number">5000</span></span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-1</span>_6: working <span class="code-snippet__keyword">on</span> img num: <span class="code-snippet__number">10000</span></span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-1</span>_10: working <span class="code-snippet__keyword">on</span> img num: <span class="code-snippet__number">15000</span></span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-1</span>_3: working <span class="code-snippet__keyword">on</span> img num: <span class="code-snippet__number">17360</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">Loading Validation Images:</span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-1</span>_13: working <span class="code-snippet__keyword">on</span> img num: <span class="code-snippet__number">0</span></span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-1</span>_18: working <span class="code-snippet__keyword">on</span> img num: <span class="code-snippet__number">1928</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">Loading Test Images:</span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-1</span>_5: working <span class="code-snippet__keyword">on</span> img num: <span class="code-snippet__number">0</span></span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-1</span>_19: working <span class="code-snippet__keyword">on</span> img num: <span class="code-snippet__number">5000</span></span></code><code><span class="code-snippet_outer">ThreadPoolExecutor<span class="code-snippet__number">-1</span>_8: working <span class="code-snippet__keyword">on</span> img num: <span class="code-snippet__number">8267</span></span></code><code><span class="code-snippet_outer">((<span class="code-snippet__number">17361</span>, <span class="code-snippet__number">125</span>, <span class="code-snippet__number">125</span>, <span class="code-snippet__number">3</span>), (<span class="code-snippet__number">1929</span>, <span class="code-snippet__number">125</span>, <span class="code-snippet__number">125</span>, <span class="code-snippet__number">3</span>), (<span class="code-snippet__number">8268</span>, <span class="code-snippet__number">125</span>, <span class="code-snippet__number">125</span>, <span class="code-snippet__number">3</span>))</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们再次运用了并行处理策略来加速图像加载和尺寸调整的计算，如上面输出结果中展示的，我们最终得到了所需尺寸的图像张量。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">现在我们可以查看一些样本的细胞图像，从而从直观上认识一下我们的数据的情况。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> matplotlib.pyplot <span class="code-snippet__keyword">as</span> plt</span></code><code><span class="code-snippet_outer">%matplotlib <span class="code-snippet__keyword">inline</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">plt.figure(<span class="code-snippet__number">1</span> , figsize = (<span class="code-snippet__number">8</span> , <span class="code-snippet__number">8</span>))</span></code><code><span class="code-snippet_outer">n = <span class="code-snippet__number">0</span> </span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">for</span> i <span class="code-snippet__keyword">in</span> range(<span class="code-snippet__number">16</span>):</span></code><code><span class="code-snippet_outer"> n += <span class="code-snippet__number">1</span> </span></code><code><span class="code-snippet_outer"> r = np.random.randint(<span class="code-snippet__number">0</span> , train_data.shape[<span class="code-snippet__number">0</span>] , <span class="code-snippet__number">1</span>)</span></code><code><span class="code-snippet_outer"> plt.subplot(<span class="code-snippet__number">4</span> , <span class="code-snippet__number">4</span> , n)</span></code><code><span class="code-snippet_outer"> plt.subplots_adjust(hspace = <span class="code-snippet__number">0.5</span> , wspace = <span class="code-snippet__number">0.5</span>)</span></code><code><span class="code-snippet_outer"> plt.imshow(train_data[r[<span class="code-snippet__number">0</span>]]/<span class="code-snippet__number">255</span>.)</span></code><code><span class="code-snippet_outer"> plt.title(<span class="code-snippet__string">'{}'</span>.format(train_labels[r[<span class="code-snippet__number">0</span>]]))</span></code><code><span class="code-snippet_outer"> plt.xticks([]) , plt.yticks([])</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img img-lazyload" style="width:452px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4GnuUL1KibKnJf7bicpl6aibHpLoR0jEibA7nEV95GM4XjDTEN5dicH6cGwPqGw/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从上面的样本图像可以看出，疟疾和健康细胞图像之间存在一些细微差别。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们将构建深度学习模型，通过不断训练来使模型尝试学习这些模式。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">在开始训练模型之前，我们先对模型的参数进行一些基本的设置。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">BATCH_SIZE = 64</span></code><code><span class="code-snippet_outer">NUM_CLASSES = 2</span></code><code><span class="code-snippet_outer">EPOCHS = 25</span></code><code><span class="code-snippet_outer">INPUT_SHAPE = (125, 125, 3)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">train_imgs_scaled = train_data / 255.</span></code><code><span class="code-snippet_outer">val_imgs_scaled = val_data / 255.</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># encode text category labels</span></span></code><code><span class="code-snippet_outer">from sklearn.preprocessing import LabelEncoder</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">le = LabelEncoder()</span></code><code><span class="code-snippet_outer">le.fit(train_labels)</span></code><code><span class="code-snippet_outer">train_labels_enc = le.transform(train_labels)</span></code><code><span class="code-snippet_outer">val_labels_enc = le.transform(val_labels)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__section">print(train_labels[:6], train_labels_enc[:6])</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Output</span></span></code><code><span class="code-snippet_outer">['malaria' 'malaria' 'malaria' 'healthy' 'healthy' 'malaria'][1 1 1 0 0 1]</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">上面的代码设定了图像的维度，批尺寸，epoch 的次数，并且对我们的类别标签进行了编码。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">TensorFLow 2.0 alpha 版本在2019年3月发布，它为我们项目的实施提供了一个完美的接口。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> tensorflow <span class="code-snippet__keyword">as</span> tf</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Load the TensorBoard notebook extension (optional)</span></span></code><code><span class="code-snippet_outer">%load_ext tensorboard.notebook</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">tf.random.set_seed(<span class="code-snippet__number">42</span>)</span></code><code><span class="code-snippet_outer">tf.__version__</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Output</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__string">'2.0.0-alpha0'</span></span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">深度学习模型的训练阶段</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">在模型训练阶段，我们将构建几个深度学习模型，利用前面构建的训练集进行训练，并在验证集上比较它们的性能。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">然后，我们将保存这些模型，并在模型评估阶段再次使用它们。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">模型1：</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从头开始训练CNN</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">对于本文的第一个疟疾检测模型，我们将构建并从头开始训练一个基本的卷积神经网络（CNN）。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">首先，我们需要定义模型的结构。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">inp = tf.keras.layers.Input(shape=INPUT_SHAPE)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">conv1 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), </span></code><code><span class="code-snippet_outer"><span class="code-snippet__code"> activation='relu', padding='same')(inp)</span></span></code><code><span class="code-snippet_outer">pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)</span></code><code><span class="code-snippet_outer">conv2 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), </span></code><code><span class="code-snippet_outer"><span class="code-snippet__code"> activation='relu', padding='same')(pool1)</span></span></code><code><span class="code-snippet_outer">pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)</span></code><code><span class="code-snippet_outer">conv3 = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), </span></code><code><span class="code-snippet_outer"><span class="code-snippet__code"> activation='relu', padding='same')(pool2)</span></span></code><code><span class="code-snippet_outer">pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">flat = tf.keras.layers.Flatten()(pool3)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">hidden1 = tf.keras.layers.Dense(512, activation='relu')(flat)</span></code><code><span class="code-snippet_outer">drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)</span></code><code><span class="code-snippet_outer">hidden2 = tf.keras.layers.Dense(512, activation='relu')(drop1)</span></code><code><span class="code-snippet_outer">drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">out = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">model = tf.keras.Model(inputs=inp, outputs=out)</span></code><code><span class="code-snippet_outer">model.compile(optimizer='adam',</span></code><code><span class="code-snippet_outer"><span class="code-snippet__code"> loss='binary_crossentropy',</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__code"> metrics=['accuracy'])</span></span></code><code><span class="code-snippet_outer">model.summary()</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__section"># Output</span></span></code><code><span class="code-snippet_outer">Model: "model"</span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__section">Layer (type) Output Shape Param # </span></span></code><code><span class="code-snippet_outer">=================================================================</span></code><code><span class="code-snippet_outer">input_1 (InputLayer) [(None, 125, 125, 3)] 0 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">conv2d (Conv2D) (None, 125, 125, 32) 896 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">max_pooling2d (MaxPooling2D) (None, 62, 62, 32) 0 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">conv2d_1 (Conv2D) (None, 62, 62, 64) 18496 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">...</span></code><code><span class="code-snippet_outer">...</span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">dense_1 (Dense) (None, 512) 262656 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">dropout_1 (Dropout) (None, 512) 0 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__section">dense_2 (Dense) (None, 1) 513 </span></span></code><code><span class="code-snippet_outer">=================================================================</span></code><code><span class="code-snippet_outer">Total params: 15,102,529</span></code><code><span class="code-snippet_outer">Trainable params: 15,102,529</span></code><code><span class="code-snippet_outer">Non-trainable params: 0</span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">上述代码所构建的 CNN 模型，包含3个卷积层、1个池化层以及2个全连接层，并对全连接层设置 dropout 参数用于正则化。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">现在让我们开始训练模型吧！</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__attr">import</span> <span class="code-snippet__string">datetime</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">logdir</span> = <span class="code-snippet__string">os.path.join('/home/dipanzan_sarkar/projects/tensorboard_logs', </span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">tensorboard_callback</span> = <span class="code-snippet__string">tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">reduce_lr</span> = <span class="code-snippet__string">tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=2, min_lr=0.000001)</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">callbacks</span> = <span class="code-snippet__string">[reduce_lr, tensorboard_callback]</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">history</span> = <span class="code-snippet__string">model.fit(x=train_imgs_scaled, y=train_labels_enc, </span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">batch_size</span>=<span class="code-snippet__string">BATCH_SIZE,</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">epochs</span>=<span class="code-snippet__string">EPOCHS, </span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">validation_data</span>=<span class="code-snippet__string">(val_imgs_scaled, val_labels_enc), </span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">callbacks</span>=<span class="code-snippet__string">callbacks,</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">verbose</span>=<span class="code-snippet__string">1) </span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Output</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">Train</span> <span class="code-snippet__string">on 17361 samples, validate on 1929 samples</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">Epoch</span> <span class="code-snippet__string">1/25</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__meta">17361/17361</span> <span class="code-snippet__string">[====] - 32s 2ms/sample - loss: 0.4373 - accuracy: 0.7814 - val_loss: 0.1834 - val_accuracy: 0.9393</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">Epoch</span> <span class="code-snippet__string">2/25</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__meta">17361/17361</span> <span class="code-snippet__string">[====] - 30s 2ms/sample - loss: 0.1725 - accuracy: 0.9434 - val_loss: 0.1567 - val_accuracy: 0.9513</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">...</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">...</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">Epoch</span> <span class="code-snippet__string">24/25</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__meta">17361/17361</span> <span class="code-snippet__string">[====] - 30s 2ms/sample - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.3693 - val_accuracy: 0.9565</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">Epoch</span> <span class="code-snippet__string">25/25</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__meta">17361/17361</span> <span class="code-snippet__string">[====] - 30s 2ms/sample - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.3699 - val_accuracy: 0.9559 </span></span></code></pre>
   <p style="line-height:1.75em;"><span style="color:rgb(63,63,63);font-family:'-webkit-standard';font-size:15px;letter-spacing:1px;">&nbsp;</span><br></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从上面的结果可以看到，我们的模型在验证集上的准确率为 95.6% ，这是非常好的。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们注意到模型在训练集上的准确率为 99.9% ，这看起来有一些过拟合。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">为了更加清晰地查看这个问题，我们可以分别绘制在训练和验证阶段的准确度曲线和损失曲线。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">f, (ax1, ax2) = plt.subplots(<span class="code-snippet__number">1</span>, <span class="code-snippet__number">2</span>, figsize=(<span class="code-snippet__number">12</span>, <span class="code-snippet__number">4</span>))</span></code><code><span class="code-snippet_outer">t = f.suptitle(<span class="code-snippet__string">'Basic CNN Performance'</span>, fontsize=<span class="code-snippet__number">12</span>)</span></code><code><span class="code-snippet_outer">f.subplots_adjust(top=<span class="code-snippet__number">0.85</span>, wspace=<span class="code-snippet__number">0.3</span>)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">max_epoch = <span class="code-snippet__built_in">len</span>(history.history[<span class="code-snippet__string">'accuracy'</span>])+<span class="code-snippet__number">1</span></span></code><code><span class="code-snippet_outer">epoch_list = list(<span class="code-snippet__keyword">range</span>(<span class="code-snippet__number">1</span>,max_epoch))</span></code><code><span class="code-snippet_outer">ax1.plot(epoch_list, history.history[<span class="code-snippet__string">'accuracy'</span>], label=<span class="code-snippet__string">'Train Accuracy'</span>)</span></code><code><span class="code-snippet_outer">ax1.plot(epoch_list, history.history[<span class="code-snippet__string">'val_accuracy'</span>], label=<span class="code-snippet__string">'Validation Accuracy'</span>)</span></code><code><span class="code-snippet_outer">ax1.set_xticks(np.arange(<span class="code-snippet__number">1</span>, max_epoch, <span class="code-snippet__number">5</span>))</span></code><code><span class="code-snippet_outer">ax1.set_ylabel(<span class="code-snippet__string">'Accuracy Value'</span>)</span></code><code><span class="code-snippet_outer">ax1.set_xlabel(<span class="code-snippet__string">'Epoch'</span>)</span></code><code><span class="code-snippet_outer">ax1.set_title(<span class="code-snippet__string">'Accuracy'</span>)</span></code><code><span class="code-snippet_outer">l1 = ax1.legend(loc=<span class="code-snippet__string">"best"</span>)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">ax2.plot(epoch_list, history.history[<span class="code-snippet__string">'loss'</span>], label=<span class="code-snippet__string">'Train Loss'</span>)</span></code><code><span class="code-snippet_outer">ax2.plot(epoch_list, history.history[<span class="code-snippet__string">'val_loss'</span>], label=<span class="code-snippet__string">'Validation Loss'</span>)</span></code><code><span class="code-snippet_outer">ax2.set_xticks(np.arange(<span class="code-snippet__number">1</span>, max_epoch, <span class="code-snippet__number">5</span>))</span></code><code><span class="code-snippet_outer">ax2.set_ylabel(<span class="code-snippet__string">'Loss Value'</span>)</span></code><code><span class="code-snippet_outer">ax2.set_xlabel(<span class="code-snippet__string">'Epoch'</span>)</span></code><code><span class="code-snippet_outer">ax2.set_title(<span class="code-snippet__string">'Loss'</span>)</span></code><code><span class="code-snippet_outer">l2 = ax2.legend(loc=<span class="code-snippet__string">"best"</span>)</span></code></pre>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img img-lazyload" style="width:547px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4Gnu3XxejzBtK0JRFqLlj0wZl5SN27TNnfMCnt4DuXUUumibNM7ZhdpzFQQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;text-align:center;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">Learning Curves for Basic&nbsp;CNN</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从图中可以看出，在第5个 epoch 之后，在验证集上的精度似乎不再提高。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们先将这个模型保存，在后面我们会再次用到它。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">model.save(<span class="code-snippet__string">'basic_cnn.h5'</span>)</span></code></pre>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">深度迁移学习</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">就像人类能够运用知识完成跨任务工作一样，迁移学习使得我们能够利用在先前任务中学习到的知识，来处理新的任务，在机器学习和深度学习的环境下也是如此。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">这些</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">文章</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">涵盖了迁移学习的详细介绍和讨论，有兴趣的读者可以参考学习。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img img-lazyload" style="width:547px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4Gnua0YZ76b03ZU7SBQob7LFCF3GQE2T4Wbz01qmbI7fvs1AubkSY66ygg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;text-align:center;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">Ideas for deep transfer&nbsp;learning</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们能否采用迁移学习的思想，将预训练的深度学习模型（已在大型数据集上进行过训练的模型——例如 ImageNet）的知识应用到我们的问题——进行疟疾检测上呢？</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们将采用两种目前最主流的迁移学习策略。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul type="disc" class="undefined list-paddingleft-2" style="margin-left:8px;">
    <li><p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">将预训练模型作为特征提取器</span></p></li>
   </ul>
   <ul type="disc" class="undefined list-paddingleft-2" style="margin-left:8px;">
    <li><p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">对预训练模型进行微调</span></p></li>
   </ul>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们将使用由牛津大学视觉几何组（VGG）所开发的预训练模型 VGG-19 进行实验。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">像 VGG-19 这样的预训练模型，一般已经在大型数据集上进行过训练，这些数据集涵盖多种类别的图像。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">基于此，这些预训练模型应该已经使用CNN模型学习到了一个具有高度鲁棒性的特征的层次结构，并且其应具有尺度、旋转和平移不变性。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">因此，这个已经学习了超过一百万个图像的具有良好特征表示的模型，可以作为一个很棒的图像特征提取器，为包括疟疾检测问题在内的其他计算机视觉问题服务。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">在引入强大的迁移学习之前，我们先简要讨论一下 VGG-19 的结构。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">理解VGG-19模型</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">VGG-19 是一个具有 19 个层（包括卷积层和全连接层）的深度学习网络，该模型基于 ImageNet 数据集进行训练，该数据集是专门为图像识别和分类所构建的。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">VGG-19 是由 Karen Simonyan 和 Andrew Zisserman 提出的，该模型在他们的论文《</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">Very Deep Convolutional Networks for Large-Scale Image Recognition</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">》中有详细介绍，建议有兴趣的读者可以去读一读这篇优秀的论文。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">VGG-19 模型的结构如下图所示。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img img-lazyload" style="width:543px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4Gnu6icicOcfRibhmibuGicolKia4gQfebD3D8b3gKMVOt0IMDmbyLqmWemClmibQ/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;text-align:center;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">VGG-19 Model Architecture</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从上图可以清楚地看到，该模型具有 16 个使用 3x3 卷积核的卷积层，其中部分卷积层后面接了一个最大池化层，用于下采样；</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">随后依次连接了两个具有 4096 个隐层神经元的全连接层，接着连接了一个具有 1000 个隐层神经元的全连接层， 最后一个全连接层的每个神经元都代表 ImageNet 数据集中的一个图像类别。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">由于我们需要使用新的全连接层来分类疟疾，因此我们不需要最后的三个全连接层。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们更关心的是前五个块，以便我们可以利用 VGG 模型作为有效的特征提取器。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">前文提到有两种迁移学习的策略，对于第一种策略，我们将把 VGG 模型当做一个特征提取器，这可以通过冻结前五个卷积块，使得它们的权重参数不会随着新的训练过程而更新来实现。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">对于第二种策略，我们将会解冻最后的两个卷积块（模块4和模块5），从而使得它们的参数会随着新的训练过程而不断更新。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">模型2：</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">将预训练模型作为特征提取机</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">为了构建这个模型，我们将利用 TensorFlow 加载 VGG-19 模型，并冻结它的卷积块，以便我们可以将其用作图像特征提取器。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们将在该模型的末尾插入自己的全连接层，用于执行本文的分类任务。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">vgg = tf.keras.applications.vgg19.VGG19(include<span class="code-snippet__emphasis">_top=False, weights='imagenet',input_</span>shape=INPUT_SHAPE)</span></code><code><span class="code-snippet_outer">vgg.trainable = False</span></code><code><span class="code-snippet_outer"><span class="code-snippet__section"># Freeze the layers</span></span></code><code><span class="code-snippet_outer">for layer in vgg.layers:</span></code><code><span class="code-snippet_outer"> layer.trainable = False</span></code><code><span class="code-snippet_outer"> </span></code><code><span class="code-snippet_outer">base_vgg = vgg</span></code><code><span class="code-snippet_outer">base<span class="code-snippet__emphasis">_out = base_</span>vgg.output</span></code><code><span class="code-snippet_outer">pool<span class="code-snippet__emphasis">_out = tf.keras.layers.Flatten()(base_</span>out)</span></code><code><span class="code-snippet_outer">hidden1 = tf.keras.layers.Dense(512, activation='relu')(pool_out)</span></code><code><span class="code-snippet_outer">drop1 = tf.keras.layers.Dropout(rate=0.3)(hidden1)</span></code><code><span class="code-snippet_outer">hidden2 = tf.keras.layers.Dense(512, activation='relu')(drop1)</span></code><code><span class="code-snippet_outer">drop2 = tf.keras.layers.Dropout(rate=0.3)(hidden2)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">out = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">model = tf.keras.Model(inputs=base_vgg.input, outputs=out)</span></code><code><span class="code-snippet_outer">model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),loss='binary_crossentropy',metrics=['accuracy'])</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">model.summary()</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__section"># Output</span></span></code><code><span class="code-snippet_outer">Model: "model_1"</span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__section">Layer (type) Output Shape Param # </span></span></code><code><span class="code-snippet_outer">=================================================================</span></code><code><span class="code-snippet_outer">input_2 (InputLayer) [(None, 125, 125, 3)] 0 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">block1_conv1 (Conv2D) (None, 125, 125, 64) 1792 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">block1_conv2 (Conv2D) (None, 125, 125, 64) 36928 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">...</span></code><code><span class="code-snippet_outer">...</span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">block5_pool (MaxPooling2D) (None, 3, 3, 512) 0 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">flatten_1 (Flatten) (None, 4608) 0 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">dense_3 (Dense) (None, 512) 2359808 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">dropout_2 (Dropout) (None, 512) 0 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">dense_4 (Dense) (None, 512) 262656 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer">dropout_3 (Dropout) (None, 512) 0 </span></code><code><span class="code-snippet_outer"><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span><span class="code-snippet__strong">_____</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__section">dense_5 (Dense) (None, 1) 513 </span></span></code><code><span class="code-snippet_outer">=================================================================</span></code><code><span class="code-snippet_outer">Total params: 22,647,361</span></code><code><span class="code-snippet_outer">Trainable params: 2,622,977</span></code><code><span class="code-snippet_outer">Non-trainable params: 20,024,384</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从上面代码的输出可以看到，我们的模型有很多层，并且我们仅仅只利用了 VGG-19 的冻结层来提取特征。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">下面的代码可以验证本模型中有多少层用于训练，以及检验本模型中一共有多少层。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__built_in">print</span>(<span class="code-snippet__string">"Total Layers:"</span>, len(model.layers))</span></code><code><span class="code-snippet_outer"><span class="code-snippet__built_in">print</span>(<span class="code-snippet__string">"Total trainable layers:"</span>,sum([1 <span class="code-snippet__keyword">for</span> l <span class="code-snippet__keyword">in</span> model.layers <span class="code-snippet__keyword">if</span> l.trainable]))</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Output</span></span></code><code><span class="code-snippet_outer">Total Layers: 28</span></code><code><span class="code-snippet_outer">Total trainable layers: 6</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">现在我们将训练该模型，在训练过程中所用到的配置和回调函数与模型1中的类似，完整的代码可以参考</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">github链接</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">下图展示了在训练过程中，模型的准确度曲线和损失曲线。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img img-lazyload" style="width:547px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4GnuTjicviajMb7iaiaN9Rs05GgWic5eKD8AY6OBUYuaib2G4bgNf1QfQoslzBog/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;text-align:center;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">Learning Curves for frozen pre-trained CNN</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从上图可以看出，该模型不像模型1中基本的 CNN 模型那样存在过拟合的现象，但是性能并不是很好。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">事实上，它的性能还没有基本的 CNN 模型好。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">现在我们将模型保存，用于后续的评估。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">model.save（ <span class="code-snippet__string">'vgg_frozen.h5'</span>）</span></code></pre>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">模型3：</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">具有图像增广的微调的预训练模型</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">在这个模型中，我们将微调预训练 VGG-19 模型的最后两个区块中层的权重。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">除此之外，我们还将介绍图像增广的概念。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">图像增广背后的原理与它的名称听起来完全一样。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们首先从训练数据集中加载现有的图像，然后对它们进行一些图像变换的操作，例如旋转，剪切，平移，缩放等，从而生成现有图像的新的、变化的版本。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">由于这些随机变换的操作，我们每次都会得到不同的图像。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们将使用 tf.keras 中的 ImageDataGenerator 工具，它能够帮助我们实现图像增广。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__attr">train_datagen</span> = tf.keras.preprocessing.image.ImageDataGenerator(rescale=<span class="code-snippet__number">1</span>./<span class="code-snippet__number">255</span>, zoom_range=<span class="code-snippet__number">0.05</span>, rotation_range=<span class="code-snippet__number">25</span>, width_shift_range=<span class="code-snippet__number">0.05</span>, height_shift_range=<span class="code-snippet__number">0.05</span>, shear_range=<span class="code-snippet__number">0.05</span>, horizontal_flip=<span class="code-snippet__literal">True</span>, fill_mode=<span class="code-snippet__string">'nearest'</span>)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">val_datagen</span> = tf.keras.preprocessing.image.ImageDataGenerator(rescale=<span class="code-snippet__number">1</span>./<span class="code-snippet__number">255</span>)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># build image augmentation generators</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">train_generator</span> = train_datagen.flow(train_data, train_labels_enc, batch_size=BATCH_SIZE, shuffle=<span class="code-snippet__literal">True</span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">val_generator</span> = val_datagen.flow(val_data, val_labels_enc, batch_size=BATCH_SIZE, shuffle=<span class="code-snippet__literal">False</span>)</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">在验证集上，我们只会对图像进行缩放操作，而不进行其他的转换，这是因为我们需要在每个训练的 epoch 结束后，用验证集来评估我们的模型。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">有关图像增广的详细说明，可以参考这篇</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">文章</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">让我们来看看进行图像增广变换后的一些样本结果。</span><br></p>
   <p style="line-height:1.75em;"><br></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">img_id = 0</span></code><code><span class="code-snippet_outer">sample<span class="code-snippet__emphasis">_generator = train_</span>datagen.flow(train<span class="code-snippet__emphasis">_data[img_</span>id:img<span class="code-snippet__emphasis">_id+1], train_</span>labels[img<span class="code-snippet__emphasis">_id:img_</span>id+1],batch_size=1)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">sample = [next(sample_generator) for i in range(0,5)]</span></code><code><span class="code-snippet_outer">fig, ax = plt.subplots(1,5, figsize=(16, 6))</span></code><code><span class="code-snippet_outer">print('Labels:', [<span class="code-snippet__string">item[1</span>][<span class="code-snippet__symbol">0</span>] for item in sample])</span></code><code><span class="code-snippet_outer">l = [<span class="code-snippet__string">ax[i</span>].imshow(sample[<span class="code-snippet__string">i</span>][<span class="code-snippet__symbol">0</span>][<span class="code-snippet__string">0</span>]) for i in range(0,5)]</span></code></pre>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img img-lazyload" style="width:547px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4GnuLZe0vteibGeqm1icVdDC0sM58VeialkQENtJQeE9FZ8QsPjct3wVN50Ag/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;text-align:center;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">Sample Augmented Images</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从上图可以清楚的看到图像发生了轻微的变化。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">现在我们将构建新的深度模型，该模型需要确保 VGG-19 模型的最后两个块可以进行训练。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">vgg = tf.keras.applications.vgg19.VGG19(include_top=<span class="code-snippet__keyword">False</span>, weights=<span class="code-snippet__string">'imagenet'</span>,input_shape=INPUT_SHAPE)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Freeze the layers</span></span></code><code><span class="code-snippet_outer">vgg.trainable = <span class="code-snippet__keyword">True</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">set_trainable = <span class="code-snippet__keyword">False</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">for</span> layer <span class="code-snippet__keyword">in</span> vgg.layers:</span></code><code><span class="code-snippet_outer"> <span class="code-snippet__keyword">if</span> layer.name <span class="code-snippet__keyword">in</span> [<span class="code-snippet__string">'block5_conv1'</span>, <span class="code-snippet__string">'block4_conv1'</span>]:</span></code><code><span class="code-snippet_outer"> set_trainable = <span class="code-snippet__keyword">True</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__keyword">if</span> set_trainable:</span></code><code><span class="code-snippet_outer"> layer.trainable = <span class="code-snippet__keyword">True</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__keyword">else</span>:</span></code><code><span class="code-snippet_outer"> layer.trainable = <span class="code-snippet__keyword">False</span></span></code><code><span class="code-snippet_outer"> </span></code><code><span class="code-snippet_outer">base_vgg = vgg</span></code><code><span class="code-snippet_outer">base_out = base_vgg.output</span></code><code><span class="code-snippet_outer">pool_out = tf.keras.layers.Flatten()(base_out)</span></code><code><span class="code-snippet_outer">hidden1 = tf.keras.layers.Dense(<span class="code-snippet__number">512</span>, activation=<span class="code-snippet__string">'relu'</span>)(pool_out)</span></code><code><span class="code-snippet_outer">drop1 = tf.keras.layers.Dropout(rate=<span class="code-snippet__number">0.3</span>)(hidden1)</span></code><code><span class="code-snippet_outer">hidden2 = tf.keras.layers.Dense(<span class="code-snippet__number">512</span>, activation=<span class="code-snippet__string">'relu'</span>)(drop1)</span></code><code><span class="code-snippet_outer">drop2 = tf.keras.layers.Dropout(rate=<span class="code-snippet__number">0.3</span>)(hidden2)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">out = tf.keras.layers.Dense(<span class="code-snippet__number">1</span>, activation=<span class="code-snippet__string">'sigmoid'</span>)(drop2)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">model = tf.keras.Model(inputs=base_vgg.input, outputs=out)</span></code><code><span class="code-snippet_outer">model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=<span class="code-snippet__number">1e-5</span>),loss=<span class="code-snippet__string">'binary_crossentropy'</span>,metrics=[<span class="code-snippet__string">'accuracy'</span>])</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">print(<span class="code-snippet__string">"Total Layers:"</span>, len(model.layers))</span></code><code><span class="code-snippet_outer">print(<span class="code-snippet__string">"Total trainable layers:"</span>, sum([<span class="code-snippet__number">1</span> <span class="code-snippet__keyword">for</span> l <span class="code-snippet__keyword">in</span> model.layers <span class="code-snippet__keyword">if</span> l.trainable]))</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Output</span></span></code><code><span class="code-snippet_outer">Total Layers: <span class="code-snippet__number">28</span></span></code><code><span class="code-snippet_outer">Total trainable layers: <span class="code-snippet__number">16</span></span></code></pre>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">由于我们不希望在微调过程中，对预训练的层进行较大的权重更新，我们降低了模型的学习率。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">由于我们使用数据生成器来加载数据，本模型的训练过程会和之前稍稍不同，在这里，我们需要用到函数 fit_generator(…) 。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=2, min_lr=0.000001)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">callbacks = [reduce_lr, tensorboard_callback]</span></code><code><span class="code-snippet_outer">train_steps_per_epoch = train_generator.n //train_generator.batch_size</span></code><code><span class="code-snippet_outer">val_steps_per_epoch = val_generator.n //val_generator.batch_size</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">history = model.fit_generator(train_generator, steps_per_epoch=train_steps_per_epoch,epochs=EPOCHS,validation_data=val_generator,validation_steps=val_steps_per_epoch,verbose=1)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Output</span></span></code><code><span class="code-snippet_outer">Epoch 1/25</span></code><code><span class="code-snippet_outer">271/271 [====] - 133s 489ms/step - loss: 0.2267 - accuracy: 0.9117 - val_loss: 0.1414 - val_accuracy: 0.9531</span></code><code><span class="code-snippet_outer">Epoch 2/25</span></code><code><span class="code-snippet_outer">271/271 [====] - 129s 475ms/step - loss: 0.1399 - accuracy: 0.9552 - val_loss: 0.1292 - val_accuracy: 0.9589</span></code><code><span class="code-snippet_outer">...</span></code><code><span class="code-snippet_outer">...</span></code><code><span class="code-snippet_outer">Epoch 24/25</span></code><code><span class="code-snippet_outer">271/271 [====] - 128s 473ms/step - loss: 0.0815 - accuracy: 0.9727 - val_loss: 0.1466 - val_accuracy: 0.9682</span></code><code><span class="code-snippet_outer">Epoch 25/25</span></code><code><span class="code-snippet_outer">271/271 [====] - 128s 473ms/step - loss: 0.0792 - accuracy: 0.9729 - val_loss: 0.1127 - val_accuracy: 0.9641</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">下图展示了该模型的训练曲线，可以看出该模型是这三个模型中最好的模型，其验证准确度几乎达到了 96.5% ，而且从训练准确度上看，我们的模型也没有像第一个模型那样出现过拟合。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img" style="width:547px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4GnumB04Rz01m11IO4h1iclvdlUpqR6btJRYVvA6fqmjOu6Zp0tibeV3Kniaw/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;text-align:center;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">Learning Curves for fine-tuned pre-trained CNN</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">现在让我们保存这个模型，很快我们将在测试集上用到它进行性能评估。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">model.save（ <span class="code-snippet__string">'vgg_finetuned.h5'</span>）</span></code></pre>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">至此，模型训练阶段告一段落，我们即将在真实的测试集上去测试这些模型的性能。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">深度学习模型的性能评估阶段</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">现在，我们将对之前训练好的三个模型进行评估。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">仅仅使用验证集来评估模型的好坏是不够的， 因此，我们将使用测试集来进一步评估模型的性能。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">我们构建了一个实用的模块 model_evaluation_utils，该模块采用相关的分类指标，用于评估深度学习模型的性能。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">首先我们需要将测试数据进行缩放。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">test_imgs_scaled = test_data / 255.</span></code><code><span class="code-snippet_outer">test_imgs_scaled.shape, test_labels.shape</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Output</span></span></code><code><span class="code-snippet_outer">((8268, 125, 125, 3), (8268,))</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">第二步是加载之前所保存的深度学习模型，然后在测试集上进行预测。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Load Saved Deep Learning Models</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">basic_cnn</span> = tf.keras.models.load_model(<span class="code-snippet__string">'./basic_cnn.h5'</span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">vgg_frz</span> = tf.keras.models.load_model(<span class="code-snippet__string">'./vgg_frozen.h5'</span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">vgg_ft</span> = tf.keras.models.load_model(<span class="code-snippet__string">'./vgg_finetuned.h5'</span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__comment"># Make Predictions on Test Data</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">basic_cnn_preds</span> = basic_cnn.predict(test_imgs_scaled, batch_size=<span class="code-snippet__number">512</span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">vgg_frz_preds</span> = vgg_frz.predict(test_imgs_scaled, batch_size=<span class="code-snippet__number">512</span>)</span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">vgg_ft_preds</span> = vgg_ft.predict(test_imgs_scaled, batch_size=<span class="code-snippet__number">512</span>)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">basic_cnn_pred_labels</span> = le.inverse_transform([<span class="code-snippet__number">1</span> if pred &gt; <span class="code-snippet__number">0.5</span> else <span class="code-snippet__number">0</span> for pred in basic_cnn_preds.ravel()])</span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">vgg_frz_pred_labels</span> = le.inverse_transform([<span class="code-snippet__number">1</span> if pred &gt; <span class="code-snippet__number">0.5</span> else <span class="code-snippet__number">0</span> for pred in vgg_frz_preds.ravel()])</span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">vgg_ft_pred_labels</span>&nbsp;=&nbsp;le.inverse_transform([<span class="code-snippet__number">1</span>&nbsp;if&nbsp;pred&nbsp;&gt;&nbsp;<span class="code-snippet__number">0.5</span>&nbsp;else&nbsp;<span class="code-snippet__number">0</span>&nbsp;for&nbsp;pred&nbsp;in&nbsp;vgg_ft_preds.ravel()])</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">最后一步是利用 model_evaluation_utils 模块，根据不同的分类评价指标，来评估每个模型的性能。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> model_evaluation_utils <span class="code-snippet__keyword">as</span> meu</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> pandas <span class="code-snippet__keyword">as</span> pd</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">basic_cnn_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=basic_cnn_pred_labels)</span></code><code><span class="code-snippet_outer">vgg_frz_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=vgg_frz_pred_labels)</span></code><code><span class="code-snippet_outer">vgg_ft_metrics = meu.get_metrics(true_labels=test_labels, predicted_labels=vgg_ft_pred_labels)</span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer">pd.DataFrame([basic_cnn_metrics, vgg_frz_metrics, vgg_ft_metrics], </span></code><code><span class="code-snippet_outer"> index=[<span class="code-snippet__string">'Basic CNN'</span>, <span class="code-snippet__string">'VGG-19 Frozen'</span>, <span class="code-snippet__string">'VGG-19 Fine-tuned'</span>])</span></code></pre>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;text-align:center;"><img class="inline-img" style="width:355px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAbYhzxb2DHPBVlu8MOo4Gnulr1t6PAl8G9mtnq844xaRTRpkOweEQ6fw3iap9ZFQaFejiawxibx8cubg/640?wx_fmt=png" alt="640?wx_fmt=png"></p>
   <p style="line-height:1.75em;"><br></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">从图中可以看到，第三个模型在测试集上的性能是最好的，其准确度和 f1-score 都达到了96%，这是一个非常好的结果，而且这个结果和论文中提到的更为复杂的模型所得到的结果具有相当的 可比性！</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">结论</span></strong></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本文研究了一个有趣的医学影像案例——疟疾检测。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">疟疾检测是一个复杂的过程，而且能够进行正确操作的医疗人员也很少，这是一个很严重的问题。</span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">本文利用 AI 技术构建了一个开源的项目，该项目在疟疾检测问题上具有最高的准确率，并使AI技术为社会带来了效益。</span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><span style="color:rgb(136,136,136);font-size:14px;letter-spacing:.5px;">相关链接：https://towardsdatascience.com/detecting-malaria-with-deep-learning-9e45c1e34b60</span></span></p>
   <p style="line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><span style="color:rgb(136,136,136);font-size:14px;letter-spacing:.5px;"><br></span></span></p>
   <p style="margin-left:8px;"><span style="color:rgb(136,136,136);font-size:14px;font-style:italic;letter-spacing:1px;text-align:left;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;">（本文为AI科技大本营编译文章，转载请微信联系 1092722531）</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><br></p>
   <p style="min-height:1em;">◆</p>
   <p style="min-height:1em;"><span style="color:rgb(255,76,0);letter-spacing:1px;"><strong><span><strong style="letter-spacing:.544px;">CTA核心技术及应用峰会</strong></span></strong></span></p>
   <p style="min-height:1em;">◆</p>
   <p><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">5月25-27日，由中国IT社区CSDN与数字经济人才发展中心联合主办的第一届CTA核心技术及应用峰会将在杭州国际博览中心隆重召开，峰会将围绕人工智能领域，邀请技术领航者，与开发者共同探讨机器学习和知识图谱的前沿研究及应用。</span><br></p>
   <p><br></p>
   <p><span style="letter-spacing:1px;font-size:15px;color:rgb(63,63,63);">更多重磅嘉宾请识别海报二维码查看，</span><span style="letter-spacing:1px;font-size:15px;color:rgb(63,63,63);">目前会议早鸟票发售中（原票价1099元），</span><strong style="letter-spacing:.544px;"><span style="color:rgb(255,76,0);letter-spacing:1px;font-size:15px;">点击阅读原文即刻抢购</span></strong><span style="letter-spacing:1px;font-size:15px;color:rgb(63,63,63);">。添加小助手微信</span><span style="letter-spacing:1px;font-size:15px;color:rgb(255,76,0);"><strong>15101014297</strong></span><span style="letter-spacing:1px;font-size:15px;color:rgb(63,63,63);">，备注“</span><strong style="letter-spacing:.544px;"><span style="letter-spacing:1px;font-size:15px;color:rgb(255,76,0);">CTA</span></strong><span style="letter-spacing:1px;font-size:15px;color:rgb(63,63,63);">”，了解票务以及会务详情。</span></p>
   <p><br></p>
   <p style="margin-left:8px;text-align:center;"><img class="rich_pages" style="width:540px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/BnSNEaficFAaEvfrVTwTOJmJOdd48GLj5vdEsysxDn5IbBMxPae10gfYod2Px8DcIZZEOTiaK2vib6T4fZko637ibw/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></p>
   <p style="margin-left:8px;min-height:1em;letter-spacing:.544px;line-height:27.2px;"><br></p>
   <p style="margin-left:8px;min-height:1em;letter-spacing:.544px;line-height:27.2px;text-align:left;"><strong style="text-align:justify;"><span style="font-size:15px;">推荐阅读</span></strong></p>
   <ul class="list-paddingleft-2">
    <li><h2 class="rich_media_title"><a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/89715015" rel="nofollow">他25岁进贝尔实验室，32岁提信息论，40岁办达特茅斯会议，晚年患上阿兹海默 | 人物志</a></h2></li>
    <li><p><a href="http://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&amp;mid=2247504776&amp;idx=1&amp;sn=1fe4d56930902426bc3e2605ba2ca025&amp;chksm=e99ee071dee96967d817d2973521ab016a0d3d823d35b6e4aebc75f8bca4deeb61aebeda9d1b&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">微软沈向洋，百度李彦宏、王海峰，阿里王坚均候选中国工程院院士</a><br></p></li>
    <li><p><a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/89667051" rel="nofollow">小样，加张图你就不认识我了？“补丁”模型骗你没商量！| 技术头条</a></p></li>
    <li><p><a href="https://mp.weixin.qq.com/s?__biz=MzU5MjEwMTE2OQ==&amp;mid=2247485808&amp;idx=1&amp;sn=214ad17daba746cf6ac22da2f8787633&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">东大漆桂林、清华李涓子、复旦肖仰华等大牛确认出席CTA峰会！5月一起打卡杭州</a></p></li>
    <li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5MjAwODM4MA==&amp;mid=2650719196&amp;idx=1&amp;sn=08ddf7ad2a591aa70ab21940ad7ed46f&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">京东 60 天哗变！CTO 成优化第一人 | 畅言</a></p></li>
    <li><p><a href="https://blog.csdn.net/FL63Zv9Zou86950w/article/details/89667030" rel="nofollow">异构计算=未来？一文带你秒懂3大主流异构</a></p></li>
    <li><p><a href="https://blog.csdn.net/csdnsevenn/article/details/89666825" rel="nofollow">《互联网人叹气图鉴》</a></p></li>
    <li><p><a href="https://blog.csdn.net/Blockchain_lemon/article/details/89629591" rel="nofollow">回报率29%! 大神用情感分析创建一个比特币交易算法, 原来交易玩的是心理战</a></p></li>
    <li><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5MjcxNjc2Ng==&amp;mid=2650559816&amp;idx=1&amp;sn=380cfd3d18fb987c0073bf1b8289155a&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">她说：</a><a href="https://mp.weixin.qq.com/s?__biz=MzA5MjcxNjc2Ng==&amp;mid=2650559816&amp;idx=1&amp;sn=380cfd3d18fb987c0073bf1b8289155a&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">为啥程序员都特想要机械键盘？</a><a href="https://mp.weixin.qq.com/s?__biz=MzA5MjcxNjc2Ng==&amp;mid=2650559816&amp;idx=1&amp;sn=380cfd3d18fb987c0073bf1b8289155a&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">这答案我服！</a></p></li>
   </ul>
   <p style="min-height:1em;text-align:right;"><br></p>
   <p style="min-height:1em;text-align:right;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAaBKZcUCzAia226XxjflVWcEefX4n29GVz3bwHiaeKYTz99IUdicIUic28RH1uMCQicuBRxgUhgsmqLP3A/640?wx_fmt=png" alt="640?wx_fmt=png"></p> 
  </div> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
