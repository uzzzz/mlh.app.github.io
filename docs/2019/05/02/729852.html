<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>如何为回归问题选择最合适的机器学习方法？ | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="如何为回归问题选择最合适的机器学习方法？" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="作者 | 何从庆 本文经授权转载自 AI算法之心（id：AIHeartForYou） 在目前的机器学习领域中，最常见的三种任务就是：回归分析、分类分析、聚类分析。在之前的文章中，我曾写过一篇《15分钟带你入门sklearn与机器学习——分类算法篇》。 那么什么是回归呢？回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。回归分析在机器学习领域应用非常广泛，例如，商品的销量预测问题，交通流量预测问题。那么，如何为这些回归问题选择最合适的机器学习算法呢？这篇文章将从以下一个方面介绍： 常用的回归算法 回归竞赛问题以及解决方案 正在进行中的回归竞赛问题 常用的回归算法 这里介绍一些回归问题中常用的机器学习方法，sklearn作为机器学习中一个强大的算法包，内置了许多经典的回归算法，下面将一一介绍各个算法： 注:下面回归算法的代码已上传至网盘，如果有小伙伴感兴趣，欢迎关注&quot;AI算法之心&quot;，后台回复&quot;回归算法&quot;。 1、线性回归 线性回归拟合一个带系数的线性模型，以最小化数据中的观测值与线性预测值之间的残差平方和。 sklearn中也存在线性回归的算法库的接口，代码示例如下所示： #加载线性模型算法库from sklearn import linear_model# 创建线性回归模型的对象regr = linear_model.LinearRegression()# 利用训练集训练线性模型regr.fit(X_train, y_train)# 使用测试集做预测y_pred = regr.predict(X_test) 2、岭回归 上述的线性回归算法使用最小二乘法优化各个系数，对于岭回归来说，岭回归通过对系数进行惩罚(L2范式)来解决普通最小二乘法的一些问题，例如，当特征之间完全共线性(有解)或者说特征之间高度相关，这个时候适合用岭回归。 #加载线性模型算法库from sklearn.linear_model import Ridge# 创建岭回归模型的对象reg = Ridge(alpha=.5)# 利用训练集训练岭回归模型reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) #输出各个系数reg.coef_reg.intercept_&nbsp; 3、Lasso回归 Lasso是一个估计稀疏稀疏的线性模型。它在某些情况下很有用，由于它倾向于选择参数值较少的解，有效地减少了给定解所依赖的变量的数量。Lasso模型在最小二乘法的基础上加入L1范式作为惩罚项。 #加载Lasso模型算法库from sklearn.linear_model import Lasso# 创建Lasso回归模型的对象reg = Lasso(alpha=0.1)# 利用训练集训练Lasso回归模型reg.fit([[0, 0], [1, 1]], [0, 1])&quot;&quot;&quot;Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)&quot;&quot;&quot;# 使用测试集做预测reg.predict([[1, 1]]) 4、Elastic Net回归 Elastic Net 是一个线性模型利用L1范式和L2范式共同作为惩罚项。这种组合既可以学习稀疏模型，同时可以保持岭回归的正则化属性。 #加载ElasticNet模型算法库from sklearn.linear_model import ElasticNet#加载数据集from sklearn.datasets import make_regressionX, y = make_regression(n_features=2, random_state=0)#创建ElasticNet回归模型的对象regr = ElasticNet(random_state=0)# 利用训练集训练ElasticNet回归模型regr.fit(X, y)print(regr.coef_) print(regr.intercept_) print(regr.predict([[0, 0]])) 5、贝叶斯岭回归 贝叶斯岭回归模型和岭回归类似。贝叶斯岭回归通过最大化边际对数似然来估计参数。 from sklearn.linear_model import BayesianRidgeX = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]Y = [0., 1., 2., 3.]reg = BayesianRidge()reg.fit(X, Y) 6、SGD回归 上述的线性模型通过最小二乘法来优化损失函数，SGD回归也是一种线性回归，不同的是，它通过随机梯度下降最小化正则化经验损失。 import numpy as npfrom sklearn import linear_modeln_samples, n_features = 10, 5np.random.seed(0)y = np.random.randn(n_samples)X = np.random.randn(n_samples, n_features)clf = linear_model.SGDRegressor(max_iter=1000, tol=1e-3)clf.fit(X, y)&quot;&quot;&quot;SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;invscaling&#39;, loss=&#39;squared_loss&#39;, max_iter=1000, n_iter=None, n_iter_no_change=5, penalty=&#39;l2&#39;, power_t=0.25, random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False)&quot;&quot;&quot; 7、SVR 众所周知，支持向量机在分类领域应用非常广泛，支持向量机的分类方法可以被推广到解决回归问题，这个就称为支持向量回归。支持向量回归算法生成的模型同样地只依赖训练数据集中的一个子集(和支持向量分类算法类似)。 #加载SVR模型算法库from sklearn.svm import SVR#训练集X = [[0, 0], [2, 2]]y = [0.5, 2.5]#创建SVR回归模型的对象clf = SVR()# 利用训练集训练SVR回归模型clf.fit(X, y) &quot;&quot;&quot;SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False)&quot;&quot;&quot;clf.predict([[1, 1]]) 8、KNN回归 在数据标签是连续变量而不是离散变量的情况下，可以使用KNN回归。分配给查询点的标签是根据其最近邻居标签的平均值计算的。 X = [[0], [1], [2], [3]]y = [0, 0, 1, 1]from sklearn.neighbors import KNeighborsRegressorneigh = KNeighborsRegressor(n_neighbors=2)neigh.fit(X, y) print(neigh.predict([[1.5]])) 9、决策树回归 决策树也可以应用于回归问题，使用sklearn的DecisionTreeRegressor类。 from sklearn.tree import DecisionTreeRegressor X = [[0, 0], [2, 2]]y = [0.5, 2.5]clf = DecisionTreeRegressor()clf = clf.fit(X, y)clf.predict([[1, 1]]) 10、神经网络 神经网络使用slearn中MLPRegressor类实现了一个多层感知器(MLP)，它使用在输出层中没有激活函数的反向传播进行训练，也可以将衡等函数视为激活函数。因此，它使用平方误差作为损失函数，输出是一组连续的值。 from sklearn.neural_network import MLPRegressormlp=MLPRegressor()mlp.fit(X_train,y_train)&quot;&quot;&quot;MLPRegressor(activation=&#39;relu&#39;, alpha=0.0001, batch_size=&#39;auto&#39;, beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(100,), learning_rate=&#39;constant&#39;, learning_rate_init=0.001, max_iter=200, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=None, shuffle=True, solver=&#39;adam&#39;, tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False)&quot;&quot;&quot;y_pred&nbsp;=&nbsp;mlp.predict(X_test) 11.RandomForest回归 RamdomForest回归也是一种经典的集成算法之一。 from sklearn.ensemble import RandomForestRegressorfrom sklearn.datasets import make_regressionX, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)regr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=100)regr.fit(X, y)print(regr.feature_importances_)print(regr.predict([[0, 0, 0, 0]])) 11、XGBoost回归 XGBoost近些年在学术界取得的成果连连捷报，基本所有的机器学习比赛的冠军方案都使用了XGBoost算法，对于XGBoost的算法接口有两种，这里我仅介绍XGBoost的sklearn接口。更多请参考：&nbsp; https://xgboost.readthedocs.io/en/latest/python/index.html import xgboost as xgbxgb_model = xgb.XGBRegressor(max_depth = 3, learning_rate = 0.1, n_estimators = 100, objective = &#39;reg:linear&#39;, n_jobs = -1)xgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train)], eval_metric=&#39;logloss&#39;, verbose=100)y_pred = xgb_model.predict(X_test)print(mean_squared_error(y_test, y_pred)) 12、LightGBM回归 LightGBM作为另一个使用基于树的学习算法的梯度增强框架。在算法竞赛也是每逢必用的神器，且要想在竞赛取得好成绩，LightGBM是一个不可或缺的神器。相比于XGBoost，LightGBM有如下优点，训练速度更快，效率更高效；低内存的使用量。对于LightGBM的算法接口有两种，这里我同样介绍LightGBM的sklearn接口。 更多请参考：https://lightgbm.readthedocs.io/en/latest/ import lightgbm as lgbgbm = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=20)gbm.fit(X_train, y_train, eval_set=[(X_train, y_train)], eval_metric=&#39;logloss&#39;, verbose=100)y_pred = gbm.predict(X_test)print(mean_squared_error(y_test, y_pred)) 回归竞赛问题以及解决方案 为了方便小伙伴们练习机器学习中的相关项目，这里整理一些回归竞赛问题，帮助入门机器学习的小伙伴们更加深入的掌握机器学习中的回归问题。 入门级比赛： Kaggle——房价预测 这个比赛作为最基础的回归问题之一，很适合入门机器学习的小伙伴们。 网址：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 经典解决方案：&nbsp; XGBoost解决方案：&nbsp;https://www.kaggle.com/dansbecker/xgboost Lasso解决方案：&nbsp;https://www.kaggle.com/mymkyt/simple-lasso-public-score-0-12102 进阶比赛： Kaggle——销售量预测 这个比赛作为经典的时间序列问题之一，目标是为了预测下个月每种产品和商店的总销售额。 网址：https://www.kaggle.com/c/competitive-data-science-predict-future-sales 经典解决方案： LightGBM:&nbsp;https://www.kaggle.com/sanket30/predicting-sales-using-lightgbm XGBoost:&nbsp;https://www.kaggle.com/fabianaboldrin/eda-xgboost 第一名解决方案：https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/74835#latest-503740 TOP比赛方案： Kaggle——餐厅访客预测 网址：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting 解决方案： 1st 方案：&nbsp;https://www.kaggle.com/plantsgo/solution-public-0-471-private-0-505 7th 方案：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49259#latest-284437 8th 方案：https://github.com/MaxHalford/kaggle-recruit-restaurant 12th 方案：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49251#latest-282765 Kaggle——CorporaciónFavoritaGrocery销售预测 网址：https://www.kaggle.com/c/favorita-grocery-sales-forecasting 解决方案： 1st 方案：&nbsp;https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47582#latest-360306 2st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47568#latest-278474 3st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47560#latest-302253 4st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47529#latest-271077 5st方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47556#latest-270515 6st方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47575#latest-269568 正在进行中的回归竞赛 小伙伴们看到上面的解决方案是不是跃跃欲试，最近国内也有各大回归比赛，赶紧趁热打铁，来学习学习回归比赛吧！ 2019年腾讯广告大赛——广告曝光预估 网址：https://algo.qq.com/application/home/home/index.html （本文为AI科技大本营转载文章，转载请联系原作者） ◆ CTA核心技术及应用峰会 ◆ 5月25-27日，由中国IT社区CSDN与数字经济人才发展中心联合主办的第一届CTA核心技术及应用峰会将在杭州国际博览中心隆重召开，峰会将围绕人工智能领域，邀请技术领航者，与开发者共同探讨机器学习和知识图谱的前沿研究及应用。 更多重磅嘉宾请识别海报二维码查看，目前会议早鸟票发售中（原票价1099元），点击阅读原文即刻抢购。添加小助手微信15101014297，备注“CTA”，了解票务以及会务详情。 推荐阅读 他25岁进贝尔实验室，32岁提信息论，40岁办达特茅斯会议，晚年患上阿兹海默 | 人物志 微软沈向洋，百度李彦宏、王海峰，阿里王坚均候选中国工程院院士 小样，加张图你就不认识我了？“补丁”模型骗你没商量！| 技术头条 东大漆桂林、清华李涓子、复旦肖仰华等大牛确认出席CTA峰会！5月一起打卡杭州 京东 60 天哗变！CTO 成优化第一人 | 畅言 异构计算=未来？一文带你秒懂3大主流异构 《互联网人叹气图鉴》 回报率29%! 大神用情感分析创建一个比特币交易算法, 原来交易玩的是心理战 她说：为啥程序员都特想要机械键盘？这答案我服！" />
<meta property="og:description" content="作者 | 何从庆 本文经授权转载自 AI算法之心（id：AIHeartForYou） 在目前的机器学习领域中，最常见的三种任务就是：回归分析、分类分析、聚类分析。在之前的文章中，我曾写过一篇《15分钟带你入门sklearn与机器学习——分类算法篇》。 那么什么是回归呢？回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。回归分析在机器学习领域应用非常广泛，例如，商品的销量预测问题，交通流量预测问题。那么，如何为这些回归问题选择最合适的机器学习算法呢？这篇文章将从以下一个方面介绍： 常用的回归算法 回归竞赛问题以及解决方案 正在进行中的回归竞赛问题 常用的回归算法 这里介绍一些回归问题中常用的机器学习方法，sklearn作为机器学习中一个强大的算法包，内置了许多经典的回归算法，下面将一一介绍各个算法： 注:下面回归算法的代码已上传至网盘，如果有小伙伴感兴趣，欢迎关注&quot;AI算法之心&quot;，后台回复&quot;回归算法&quot;。 1、线性回归 线性回归拟合一个带系数的线性模型，以最小化数据中的观测值与线性预测值之间的残差平方和。 sklearn中也存在线性回归的算法库的接口，代码示例如下所示： #加载线性模型算法库from sklearn import linear_model# 创建线性回归模型的对象regr = linear_model.LinearRegression()# 利用训练集训练线性模型regr.fit(X_train, y_train)# 使用测试集做预测y_pred = regr.predict(X_test) 2、岭回归 上述的线性回归算法使用最小二乘法优化各个系数，对于岭回归来说，岭回归通过对系数进行惩罚(L2范式)来解决普通最小二乘法的一些问题，例如，当特征之间完全共线性(有解)或者说特征之间高度相关，这个时候适合用岭回归。 #加载线性模型算法库from sklearn.linear_model import Ridge# 创建岭回归模型的对象reg = Ridge(alpha=.5)# 利用训练集训练岭回归模型reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) #输出各个系数reg.coef_reg.intercept_&nbsp; 3、Lasso回归 Lasso是一个估计稀疏稀疏的线性模型。它在某些情况下很有用，由于它倾向于选择参数值较少的解，有效地减少了给定解所依赖的变量的数量。Lasso模型在最小二乘法的基础上加入L1范式作为惩罚项。 #加载Lasso模型算法库from sklearn.linear_model import Lasso# 创建Lasso回归模型的对象reg = Lasso(alpha=0.1)# 利用训练集训练Lasso回归模型reg.fit([[0, 0], [1, 1]], [0, 1])&quot;&quot;&quot;Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)&quot;&quot;&quot;# 使用测试集做预测reg.predict([[1, 1]]) 4、Elastic Net回归 Elastic Net 是一个线性模型利用L1范式和L2范式共同作为惩罚项。这种组合既可以学习稀疏模型，同时可以保持岭回归的正则化属性。 #加载ElasticNet模型算法库from sklearn.linear_model import ElasticNet#加载数据集from sklearn.datasets import make_regressionX, y = make_regression(n_features=2, random_state=0)#创建ElasticNet回归模型的对象regr = ElasticNet(random_state=0)# 利用训练集训练ElasticNet回归模型regr.fit(X, y)print(regr.coef_) print(regr.intercept_) print(regr.predict([[0, 0]])) 5、贝叶斯岭回归 贝叶斯岭回归模型和岭回归类似。贝叶斯岭回归通过最大化边际对数似然来估计参数。 from sklearn.linear_model import BayesianRidgeX = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]Y = [0., 1., 2., 3.]reg = BayesianRidge()reg.fit(X, Y) 6、SGD回归 上述的线性模型通过最小二乘法来优化损失函数，SGD回归也是一种线性回归，不同的是，它通过随机梯度下降最小化正则化经验损失。 import numpy as npfrom sklearn import linear_modeln_samples, n_features = 10, 5np.random.seed(0)y = np.random.randn(n_samples)X = np.random.randn(n_samples, n_features)clf = linear_model.SGDRegressor(max_iter=1000, tol=1e-3)clf.fit(X, y)&quot;&quot;&quot;SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;invscaling&#39;, loss=&#39;squared_loss&#39;, max_iter=1000, n_iter=None, n_iter_no_change=5, penalty=&#39;l2&#39;, power_t=0.25, random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False)&quot;&quot;&quot; 7、SVR 众所周知，支持向量机在分类领域应用非常广泛，支持向量机的分类方法可以被推广到解决回归问题，这个就称为支持向量回归。支持向量回归算法生成的模型同样地只依赖训练数据集中的一个子集(和支持向量分类算法类似)。 #加载SVR模型算法库from sklearn.svm import SVR#训练集X = [[0, 0], [2, 2]]y = [0.5, 2.5]#创建SVR回归模型的对象clf = SVR()# 利用训练集训练SVR回归模型clf.fit(X, y) &quot;&quot;&quot;SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False)&quot;&quot;&quot;clf.predict([[1, 1]]) 8、KNN回归 在数据标签是连续变量而不是离散变量的情况下，可以使用KNN回归。分配给查询点的标签是根据其最近邻居标签的平均值计算的。 X = [[0], [1], [2], [3]]y = [0, 0, 1, 1]from sklearn.neighbors import KNeighborsRegressorneigh = KNeighborsRegressor(n_neighbors=2)neigh.fit(X, y) print(neigh.predict([[1.5]])) 9、决策树回归 决策树也可以应用于回归问题，使用sklearn的DecisionTreeRegressor类。 from sklearn.tree import DecisionTreeRegressor X = [[0, 0], [2, 2]]y = [0.5, 2.5]clf = DecisionTreeRegressor()clf = clf.fit(X, y)clf.predict([[1, 1]]) 10、神经网络 神经网络使用slearn中MLPRegressor类实现了一个多层感知器(MLP)，它使用在输出层中没有激活函数的反向传播进行训练，也可以将衡等函数视为激活函数。因此，它使用平方误差作为损失函数，输出是一组连续的值。 from sklearn.neural_network import MLPRegressormlp=MLPRegressor()mlp.fit(X_train,y_train)&quot;&quot;&quot;MLPRegressor(activation=&#39;relu&#39;, alpha=0.0001, batch_size=&#39;auto&#39;, beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(100,), learning_rate=&#39;constant&#39;, learning_rate_init=0.001, max_iter=200, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=None, shuffle=True, solver=&#39;adam&#39;, tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False)&quot;&quot;&quot;y_pred&nbsp;=&nbsp;mlp.predict(X_test) 11.RandomForest回归 RamdomForest回归也是一种经典的集成算法之一。 from sklearn.ensemble import RandomForestRegressorfrom sklearn.datasets import make_regressionX, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)regr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=100)regr.fit(X, y)print(regr.feature_importances_)print(regr.predict([[0, 0, 0, 0]])) 11、XGBoost回归 XGBoost近些年在学术界取得的成果连连捷报，基本所有的机器学习比赛的冠军方案都使用了XGBoost算法，对于XGBoost的算法接口有两种，这里我仅介绍XGBoost的sklearn接口。更多请参考：&nbsp; https://xgboost.readthedocs.io/en/latest/python/index.html import xgboost as xgbxgb_model = xgb.XGBRegressor(max_depth = 3, learning_rate = 0.1, n_estimators = 100, objective = &#39;reg:linear&#39;, n_jobs = -1)xgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train)], eval_metric=&#39;logloss&#39;, verbose=100)y_pred = xgb_model.predict(X_test)print(mean_squared_error(y_test, y_pred)) 12、LightGBM回归 LightGBM作为另一个使用基于树的学习算法的梯度增强框架。在算法竞赛也是每逢必用的神器，且要想在竞赛取得好成绩，LightGBM是一个不可或缺的神器。相比于XGBoost，LightGBM有如下优点，训练速度更快，效率更高效；低内存的使用量。对于LightGBM的算法接口有两种，这里我同样介绍LightGBM的sklearn接口。 更多请参考：https://lightgbm.readthedocs.io/en/latest/ import lightgbm as lgbgbm = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=20)gbm.fit(X_train, y_train, eval_set=[(X_train, y_train)], eval_metric=&#39;logloss&#39;, verbose=100)y_pred = gbm.predict(X_test)print(mean_squared_error(y_test, y_pred)) 回归竞赛问题以及解决方案 为了方便小伙伴们练习机器学习中的相关项目，这里整理一些回归竞赛问题，帮助入门机器学习的小伙伴们更加深入的掌握机器学习中的回归问题。 入门级比赛： Kaggle——房价预测 这个比赛作为最基础的回归问题之一，很适合入门机器学习的小伙伴们。 网址：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 经典解决方案：&nbsp; XGBoost解决方案：&nbsp;https://www.kaggle.com/dansbecker/xgboost Lasso解决方案：&nbsp;https://www.kaggle.com/mymkyt/simple-lasso-public-score-0-12102 进阶比赛： Kaggle——销售量预测 这个比赛作为经典的时间序列问题之一，目标是为了预测下个月每种产品和商店的总销售额。 网址：https://www.kaggle.com/c/competitive-data-science-predict-future-sales 经典解决方案： LightGBM:&nbsp;https://www.kaggle.com/sanket30/predicting-sales-using-lightgbm XGBoost:&nbsp;https://www.kaggle.com/fabianaboldrin/eda-xgboost 第一名解决方案：https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/74835#latest-503740 TOP比赛方案： Kaggle——餐厅访客预测 网址：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting 解决方案： 1st 方案：&nbsp;https://www.kaggle.com/plantsgo/solution-public-0-471-private-0-505 7th 方案：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49259#latest-284437 8th 方案：https://github.com/MaxHalford/kaggle-recruit-restaurant 12th 方案：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49251#latest-282765 Kaggle——CorporaciónFavoritaGrocery销售预测 网址：https://www.kaggle.com/c/favorita-grocery-sales-forecasting 解决方案： 1st 方案：&nbsp;https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47582#latest-360306 2st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47568#latest-278474 3st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47560#latest-302253 4st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47529#latest-271077 5st方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47556#latest-270515 6st方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47575#latest-269568 正在进行中的回归竞赛 小伙伴们看到上面的解决方案是不是跃跃欲试，最近国内也有各大回归比赛，赶紧趁热打铁，来学习学习回归比赛吧！ 2019年腾讯广告大赛——广告曝光预估 网址：https://algo.qq.com/application/home/home/index.html （本文为AI科技大本营转载文章，转载请联系原作者） ◆ CTA核心技术及应用峰会 ◆ 5月25-27日，由中国IT社区CSDN与数字经济人才发展中心联合主办的第一届CTA核心技术及应用峰会将在杭州国际博览中心隆重召开，峰会将围绕人工智能领域，邀请技术领航者，与开发者共同探讨机器学习和知识图谱的前沿研究及应用。 更多重磅嘉宾请识别海报二维码查看，目前会议早鸟票发售中（原票价1099元），点击阅读原文即刻抢购。添加小助手微信15101014297，备注“CTA”，了解票务以及会务详情。 推荐阅读 他25岁进贝尔实验室，32岁提信息论，40岁办达特茅斯会议，晚年患上阿兹海默 | 人物志 微软沈向洋，百度李彦宏、王海峰，阿里王坚均候选中国工程院院士 小样，加张图你就不认识我了？“补丁”模型骗你没商量！| 技术头条 东大漆桂林、清华李涓子、复旦肖仰华等大牛确认出席CTA峰会！5月一起打卡杭州 京东 60 天哗变！CTO 成优化第一人 | 畅言 异构计算=未来？一文带你秒懂3大主流异构 《互联网人叹气图鉴》 回报率29%! 大神用情感分析创建一个比特币交易算法, 原来交易玩的是心理战 她说：为啥程序员都特想要机械键盘？这答案我服！" />
<link rel="canonical" href="https://mlh.app/2019/05/02/729852.html" />
<meta property="og:url" content="https://mlh.app/2019/05/02/729852.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-02T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"作者 | 何从庆 本文经授权转载自 AI算法之心（id：AIHeartForYou） 在目前的机器学习领域中，最常见的三种任务就是：回归分析、分类分析、聚类分析。在之前的文章中，我曾写过一篇《15分钟带你入门sklearn与机器学习——分类算法篇》。 那么什么是回归呢？回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。回归分析在机器学习领域应用非常广泛，例如，商品的销量预测问题，交通流量预测问题。那么，如何为这些回归问题选择最合适的机器学习算法呢？这篇文章将从以下一个方面介绍： 常用的回归算法 回归竞赛问题以及解决方案 正在进行中的回归竞赛问题 常用的回归算法 这里介绍一些回归问题中常用的机器学习方法，sklearn作为机器学习中一个强大的算法包，内置了许多经典的回归算法，下面将一一介绍各个算法： 注:下面回归算法的代码已上传至网盘，如果有小伙伴感兴趣，欢迎关注&quot;AI算法之心&quot;，后台回复&quot;回归算法&quot;。 1、线性回归 线性回归拟合一个带系数的线性模型，以最小化数据中的观测值与线性预测值之间的残差平方和。 sklearn中也存在线性回归的算法库的接口，代码示例如下所示： #加载线性模型算法库from sklearn import linear_model# 创建线性回归模型的对象regr = linear_model.LinearRegression()# 利用训练集训练线性模型regr.fit(X_train, y_train)# 使用测试集做预测y_pred = regr.predict(X_test) 2、岭回归 上述的线性回归算法使用最小二乘法优化各个系数，对于岭回归来说，岭回归通过对系数进行惩罚(L2范式)来解决普通最小二乘法的一些问题，例如，当特征之间完全共线性(有解)或者说特征之间高度相关，这个时候适合用岭回归。 #加载线性模型算法库from sklearn.linear_model import Ridge# 创建岭回归模型的对象reg = Ridge(alpha=.5)# 利用训练集训练岭回归模型reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) #输出各个系数reg.coef_reg.intercept_&nbsp; 3、Lasso回归 Lasso是一个估计稀疏稀疏的线性模型。它在某些情况下很有用，由于它倾向于选择参数值较少的解，有效地减少了给定解所依赖的变量的数量。Lasso模型在最小二乘法的基础上加入L1范式作为惩罚项。 #加载Lasso模型算法库from sklearn.linear_model import Lasso# 创建Lasso回归模型的对象reg = Lasso(alpha=0.1)# 利用训练集训练Lasso回归模型reg.fit([[0, 0], [1, 1]], [0, 1])&quot;&quot;&quot;Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)&quot;&quot;&quot;# 使用测试集做预测reg.predict([[1, 1]]) 4、Elastic Net回归 Elastic Net 是一个线性模型利用L1范式和L2范式共同作为惩罚项。这种组合既可以学习稀疏模型，同时可以保持岭回归的正则化属性。 #加载ElasticNet模型算法库from sklearn.linear_model import ElasticNet#加载数据集from sklearn.datasets import make_regressionX, y = make_regression(n_features=2, random_state=0)#创建ElasticNet回归模型的对象regr = ElasticNet(random_state=0)# 利用训练集训练ElasticNet回归模型regr.fit(X, y)print(regr.coef_) print(regr.intercept_) print(regr.predict([[0, 0]])) 5、贝叶斯岭回归 贝叶斯岭回归模型和岭回归类似。贝叶斯岭回归通过最大化边际对数似然来估计参数。 from sklearn.linear_model import BayesianRidgeX = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]Y = [0., 1., 2., 3.]reg = BayesianRidge()reg.fit(X, Y) 6、SGD回归 上述的线性模型通过最小二乘法来优化损失函数，SGD回归也是一种线性回归，不同的是，它通过随机梯度下降最小化正则化经验损失。 import numpy as npfrom sklearn import linear_modeln_samples, n_features = 10, 5np.random.seed(0)y = np.random.randn(n_samples)X = np.random.randn(n_samples, n_features)clf = linear_model.SGDRegressor(max_iter=1000, tol=1e-3)clf.fit(X, y)&quot;&quot;&quot;SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;invscaling&#39;, loss=&#39;squared_loss&#39;, max_iter=1000, n_iter=None, n_iter_no_change=5, penalty=&#39;l2&#39;, power_t=0.25, random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False)&quot;&quot;&quot; 7、SVR 众所周知，支持向量机在分类领域应用非常广泛，支持向量机的分类方法可以被推广到解决回归问题，这个就称为支持向量回归。支持向量回归算法生成的模型同样地只依赖训练数据集中的一个子集(和支持向量分类算法类似)。 #加载SVR模型算法库from sklearn.svm import SVR#训练集X = [[0, 0], [2, 2]]y = [0.5, 2.5]#创建SVR回归模型的对象clf = SVR()# 利用训练集训练SVR回归模型clf.fit(X, y) &quot;&quot;&quot;SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False)&quot;&quot;&quot;clf.predict([[1, 1]]) 8、KNN回归 在数据标签是连续变量而不是离散变量的情况下，可以使用KNN回归。分配给查询点的标签是根据其最近邻居标签的平均值计算的。 X = [[0], [1], [2], [3]]y = [0, 0, 1, 1]from sklearn.neighbors import KNeighborsRegressorneigh = KNeighborsRegressor(n_neighbors=2)neigh.fit(X, y) print(neigh.predict([[1.5]])) 9、决策树回归 决策树也可以应用于回归问题，使用sklearn的DecisionTreeRegressor类。 from sklearn.tree import DecisionTreeRegressor X = [[0, 0], [2, 2]]y = [0.5, 2.5]clf = DecisionTreeRegressor()clf = clf.fit(X, y)clf.predict([[1, 1]]) 10、神经网络 神经网络使用slearn中MLPRegressor类实现了一个多层感知器(MLP)，它使用在输出层中没有激活函数的反向传播进行训练，也可以将衡等函数视为激活函数。因此，它使用平方误差作为损失函数，输出是一组连续的值。 from sklearn.neural_network import MLPRegressormlp=MLPRegressor()mlp.fit(X_train,y_train)&quot;&quot;&quot;MLPRegressor(activation=&#39;relu&#39;, alpha=0.0001, batch_size=&#39;auto&#39;, beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(100,), learning_rate=&#39;constant&#39;, learning_rate_init=0.001, max_iter=200, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=None, shuffle=True, solver=&#39;adam&#39;, tol=0.0001, validation_fraction=0.1, verbose=False, warm_start=False)&quot;&quot;&quot;y_pred&nbsp;=&nbsp;mlp.predict(X_test) 11.RandomForest回归 RamdomForest回归也是一种经典的集成算法之一。 from sklearn.ensemble import RandomForestRegressorfrom sklearn.datasets import make_regressionX, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)regr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=100)regr.fit(X, y)print(regr.feature_importances_)print(regr.predict([[0, 0, 0, 0]])) 11、XGBoost回归 XGBoost近些年在学术界取得的成果连连捷报，基本所有的机器学习比赛的冠军方案都使用了XGBoost算法，对于XGBoost的算法接口有两种，这里我仅介绍XGBoost的sklearn接口。更多请参考：&nbsp; https://xgboost.readthedocs.io/en/latest/python/index.html import xgboost as xgbxgb_model = xgb.XGBRegressor(max_depth = 3, learning_rate = 0.1, n_estimators = 100, objective = &#39;reg:linear&#39;, n_jobs = -1)xgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train)], eval_metric=&#39;logloss&#39;, verbose=100)y_pred = xgb_model.predict(X_test)print(mean_squared_error(y_test, y_pred)) 12、LightGBM回归 LightGBM作为另一个使用基于树的学习算法的梯度增强框架。在算法竞赛也是每逢必用的神器，且要想在竞赛取得好成绩，LightGBM是一个不可或缺的神器。相比于XGBoost，LightGBM有如下优点，训练速度更快，效率更高效；低内存的使用量。对于LightGBM的算法接口有两种，这里我同样介绍LightGBM的sklearn接口。 更多请参考：https://lightgbm.readthedocs.io/en/latest/ import lightgbm as lgbgbm = lgb.LGBMRegressor(num_leaves=31, learning_rate=0.05, n_estimators=20)gbm.fit(X_train, y_train, eval_set=[(X_train, y_train)], eval_metric=&#39;logloss&#39;, verbose=100)y_pred = gbm.predict(X_test)print(mean_squared_error(y_test, y_pred)) 回归竞赛问题以及解决方案 为了方便小伙伴们练习机器学习中的相关项目，这里整理一些回归竞赛问题，帮助入门机器学习的小伙伴们更加深入的掌握机器学习中的回归问题。 入门级比赛： Kaggle——房价预测 这个比赛作为最基础的回归问题之一，很适合入门机器学习的小伙伴们。 网址：https://www.kaggle.com/c/house-prices-advanced-regression-techniques 经典解决方案：&nbsp; XGBoost解决方案：&nbsp;https://www.kaggle.com/dansbecker/xgboost Lasso解决方案：&nbsp;https://www.kaggle.com/mymkyt/simple-lasso-public-score-0-12102 进阶比赛： Kaggle——销售量预测 这个比赛作为经典的时间序列问题之一，目标是为了预测下个月每种产品和商店的总销售额。 网址：https://www.kaggle.com/c/competitive-data-science-predict-future-sales 经典解决方案： LightGBM:&nbsp;https://www.kaggle.com/sanket30/predicting-sales-using-lightgbm XGBoost:&nbsp;https://www.kaggle.com/fabianaboldrin/eda-xgboost 第一名解决方案：https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/74835#latest-503740 TOP比赛方案： Kaggle——餐厅访客预测 网址：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting 解决方案： 1st 方案：&nbsp;https://www.kaggle.com/plantsgo/solution-public-0-471-private-0-505 7th 方案：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49259#latest-284437 8th 方案：https://github.com/MaxHalford/kaggle-recruit-restaurant 12th 方案：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49251#latest-282765 Kaggle——CorporaciónFavoritaGrocery销售预测 网址：https://www.kaggle.com/c/favorita-grocery-sales-forecasting 解决方案： 1st 方案：&nbsp;https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47582#latest-360306 2st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47568#latest-278474 3st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47560#latest-302253 4st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47529#latest-271077 5st方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47556#latest-270515 6st方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47575#latest-269568 正在进行中的回归竞赛 小伙伴们看到上面的解决方案是不是跃跃欲试，最近国内也有各大回归比赛，赶紧趁热打铁，来学习学习回归比赛吧！ 2019年腾讯广告大赛——广告曝光预估 网址：https://algo.qq.com/application/home/home/index.html （本文为AI科技大本营转载文章，转载请联系原作者） ◆ CTA核心技术及应用峰会 ◆ 5月25-27日，由中国IT社区CSDN与数字经济人才发展中心联合主办的第一届CTA核心技术及应用峰会将在杭州国际博览中心隆重召开，峰会将围绕人工智能领域，邀请技术领航者，与开发者共同探讨机器学习和知识图谱的前沿研究及应用。 更多重磅嘉宾请识别海报二维码查看，目前会议早鸟票发售中（原票价1099元），点击阅读原文即刻抢购。添加小助手微信15101014297，备注“CTA”，了解票务以及会务详情。 推荐阅读 他25岁进贝尔实验室，32岁提信息论，40岁办达特茅斯会议，晚年患上阿兹海默 | 人物志 微软沈向洋，百度李彦宏、王海峰，阿里王坚均候选中国工程院院士 小样，加张图你就不认识我了？“补丁”模型骗你没商量！| 技术头条 东大漆桂林、清华李涓子、复旦肖仰华等大牛确认出席CTA峰会！5月一起打卡杭州 京东 60 天哗变！CTO 成优化第一人 | 畅言 异构计算=未来？一文带你秒懂3大主流异构 《互联网人叹气图鉴》 回报率29%! 大神用情感分析创建一个比特币交易算法, 原来交易玩的是心理战 她说：为啥程序员都特想要机械键盘？这答案我服！","@type":"BlogPosting","url":"https://mlh.app/2019/05/02/729852.html","headline":"如何为回归问题选择最合适的机器学习方法？","dateModified":"2019-05-02T00:00:00+08:00","datePublished":"2019-05-02T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/02/729852.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>如何为回归问题选择最合适的机器学习方法？</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="rich_media_content" id="js_content"> 
   <p style="margin-left:8px;min-height:1em;letter-spacing:.544px;text-align:center;"><img class="rich_pages" style="color:rgb(136,136,136);font-size:15px;letter-spacing:1px;text-align:center;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/BnSNEaficFAaEvfrVTwTOJmJOdd48GLj5h1RyAH4y6ib8lZENjZq9y9IjJGT5scoiaetlqgA8yoKrqr2yugAHs58w/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></p>
   <p style="margin-left:8px;min-height:1em;letter-spacing:.544px;text-align:center;"><br></p>
   <p style="min-height:1em;letter-spacing:.544px;line-height:1.75em;"><span style="letter-spacing:.5px;color:rgb(136,136,136);font-family:arial, 'pingfang sc', stheiti, 'microsoft yahei', sans-serif;font-size:14px;text-align:left;">作者 | 何从庆</span></p>
   <p style="margin-left:8px;min-height:1em;letter-spacing:.544px;color:rgb(62,62,62);font-family:'Helvetica Neue', Helvetica, 'Hiragino Sans GB', 'Microsoft YaHei', Arial, sans-serif;font-size:16px;line-height:1.75em;"><span style="letter-spacing:.5px;color:rgb(136,136,136);font-family:arial, 'pingfang sc', stheiti, 'microsoft yahei', sans-serif;font-size:14px;text-align:left;">本文经授权转载自 AI算法之心（id：A</span><span style="color:rgb(136,136,136);font-size:14px;letter-spacing:.5px;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;text-align:justify;"><span style="color:rgb(136,136,136);font-family:arial, 'pingfang sc', stheiti, 'microsoft yahei', sans-serif;font-size:14px;text-align:left;">IHeartForYou</span>）</span></p>
   <p style="min-height:1em;letter-spacing:.544px;line-height:1.75em;"><br></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">在目前的机器学习领域中，最常见的三种任务就是：回归分析、分类分析、聚类分析。在之前的文章中，我曾写过一篇《15分钟带你入门sklearn与机器学习——分类算法篇》。</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">那么什么是回归呢？回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。回归分析在机器学习领域应用非常广泛，例如，商品的销量预测问题，交通流量预测问题。那么，如何为这些回归问题选择最合适的机器学习算法呢？这篇文章将从以下一个方面介绍：<br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <ul class="list-paddingleft-2" style="list-style-type:disc;">
    <li><p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">常用的回归算法</span></p></li>
    <li><p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">回归竞赛问题以及解决方案</span></p></li>
    <li><p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">正在进行中的回归竞赛问题</span></p></li>
   </ul>
   <p><br></p>
   <h3 style="text-align:justify;line-height:1.75em;"><span style="font-size:18px;"><strong><span style="font-family:'-webkit-standard';color:rgb(63,63,63);letter-spacing:1px;">常用的回归算法</span></strong></span></h3>
   <p><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">这里介绍一些回归问题中常用的机器学习方法，sklearn作为机器学习中一个强大的算法包，内置了许多经典的回归算法，下面将一一介绍各个算法：</span></p>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">注:下面回归算法的代码已上传至网盘，如果有小伙伴感兴趣，欢迎关注"AI算法之心"，后台回复"回归算法"。</span></p>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">1、线性回归</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">线性回归拟合一个带系数的线性模型，以最小化数据中的观测值与线性预测值之间的残差平方和。</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">sklearn中也存在线性回归的算法库的接口，代码示例如下所示：</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">#加载线性模型算法库</span></code><code><span class="code-snippet_outer">from sklearn import linear_model</span></code><code><span class="code-snippet_outer"># 创建线性回归模型的对象</span></code><code><span class="code-snippet_outer">regr = linear_model.LinearRegression()</span></code><code><span class="code-snippet_outer"># 利用训练集训练线性模型</span></code><code><span class="code-snippet_outer">regr.fit(X_train, y_train)</span></code><code><span class="code-snippet_outer"># 使用测试集做预测</span></code><code><span class="code-snippet_outer">y_pred = regr.predict(X_test)</span></code></pre>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">2、岭回归</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">上述的线性回归算法使用最小二乘法优化各个系数，对于岭回归来说，岭回归通过对系数进行惩罚(L2范式)来解决普通最小二乘法的一些问题，例如，当特征之间完全共线性(有解)或者说特征之间高度相关，这个时候适合用岭回归。</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <p style="line-height:1.75em;text-align:justify;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">#加载线性模型算法库</span></code><code><span class="code-snippet_outer">from sklearn.linear_model import Ridge</span></code><code><span class="code-snippet_outer"># 创建岭回归模型的对象</span></code><code><span class="code-snippet_outer">reg = Ridge(alpha=.5)</span></code><code><span class="code-snippet_outer"># 利用训练集训练岭回归模型</span></code><code><span class="code-snippet_outer">reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) </span></code><code><span class="code-snippet_outer">#输出各个系数</span></code><code><span class="code-snippet_outer">reg.coef_</span></code><code><span class="code-snippet_outer">reg.intercept_&nbsp;</span></code></pre>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">3、Lasso回归</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">Lasso是一个估计稀疏稀疏的线性模型。它在某些情况下很有用，由于它倾向于选择参数值较少的解，有效地减少了给定解所依赖的变量的数量。Lasso模型在最小二乘法的基础上加入L1范式作为惩罚项。</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">#加载Lasso模型算法库</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> sklearn.linear_model <span class="code-snippet__keyword">import</span> Lasso</span></code><code><span class="code-snippet_outer"># 创建Lasso回归模型的对象</span></code><code><span class="code-snippet_outer">reg = Lasso(alpha=<span class="code-snippet__number">0.1</span>)</span></code><code><span class="code-snippet_outer"># 利用训练集训练Lasso回归模型</span></code><code><span class="code-snippet_outer">reg.fit([[<span class="code-snippet__number">0</span>, <span class="code-snippet__number">0</span>], [<span class="code-snippet__number">1</span>, <span class="code-snippet__number">1</span>]], [<span class="code-snippet__number">0</span>, <span class="code-snippet__number">1</span>])</span></code><code><span class="code-snippet_outer">"""</span></code><code><span class="code-snippet_outer">Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,</span></code><code><span class="code-snippet_outer"> normalize=False, positive=False, precompute=False, random_state=None,</span></code><code><span class="code-snippet_outer"> selection='cyclic', tol=0.0001, warm_start=False)</span></code><code><span class="code-snippet_outer">"""</span></code><code><span class="code-snippet_outer"># 使用测试集做预测</span></code><code><span class="code-snippet_outer">reg.predict([[<span class="code-snippet__number">1</span>, <span class="code-snippet__number">1</span>]])</span></code></pre>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">4、Elastic Net回归</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">Elastic Net 是一个线性模型利用L1范式和L2范式共同作为惩罚项。这种组合既可以学习稀疏模型，同时可以保持岭回归的正则化属性。</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">#加载ElasticNet模型算法库</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> sklearn.linear_model <span class="code-snippet__keyword">import</span> ElasticNet</span></code><code><span class="code-snippet_outer">#加载数据集</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> sklearn.datasets <span class="code-snippet__keyword">import</span> make_regression</span></code><code><span class="code-snippet_outer">X, y = make_regression(n_features=<span class="code-snippet__number">2</span>, random_state=<span class="code-snippet__number">0</span>)</span></code><code><span class="code-snippet_outer">#创建ElasticNet回归模型的对象</span></code><code><span class="code-snippet_outer">regr = ElasticNet(random_state=<span class="code-snippet__number">0</span>)</span></code><code><span class="code-snippet_outer"># 利用训练集训练ElasticNet回归模型</span></code><code><span class="code-snippet_outer">regr.fit(X, y)</span></code><code><span class="code-snippet_outer">print(regr.coef_) </span></code><code><span class="code-snippet_outer">print(regr.intercept_) </span></code><code><span class="code-snippet_outer">print(regr.predict([[<span class="code-snippet__number">0</span>, <span class="code-snippet__number">0</span>]])) </span></code></pre>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">5、贝叶斯岭回归</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">贝叶斯岭回归模型和岭回归类似。贝叶斯岭回归通过最大化边际对数似然来估计参数。</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">from sklearn.linear_model import BayesianRidge</span></code><code><span class="code-snippet_outer">X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]</span></code><code><span class="code-snippet_outer">Y = [0., 1., 2., 3.]</span></code><code><span class="code-snippet_outer">reg = BayesianRidge()</span></code><code><span class="code-snippet_outer">reg.fit(X, Y)</span></code></pre>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">6、SGD回归</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">上述的线性模型通过最小二乘法来优化损失函数，SGD回归也是一种线性回归，不同的是，它通过随机梯度下降最小化正则化经验损失。</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__keyword">import</span> numpy <span class="code-snippet__keyword">as</span> np</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> sklearn <span class="code-snippet__keyword">import</span> linear_model</span></code><code><span class="code-snippet_outer">n_samples, n_features = <span class="code-snippet__number">10</span>, <span class="code-snippet__number">5</span></span></code><code><span class="code-snippet_outer">np.random.seed(<span class="code-snippet__number">0</span>)</span></code><code><span class="code-snippet_outer">y = np.random.randn(n_samples)</span></code><code><span class="code-snippet_outer">X = np.random.randn(n_samples, n_features)</span></code><code><span class="code-snippet_outer">clf = linear_model.SGDRegressor(max_iter=<span class="code-snippet__number">1000</span>, tol=<span class="code-snippet__number">1e-3</span>)</span></code><code><span class="code-snippet_outer">clf.fit(X, y)</span></code><code><span class="code-snippet_outer">"""</span></code><code><span class="code-snippet_outer">SGDRegressor(alpha=0.0001, average=False, early_stopping=False,</span></code><code><span class="code-snippet_outer"> epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,</span></code><code><span class="code-snippet_outer"> learning_rate='invscaling', loss='squared_loss', max_iter=1000,</span></code><code><span class="code-snippet_outer"> n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,</span></code><code><span class="code-snippet_outer"> random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1,</span></code><code><span class="code-snippet_outer"> verbose=0, warm_start=False)</span></code><code><span class="code-snippet_outer">"""</span></code></pre>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">7、SVR</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">众所周知，支持向量机在分类领域应用非常广泛，支持向量机的分类方法可以被推广到解决回归问题，这个就称为支持向量回归。支持向量回归算法生成的模型同样地只依赖训练数据集中的一个子集(和支持向量分类算法类似)。</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">#加载SVR模型算法库</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> sklearn.svm <span class="code-snippet__keyword">import</span> SVR</span></code><code><span class="code-snippet_outer">#训练集</span></code><code><span class="code-snippet_outer">X = [[<span class="code-snippet__number">0</span>, <span class="code-snippet__number">0</span>], [<span class="code-snippet__number">2</span>, <span class="code-snippet__number">2</span>]]</span></code><code><span class="code-snippet_outer">y = [<span class="code-snippet__number">0.5</span>, <span class="code-snippet__number">2.5</span>]</span></code><code><span class="code-snippet_outer">#创建SVR回归模型的对象</span></code><code><span class="code-snippet_outer">clf = SVR()</span></code><code><span class="code-snippet_outer"># 利用训练集训练SVR回归模型</span></code><code><span class="code-snippet_outer">clf.fit(X, y) </span></code><code><span class="code-snippet_outer">"""</span></code><code><span class="code-snippet_outer">SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,</span></code><code><span class="code-snippet_outer"> gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True,</span></code><code><span class="code-snippet_outer"> tol=0.001, verbose=False)</span></code><code><span class="code-snippet_outer">"""</span></code><code><span class="code-snippet_outer">clf.predict([[<span class="code-snippet__number">1</span>, <span class="code-snippet__number">1</span>]])</span></code></pre>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">8、KNN回归</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">在数据标签是连续变量而不是离散变量的情况下，可以使用KNN回归。分配给查询点的标签是根据其最近邻居标签的平均值计算的。</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">X = [[0], [1], [2], [3]]</span></code><code><span class="code-snippet_outer">y = [0, 0, 1, 1]</span></code><code><span class="code-snippet_outer">from sklearn.neighbors import KNeighborsRegressor</span></code><code><span class="code-snippet_outer">neigh = KNeighborsRegressor(n_neighbors=2)</span></code><code><span class="code-snippet_outer">neigh.fit(X, y) </span></code><code><span class="code-snippet_outer">print(neigh.predict([[1.5]]))</span></code></pre>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">9、决策树回归</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">决策树也可以应用于回归问题，使用sklearn的DecisionTreeRegressor类。</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer">from sklearn.tree import DecisionTreeRegressor </span></code><code><span class="code-snippet_outer">X = [[0, 0], [2, 2]]</span></code><code><span class="code-snippet_outer">y = [0.5, 2.5]</span></code><code><span class="code-snippet_outer">clf = DecisionTreeRegressor()</span></code><code><span class="code-snippet_outer">clf = clf.fit(X, y)</span></code><code><span class="code-snippet_outer">clf.predict([[1, 1]])</span></code></pre>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">10、神经网络<br></span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">神经网络使用slearn中MLPRegressor类实现了一个多层感知器(MLP)，它使用在输出层中没有激活函数的反向传播进行训练，也可以将衡等函数视为激活函数。因此，它使用平方误差作为损失函数，输出是一组连续的值。</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <p style="line-height:1.75em;text-align:justify;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"></span></p>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> sklearn.neural_network <span class="code-snippet__keyword">import</span> MLPRegressor</span></code><code><span class="code-snippet_outer">mlp=MLPRegressor()</span></code><code><span class="code-snippet_outer">mlp.fit(X_train,y_train)</span></code><code><span class="code-snippet_outer">"""</span></code><code><span class="code-snippet_outer">MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,</span></code><code><span class="code-snippet_outer"> beta_2=0.999, early_stopping=False, epsilon=1e-08,</span></code><code><span class="code-snippet_outer"> hidden_layer_sizes=(100,), learning_rate='constant',</span></code><code><span class="code-snippet_outer"> learning_rate_init=0.001, max_iter=200, momentum=0.9,</span></code><code><span class="code-snippet_outer"> n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,</span></code><code><span class="code-snippet_outer"> random_state=None, shuffle=True, solver='adam', tol=0.0001,</span></code><code><span class="code-snippet_outer"> validation_fraction=0.1, verbose=False, warm_start=False)</span></code><code><span class="code-snippet_outer">"""</span></code><code><span class="code-snippet_outer">y_pred&nbsp;=&nbsp;mlp.predict(X_test)</span></code></pre>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">11.RandomForest回归</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">RamdomForest回归也是一种经典的集成算法之一。</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> sklearn.ensemble <span class="code-snippet__keyword">import</span> RandomForestRegressor</span></code><code><span class="code-snippet_outer"><span class="code-snippet__keyword">from</span> sklearn.datasets <span class="code-snippet__keyword">import</span> make_regression</span></code><code><span class="code-snippet_outer">X, y = make_regression(n_features=<span class="code-snippet__number">4</span>, n_informative=<span class="code-snippet__number">2</span>,</span></code><code><span class="code-snippet_outer"> random_state=<span class="code-snippet__number">0</span>, shuffle=<span class="code-snippet__keyword">False</span>)</span></code><code><span class="code-snippet_outer">regr = RandomForestRegressor(max_depth=<span class="code-snippet__number">2</span>, random_state=<span class="code-snippet__number">0</span>,</span></code><code><span class="code-snippet_outer"> n_estimators=<span class="code-snippet__number">100</span>)</span></code><code><span class="code-snippet_outer">regr.fit(X, y)</span></code><code><span class="code-snippet_outer">print(regr.feature_importances_)</span></code><code><span class="code-snippet_outer">print(regr.predict([[<span class="code-snippet__number">0</span>, <span class="code-snippet__number">0</span>, <span class="code-snippet__number">0</span>, <span class="code-snippet__number">0</span>]]))</span></code></pre>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">11、XGBoost回归</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">XGBoost近些年在学术界取得的成果连连捷报，基本所有的机器学习比赛的冠军方案都使用了XGBoost算法，对于XGBoost的算法接口有两种，这里我仅介绍XGBoost的sklearn接口。更多请参考：&nbsp;</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;color:rgb(136,136,136);font-size:14px;">https://xgboost.readthedocs.io/en/latest/python/index.html</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__attr">import</span> <span class="code-snippet__string">xgboost as xgb</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">xgb_model</span> = <span class="code-snippet__string">xgb.XGBRegressor(max_depth = 3,</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">learning_rate</span> = <span class="code-snippet__string">0.1,</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">n_estimators</span> = <span class="code-snippet__string">100,</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">objective</span> = <span class="code-snippet__string">'reg:linear',</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">n_jobs</span> = <span class="code-snippet__string">-1)</span></span></code><code><span class="code-snippet_outer"><br></span></code><code><span class="code-snippet_outer"><span class="code-snippet__meta">xgb_model.fit(X_train,</span> <span class="code-snippet__string">y_train,</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">eval_set</span>=<span class="code-snippet__string">[(X_train, y_train)], </span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">eval_metric</span>=<span class="code-snippet__string">'logloss',</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">verbose</span>=<span class="code-snippet__string">100)</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">y_pred</span> = <span class="code-snippet__string">xgb_model.predict(X_test)</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__meta">print(mean_squared_error(y_test,</span> <span class="code-snippet__string">y_pred))</span></span></code></pre>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">12、LightGBM回归</span></strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">LightGBM作为另一个使用基于树的学习算法的梯度增强框架。在算法竞赛也是每逢必用的神器，且要想在竞赛取得好成绩，LightGBM是一个不可或缺的神器。相比于XGBoost，LightGBM有如下优点，训练速度更快，效率更高效；低内存的使用量。对于LightGBM的算法接口有两种，这里我同样介绍LightGBM的sklearn接口。</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:left;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">更多请参考：https://lightgbm.readthedocs.io/en/latest/</span></p>
   <p><br></p>
   <pre style="text-align:justify;"></pre>
   <ul class="code-snippet__line-index code-snippet__js">
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
    <li></li>
   </ul>
   <pre class="code-snippet__js"><code><span class="code-snippet_outer"><span class="code-snippet__attr">import</span> <span class="code-snippet__string">lightgbm as lgb</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">gbm</span> = <span class="code-snippet__string">lgb.LGBMRegressor(num_leaves=31,</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">learning_rate</span>=<span class="code-snippet__string">0.05,</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">n_estimators</span>=<span class="code-snippet__string">20)</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__meta">gbm.fit(X_train,</span> <span class="code-snippet__string">y_train,</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">eval_set</span>=<span class="code-snippet__string">[(X_train, y_train)], </span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">eval_metric</span>=<span class="code-snippet__string">'logloss',</span></span></code><code><span class="code-snippet_outer"> <span class="code-snippet__attr">verbose</span>=<span class="code-snippet__string">100)</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__attr">y_pred</span> = <span class="code-snippet__string">gbm.predict(X_test)</span></span></code><code><span class="code-snippet_outer"><span class="code-snippet__meta">print(mean_squared_error(y_test,</span> <span class="code-snippet__string">y_pred))</span></span></code></pre>
   <p><br></p>
   <h3 style="text-align:justify;line-height:1.75em;"><span style="font-size:18px;"><strong><span style="font-size:18px;font-family:'-webkit-standard';color:rgb(63,63,63);letter-spacing:1px;">回归竞赛问题以及解决方案</span></strong></span></h3>
   <p><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">为了方便小伙伴们练习机器学习中的相关项目，这里整理一些回归竞赛问题，帮助入门机器学习的小伙伴们更加深入的掌握机器学习中的回归问题。</span></p>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">入门级比赛：</span></strong></p>
   <p><br></p>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">Kaggle——房价预测</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">这个比赛作为最基础的回归问题之一，很适合入门机器学习的小伙伴们。</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">网址：https://www.kaggle.com/c/house-prices-advanced-regression-techniques</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">经典解决方案：&nbsp;</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">XGBoost解决方案：&nbsp;https://www.kaggle.com/dansbecker/xgboost</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">Lasso解决方案：&nbsp;https://www.kaggle.com/mymkyt/simple-lasso-public-score-0-12102</span></p>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">进阶比赛：</span></strong></p>
   <p><br></p>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">Kaggle——销售量预测</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">这个比赛作为经典的时间序列问题之一，目标是为了预测下个月每种产品和商店的总销售额。</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">网址：https://www.kaggle.com/c/competitive-data-science-predict-future-sales</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">经典解决方案：</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">LightGBM:&nbsp;https://www.kaggle.com/sanket30/predicting-sales-using-lightgbm</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">XGBoost:&nbsp;https://www.kaggle.com/fabianaboldrin/eda-xgboost</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">第一名解决方案：https://www.kaggle.com/c/competitive-data-science-predict-future-sales/discussion/74835#latest-503740</span></p>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">TOP比赛方案：</span></strong></p>
   <p><br></p>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">Kaggle——餐厅访客预测</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">网址：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">解决方案：</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">1st 方案：&nbsp;https://www.kaggle.com/plantsgo/solution-public-0-471-private-0-505</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">7th 方案：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49259#latest-284437</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">8th 方案：https://github.com/MaxHalford/kaggle-recruit-restaurant</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">12th 方案：https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49251#latest-282765</span></p>
   <p><br></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">Kaggle——CorporaciónFavoritaGrocery销售预测</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">网址：https://www.kaggle.com/c/favorita-grocery-sales-forecasting</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">解决方案：</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">1st 方案：&nbsp;https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47582#latest-360306</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">2st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47568#latest-278474</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">3st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47560#latest-302253</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">4st 方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47529#latest-271077</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">5st方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47556#latest-270515</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">6st方案：https://www.kaggle.com/c/favorita-grocery-sales-forecasting/discussion/47575#latest-269568</span></p>
   <h3 style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></h3>
   <h3 style="text-align:justify;line-height:1.75em;"><span style="font-size:18px;"><strong><span style="font-size:18px;font-family:'-webkit-standard';color:rgb(63,63,63);letter-spacing:1px;">正在进行中的回归竞赛</span></strong></span><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></h3>
   <p><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">小伙伴们看到上面的解决方案是不是跃跃欲试，最近国内也有各大回归比赛，赶紧趁热打铁，来学习学习回归比赛吧！</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><strong><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;">2019年腾讯广告大赛——广告曝光预估</span></strong></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';font-size:15px;color:rgb(63,63,63);letter-spacing:1px;"><br></span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);">网址：https://algo.qq.com/application/home/home/index.html</span></p>
   <p style="text-align:justify;line-height:1.75em;"><span style="font-family:'-webkit-standard';letter-spacing:1px;font-size:14px;color:rgb(136,136,136);"><br></span></p>
   <p style="margin-left:8px;"><span style="color:rgb(136,136,136);font-size:14px;font-style:italic;letter-spacing:1px;text-align:left;font-family:'-apple-system-font', BlinkMacSystemFont, 'Helvetica Neue', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei UI', 'Microsoft YaHei', Arial, sans-serif;">（本文为AI科技大本营转载文章，转载请联系原作者）</span></p>
   <p style="margin-left:8px;line-height:1.75em;"><br></p>
   <p style="margin-left:8px;line-height:1.75em;"><br></p>
   <p style="min-height:1em;">◆</p>
   <p style="min-height:1em;"><span style="color:rgb(255,76,0);letter-spacing:1px;"><strong><span><strong style="letter-spacing:.544px;">CTA核心技术及应用峰会</strong></span></strong></span></p>
   <p style="min-height:1em;">◆</p>
   <p><span style="color:rgb(63,63,63);font-size:15px;letter-spacing:1px;">5月25-27日，由中国IT社区CSDN与数字经济人才发展中心联合主办的第一届CTA核心技术及应用峰会将在杭州国际博览中心隆重召开，峰会将围绕人工智能领域，邀请技术领航者，与开发者共同探讨机器学习和知识图谱的前沿研究及应用。</span><br></p>
   <p><br></p>
   <p><span style="letter-spacing:1px;font-size:15px;color:rgb(63,63,63);">更多重磅嘉宾请识别海报二维码查看，</span><span style="letter-spacing:1px;font-size:15px;color:rgb(63,63,63);">目前会议早鸟票发售中（原票价1099元），</span><strong style="letter-spacing:.544px;"><span style="color:rgb(255,76,0);letter-spacing:1px;font-size:15px;">点击阅读原文即刻抢购</span></strong><span style="letter-spacing:1px;font-size:15px;color:rgb(63,63,63);">。添加小助手微信</span><span style="letter-spacing:1px;font-size:15px;color:rgb(255,76,0);"><strong>15101014297</strong></span><span style="letter-spacing:1px;font-size:15px;color:rgb(63,63,63);">，备注“</span><strong style="letter-spacing:.544px;"><span style="letter-spacing:1px;font-size:15px;color:rgb(255,76,0);">CTA</span></strong><span style="letter-spacing:1px;font-size:15px;color:rgb(63,63,63);">”，了解票务以及会务详情。</span></p>
   <p><br></p>
   <p style="margin-left:8px;text-align:center;"><img class="rich_pages" style="width:540px;" src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/BnSNEaficFAaEvfrVTwTOJmJOdd48GLj5vdEsysxDn5IbBMxPae10gfYod2Px8DcIZZEOTiaK2vib6T4fZko637ibw/640?wx_fmt=jpeg" alt="640?wx_fmt=jpeg"></p>
   <p style="margin-left:8px;min-height:1em;letter-spacing:.544px;line-height:27.2px;"><br></p>
   <p style="margin-left:8px;min-height:1em;letter-spacing:.544px;line-height:27.2px;text-align:left;"><strong style="text-align:justify;"><span style="font-size:15px;">推荐阅读</span></strong></p>
   <ul class="list-paddingleft-2">
    <li><h2 class="rich_media_title"><a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/89715015" rel="nofollow">他25岁进贝尔实验室，32岁提信息论，40岁办达特茅斯会议，晚年患上阿兹海默 | 人物志</a></h2></li>
    <li><p><a href="http://mp.weixin.qq.com/s?__biz=MzI0ODcxODk5OA==&amp;mid=2247504776&amp;idx=1&amp;sn=1fe4d56930902426bc3e2605ba2ca025&amp;chksm=e99ee071dee96967d817d2973521ab016a0d3d823d35b6e4aebc75f8bca4deeb61aebeda9d1b&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">微软沈向洋，百度李彦宏、王海峰，阿里王坚均候选中国工程院院士</a><br></p></li>
    <li><p><a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/89667051" rel="nofollow">小样，加张图你就不认识我了？“补丁”模型骗你没商量！| 技术头条</a></p></li>
    <li><p><a href="https://mp.weixin.qq.com/s?__biz=MzU5MjEwMTE2OQ==&amp;mid=2247485808&amp;idx=1&amp;sn=214ad17daba746cf6ac22da2f8787633&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">东大漆桂林、清华李涓子、复旦肖仰华等大牛确认出席CTA峰会！5月一起打卡杭州</a></p></li>
    <li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5MjAwODM4MA==&amp;mid=2650719196&amp;idx=1&amp;sn=08ddf7ad2a591aa70ab21940ad7ed46f&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">京东 60 天哗变！CTO 成优化第一人 | 畅言</a></p></li>
    <li><p><a href="https://blog.csdn.net/FL63Zv9Zou86950w/article/details/89667030" rel="nofollow">异构计算=未来？一文带你秒懂3大主流异构</a></p></li>
    <li><p><a href="https://blog.csdn.net/csdnsevenn/article/details/89666825" rel="nofollow">《互联网人叹气图鉴》</a></p></li>
    <li><p><a href="https://blog.csdn.net/Blockchain_lemon/article/details/89629591" rel="nofollow">回报率29%! 大神用情感分析创建一个比特币交易算法, 原来交易玩的是心理战</a></p></li>
    <li><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5MjcxNjc2Ng==&amp;mid=2650559816&amp;idx=1&amp;sn=380cfd3d18fb987c0073bf1b8289155a&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">她说：</a><a href="https://mp.weixin.qq.com/s?__biz=MzA5MjcxNjc2Ng==&amp;mid=2650559816&amp;idx=1&amp;sn=380cfd3d18fb987c0073bf1b8289155a&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">为啥程序员都特想要机械键盘？</a><a href="https://mp.weixin.qq.com/s?__biz=MzA5MjcxNjc2Ng==&amp;mid=2650559816&amp;idx=1&amp;sn=380cfd3d18fb987c0073bf1b8289155a&amp;scene=21#wechat_redirect" rel="nofollow" style="text-decoration:underline;font-size:14px;">这答案我服！</a></p></li>
   </ul>
   <p style="min-height:1em;text-align:right;"><br></p>
   <p style="min-height:1em;text-align:right;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAaBKZcUCzAia226XxjflVWcEefX4n29GVz3bwHiaeKYTz99IUdicIUic28RH1uMCQicuBRxgUhgsmqLP3A/640?wx_fmt=png" alt="640?wx_fmt=png"></p> 
  </div> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?aee162ba0c05d8e682055e28ff964fa2";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
