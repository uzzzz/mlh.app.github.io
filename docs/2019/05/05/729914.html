<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>大数据和人工智能概念全面解析 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="大数据和人工智能概念全面解析" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一、大数据和人工智能 &nbsp; 大数据是伴随着信息数据爆炸式增长和网络计算技术迅速发展而兴起的一个新型概念。根据麦肯锡全球研究所的定义，大数据是一种规模大到在获取、存储、管理、分析方面大大超出了传统数据库软件工具能力范围的数据集合，具有海量的数据规模、快速的数据流转、多样的数据类型和价值密度低四大特征。大数据能够帮助各行各业的企业从原本毫无价值的海量数据中挖掘出用户的需求，使数据能够从量变到质变，真正产生价值。随着大数据的发展，其应用已经渗透到农业、工业、商业、服务业、医疗领域等各个方面，成为影响产业发展的一个重要因素。 当前人们所说的人工智能，是指研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术以及应用系统的一门新的技术科学，是由人工制造出来的系统所表现出来的智能。 &nbsp; &nbsp;传统人工智能受制于计算能力，并没能完成大规模的并行计算和并行处理，人工智能系统的能力较差。2006年，Hinton教授提出“深度学习”神经网络使得人工智能性能获得突破性进展，进而促使人工智能产业又一次进入快速发展阶段。“深度学习”神经网络主要机理是通过深层神经网络算法来模拟人的大脑学习过程，通过输入与输出的非线性关系将低层特征组合成更高层的抽象表示，最终达到掌握运用的水平。数据量的丰富程度决定了是否有充足数据对神经网络进行训练，进而使人工智能系统经过深度学习训练后达到强人工智能水平。因此，能否有足够多的数据对人工神经网络进行深度训练，提升算法有效性是人工智能能否达到类人或超人水平的决定因素之一。 随着移动互联网的爆发，数据量呈现出指数级的增长，大数据的积累为人工智能提供了基础支撑。同时受益于计算机技术在数据采集、存储、计算等环节的突破，人工智能已从简单的算法+数据库发展演化到了机器学习+深度理解的状态。 如果想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，我也被圈粉了。教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 &nbsp;二、人工智能产业及生态 &nbsp; 按产业链结构划分，人工智能可以分为基础技术层、AI技术层和AI应用层。基础技术层主要聚焦于数据资源、计算能力和硬件平台，数据资源主要是各类大数据，硬件资源包括芯片研发、存储设备开发等。AI技术层着重于算法、模型及可应用技术，如计算智能算法、感知智能算法、认知智能算法。AI应用层则主要关注将人工智能与下游各领域结合起来，如无人机、机器人、虚拟客服、语音输入法等。 图1 人工智能产业链 &nbsp; &nbsp; 资料来源：中国产业信息网，《2017年中国人工智能行业发展概况及未来发展趋势分析》 &nbsp; （一）基础技术层&nbsp; 1.1 大数据 数据资源是机器学习训练的基本素材，通过对于数据的学习，机器能不断积累经验和优化决策参数，逐渐变得更贴近人类智能。 从数据流动方向的角度来看，大数据的产业链可分为底层平台、处理分析、应用三个层次。底层平台由基础设施与数据资产池构成，主要提供数据采集、分享和交易服务，处理分析则是在原始数据的基础上对数据进行清洗后以不同方式呈现。在数据处理分析的基础之上，挖掘各行业的数据需求，最终为用户提供服务。 根据数据应用程度不同，大数据产业链下各参与方功能可细分为数据标准与规范化、数据采集、数据安全、数据储存与管理、数据分析与挖掘、数据运维和数据运用七个方面。&nbsp; 1.2 计算能力和硬件平台 数据资源、核心算法、运算能力是人工智能的三大核心要素。随着全球移动互联网和物联网等快速发展，人类可获取利用的数据正以爆炸式增长。海量的大数据通过最新的深度学习技术将为人工智能的发展与应用带来难以估量的价值，而运算能力提升是人工智能发展的前提保障。其中，芯片是运算能力的核心。 就目前而言，AI 芯片主要类型有GPU、FPGA、ASIC和类人脑芯片四种。 1.2.1 GPU 1.2.1.1 GPU简介 GPU 即图形处理器，最初是用来做图像运算的微处理器。GPU 优化和调整了CPU 结构，使其运算速度突飞猛进，拥有了更强大的处理浮点运算的能力。2009 年，斯坦福大学的吴恩达及其团队发现GPU 芯片可以并行运行神经网络。用GPU来运行机器学习模型，同样的大训练集，GPU在耗费功率更低、占用基础设施更少的情况下能够支持远比单纯使用CPU时10-100倍的应用吞吐量。因此GPU已经成为数据科学家处理大数据的处理器。 1.2.1.2 GPU行业现状 目前国际GPU市场被NVIDIA 和AMD 两大公司瓜分，全球GPU 行业的市场份额有超过70％被NVIDIA占据，而应用在人工智能领域的可进行通用计算的GPU 市场则基本被NVIDIA垄断。目前公司已与谷歌、微软、IBM、丰田、百度等多家尝试利用深度神经网络来解决海量复杂计算问题的企业建立和合作关系。NVIDIA与下游客户在深度学习领域的合作不断加深，已经开发出多款针对深度学习的GPU产品。从产品成熟度、生态圈的规模角度而言，NVIDIA的GPU 已具备统治性的地位。 中国在GPU芯片设计领域起步较晚，目前只有景嘉微和兆芯两家掌握核心技术的公司正在逐步打破国外芯片在我国GPU市场的垄断局面，但产品还是主要用于GPU最初的图形显控领域，距人工智能所需要的GPU技术还有很远的距离。 1.2.2 FPGA 1.2.2.1 FPGA简介 FPGA，即场效可编程逻辑闸阵列，最初是从专用集成电路上发展起来的半定制化的可编程电路，FPGA 还具有静态可重复编程和动态在系统重构的特性，使得硬件的功能可以像软件一样通过编程来修改，不同的编程数据在同一片FPGA上可以产生不同的电路功能，具有很强的灵活性和适应性。 FPGA 和GPU 内都有大量的计算单元，因此它们的计算能力都很强。在进行神经网络运算的时候，两者的速度会比CPU 快很多。但是GPU 由于架构固定，硬件原生支持的指令也就固定了，而FPGA 则是可编程的。其可编程性是关键，因为它让软件与终端应用公司能够提供与其竞争对手不同的解决方案，并且能够灵活地针对自己所用的算法修改电路。与GPU相比，FPGA具有性能高、能耗低及可硬件编程的特点。 1.2.2.2 FPGA行业现状 目前FPGA 整个市场被国外的两大巨头所寡占，据东方证券研究所数据显示，Xilinx 和Altera 占了近90%的份额，合计专利达到6000多项，剩余份额被Lattice和Microsemi两家占据，两家专利合计共有超过3000项。技术专利的限制和漫长的开发周期使得FPGA行业有着极高的壁垒。 尽管我国政府多年来在此领域投入了数百亿的科研经费，但FPGA的专利限制及技术门槛使得中国FPGA的研发之路十分艰辛，国内如同创国芯、京微雅格、高云等公司在FPGA研发方面已获得一定进展，但产品性能、功耗、容量和应用领域上都同国外先进技术存在着较大差距。当前国内部分资本已经试图走出国门，通过并购半导体类公司的方法进入FPGA的行业，实现弯道超车。 1.2.3 ASIC 1.2.3.1 ASIC简介 ASIC，即专用集成电路，是指应特定用户要求或特定电子系统的需要而设计、制造的集成电路。ASIC 作为集成电路技术与特定用户的整机或系统技术紧密结合的产物，与通用集成电路相比，具有以下几个方面的优越性：体积更小、功耗更低、可靠性提高、性能提高、保密性增强。FPGA一般来说比ASIC的速度要慢，而且无法完成更复杂的设计，并且会消耗更多的电能，因此就算力而言ASIC远优于FPGA；但ASIC的专用特点使得其生产成本很高，如果出货量较小，则采用ASIC在经济上不太实惠。一旦人工智能技术成熟，ASIC专用集成的特点反而会达到规模效应，较通用集成电路而言，成本大大降低。 当前ASIC 在人工智能深度学习方面的应用还不多，但是我们可以拿比特币矿机芯片的发展做类似的推理。比特币挖矿和人工智能深度学习有类似之处，都是依赖于底层的芯片进行大规模的并行计算。比特币矿机的芯片经历了四个阶段：CPU、GPU、FPGA 和ASIC。其中ASIC 在比特币挖矿领域，展现出了得天独厚的优势。随着人工智能越来越多的应用在各个领域并表现出优越的性能，长期来看ASIC大有可为。 1.2.3.2 ASIC市场现状 随着人工智能的兴起，科技巨头纷纷布局芯片制造。高通、AMD、ARM、Intel和NVIDIA都在致力于将定制化芯片整合进它们的现有解决方案中。Nervana 和 Movidius（目前都在Intel旗下）据说正在开发集合方案。ASIC中较为成熟的产品是谷歌针对AlphaGo研发的TPU。第一代TPU产品由谷歌在2016年I/O大会上正式推出，今年5月的开发者I/O大会上，谷歌正式公布了第二代TPU，又称Cloud TPU，相较于初代TPU，既能用于训练神经网络，又可以用于推理，浮点性能方面较传统的GPU提升了15倍。 ASIC在人工智能领域的应用起步较晚，国内外水平相差不大。目前国内已有数家公司致力于人工智能相关ASIC芯片研究，代表公司为地平线机器人、中科寒武纪与中星微电子。其中地平线机器人公司作为初创企业，致力于打造基于深度神经网络的人工智能“大脑”平台-包括软件和芯片，可以做到低功耗、本地化的解决环境感知、人机交互、决策控制等问题。其关于芯片的研发目前还未成熟。中科寒武纪和中星微电子则已经有了相对成熟的产品。寒武纪芯片专门面向深度学习技术，研制了国际首个深度学习专用处理器芯片NPU，目前已研发的三款芯片分别面向神经网络的原型处理器结构、大规模神经网络和多种机器学习算法，预计将于2018年实现芯片的产业化。中星微电子于2016年6月推出中国首款嵌入式神经网络处理器（NPU）芯片，这是全球首颗具备深度学习人工智能的嵌入式视频采集压缩编码系统级芯片。这款基于深度学习的芯片运用在人脸识别上，最高能达到98%的准确率，超过人眼的识别率。该芯片于2017年3月6日实现量产，截止到今年5月出货量为十几万件。 1.2.4 类人脑芯片 1.2.4.1 类人脑芯片简介 类人脑芯片是一种基于神经形态工程、借鉴人脑信息处理方式，旨在打破“冯·诺依曼”架构束缚，适于实时处理非结构化信息、具有学习能力的超低功耗新型计算芯片。从理论上来看，类人脑芯片更加接近于人工智能目标的芯片，力图在基本架构上模仿人脑的工作原理，使用神经元和突触的方式替代传统架构体系，使芯片能够进行异步、并行、低俗和分布式处理信息数据的能力，同时具备自护感知、识别和学习的能力。 1.2.4.2 类人脑芯片市场现状 类人脑芯片是人工智能芯片发展的重点方向。目前各国政府及科技巨头都在大力推动类人脑芯片的研发进程，包括美国、日本、德国、英国、瑞士等发达国家已经制定相应的发展战略，中国的类人脑科学研究项目目前也已经正式启动。当前世界上已有一批科技公司走在前列，在类人脑芯片研发中取得了突破，代表产品包括IBM的TrueNorth芯片、高通Zeroth芯片、谷歌的“神经网络图灵机”等。 （二）AI技术层 AI技术层主要着眼于算法、模型及可应用技术。按照智能程度不同，人工智能可分为运算智能、感知智能、认知智能三个阶段。运算智能，即快速计算和记忆存储能力，在这一阶段主要是算法与数据库相结合，使得机器开始像人类一样会计算和传递信息； 感知智能，即视觉、听觉、触觉等感知能力，在这一阶段，数据库与浅层学习算法结合，使得机器开始看懂和听懂，并做出判断、采取行动；认知智能，即能理解会思考的能力，这一阶段主要是采用深度学习算法，使得机器能够像人一样思考，主动采取行动。 AI技术层可以分为框架层和算法层，其中框架层指TensorFlow，Caffe，Theano，Torch，DMTK，DTPAR，ROS等框架或操作系统，算法层指的是对数据的处理方法。 根据数据类型的不同，对一个问题会采用不同的建模方式，即学习方式。按照学习方式来分类，人工智能算法可以分为传统机器学习和神经网络算法，其中传统机器学习又可细分为监督式学习、非监督式学习、半监督式学习、强化学习。 2.1 传统机器学习 2.1.1 监督式学习 在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常用算法有回归算法、朴素贝叶斯、SVM等。 2.1.2 非监督式学习 在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。关联规则学习的常见算法主要为Apriori算法及其拓展算法，聚类的常用算法有k-Means算法及其相似算法。 2.1.3 半监督式学习 在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。 2.1.4 强化学习 在此学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。 2.2 神经网络 人工神经网络是模拟生物神经网络，由众多的神经元可调的连接权值连接而成，具有大规模并行处理、分布式信息存储、良好的组织学习能力特点，并通过一定学习准则进行学习，进而建立相关模型，解决一定工作。在人工神经网络的学习算法设计方面，一般对人工神经网络进行大量的数据训练和调整，不断修正各层级节点参数，通过不断学习使得人工神经网络具有初步的自适应能力和自我组织能力及较强的泛化能力，进而较快适应周边环境要求，基于其众多优点，人工神经网络已然成为人工智能算法的核心。深度学习算法是人工神经网络当前最新算法，其实质是通过很多隐层的机器学习模型和海量的训练数据来学习更有用的特征，从而提升分类或预测的准确性。 &nbsp; （三）AI应用层&nbsp; 人工智能的应用主要是采用了“AI+垂直行业”的方式渗透到传统各行业，按发展层次的不同可以分为专用人工智能、通用人工智能和超级人工智能三个层次。其中，专用人工智能以一个或多个专门的领域和功能为主；通用人工智能即机器与人类一样拥有进行所有工作的可能，关键在于自动地认知和拓展；超级人工智能是指具有自我意识，包括独立自主的价值观、世界观等，目前仅存在于文化作品构想中。 按应用技术类型进行划分，人工智能的应用技术可以分为计算机视觉、机器学习、自然语言处理和机器人四块。 3.1 计算机视觉 计算机视觉，是指计算机从图像中识别出物体、场景和活动的能力。计算机视觉技术运用由图像处理操作及其他技术所组成的序列来将图像分析任务分解为便于管理的小块任务目前计算机视觉主要应用在人脸识别、图像识别方面（包括静态、动态两类信息）。 人脸识别，亦叫人像识别、面部识别，是基于人的脸部特征信息进行身份识别的一种生物识别技术。用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行处理的一系列相关技术。 图像识别，是计算机对图像进行处理、分析和理解，以识别各种不同模式的目标和对象的技术。识别过程包括图像预处理、图像分割、特征提取和判断匹配。由于动态监测与识别的技术限制，静态图像识别与人脸识别的研究暂时处于领先位置。 当前国外科技巨头自行研发和收购双管齐下布局计算机视觉领域，将技术广泛用于自身产品升级，并基于自身基因打造技术服务平台和新品类持续提升影响力。中国国内BAT都已纷纷布局相关领域，并基于自身产品进行功能研发。百度相对更加激进，成立了独立风投公司，专注于AI早期投资。 除BAT三巨头外，国内也有不少初创公司涉足计算机视觉技术，主要聚焦于技术应用。其中典型代表当属旷视科技。公司成立于2012年11月，公司专注于人脸识别技术和相关产品应用研究，面向开发者提供服务，能提供一整套人脸检测、人脸识别、人脸分析以及人脸3D技术的视觉技术服务，主要通过提供云端API、离线SDK、以及面向用户的自主研发产品形式，将人脸识别技术广泛应用到互联网及移动应用场景中。Face++通过和众多互联网公司合作，并通过“脱敏”技术掌握到了500万张人脸图片数据库，在互联网图片人脸识别LFW的准确率达到99.6%，合作伙伴包括阿里、360等一批大型的图片、社交、设备类企业。 当前国内计算机视觉创业热度不断提高，iiMedia Research(艾媒咨询)数据显示， 中国人工智能创业公司所属领域分布中，计算机视觉领域拥有最多创业公司，高达35家。&nbsp; 3.2 机器学习 机器学习是指计算机通过对大量已有数据的处理分析和学习，从而拥有预测判断和做出最佳决策的能力。其核心在于，机器学习是从数据中自动发现模式，模式一旦被发现便可用于做预测。 机器学习的应用范围非常广泛，针对那些产生庞大数据的活动，它几乎拥有改进一切性能的潜力。除了欺诈甄别之外，这些活动还包括销售预测、库存管理、石油和天然气勘探、以及公共卫生。机器学习技术在其他的认知技术领域也扮演着重要角色，比如计算机视觉，它能在海量图像中通过不断训练和改进视觉模型来提高其识别对象的能力。 现如今，机器学习已经成为认知技术中最炙手可热的研究领域之一，在2011-2014年中这段时间内就已吸引了近十亿美元的风险投资。谷歌也在2014年斥资4亿美金收购Deepmind这家研究机器学习技术的公司。目前国内机器学习相关企业数量相对较少。BAT在机器学习方面有着先天的优势，国内初创公司第四范式是基于机器学习的解决方案提供商。&nbsp; 3.3 自然语言处理 自然语言处理就是用人工智能来处理、理解以及运用人类语言，通过建立语言模型来预测语言表达的概率分布，从而实现目标。 &nbsp;&nbsp;自然语言处理技术在生活中应用广泛，例如机器翻译、手写体和印刷体字符识别、语音识别后实现文字转换、信息检索、抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等。它们分别应用了自然语言处理当中的语法分析、语义分析、篇章理解等技术，是人工智能界最前沿的研究领域。时至今日AI在这些技术领域的发展已经把识别准确率从70%提高到了90%以上，但只有当准确率提高到99%及以上时，才能被认定为自然语言处理的技术达到人类水平。 在资本与产业助力之下，我国人工智能的语音识别技术已处于国际领先水平，技术成熟，通用识别率上，各企业均维持在了95%左右的水平。类似百度、科大讯飞等上市公司凭借深厚的技术和数据积累在市场上占据前列，且通过软硬件服务的开发不断进化着自身的服务能力。在科大讯飞之后发布国内第二家“语音识别公有云”的云知声在各项通用语音服务技术的提供上也占据着不小的市场空间。除此之外，依托中科院自动化所的紫冬锐意和纳象立方以及有着海外背景的苏州思必驰在教育领域的语音识别上占据着领先的位置。 3.4 机器人 将机器视觉、自动规划等认知技术整合至极小却高性能的传感器、致动器、以及设计巧妙的硬件中，这就催生了新一代的机器人，它有能力与人类一起工作，能在各种未知环境中灵活处理不同的任务。 目前世界上至少有48个国家在发展机器人，其中25个国家已涉足服务型机器人开发。在日本、北美和欧洲，迄今已有7种类型计40余款服务型机器人进入实验和半商业化应用在服务机器人领域。美国是机器人的发源地，美国的机器人技术在国际上仍一直处于领先地位，其技术全面、先进，适应性十分强，在军用、医疗、家用服务机器人产业都占有绝对的优势，占服务机器人市场约60%的份额。国内智能机器人行业的研发主要集中于家庭机器人、工业/企业服务和智能助手三个方面。其中工业及企业服务类的机器人研发企业依托政策背景和市场需求处于相对领先的发展阶段。然而在中国涉足智能机器人的企业中，从事家庭机器人和智能助手研发的企业占据了绝大多数比例。 因为服务一般都要结合特定市场进行开发，本土企业更容易结合特定的环境和文化进行开发占据良好的市场定位，从而保持一定的竞争优势；另一方面，外国的服务机器人公司也属于新兴产业，大部分成立的时候还比较短，因而我国的服务机器人产业面临着比较大的机遇和可发展空间。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一、大数据和人工智能 &nbsp; 大数据是伴随着信息数据爆炸式增长和网络计算技术迅速发展而兴起的一个新型概念。根据麦肯锡全球研究所的定义，大数据是一种规模大到在获取、存储、管理、分析方面大大超出了传统数据库软件工具能力范围的数据集合，具有海量的数据规模、快速的数据流转、多样的数据类型和价值密度低四大特征。大数据能够帮助各行各业的企业从原本毫无价值的海量数据中挖掘出用户的需求，使数据能够从量变到质变，真正产生价值。随着大数据的发展，其应用已经渗透到农业、工业、商业、服务业、医疗领域等各个方面，成为影响产业发展的一个重要因素。 当前人们所说的人工智能，是指研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术以及应用系统的一门新的技术科学，是由人工制造出来的系统所表现出来的智能。 &nbsp; &nbsp;传统人工智能受制于计算能力，并没能完成大规模的并行计算和并行处理，人工智能系统的能力较差。2006年，Hinton教授提出“深度学习”神经网络使得人工智能性能获得突破性进展，进而促使人工智能产业又一次进入快速发展阶段。“深度学习”神经网络主要机理是通过深层神经网络算法来模拟人的大脑学习过程，通过输入与输出的非线性关系将低层特征组合成更高层的抽象表示，最终达到掌握运用的水平。数据量的丰富程度决定了是否有充足数据对神经网络进行训练，进而使人工智能系统经过深度学习训练后达到强人工智能水平。因此，能否有足够多的数据对人工神经网络进行深度训练，提升算法有效性是人工智能能否达到类人或超人水平的决定因素之一。 随着移动互联网的爆发，数据量呈现出指数级的增长，大数据的积累为人工智能提供了基础支撑。同时受益于计算机技术在数据采集、存储、计算等环节的突破，人工智能已从简单的算法+数据库发展演化到了机器学习+深度理解的状态。 如果想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，我也被圈粉了。教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 &nbsp;二、人工智能产业及生态 &nbsp; 按产业链结构划分，人工智能可以分为基础技术层、AI技术层和AI应用层。基础技术层主要聚焦于数据资源、计算能力和硬件平台，数据资源主要是各类大数据，硬件资源包括芯片研发、存储设备开发等。AI技术层着重于算法、模型及可应用技术，如计算智能算法、感知智能算法、认知智能算法。AI应用层则主要关注将人工智能与下游各领域结合起来，如无人机、机器人、虚拟客服、语音输入法等。 图1 人工智能产业链 &nbsp; &nbsp; 资料来源：中国产业信息网，《2017年中国人工智能行业发展概况及未来发展趋势分析》 &nbsp; （一）基础技术层&nbsp; 1.1 大数据 数据资源是机器学习训练的基本素材，通过对于数据的学习，机器能不断积累经验和优化决策参数，逐渐变得更贴近人类智能。 从数据流动方向的角度来看，大数据的产业链可分为底层平台、处理分析、应用三个层次。底层平台由基础设施与数据资产池构成，主要提供数据采集、分享和交易服务，处理分析则是在原始数据的基础上对数据进行清洗后以不同方式呈现。在数据处理分析的基础之上，挖掘各行业的数据需求，最终为用户提供服务。 根据数据应用程度不同，大数据产业链下各参与方功能可细分为数据标准与规范化、数据采集、数据安全、数据储存与管理、数据分析与挖掘、数据运维和数据运用七个方面。&nbsp; 1.2 计算能力和硬件平台 数据资源、核心算法、运算能力是人工智能的三大核心要素。随着全球移动互联网和物联网等快速发展，人类可获取利用的数据正以爆炸式增长。海量的大数据通过最新的深度学习技术将为人工智能的发展与应用带来难以估量的价值，而运算能力提升是人工智能发展的前提保障。其中，芯片是运算能力的核心。 就目前而言，AI 芯片主要类型有GPU、FPGA、ASIC和类人脑芯片四种。 1.2.1 GPU 1.2.1.1 GPU简介 GPU 即图形处理器，最初是用来做图像运算的微处理器。GPU 优化和调整了CPU 结构，使其运算速度突飞猛进，拥有了更强大的处理浮点运算的能力。2009 年，斯坦福大学的吴恩达及其团队发现GPU 芯片可以并行运行神经网络。用GPU来运行机器学习模型，同样的大训练集，GPU在耗费功率更低、占用基础设施更少的情况下能够支持远比单纯使用CPU时10-100倍的应用吞吐量。因此GPU已经成为数据科学家处理大数据的处理器。 1.2.1.2 GPU行业现状 目前国际GPU市场被NVIDIA 和AMD 两大公司瓜分，全球GPU 行业的市场份额有超过70％被NVIDIA占据，而应用在人工智能领域的可进行通用计算的GPU 市场则基本被NVIDIA垄断。目前公司已与谷歌、微软、IBM、丰田、百度等多家尝试利用深度神经网络来解决海量复杂计算问题的企业建立和合作关系。NVIDIA与下游客户在深度学习领域的合作不断加深，已经开发出多款针对深度学习的GPU产品。从产品成熟度、生态圈的规模角度而言，NVIDIA的GPU 已具备统治性的地位。 中国在GPU芯片设计领域起步较晚，目前只有景嘉微和兆芯两家掌握核心技术的公司正在逐步打破国外芯片在我国GPU市场的垄断局面，但产品还是主要用于GPU最初的图形显控领域，距人工智能所需要的GPU技术还有很远的距离。 1.2.2 FPGA 1.2.2.1 FPGA简介 FPGA，即场效可编程逻辑闸阵列，最初是从专用集成电路上发展起来的半定制化的可编程电路，FPGA 还具有静态可重复编程和动态在系统重构的特性，使得硬件的功能可以像软件一样通过编程来修改，不同的编程数据在同一片FPGA上可以产生不同的电路功能，具有很强的灵活性和适应性。 FPGA 和GPU 内都有大量的计算单元，因此它们的计算能力都很强。在进行神经网络运算的时候，两者的速度会比CPU 快很多。但是GPU 由于架构固定，硬件原生支持的指令也就固定了，而FPGA 则是可编程的。其可编程性是关键，因为它让软件与终端应用公司能够提供与其竞争对手不同的解决方案，并且能够灵活地针对自己所用的算法修改电路。与GPU相比，FPGA具有性能高、能耗低及可硬件编程的特点。 1.2.2.2 FPGA行业现状 目前FPGA 整个市场被国外的两大巨头所寡占，据东方证券研究所数据显示，Xilinx 和Altera 占了近90%的份额，合计专利达到6000多项，剩余份额被Lattice和Microsemi两家占据，两家专利合计共有超过3000项。技术专利的限制和漫长的开发周期使得FPGA行业有着极高的壁垒。 尽管我国政府多年来在此领域投入了数百亿的科研经费，但FPGA的专利限制及技术门槛使得中国FPGA的研发之路十分艰辛，国内如同创国芯、京微雅格、高云等公司在FPGA研发方面已获得一定进展，但产品性能、功耗、容量和应用领域上都同国外先进技术存在着较大差距。当前国内部分资本已经试图走出国门，通过并购半导体类公司的方法进入FPGA的行业，实现弯道超车。 1.2.3 ASIC 1.2.3.1 ASIC简介 ASIC，即专用集成电路，是指应特定用户要求或特定电子系统的需要而设计、制造的集成电路。ASIC 作为集成电路技术与特定用户的整机或系统技术紧密结合的产物，与通用集成电路相比，具有以下几个方面的优越性：体积更小、功耗更低、可靠性提高、性能提高、保密性增强。FPGA一般来说比ASIC的速度要慢，而且无法完成更复杂的设计，并且会消耗更多的电能，因此就算力而言ASIC远优于FPGA；但ASIC的专用特点使得其生产成本很高，如果出货量较小，则采用ASIC在经济上不太实惠。一旦人工智能技术成熟，ASIC专用集成的特点反而会达到规模效应，较通用集成电路而言，成本大大降低。 当前ASIC 在人工智能深度学习方面的应用还不多，但是我们可以拿比特币矿机芯片的发展做类似的推理。比特币挖矿和人工智能深度学习有类似之处，都是依赖于底层的芯片进行大规模的并行计算。比特币矿机的芯片经历了四个阶段：CPU、GPU、FPGA 和ASIC。其中ASIC 在比特币挖矿领域，展现出了得天独厚的优势。随着人工智能越来越多的应用在各个领域并表现出优越的性能，长期来看ASIC大有可为。 1.2.3.2 ASIC市场现状 随着人工智能的兴起，科技巨头纷纷布局芯片制造。高通、AMD、ARM、Intel和NVIDIA都在致力于将定制化芯片整合进它们的现有解决方案中。Nervana 和 Movidius（目前都在Intel旗下）据说正在开发集合方案。ASIC中较为成熟的产品是谷歌针对AlphaGo研发的TPU。第一代TPU产品由谷歌在2016年I/O大会上正式推出，今年5月的开发者I/O大会上，谷歌正式公布了第二代TPU，又称Cloud TPU，相较于初代TPU，既能用于训练神经网络，又可以用于推理，浮点性能方面较传统的GPU提升了15倍。 ASIC在人工智能领域的应用起步较晚，国内外水平相差不大。目前国内已有数家公司致力于人工智能相关ASIC芯片研究，代表公司为地平线机器人、中科寒武纪与中星微电子。其中地平线机器人公司作为初创企业，致力于打造基于深度神经网络的人工智能“大脑”平台-包括软件和芯片，可以做到低功耗、本地化的解决环境感知、人机交互、决策控制等问题。其关于芯片的研发目前还未成熟。中科寒武纪和中星微电子则已经有了相对成熟的产品。寒武纪芯片专门面向深度学习技术，研制了国际首个深度学习专用处理器芯片NPU，目前已研发的三款芯片分别面向神经网络的原型处理器结构、大规模神经网络和多种机器学习算法，预计将于2018年实现芯片的产业化。中星微电子于2016年6月推出中国首款嵌入式神经网络处理器（NPU）芯片，这是全球首颗具备深度学习人工智能的嵌入式视频采集压缩编码系统级芯片。这款基于深度学习的芯片运用在人脸识别上，最高能达到98%的准确率，超过人眼的识别率。该芯片于2017年3月6日实现量产，截止到今年5月出货量为十几万件。 1.2.4 类人脑芯片 1.2.4.1 类人脑芯片简介 类人脑芯片是一种基于神经形态工程、借鉴人脑信息处理方式，旨在打破“冯·诺依曼”架构束缚，适于实时处理非结构化信息、具有学习能力的超低功耗新型计算芯片。从理论上来看，类人脑芯片更加接近于人工智能目标的芯片，力图在基本架构上模仿人脑的工作原理，使用神经元和突触的方式替代传统架构体系，使芯片能够进行异步、并行、低俗和分布式处理信息数据的能力，同时具备自护感知、识别和学习的能力。 1.2.4.2 类人脑芯片市场现状 类人脑芯片是人工智能芯片发展的重点方向。目前各国政府及科技巨头都在大力推动类人脑芯片的研发进程，包括美国、日本、德国、英国、瑞士等发达国家已经制定相应的发展战略，中国的类人脑科学研究项目目前也已经正式启动。当前世界上已有一批科技公司走在前列，在类人脑芯片研发中取得了突破，代表产品包括IBM的TrueNorth芯片、高通Zeroth芯片、谷歌的“神经网络图灵机”等。 （二）AI技术层 AI技术层主要着眼于算法、模型及可应用技术。按照智能程度不同，人工智能可分为运算智能、感知智能、认知智能三个阶段。运算智能，即快速计算和记忆存储能力，在这一阶段主要是算法与数据库相结合，使得机器开始像人类一样会计算和传递信息； 感知智能，即视觉、听觉、触觉等感知能力，在这一阶段，数据库与浅层学习算法结合，使得机器开始看懂和听懂，并做出判断、采取行动；认知智能，即能理解会思考的能力，这一阶段主要是采用深度学习算法，使得机器能够像人一样思考，主动采取行动。 AI技术层可以分为框架层和算法层，其中框架层指TensorFlow，Caffe，Theano，Torch，DMTK，DTPAR，ROS等框架或操作系统，算法层指的是对数据的处理方法。 根据数据类型的不同，对一个问题会采用不同的建模方式，即学习方式。按照学习方式来分类，人工智能算法可以分为传统机器学习和神经网络算法，其中传统机器学习又可细分为监督式学习、非监督式学习、半监督式学习、强化学习。 2.1 传统机器学习 2.1.1 监督式学习 在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常用算法有回归算法、朴素贝叶斯、SVM等。 2.1.2 非监督式学习 在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。关联规则学习的常见算法主要为Apriori算法及其拓展算法，聚类的常用算法有k-Means算法及其相似算法。 2.1.3 半监督式学习 在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。 2.1.4 强化学习 在此学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。 2.2 神经网络 人工神经网络是模拟生物神经网络，由众多的神经元可调的连接权值连接而成，具有大规模并行处理、分布式信息存储、良好的组织学习能力特点，并通过一定学习准则进行学习，进而建立相关模型，解决一定工作。在人工神经网络的学习算法设计方面，一般对人工神经网络进行大量的数据训练和调整，不断修正各层级节点参数，通过不断学习使得人工神经网络具有初步的自适应能力和自我组织能力及较强的泛化能力，进而较快适应周边环境要求，基于其众多优点，人工神经网络已然成为人工智能算法的核心。深度学习算法是人工神经网络当前最新算法，其实质是通过很多隐层的机器学习模型和海量的训练数据来学习更有用的特征，从而提升分类或预测的准确性。 &nbsp; （三）AI应用层&nbsp; 人工智能的应用主要是采用了“AI+垂直行业”的方式渗透到传统各行业，按发展层次的不同可以分为专用人工智能、通用人工智能和超级人工智能三个层次。其中，专用人工智能以一个或多个专门的领域和功能为主；通用人工智能即机器与人类一样拥有进行所有工作的可能，关键在于自动地认知和拓展；超级人工智能是指具有自我意识，包括独立自主的价值观、世界观等，目前仅存在于文化作品构想中。 按应用技术类型进行划分，人工智能的应用技术可以分为计算机视觉、机器学习、自然语言处理和机器人四块。 3.1 计算机视觉 计算机视觉，是指计算机从图像中识别出物体、场景和活动的能力。计算机视觉技术运用由图像处理操作及其他技术所组成的序列来将图像分析任务分解为便于管理的小块任务目前计算机视觉主要应用在人脸识别、图像识别方面（包括静态、动态两类信息）。 人脸识别，亦叫人像识别、面部识别，是基于人的脸部特征信息进行身份识别的一种生物识别技术。用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行处理的一系列相关技术。 图像识别，是计算机对图像进行处理、分析和理解，以识别各种不同模式的目标和对象的技术。识别过程包括图像预处理、图像分割、特征提取和判断匹配。由于动态监测与识别的技术限制，静态图像识别与人脸识别的研究暂时处于领先位置。 当前国外科技巨头自行研发和收购双管齐下布局计算机视觉领域，将技术广泛用于自身产品升级，并基于自身基因打造技术服务平台和新品类持续提升影响力。中国国内BAT都已纷纷布局相关领域，并基于自身产品进行功能研发。百度相对更加激进，成立了独立风投公司，专注于AI早期投资。 除BAT三巨头外，国内也有不少初创公司涉足计算机视觉技术，主要聚焦于技术应用。其中典型代表当属旷视科技。公司成立于2012年11月，公司专注于人脸识别技术和相关产品应用研究，面向开发者提供服务，能提供一整套人脸检测、人脸识别、人脸分析以及人脸3D技术的视觉技术服务，主要通过提供云端API、离线SDK、以及面向用户的自主研发产品形式，将人脸识别技术广泛应用到互联网及移动应用场景中。Face++通过和众多互联网公司合作，并通过“脱敏”技术掌握到了500万张人脸图片数据库，在互联网图片人脸识别LFW的准确率达到99.6%，合作伙伴包括阿里、360等一批大型的图片、社交、设备类企业。 当前国内计算机视觉创业热度不断提高，iiMedia Research(艾媒咨询)数据显示， 中国人工智能创业公司所属领域分布中，计算机视觉领域拥有最多创业公司，高达35家。&nbsp; 3.2 机器学习 机器学习是指计算机通过对大量已有数据的处理分析和学习，从而拥有预测判断和做出最佳决策的能力。其核心在于，机器学习是从数据中自动发现模式，模式一旦被发现便可用于做预测。 机器学习的应用范围非常广泛，针对那些产生庞大数据的活动，它几乎拥有改进一切性能的潜力。除了欺诈甄别之外，这些活动还包括销售预测、库存管理、石油和天然气勘探、以及公共卫生。机器学习技术在其他的认知技术领域也扮演着重要角色，比如计算机视觉，它能在海量图像中通过不断训练和改进视觉模型来提高其识别对象的能力。 现如今，机器学习已经成为认知技术中最炙手可热的研究领域之一，在2011-2014年中这段时间内就已吸引了近十亿美元的风险投资。谷歌也在2014年斥资4亿美金收购Deepmind这家研究机器学习技术的公司。目前国内机器学习相关企业数量相对较少。BAT在机器学习方面有着先天的优势，国内初创公司第四范式是基于机器学习的解决方案提供商。&nbsp; 3.3 自然语言处理 自然语言处理就是用人工智能来处理、理解以及运用人类语言，通过建立语言模型来预测语言表达的概率分布，从而实现目标。 &nbsp;&nbsp;自然语言处理技术在生活中应用广泛，例如机器翻译、手写体和印刷体字符识别、语音识别后实现文字转换、信息检索、抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等。它们分别应用了自然语言处理当中的语法分析、语义分析、篇章理解等技术，是人工智能界最前沿的研究领域。时至今日AI在这些技术领域的发展已经把识别准确率从70%提高到了90%以上，但只有当准确率提高到99%及以上时，才能被认定为自然语言处理的技术达到人类水平。 在资本与产业助力之下，我国人工智能的语音识别技术已处于国际领先水平，技术成熟，通用识别率上，各企业均维持在了95%左右的水平。类似百度、科大讯飞等上市公司凭借深厚的技术和数据积累在市场上占据前列，且通过软硬件服务的开发不断进化着自身的服务能力。在科大讯飞之后发布国内第二家“语音识别公有云”的云知声在各项通用语音服务技术的提供上也占据着不小的市场空间。除此之外，依托中科院自动化所的紫冬锐意和纳象立方以及有着海外背景的苏州思必驰在教育领域的语音识别上占据着领先的位置。 3.4 机器人 将机器视觉、自动规划等认知技术整合至极小却高性能的传感器、致动器、以及设计巧妙的硬件中，这就催生了新一代的机器人，它有能力与人类一起工作，能在各种未知环境中灵活处理不同的任务。 目前世界上至少有48个国家在发展机器人，其中25个国家已涉足服务型机器人开发。在日本、北美和欧洲，迄今已有7种类型计40余款服务型机器人进入实验和半商业化应用在服务机器人领域。美国是机器人的发源地，美国的机器人技术在国际上仍一直处于领先地位，其技术全面、先进，适应性十分强，在军用、医疗、家用服务机器人产业都占有绝对的优势，占服务机器人市场约60%的份额。国内智能机器人行业的研发主要集中于家庭机器人、工业/企业服务和智能助手三个方面。其中工业及企业服务类的机器人研发企业依托政策背景和市场需求处于相对领先的发展阶段。然而在中国涉足智能机器人的企业中，从事家庭机器人和智能助手研发的企业占据了绝大多数比例。 因为服务一般都要结合特定市场进行开发，本土企业更容易结合特定的环境和文化进行开发占据良好的市场定位，从而保持一定的竞争优势；另一方面，外国的服务机器人公司也属于新兴产业，大部分成立的时候还比较短，因而我国的服务机器人产业面临着比较大的机遇和可发展空间。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/05/729914.html" />
<meta property="og:url" content="https://mlh.app/2019/05/05/729914.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-05T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一、大数据和人工智能 &nbsp; 大数据是伴随着信息数据爆炸式增长和网络计算技术迅速发展而兴起的一个新型概念。根据麦肯锡全球研究所的定义，大数据是一种规模大到在获取、存储、管理、分析方面大大超出了传统数据库软件工具能力范围的数据集合，具有海量的数据规模、快速的数据流转、多样的数据类型和价值密度低四大特征。大数据能够帮助各行各业的企业从原本毫无价值的海量数据中挖掘出用户的需求，使数据能够从量变到质变，真正产生价值。随着大数据的发展，其应用已经渗透到农业、工业、商业、服务业、医疗领域等各个方面，成为影响产业发展的一个重要因素。 当前人们所说的人工智能，是指研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术以及应用系统的一门新的技术科学，是由人工制造出来的系统所表现出来的智能。 &nbsp; &nbsp;传统人工智能受制于计算能力，并没能完成大规模的并行计算和并行处理，人工智能系统的能力较差。2006年，Hinton教授提出“深度学习”神经网络使得人工智能性能获得突破性进展，进而促使人工智能产业又一次进入快速发展阶段。“深度学习”神经网络主要机理是通过深层神经网络算法来模拟人的大脑学习过程，通过输入与输出的非线性关系将低层特征组合成更高层的抽象表示，最终达到掌握运用的水平。数据量的丰富程度决定了是否有充足数据对神经网络进行训练，进而使人工智能系统经过深度学习训练后达到强人工智能水平。因此，能否有足够多的数据对人工神经网络进行深度训练，提升算法有效性是人工智能能否达到类人或超人水平的决定因素之一。 随着移动互联网的爆发，数据量呈现出指数级的增长，大数据的积累为人工智能提供了基础支撑。同时受益于计算机技术在数据采集、存储、计算等环节的突破，人工智能已从简单的算法+数据库发展演化到了机器学习+深度理解的状态。 如果想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，我也被圈粉了。教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 &nbsp;二、人工智能产业及生态 &nbsp; 按产业链结构划分，人工智能可以分为基础技术层、AI技术层和AI应用层。基础技术层主要聚焦于数据资源、计算能力和硬件平台，数据资源主要是各类大数据，硬件资源包括芯片研发、存储设备开发等。AI技术层着重于算法、模型及可应用技术，如计算智能算法、感知智能算法、认知智能算法。AI应用层则主要关注将人工智能与下游各领域结合起来，如无人机、机器人、虚拟客服、语音输入法等。 图1 人工智能产业链 &nbsp; &nbsp; 资料来源：中国产业信息网，《2017年中国人工智能行业发展概况及未来发展趋势分析》 &nbsp; （一）基础技术层&nbsp; 1.1 大数据 数据资源是机器学习训练的基本素材，通过对于数据的学习，机器能不断积累经验和优化决策参数，逐渐变得更贴近人类智能。 从数据流动方向的角度来看，大数据的产业链可分为底层平台、处理分析、应用三个层次。底层平台由基础设施与数据资产池构成，主要提供数据采集、分享和交易服务，处理分析则是在原始数据的基础上对数据进行清洗后以不同方式呈现。在数据处理分析的基础之上，挖掘各行业的数据需求，最终为用户提供服务。 根据数据应用程度不同，大数据产业链下各参与方功能可细分为数据标准与规范化、数据采集、数据安全、数据储存与管理、数据分析与挖掘、数据运维和数据运用七个方面。&nbsp; 1.2 计算能力和硬件平台 数据资源、核心算法、运算能力是人工智能的三大核心要素。随着全球移动互联网和物联网等快速发展，人类可获取利用的数据正以爆炸式增长。海量的大数据通过最新的深度学习技术将为人工智能的发展与应用带来难以估量的价值，而运算能力提升是人工智能发展的前提保障。其中，芯片是运算能力的核心。 就目前而言，AI 芯片主要类型有GPU、FPGA、ASIC和类人脑芯片四种。 1.2.1 GPU 1.2.1.1 GPU简介 GPU 即图形处理器，最初是用来做图像运算的微处理器。GPU 优化和调整了CPU 结构，使其运算速度突飞猛进，拥有了更强大的处理浮点运算的能力。2009 年，斯坦福大学的吴恩达及其团队发现GPU 芯片可以并行运行神经网络。用GPU来运行机器学习模型，同样的大训练集，GPU在耗费功率更低、占用基础设施更少的情况下能够支持远比单纯使用CPU时10-100倍的应用吞吐量。因此GPU已经成为数据科学家处理大数据的处理器。 1.2.1.2 GPU行业现状 目前国际GPU市场被NVIDIA 和AMD 两大公司瓜分，全球GPU 行业的市场份额有超过70％被NVIDIA占据，而应用在人工智能领域的可进行通用计算的GPU 市场则基本被NVIDIA垄断。目前公司已与谷歌、微软、IBM、丰田、百度等多家尝试利用深度神经网络来解决海量复杂计算问题的企业建立和合作关系。NVIDIA与下游客户在深度学习领域的合作不断加深，已经开发出多款针对深度学习的GPU产品。从产品成熟度、生态圈的规模角度而言，NVIDIA的GPU 已具备统治性的地位。 中国在GPU芯片设计领域起步较晚，目前只有景嘉微和兆芯两家掌握核心技术的公司正在逐步打破国外芯片在我国GPU市场的垄断局面，但产品还是主要用于GPU最初的图形显控领域，距人工智能所需要的GPU技术还有很远的距离。 1.2.2 FPGA 1.2.2.1 FPGA简介 FPGA，即场效可编程逻辑闸阵列，最初是从专用集成电路上发展起来的半定制化的可编程电路，FPGA 还具有静态可重复编程和动态在系统重构的特性，使得硬件的功能可以像软件一样通过编程来修改，不同的编程数据在同一片FPGA上可以产生不同的电路功能，具有很强的灵活性和适应性。 FPGA 和GPU 内都有大量的计算单元，因此它们的计算能力都很强。在进行神经网络运算的时候，两者的速度会比CPU 快很多。但是GPU 由于架构固定，硬件原生支持的指令也就固定了，而FPGA 则是可编程的。其可编程性是关键，因为它让软件与终端应用公司能够提供与其竞争对手不同的解决方案，并且能够灵活地针对自己所用的算法修改电路。与GPU相比，FPGA具有性能高、能耗低及可硬件编程的特点。 1.2.2.2 FPGA行业现状 目前FPGA 整个市场被国外的两大巨头所寡占，据东方证券研究所数据显示，Xilinx 和Altera 占了近90%的份额，合计专利达到6000多项，剩余份额被Lattice和Microsemi两家占据，两家专利合计共有超过3000项。技术专利的限制和漫长的开发周期使得FPGA行业有着极高的壁垒。 尽管我国政府多年来在此领域投入了数百亿的科研经费，但FPGA的专利限制及技术门槛使得中国FPGA的研发之路十分艰辛，国内如同创国芯、京微雅格、高云等公司在FPGA研发方面已获得一定进展，但产品性能、功耗、容量和应用领域上都同国外先进技术存在着较大差距。当前国内部分资本已经试图走出国门，通过并购半导体类公司的方法进入FPGA的行业，实现弯道超车。 1.2.3 ASIC 1.2.3.1 ASIC简介 ASIC，即专用集成电路，是指应特定用户要求或特定电子系统的需要而设计、制造的集成电路。ASIC 作为集成电路技术与特定用户的整机或系统技术紧密结合的产物，与通用集成电路相比，具有以下几个方面的优越性：体积更小、功耗更低、可靠性提高、性能提高、保密性增强。FPGA一般来说比ASIC的速度要慢，而且无法完成更复杂的设计，并且会消耗更多的电能，因此就算力而言ASIC远优于FPGA；但ASIC的专用特点使得其生产成本很高，如果出货量较小，则采用ASIC在经济上不太实惠。一旦人工智能技术成熟，ASIC专用集成的特点反而会达到规模效应，较通用集成电路而言，成本大大降低。 当前ASIC 在人工智能深度学习方面的应用还不多，但是我们可以拿比特币矿机芯片的发展做类似的推理。比特币挖矿和人工智能深度学习有类似之处，都是依赖于底层的芯片进行大规模的并行计算。比特币矿机的芯片经历了四个阶段：CPU、GPU、FPGA 和ASIC。其中ASIC 在比特币挖矿领域，展现出了得天独厚的优势。随着人工智能越来越多的应用在各个领域并表现出优越的性能，长期来看ASIC大有可为。 1.2.3.2 ASIC市场现状 随着人工智能的兴起，科技巨头纷纷布局芯片制造。高通、AMD、ARM、Intel和NVIDIA都在致力于将定制化芯片整合进它们的现有解决方案中。Nervana 和 Movidius（目前都在Intel旗下）据说正在开发集合方案。ASIC中较为成熟的产品是谷歌针对AlphaGo研发的TPU。第一代TPU产品由谷歌在2016年I/O大会上正式推出，今年5月的开发者I/O大会上，谷歌正式公布了第二代TPU，又称Cloud TPU，相较于初代TPU，既能用于训练神经网络，又可以用于推理，浮点性能方面较传统的GPU提升了15倍。 ASIC在人工智能领域的应用起步较晚，国内外水平相差不大。目前国内已有数家公司致力于人工智能相关ASIC芯片研究，代表公司为地平线机器人、中科寒武纪与中星微电子。其中地平线机器人公司作为初创企业，致力于打造基于深度神经网络的人工智能“大脑”平台-包括软件和芯片，可以做到低功耗、本地化的解决环境感知、人机交互、决策控制等问题。其关于芯片的研发目前还未成熟。中科寒武纪和中星微电子则已经有了相对成熟的产品。寒武纪芯片专门面向深度学习技术，研制了国际首个深度学习专用处理器芯片NPU，目前已研发的三款芯片分别面向神经网络的原型处理器结构、大规模神经网络和多种机器学习算法，预计将于2018年实现芯片的产业化。中星微电子于2016年6月推出中国首款嵌入式神经网络处理器（NPU）芯片，这是全球首颗具备深度学习人工智能的嵌入式视频采集压缩编码系统级芯片。这款基于深度学习的芯片运用在人脸识别上，最高能达到98%的准确率，超过人眼的识别率。该芯片于2017年3月6日实现量产，截止到今年5月出货量为十几万件。 1.2.4 类人脑芯片 1.2.4.1 类人脑芯片简介 类人脑芯片是一种基于神经形态工程、借鉴人脑信息处理方式，旨在打破“冯·诺依曼”架构束缚，适于实时处理非结构化信息、具有学习能力的超低功耗新型计算芯片。从理论上来看，类人脑芯片更加接近于人工智能目标的芯片，力图在基本架构上模仿人脑的工作原理，使用神经元和突触的方式替代传统架构体系，使芯片能够进行异步、并行、低俗和分布式处理信息数据的能力，同时具备自护感知、识别和学习的能力。 1.2.4.2 类人脑芯片市场现状 类人脑芯片是人工智能芯片发展的重点方向。目前各国政府及科技巨头都在大力推动类人脑芯片的研发进程，包括美国、日本、德国、英国、瑞士等发达国家已经制定相应的发展战略，中国的类人脑科学研究项目目前也已经正式启动。当前世界上已有一批科技公司走在前列，在类人脑芯片研发中取得了突破，代表产品包括IBM的TrueNorth芯片、高通Zeroth芯片、谷歌的“神经网络图灵机”等。 （二）AI技术层 AI技术层主要着眼于算法、模型及可应用技术。按照智能程度不同，人工智能可分为运算智能、感知智能、认知智能三个阶段。运算智能，即快速计算和记忆存储能力，在这一阶段主要是算法与数据库相结合，使得机器开始像人类一样会计算和传递信息； 感知智能，即视觉、听觉、触觉等感知能力，在这一阶段，数据库与浅层学习算法结合，使得机器开始看懂和听懂，并做出判断、采取行动；认知智能，即能理解会思考的能力，这一阶段主要是采用深度学习算法，使得机器能够像人一样思考，主动采取行动。 AI技术层可以分为框架层和算法层，其中框架层指TensorFlow，Caffe，Theano，Torch，DMTK，DTPAR，ROS等框架或操作系统，算法层指的是对数据的处理方法。 根据数据类型的不同，对一个问题会采用不同的建模方式，即学习方式。按照学习方式来分类，人工智能算法可以分为传统机器学习和神经网络算法，其中传统机器学习又可细分为监督式学习、非监督式学习、半监督式学习、强化学习。 2.1 传统机器学习 2.1.1 监督式学习 在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常用算法有回归算法、朴素贝叶斯、SVM等。 2.1.2 非监督式学习 在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。关联规则学习的常见算法主要为Apriori算法及其拓展算法，聚类的常用算法有k-Means算法及其相似算法。 2.1.3 半监督式学习 在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。 2.1.4 强化学习 在此学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。 2.2 神经网络 人工神经网络是模拟生物神经网络，由众多的神经元可调的连接权值连接而成，具有大规模并行处理、分布式信息存储、良好的组织学习能力特点，并通过一定学习准则进行学习，进而建立相关模型，解决一定工作。在人工神经网络的学习算法设计方面，一般对人工神经网络进行大量的数据训练和调整，不断修正各层级节点参数，通过不断学习使得人工神经网络具有初步的自适应能力和自我组织能力及较强的泛化能力，进而较快适应周边环境要求，基于其众多优点，人工神经网络已然成为人工智能算法的核心。深度学习算法是人工神经网络当前最新算法，其实质是通过很多隐层的机器学习模型和海量的训练数据来学习更有用的特征，从而提升分类或预测的准确性。 &nbsp; （三）AI应用层&nbsp; 人工智能的应用主要是采用了“AI+垂直行业”的方式渗透到传统各行业，按发展层次的不同可以分为专用人工智能、通用人工智能和超级人工智能三个层次。其中，专用人工智能以一个或多个专门的领域和功能为主；通用人工智能即机器与人类一样拥有进行所有工作的可能，关键在于自动地认知和拓展；超级人工智能是指具有自我意识，包括独立自主的价值观、世界观等，目前仅存在于文化作品构想中。 按应用技术类型进行划分，人工智能的应用技术可以分为计算机视觉、机器学习、自然语言处理和机器人四块。 3.1 计算机视觉 计算机视觉，是指计算机从图像中识别出物体、场景和活动的能力。计算机视觉技术运用由图像处理操作及其他技术所组成的序列来将图像分析任务分解为便于管理的小块任务目前计算机视觉主要应用在人脸识别、图像识别方面（包括静态、动态两类信息）。 人脸识别，亦叫人像识别、面部识别，是基于人的脸部特征信息进行身份识别的一种生物识别技术。用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行处理的一系列相关技术。 图像识别，是计算机对图像进行处理、分析和理解，以识别各种不同模式的目标和对象的技术。识别过程包括图像预处理、图像分割、特征提取和判断匹配。由于动态监测与识别的技术限制，静态图像识别与人脸识别的研究暂时处于领先位置。 当前国外科技巨头自行研发和收购双管齐下布局计算机视觉领域，将技术广泛用于自身产品升级，并基于自身基因打造技术服务平台和新品类持续提升影响力。中国国内BAT都已纷纷布局相关领域，并基于自身产品进行功能研发。百度相对更加激进，成立了独立风投公司，专注于AI早期投资。 除BAT三巨头外，国内也有不少初创公司涉足计算机视觉技术，主要聚焦于技术应用。其中典型代表当属旷视科技。公司成立于2012年11月，公司专注于人脸识别技术和相关产品应用研究，面向开发者提供服务，能提供一整套人脸检测、人脸识别、人脸分析以及人脸3D技术的视觉技术服务，主要通过提供云端API、离线SDK、以及面向用户的自主研发产品形式，将人脸识别技术广泛应用到互联网及移动应用场景中。Face++通过和众多互联网公司合作，并通过“脱敏”技术掌握到了500万张人脸图片数据库，在互联网图片人脸识别LFW的准确率达到99.6%，合作伙伴包括阿里、360等一批大型的图片、社交、设备类企业。 当前国内计算机视觉创业热度不断提高，iiMedia Research(艾媒咨询)数据显示， 中国人工智能创业公司所属领域分布中，计算机视觉领域拥有最多创业公司，高达35家。&nbsp; 3.2 机器学习 机器学习是指计算机通过对大量已有数据的处理分析和学习，从而拥有预测判断和做出最佳决策的能力。其核心在于，机器学习是从数据中自动发现模式，模式一旦被发现便可用于做预测。 机器学习的应用范围非常广泛，针对那些产生庞大数据的活动，它几乎拥有改进一切性能的潜力。除了欺诈甄别之外，这些活动还包括销售预测、库存管理、石油和天然气勘探、以及公共卫生。机器学习技术在其他的认知技术领域也扮演着重要角色，比如计算机视觉，它能在海量图像中通过不断训练和改进视觉模型来提高其识别对象的能力。 现如今，机器学习已经成为认知技术中最炙手可热的研究领域之一，在2011-2014年中这段时间内就已吸引了近十亿美元的风险投资。谷歌也在2014年斥资4亿美金收购Deepmind这家研究机器学习技术的公司。目前国内机器学习相关企业数量相对较少。BAT在机器学习方面有着先天的优势，国内初创公司第四范式是基于机器学习的解决方案提供商。&nbsp; 3.3 自然语言处理 自然语言处理就是用人工智能来处理、理解以及运用人类语言，通过建立语言模型来预测语言表达的概率分布，从而实现目标。 &nbsp;&nbsp;自然语言处理技术在生活中应用广泛，例如机器翻译、手写体和印刷体字符识别、语音识别后实现文字转换、信息检索、抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等。它们分别应用了自然语言处理当中的语法分析、语义分析、篇章理解等技术，是人工智能界最前沿的研究领域。时至今日AI在这些技术领域的发展已经把识别准确率从70%提高到了90%以上，但只有当准确率提高到99%及以上时，才能被认定为自然语言处理的技术达到人类水平。 在资本与产业助力之下，我国人工智能的语音识别技术已处于国际领先水平，技术成熟，通用识别率上，各企业均维持在了95%左右的水平。类似百度、科大讯飞等上市公司凭借深厚的技术和数据积累在市场上占据前列，且通过软硬件服务的开发不断进化着自身的服务能力。在科大讯飞之后发布国内第二家“语音识别公有云”的云知声在各项通用语音服务技术的提供上也占据着不小的市场空间。除此之外，依托中科院自动化所的紫冬锐意和纳象立方以及有着海外背景的苏州思必驰在教育领域的语音识别上占据着领先的位置。 3.4 机器人 将机器视觉、自动规划等认知技术整合至极小却高性能的传感器、致动器、以及设计巧妙的硬件中，这就催生了新一代的机器人，它有能力与人类一起工作，能在各种未知环境中灵活处理不同的任务。 目前世界上至少有48个国家在发展机器人，其中25个国家已涉足服务型机器人开发。在日本、北美和欧洲，迄今已有7种类型计40余款服务型机器人进入实验和半商业化应用在服务机器人领域。美国是机器人的发源地，美国的机器人技术在国际上仍一直处于领先地位，其技术全面、先进，适应性十分强，在军用、医疗、家用服务机器人产业都占有绝对的优势，占服务机器人市场约60%的份额。国内智能机器人行业的研发主要集中于家庭机器人、工业/企业服务和智能助手三个方面。其中工业及企业服务类的机器人研发企业依托政策背景和市场需求处于相对领先的发展阶段。然而在中国涉足智能机器人的企业中，从事家庭机器人和智能助手研发的企业占据了绝大多数比例。 因为服务一般都要结合特定市场进行开发，本土企业更容易结合特定的环境和文化进行开发占据良好的市场定位，从而保持一定的竞争优势；另一方面，外国的服务机器人公司也属于新兴产业，大部分成立的时候还比较短，因而我国的服务机器人产业面临着比较大的机遇和可发展空间。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/05/729914.html","headline":"大数据和人工智能概念全面解析","dateModified":"2019-05-05T00:00:00+08:00","datePublished":"2019-05-05T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/05/729914.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>大数据和人工智能概念全面解析</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div class="markdown_views prism-tomorrow-night" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <svg xmlns="http://www.w3.org/2000/svg">
    <path id="raphael-marker-block" stroke-linecap="round" d="M 5 0 L 0 2.5 L 5 5 Z" />
   </svg>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <div class="htmledit_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <div class="dtl-md pr10 pl10">
     一、大数据和人工智能
     <p>&nbsp;</p>
     <p><span>大数据是伴随着信息数据爆炸式增长和网络计算技术迅速发展而兴起的一个新型概念。根据麦肯锡全球研究所的定义，大数据是一种规模大到在获取、存储、管理、分析方面大大超出了传统数据库软件工具能力范围的数据集合，具有海量的数据规模、快速的数据流转、多样的数据类型和价值密度低四大特征。大数据能够帮助各行各业的企业从原本毫无价值的海量数据中挖掘出用户的需求，使数据能够从量变到质变，真正产生价值。随着大数据的发展，其应用已经渗透到农业、工业、商业、服务业、医疗领域等各个方面，成为影响产业发展的一个重要因素。</span></p>
     <p><span>当前人们所说的人工智能，是指研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术以及应用系统的一门新的技术科学，是由人工制造出来的系统所表现出来的智能。</span></p>
     <p><span>&nbsp; &nbsp;传统人工智能受制于计算能力，并没能完成大规模的并行计算和并行处理，人工智能系统的能力较差。2006年，Hinton教授提出“深度学习”神经网络使得人工智能性能获得突破性进展，进而促使人工智能产业又一次进入快速发展阶段。“深度学习”神经网络主要机理是通过深层神经网络算法来模拟人的大脑学习过程，通过输入与输出的非线性关系将低层特征组合成更高层的抽象表示，最终达到掌握运用的水平。数据量的丰富程度决定了是否有充足数据对神经网络进行训练，进而使人工智能系统经过深度学习训练后达到强人工智能水平。因此，能否有足够多的数据对人工神经网络进行深度训练，提升算法有效性是人工智能能否达到类人或超人水平的决定因素之一。</span></p>
     <p><span>随着移动互联网的爆发，数据量呈现出指数级的增长，大数据的积累为人工智能提供了基础支撑。同时受益于计算机技术在数据采集、存储、计算等环节的突破，人工智能已从简单的算法+数据库发展演化到了机器学习+深度理解的状态。</span></p>
     <p>如果想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，我也被圈粉了。教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
    </div>
   </div>
   <p>&nbsp;</p>二、人工智能产业及生态
   <p>&nbsp;</p>
   <p><span>按产业链结构划分，人工智能可以分为基础技术层、AI技术层和AI应用层。基础技术层主要聚焦于数据资源、计算能力和硬件平台，数据资源主要是各类大数据，硬件资源包括芯片研发、存储设备开发等。AI技术层着重于算法、模型及可应用技术，如计算智能算法、感知智能算法、认知智能算法。AI应用层则主要关注将人工智能与下游各领域结合起来，如无人机、机器人、虚拟客服、语音输入法等。</span></p>
   <p><span>图1 人工智能产业链</span></p>
   <p>&nbsp;</p>
   <p><img alt="" src="http://image2.135editor.com/cache/remote/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy96R0tFell3aWFMRGhtOUR1WWozcEYyQ1o4U0JsdTlvakgxcENKVGZPbzdXMkdLSmJDYkhQcm9zeGN2Uzd4UW9IWXplS2NnRW1XM0FBRUMzaGxCTWRnUUEvMD93eF9mbXQ9cG5n"></p>
   <p>&nbsp;</p>
   <p>资料来源：中国产业信息网，《2017年中国人工智能行业发展概况及未来发展趋势分析》</p>
   <p>&nbsp;</p>
   <p>（一）基础技术层</p>&nbsp;
   <p><span>1.1 大数据</span></p>
   <p><span>数据资源是机器学习训练的基本素材，通过对于数据的学习，机器能不断积累经验和优化决策参数，逐渐变得更贴近人类智能。</span></p>
   <p><span>从数据流动方向的角度来看，大数据的产业链可分为底层平台、处理分析、应用三个层次。底层平台由基础设施与数据资产池构成，主要提供数据采集、分享和交易服务，处理分析则是在原始数据的基础上对数据进行清洗后以不同方式呈现。在数据处理分析的基础之上，挖掘各行业的数据需求，最终为用户提供服务。</span></p>
   <p><span>根据数据应用程度不同，大数据产业链下各参与方功能可细分为数据标准与规范化、数据采集、数据安全、数据储存与管理、数据分析与挖掘、数据运维和数据运用七个方面。</span><span>&nbsp;</span></p>
   <p><span>1.2 计算能力和硬件平台</span></p>
   <p><span>数据资源、核心算法、运算能力是人工智能的三大核心要素。随着全球移动互联网和物联网等快速发展，人类可获取利用的数据正以爆炸式增长。海量的大数据通过最新的深度学习技术将为人工智能的发展与应用带来难以估量的价值，而运算能力提升是人工智能发展的前提保障。其中，芯片是运算能力的核心。</span></p>
   <p><span>就目前而言，AI 芯片主要类型有GPU、FPGA、ASIC和类人脑芯片四种。</span></p>
   <p><span>1.2.1 GPU</span></p>
   <p><span>1.2.1.1 GPU简介</span></p>
   <p><span>GPU 即图形处理器，最初是用来做图像运算的微处理器。GPU 优化和调整了CPU 结构，使其运算速度突飞猛进，拥有了更强大的处理浮点运算的能力。2009 年，斯坦福大学的吴恩达及其团队发现GPU 芯片可以并行运行神经网络。用GPU来运行机器学习模型，同样的大训练集，GPU在耗费功率更低、占用基础设施更少的情况下能够支持远比单纯使用CPU时10-100倍的应用吞吐量。因此GPU已经成为数据科学家处理大数据的处理器。</span></p>
   <p><span>1.2.1.2 GPU行业现状</span></p>
   <p><span>目前国际GPU市场被NVIDIA 和AMD 两大公司瓜分，全球GPU 行业的市场份额有超过70％被NVIDIA占据，而应用在人工智能领域的可进行通用计算的GPU 市场则基本被NVIDIA垄断。目前公司已与谷歌、微软、IBM、丰田、百度等多家尝试利用深度神经网络来解决海量复杂计算问题的企业建立和合作关系。NVIDIA与下游客户在深度学习领域的合作不断加深，已经开发出多款针对深度学习的GPU产品。从产品成熟度、生态圈的规模角度而言，NVIDIA的GPU 已具备统治性的地位。</span></p>
   <p><span>中国在GPU芯片设计领域起步较晚，目前只有景嘉微和兆芯两家掌握核心技术的公司正在逐步打破国外芯片在我国GPU市场的垄断局面，但产品还是主要用于GPU最初的图形显控领域，距人工智能所需要的GPU技术还有很远的距离。</span></p>
   <p><span>1.2.2 FPGA</span></p>
   <p><span>1.2.2.1 FPGA简介</span></p>
   <p><span>FPGA，即场效可编程逻辑闸阵列，最初是从专用集成电路上发展起来的半定制化的可编程电路，FPGA 还具有静态可重复编程和动态在系统重构的特性，使得硬件的功能可以像软件一样通过编程来修改，不同的编程数据在同一片FPGA上可以产生不同的电路功能，具有很强的灵活性和适应性。</span></p>
   <p><span>FPGA 和GPU 内都有大量的计算单元，因此它们的计算能力都很强。在进行神经网络运算的时候，两者的速度会比CPU 快很多。但是GPU 由于架构固定，硬件原生支持的指令也就固定了，而FPGA 则是可编程的。其可编程性是关键，因为它让软件与终端应用公司能够提供与其竞争对手不同的解决方案，并且能够灵活地针对自己所用的算法修改电路。与GPU相比，FPGA具有性能高、能耗低及可硬件编程的特点。</span></p>
   <p><span>1.2.2.2 FPGA行业现状</span></p>
   <p><span>目前FPGA 整个市场被国外的两大巨头所寡占，据东方证券研究所数据显示，Xilinx 和Altera 占了近90%的份额，合计专利达到6000多项，剩余份额被Lattice和Microsemi两家占据，两家专利合计共有超过3000项。技术专利的限制和漫长的开发周期使得FPGA行业有着极高的壁垒。</span></p>
   <p><span>尽管我国政府多年来在此领域投入了数百亿的科研经费，但FPGA的专利限制及技术门槛使得中国FPGA的研发之路十分艰辛，国内如同创国芯、京微雅格、高云等公司在FPGA研发方面已获得一定进展，但产品性能、功耗、容量和应用领域上都同国外先进技术存在着较大差距。当前国内部分资本已经试图走出国门，通过并购半导体类公司的方法进入FPGA的行业，实现弯道超车。</span></p>
   <p><span>1.2.3 ASIC</span></p>
   <p><span>1.2.3.1 ASIC简介</span></p>
   <p><span>ASIC，即专用集成电路，是指应特定用户要求或特定电子系统的需要而设计、制造的集成电路。ASIC 作为集成电路技术与特定用户的整机或系统技术紧密结合的产物，与通用集成电路相比，具有以下几个方面的优越性：体积更小、功耗更低、可靠性提高、性能提高、保密性增强。FPGA一般来说比ASIC的速度要慢，而且无法完成更复杂的设计，并且会消耗更多的电能，因此就算力而言ASIC远优于FPGA；但ASIC的专用特点使得其生产成本很高，如果出货量较小，则采用ASIC在经济上不太实惠。一旦人工智能技术成熟，ASIC专用集成的特点反而会达到规模效应，较通用集成电路而言，成本大大降低。</span></p>
   <p><span>当前ASIC 在人工智能深度学习方面的应用还不多，但是我们可以拿比特币矿机芯片的发展做类似的推理。比特币挖矿和人工智能深度学习有类似之处，都是依赖于底层的芯片进行大规模的并行计算。比特币矿机的芯片经历了四个阶段：CPU、GPU、FPGA 和ASIC。其中ASIC 在比特币挖矿领域，展现出了得天独厚的优势。随着人工智能越来越多的应用在各个领域并表现出优越的性能，长期来看ASIC大有可为。</span></p>
   <p><span>1.2.3.2 ASIC市场现状</span></p>
   <p><span>随着人工智能的兴起，科技巨头纷纷布局芯片制造。高通、AMD、ARM、Intel和NVIDIA都在致力于将定制化芯片整合进它们的现有解决方案中。Nervana 和 Movidius（目前都在Intel旗下）据说正在开发集合方案。ASIC中较为成熟的产品是谷歌针对AlphaGo研发的TPU。第一代TPU产品由谷歌在2016年I/O大会上正式推出，今年5月的开发者I/O大会上，谷歌正式公布了第二代TPU，又称Cloud TPU，相较于初代TPU，既能用于训练神经网络，又可以用于推理，浮点性能方面较传统的GPU提升了15倍。</span></p>
   <p><span>ASIC在人工智能领域的应用起步较晚，国内外水平相差不大。目前国内已有数家公司致力于人工智能相关ASIC芯片研究，代表公司为地平线机器人、中科寒武纪与中星微电子。其中地平线机器人公司作为初创企业，致力于打造基于深度神经网络的人工智能“大脑”平台-包括软件和芯片，可以做到低功耗、本地化的解决环境感知、人机交互、决策控制等问题。其关于芯片的研发目前还未成熟。中科寒武纪和中星微电子则已经有了相对成熟的产品。寒武纪芯片专门面向深度学习技术，研制了国际首个深度学习专用处理器芯片NPU，目前已研发的三款芯片分别面向神经网络的原型处理器结构、大规模神经网络和多种机器学习算法，预计将于2018年实现芯片的产业化。中星微电子于2016年6月推出中国首款嵌入式神经网络处理器（NPU）芯片，这是全球首颗具备深度学习人工智能的嵌入式视频采集压缩编码系统级芯片。这款基于深度学习的芯片运用在人脸识别上，最高能达到98%的准确率，超过人眼的识别率。该芯片于2017年3月6日实现量产，截止到今年5月出货量为十几万件。</span></p>
   <p><span>1.2.4 类人脑芯片</span></p>
   <p><span>1.2.4.1 类人脑芯片简介</span></p>
   <p><span>类人脑芯片是一种基于神经形态工程、借鉴人脑信息处理方式，旨在打破“冯·诺依曼”架构束缚，适于实时处理非结构化信息、具有学习能力的超低功耗新型计算芯片。从理论上来看，类人脑芯片更加接近于人工智能目标的芯片，力图在基本架构上模仿人脑的工作原理，使用神经元和突触的方式替代传统架构体系，使芯片能够进行异步、并行、低俗和分布式处理信息数据的能力，同时具备自护感知、识别和学习的能力。</span></p>
   <p><span>1.2.4.2 类人脑芯片市场现状</span></p>
   <p><span>类人脑芯片是人工智能芯片发展的重点方向。目前各国政府及科技巨头都在大力推动类人脑芯片的研发进程，包括美国、日本、德国、英国、瑞士等发达国家已经制定相应的发展战略，中国的类人脑科学研究项目目前也已经正式启动。当前世界上已有一批科技公司走在前列，在类人脑芯片研发中取得了突破，代表产品包括IBM的TrueNorth芯片、高通Zeroth芯片、谷歌的“神经网络图灵机”等。</span></p>
   <p>（二）AI技术层</p>
   <p><span>AI技术层主要着眼于算法、模型及可应用技术。按照智能程度不同，人工智能可分为运算智能、感知智能、认知智能三个阶段。运算智能，即快速计算和记忆存储能力，在这一阶段主要是算法与数据库相结合，使得机器开始像人类一样会计算和传递信息； 感知智能，即视觉、听觉、触觉等感知能力，在这一阶段，数据库与浅层学习算法结合，使得机器开始看懂和听懂，并做出判断、采取行动；认知智能，即能理解会思考的能力，这一阶段主要是采用深度学习算法，使得机器能够像人一样思考，主动采取行动。</span></p>
   <p><span>AI技术层可以分为框架层和算法层，其中框架层指TensorFlow，Caffe，Theano，Torch，DMTK，DTPAR，ROS等框架或操作系统，算法层指的是对数据的处理方法。</span></p>
   <p><span>根据数据类型的不同，对一个问题会采用不同的建模方式，即学习方式。按照学习方式来分类，人工智能算法可以分为传统机器学习和神经网络算法，其中传统机器学习又可细分为监督式学习、非监督式学习、半监督式学习、强化学习。</span></p>
   <p><span>2.1 传统机器学习</span></p>
   <p><span>2.1.1 监督式学习</span></p>
   <p><span>在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常用算法有回归算法、朴素贝叶斯、SVM等。</span></p>
   <p><span>2.1.2 非监督式学习</span></p>
   <p><span>在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。关联规则学习的常见算法主要为Apriori算法及其拓展算法，聚类的常用算法有k-Means算法及其相似算法。</span></p>
   <p><span>2.1.3 半监督式学习</span></p>
   <p><span>在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。</span></p>
   <p><span>2.1.4 强化学习</span></p>
   <p><span>在此学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。</span></p>
   <p><span>2.2 神经网络</span></p>
   <p><span>人工神经网络是模拟生物神经网络，由众多的神经元可调的连接权值连接而成，具有大规模并行处理、分布式信息存储、良好的组织学习能力特点，并通过一定学习准则进行学习，进而建立相关模型，解决一定工作。在人工神经网络的学习算法设计方面，一般对人工神经网络进行大量的数据训练和调整，不断修正各层级节点参数，通过不断学习使得人工神经网络具有初步的自适应能力和自我组织能力及较强的泛化能力，进而较快适应周边环境要求，基于其众多优点，人工神经网络已然成为人工智能算法的核心。深度学习算法是人工神经网络当前最新算法，其实质是通过很多隐层的机器学习模型和海量的训练数据来学习更有用的特征，从而提升分类或预测的准确性。</span></p>
   <p>&nbsp;</p>
   <p>（三）AI应用层</p>&nbsp;
   <p><span>人工智能的应用主要是采用了“AI+垂直行业”的方式渗透到传统各行业，按发展层次的不同可以分为专用人工智能、通用人工智能和超级人工智能三个层次。其中，专用人工智能以一个或多个专门的领域和功能为主；通用人工智能即机器与人类一样拥有进行所有工作的可能，关键在于自动地认知和拓展；超级人工智能是指具有自我意识，包括独立自主的价值观、世界观等，目前仅存在于文化作品构想中。</span></p>
   <p><span>按应用技术类型进行划分，人工智能的应用技术可以分为计算机视觉、机器学习、自然语言处理和机器人四块。</span></p>
   <p><span>3.1 计算机视觉</span></p>
   <p><span>计算机视觉，是指计算机从图像中识别出物体、场景和活动的能力。计算机视觉技术运用由图像处理操作及其他技术所组成的序列来将图像分析任务分解为便于管理的小块任务目前计算机视觉主要应用在人脸识别、图像识别方面（包括静态、动态两类信息）。</span></p>
   <p><span>人脸识别，亦叫人像识别、面部识别，是基于人的脸部特征信息进行身份识别的一种生物识别技术。用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行处理的一系列相关技术。</span></p>
   <p><span>图像识别，是计算机对图像进行处理、分析和理解，以识别各种不同模式的目标和对象的技术。识别过程包括图像预处理、图像分割、特征提取和判断匹配。由于动态监测与识别的技术限制，静态图像识别与人脸识别的研究暂时处于领先位置。</span></p>
   <p><span>当前国外科技巨头自行研发和收购双管齐下布局计算机视觉领域，将技术广泛用于自身产品升级，并基于自身基因打造技术服务平台和新品类持续提升影响力。中国国内BAT都已纷纷布局相关领域，并基于自身产品进行功能研发。百度相对更加激进，成立了独立风投公司，专注于AI早期投资。</span></p>
   <p><span>除BAT三巨头外，国内也有不少初创公司涉足计算机视觉技术，主要聚焦于技术应用。其中典型代表当属旷视科技。公司成立于2012年11月，公司专注于人脸识别技术和相关产品应用研究，面向开发者提供服务，能提供一整套人脸检测、人脸识别、人脸分析以及人脸3D技术的视觉技术服务，主要通过提供云端API、离线SDK、以及面向用户的自主研发产品形式，将人脸识别技术广泛应用到互联网及移动应用场景中。Face++通过和众多互联网公司合作，并通过“脱敏”技术掌握到了500万张人脸图片数据库，在互联网图片人脸识别LFW的准确率达到99.6%，合作伙伴包括阿里、360等一批大型的图片、社交、设备类企业。</span></p>
   <p><span>当前国内计算机视觉创业热度不断提高，iiMedia Research(艾媒咨询)数据显示， 中国人工智能创业公司所属领域分布中，计算机视觉领域拥有最多创业公司，高达35家。</span><span>&nbsp;</span></p>
   <p><span>3.2 机器学习</span></p>
   <p><span>机器学习是指计算机通过对大量已有数据的处理分析和学习，从而拥有预测判断和做出最佳决策的能力。其核心在于，机器学习是从数据中自动发现模式，模式一旦被发现便可用于做预测。</span></p>
   <p><span>机器学习的应用范围非常广泛，针对那些产生庞大数据的活动，它几乎拥有改进一切性能的潜力。除了欺诈甄别之外，这些活动还包括销售预测、库存管理、石油和天然气勘探、以及公共卫生。机器学习技术在其他的认知技术领域也扮演着重要角色，比如计算机视觉，它能在海量图像中通过不断训练和改进视觉模型来提高其识别对象的能力。</span></p>
   <p><span>现如今，机器学习已经成为认知技术中最炙手可热的研究领域之一，在2011-2014年中这段时间内就已吸引了近十亿美元的风险投资。谷歌也在2014年斥资4亿美金收购Deepmind这家研究机器学习技术的公司。目前国内机器学习相关企业数量相对较少。BAT在机器学习方面有着先天的优势，国内初创公司第四范式是基于机器学习的解决方案提供商。</span><span>&nbsp;</span></p>
   <p><span>3.3 自然语言处理</span></p>
   <p><span>自然语言处理就是用人工智能来处理、理解以及运用人类语言，通过建立语言模型来预测语言表达的概率分布，从而实现目标。</span></p>
   <p><span>&nbsp;&nbsp;自然语言处理技术在生活中应用广泛，例如机器翻译、手写体和印刷体字符识别、语音识别后实现文字转换、信息检索、抽取与过滤、文本分类与聚类、舆情分析和观点挖掘等。它们分别应用了自然语言处理当中的语法分析、语义分析、篇章理解等技术，是人工智能界最前沿的研究领域。时至今日AI在这些技术领域的发展已经把识别准确率从70%提高到了90%以上，但只有当准确率提高到99%及以上时，才能被认定为自然语言处理的技术达到人类水平。</span></p>
   <p><span>在资本与产业助力之下，我国人工智能的语音识别技术已处于国际领先水平，技术成熟，通用识别率上，各企业均维持在了95%左右的水平。类似百度、科大讯飞等上市公司凭借深厚的技术和数据积累在市场上占据前列，且通过软硬件服务的开发不断进化着自身的服务能力。在科大讯飞之后发布国内第二家“语音识别公有云”的云知声在各项通用语音服务技术的提供上也占据着不小的市场空间。除此之外，依托中科院自动化所的紫冬锐意和纳象立方以及有着海外背景的苏州思必驰在教育领域的语音识别上占据着领先的位置。</span></p>
   <p><span>3.4 机器人</span></p>
   <p><span>将机器视觉、自动规划等认知技术整合至极小却高性能的传感器、致动器、以及设计巧妙的硬件中，这就催生了新一代的机器人，它有能力与人类一起工作，能在各种未知环境中灵活处理不同的任务。</span></p>
   <p><span>目前世界上至少有48个国家在发展机器人，其中25个国家已涉足服务型机器人开发。在日本、北美和欧洲，迄今已有7种类型计40余款服务型机器人进入实验和半商业化应用在服务机器人领域。美国是机器人的发源地，美国的机器人技术在国际上仍一直处于领先地位，其技术全面、先进，适应性十分强，在军用、医疗、家用服务机器人产业都占有绝对的优势，占服务机器人市场约60%的份额。国内智能机器人行业的研发主要集中于家庭机器人、工业/企业服务和智能助手三个方面。其中工业及企业服务类的机器人研发企业依托政策背景和市场需求处于相对领先的发展阶段。然而在中国涉足智能机器人的企业中，从事家庭机器人和智能助手研发的企业占据了绝大多数比例。</span></p>
   <p><span>因为服务一般都要结合特定市场进行开发，本土企业更容易结合特定的环境和文化进行开发占据良好的市场定位，从而保持一定的竞争优势；另一方面，外国的服务机器人公司也属于新兴产业，大部分成立的时候还比较短，因而我国的服务机器人产业面临着比较大的机遇和可发展空间。</span></p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
