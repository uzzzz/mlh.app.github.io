<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>手动搭建kubernetes集群（二） | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="手动搭建kubernetes集群（二）" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="根据前文准备好的环境，我们现在来一步步的搭建一个基础的k8s集群 注意，这里的配置信息都是按照我自己的虚拟环境来写的。 把server01作为master节点，server02和server03作为worker节点 各个节点需要配置的服务和命令如下： master节点上需要部署的服务包括：etcd服务、APIServer服务、Scheduler服务、ControllerManager服务、CalicoNode服务、kube-proxy服务、kubectl命令 worker节点上需要部署的服务包括：CalicoNode服务、kubelet服务、kube-proxy服务 步骤1：准备文件 安装k8s集群有几种方式可以选择，比如容器化的方式，比如用kubeadmin的方式，这次我们打算尝试的是使用二进制文件的方式。 登录到master虚拟机上（server01），从github上下载安装文件的压缩包，我们使用的是1.13.6版本： wget https://github.com/kubernetes/kubernetes/releases/download/v1.13.6/kubernetes.tar.gz 解压缩 tar zxvf kubernetes.tar.gz 下载文件，进入刚刚解压好的文件夹 cd kubernetes ./cluster/get-kube-binaries.sh 这个步骤因为涉及到从官网下载文件，由于墙的原因会非常缓慢或者失败，请自行上网解决。 步骤2：master环境部署 etcd部署： 编写etcd服务的启动配置文件etcd.service，内容如下： [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target Documentation=https://github.com/coreos [Service] Type=notify WorkingDirectory=/var/lib/etcd/ ExecStart=/home/anakin/bin/etcd \ --name=192.168.32.131 \ --listen-client-urls=http://192.168.32.131:2379,http://127.0.0.1:2379 \ --advertise-client-urls=http://192.168.32.131:2379 \ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 然后执行以下命令： cp etcd.service /lib/systemd/system/ systemctl enable etcd.service mkdir -p /var/lib/etcd service etcd start 如果没有问题的话，etcd服务应该已经跑起来了。 APIServer部署 编写kube-apiserver.service文件，内容如下： [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] ExecStart=/home/anakin/bin/kube-apiserver \ --admission-control=NamespaceLifecycle,LimitRanger,DefaultStorageClass,ResourceQuota,NodeRestriction \ --insecure-bind-address=0.0.0.0 \ --kubelet-https=false \ --service-cluster-ip-range=10.68.0.0/16 \ --service-node-port-range=20000-40000 \ --etcd-servers=http://192.168.32.131:2379 \ --enable-swagger-ui=true \ --allow-privileged=true \ --audit-log-maxage=30 \ --audit-log-maxbackup=3 \ --audit-log-maxsize=100 \ --audit-log-path=/var/lib/audit.log \ --event-ttl=1h \ --v=2 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target 然后执行和启动etcd类似的命令 cp kube-apiserver.service /lib/systemd/system/ systemctl enable kube-apiserver.service service kube-apiserver start ControllerManager部署 编写kube-controller-manager.service文件，内容如下： [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/home/anakin/bin/kube-controller-manager \ --address=127.0.0.1 \ --master=http://127.0.0.1:8080 \ --allocate-node-cidrs=true \ --service-cluster-ip-range=10.68.0.0/16 \ --cluster-cidr=172.20.0.0/16 \ --cluster-name=kubernetes \ --leader-elect=true \ --cluster-signing-cert-file= \ --cluster-signing-key-file= \ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 然后，同样的方式启动服务: cp kube-controller-manager.service /lib/systemd/system/ systemctl enable kube-controller-manager.service service kube-controller-manager start Scheduler部署 编写kube-scheduler.service文件，内容如下： [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/home/anakin/bin/kube-scheduler \ --address=127.0.0.1 \ --master=http://127.0.0.1:8080 \ --leader-elect=true \ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 然后，继续上面的启动方式: cp kube-scheduler.service /lib/systemd/system/ systemctl enable kube-scheduler.service service kube-scheduler start CalicoNode部署 编写kube-calico.service文件，内容如下： [Unit] Description=calico node After=docker.service Requires=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run --net=host --privileged --name=calico-node \ -e ETCD_ENDPOINTS=http://192.168.32.131:2379 \ -e CALICO_LIBNETWORK_ENABLED=true \ -e CALICO_NETWORKING_BACKEND=bird \ -e CALICO_DISABLE_FILE_LOGGING=true \ -e CALICO_IPV4POOL_CIDR=172.20.0.0/16 \ -e CALICO_IPV4POOL_IPIP=off \ -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \ -e FELIX_IPV6SUPPORT=false \ -e FELIX_LOGSEVERITYSCREEN=info \ -e FELIX_IPINIPMTU=1440 \ -e FELIX_HEALTHENABLED=true \ -e IP=192.168.32.131 \ -v /var/run/calico:/var/run/calico \ -v /lib/modules:/lib/modules \ -v /run/docker/plugins:/run/docker/plugins \ -v /var/run/docker.sock:/var/run/docker.sock \ -v /var/log/calico:/var/log/calico \ registry.anakin.sun.com/k8s/calico-node:v2.6.2 ExecStop=/usr/bin/docker rm -f calico-node Restart=always RestartSec=10 [Install] WantedBy=multi-user.target 同样的方式启动： cp kube-calico.service /lib/systemd/system/ systemctl enable kube-calico.service service kube-calico start kubectl命令配置 执行以下命令(root)： kubectl config set-cluster kubernetes --server=http://192.168.1.102:8080 kubectl config set-context kubernetes --cluster=kubernetes kubectl config use-context kubernetes 如果有问题，可以手动修改配置文件：~/.kube/config kube-proxy服务部署 编写kube-proxy.service文件，内容如下： [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/home/anakin/bin/kube-proxy \ --bind-address=192.168.32.131 \ --hostname-override=192.168.32.131 \ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \ --logtostderr=true \ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 编写kube-proxy.kubeconfig文件，内容如下： apiVersion: v1 clusters: - cluster: server: http://192.168.32.131:8080 name: kubernetes contexts: - context: cluster: kubernetes name: default current-context: default kind: Config preferences: {} users: [] 然后执行如下命令： mkdir -p /var/lib/kube-proxy cp kube-proxy.service /lib/systemd/system/ cp kube-proxy.kubeconfig /etc/kubernetes/ systemctl enable kube-proxy.service OK,至此，master节点应该已经配置完成了。 步骤3：worker环境部署 CalicoNode部署 参考master部分的内容 kube-proxy服务部署 参考master部分的内容 kubelet服务配置 编写kubelet.service文件，内容如下： [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/home/anakin/bin/kubelet \ --address=192.168.32.131 \ --hostname-override=192.168.32.131 \ --pod-infra-container-image=registry.anakin.sun.com/imooc/pause-amd64:3.0 \ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \ --network-plugin=cni \ --cni-conf-dir=/etc/cni/net.d \ --cni-bin-dir=/home/anakin/bin \ --cluster-dns=10.68.0.2 \ --cluster-domain=cluster.local. \ --allow-privileged=true \ --fail-swap-on=false \ --logtostderr=true \ --v=2 ExecStartPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT ExecStartPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT ExecStartPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 编写kubelet.kubeconfig文件，内容如下： apiVersion: v1 clusters: - cluster: insecure-skip-tls-verify: true server: http://192.168.32.131:8080 name: kubernetes contexts: - context: cluster: kubernetes user: &quot;&quot; name: system:node:kube-master current-context: system:node:kube-master kind: Config preferences: {} users: [] 编写10-calico.conf文件，内容如下： { &quot;name&quot;: &quot;calico-k8s-network&quot;, &quot;cniVersion&quot;: &quot;0.1.0&quot;, &quot;type&quot;: &quot;calico&quot;, &quot;etcd_endpoints&quot;: &quot;http://192.168.32.131:2379&quot;, &quot;log_level&quot;: &quot;info&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;calico-ipam&quot; }, &quot;kubernetes&quot;: { &quot;k8s_api_root&quot;: &quot;http://192.168.32.131:8080&quot; } } 然后，执行如下命令： mkdir -p /var/lib/kubelet mkdir -p /etc/kubernetes mkdir -p /etc/cni/net.d cp kubelet.service /lib/systemd/system/ cp kubelet.kubeconfig /etc/kubernetes/ cp 10-calico.conf /etc/cni/net.d/ systemctl enable kubelet.service service kubelet start 至此，整个集群应该已经搭建好了，如果中间遇到什么问题，可以通过查看系统日志，或者google解决。" />
<meta property="og:description" content="根据前文准备好的环境，我们现在来一步步的搭建一个基础的k8s集群 注意，这里的配置信息都是按照我自己的虚拟环境来写的。 把server01作为master节点，server02和server03作为worker节点 各个节点需要配置的服务和命令如下： master节点上需要部署的服务包括：etcd服务、APIServer服务、Scheduler服务、ControllerManager服务、CalicoNode服务、kube-proxy服务、kubectl命令 worker节点上需要部署的服务包括：CalicoNode服务、kubelet服务、kube-proxy服务 步骤1：准备文件 安装k8s集群有几种方式可以选择，比如容器化的方式，比如用kubeadmin的方式，这次我们打算尝试的是使用二进制文件的方式。 登录到master虚拟机上（server01），从github上下载安装文件的压缩包，我们使用的是1.13.6版本： wget https://github.com/kubernetes/kubernetes/releases/download/v1.13.6/kubernetes.tar.gz 解压缩 tar zxvf kubernetes.tar.gz 下载文件，进入刚刚解压好的文件夹 cd kubernetes ./cluster/get-kube-binaries.sh 这个步骤因为涉及到从官网下载文件，由于墙的原因会非常缓慢或者失败，请自行上网解决。 步骤2：master环境部署 etcd部署： 编写etcd服务的启动配置文件etcd.service，内容如下： [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target Documentation=https://github.com/coreos [Service] Type=notify WorkingDirectory=/var/lib/etcd/ ExecStart=/home/anakin/bin/etcd \ --name=192.168.32.131 \ --listen-client-urls=http://192.168.32.131:2379,http://127.0.0.1:2379 \ --advertise-client-urls=http://192.168.32.131:2379 \ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 然后执行以下命令： cp etcd.service /lib/systemd/system/ systemctl enable etcd.service mkdir -p /var/lib/etcd service etcd start 如果没有问题的话，etcd服务应该已经跑起来了。 APIServer部署 编写kube-apiserver.service文件，内容如下： [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] ExecStart=/home/anakin/bin/kube-apiserver \ --admission-control=NamespaceLifecycle,LimitRanger,DefaultStorageClass,ResourceQuota,NodeRestriction \ --insecure-bind-address=0.0.0.0 \ --kubelet-https=false \ --service-cluster-ip-range=10.68.0.0/16 \ --service-node-port-range=20000-40000 \ --etcd-servers=http://192.168.32.131:2379 \ --enable-swagger-ui=true \ --allow-privileged=true \ --audit-log-maxage=30 \ --audit-log-maxbackup=3 \ --audit-log-maxsize=100 \ --audit-log-path=/var/lib/audit.log \ --event-ttl=1h \ --v=2 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target 然后执行和启动etcd类似的命令 cp kube-apiserver.service /lib/systemd/system/ systemctl enable kube-apiserver.service service kube-apiserver start ControllerManager部署 编写kube-controller-manager.service文件，内容如下： [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/home/anakin/bin/kube-controller-manager \ --address=127.0.0.1 \ --master=http://127.0.0.1:8080 \ --allocate-node-cidrs=true \ --service-cluster-ip-range=10.68.0.0/16 \ --cluster-cidr=172.20.0.0/16 \ --cluster-name=kubernetes \ --leader-elect=true \ --cluster-signing-cert-file= \ --cluster-signing-key-file= \ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 然后，同样的方式启动服务: cp kube-controller-manager.service /lib/systemd/system/ systemctl enable kube-controller-manager.service service kube-controller-manager start Scheduler部署 编写kube-scheduler.service文件，内容如下： [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/home/anakin/bin/kube-scheduler \ --address=127.0.0.1 \ --master=http://127.0.0.1:8080 \ --leader-elect=true \ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 然后，继续上面的启动方式: cp kube-scheduler.service /lib/systemd/system/ systemctl enable kube-scheduler.service service kube-scheduler start CalicoNode部署 编写kube-calico.service文件，内容如下： [Unit] Description=calico node After=docker.service Requires=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run --net=host --privileged --name=calico-node \ -e ETCD_ENDPOINTS=http://192.168.32.131:2379 \ -e CALICO_LIBNETWORK_ENABLED=true \ -e CALICO_NETWORKING_BACKEND=bird \ -e CALICO_DISABLE_FILE_LOGGING=true \ -e CALICO_IPV4POOL_CIDR=172.20.0.0/16 \ -e CALICO_IPV4POOL_IPIP=off \ -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \ -e FELIX_IPV6SUPPORT=false \ -e FELIX_LOGSEVERITYSCREEN=info \ -e FELIX_IPINIPMTU=1440 \ -e FELIX_HEALTHENABLED=true \ -e IP=192.168.32.131 \ -v /var/run/calico:/var/run/calico \ -v /lib/modules:/lib/modules \ -v /run/docker/plugins:/run/docker/plugins \ -v /var/run/docker.sock:/var/run/docker.sock \ -v /var/log/calico:/var/log/calico \ registry.anakin.sun.com/k8s/calico-node:v2.6.2 ExecStop=/usr/bin/docker rm -f calico-node Restart=always RestartSec=10 [Install] WantedBy=multi-user.target 同样的方式启动： cp kube-calico.service /lib/systemd/system/ systemctl enable kube-calico.service service kube-calico start kubectl命令配置 执行以下命令(root)： kubectl config set-cluster kubernetes --server=http://192.168.1.102:8080 kubectl config set-context kubernetes --cluster=kubernetes kubectl config use-context kubernetes 如果有问题，可以手动修改配置文件：~/.kube/config kube-proxy服务部署 编写kube-proxy.service文件，内容如下： [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/home/anakin/bin/kube-proxy \ --bind-address=192.168.32.131 \ --hostname-override=192.168.32.131 \ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \ --logtostderr=true \ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 编写kube-proxy.kubeconfig文件，内容如下： apiVersion: v1 clusters: - cluster: server: http://192.168.32.131:8080 name: kubernetes contexts: - context: cluster: kubernetes name: default current-context: default kind: Config preferences: {} users: [] 然后执行如下命令： mkdir -p /var/lib/kube-proxy cp kube-proxy.service /lib/systemd/system/ cp kube-proxy.kubeconfig /etc/kubernetes/ systemctl enable kube-proxy.service OK,至此，master节点应该已经配置完成了。 步骤3：worker环境部署 CalicoNode部署 参考master部分的内容 kube-proxy服务部署 参考master部分的内容 kubelet服务配置 编写kubelet.service文件，内容如下： [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/home/anakin/bin/kubelet \ --address=192.168.32.131 \ --hostname-override=192.168.32.131 \ --pod-infra-container-image=registry.anakin.sun.com/imooc/pause-amd64:3.0 \ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \ --network-plugin=cni \ --cni-conf-dir=/etc/cni/net.d \ --cni-bin-dir=/home/anakin/bin \ --cluster-dns=10.68.0.2 \ --cluster-domain=cluster.local. \ --allow-privileged=true \ --fail-swap-on=false \ --logtostderr=true \ --v=2 ExecStartPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT ExecStartPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT ExecStartPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 编写kubelet.kubeconfig文件，内容如下： apiVersion: v1 clusters: - cluster: insecure-skip-tls-verify: true server: http://192.168.32.131:8080 name: kubernetes contexts: - context: cluster: kubernetes user: &quot;&quot; name: system:node:kube-master current-context: system:node:kube-master kind: Config preferences: {} users: [] 编写10-calico.conf文件，内容如下： { &quot;name&quot;: &quot;calico-k8s-network&quot;, &quot;cniVersion&quot;: &quot;0.1.0&quot;, &quot;type&quot;: &quot;calico&quot;, &quot;etcd_endpoints&quot;: &quot;http://192.168.32.131:2379&quot;, &quot;log_level&quot;: &quot;info&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;calico-ipam&quot; }, &quot;kubernetes&quot;: { &quot;k8s_api_root&quot;: &quot;http://192.168.32.131:8080&quot; } } 然后，执行如下命令： mkdir -p /var/lib/kubelet mkdir -p /etc/kubernetes mkdir -p /etc/cni/net.d cp kubelet.service /lib/systemd/system/ cp kubelet.kubeconfig /etc/kubernetes/ cp 10-calico.conf /etc/cni/net.d/ systemctl enable kubelet.service service kubelet start 至此，整个集群应该已经搭建好了，如果中间遇到什么问题，可以通过查看系统日志，或者google解决。" />
<link rel="canonical" href="https://mlh.app/2019/05/20/787563.html" />
<meta property="og:url" content="https://mlh.app/2019/05/20/787563.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-20T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"根据前文准备好的环境，我们现在来一步步的搭建一个基础的k8s集群 注意，这里的配置信息都是按照我自己的虚拟环境来写的。 把server01作为master节点，server02和server03作为worker节点 各个节点需要配置的服务和命令如下： master节点上需要部署的服务包括：etcd服务、APIServer服务、Scheduler服务、ControllerManager服务、CalicoNode服务、kube-proxy服务、kubectl命令 worker节点上需要部署的服务包括：CalicoNode服务、kubelet服务、kube-proxy服务 步骤1：准备文件 安装k8s集群有几种方式可以选择，比如容器化的方式，比如用kubeadmin的方式，这次我们打算尝试的是使用二进制文件的方式。 登录到master虚拟机上（server01），从github上下载安装文件的压缩包，我们使用的是1.13.6版本： wget https://github.com/kubernetes/kubernetes/releases/download/v1.13.6/kubernetes.tar.gz 解压缩 tar zxvf kubernetes.tar.gz 下载文件，进入刚刚解压好的文件夹 cd kubernetes ./cluster/get-kube-binaries.sh 这个步骤因为涉及到从官网下载文件，由于墙的原因会非常缓慢或者失败，请自行上网解决。 步骤2：master环境部署 etcd部署： 编写etcd服务的启动配置文件etcd.service，内容如下： [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target Documentation=https://github.com/coreos [Service] Type=notify WorkingDirectory=/var/lib/etcd/ ExecStart=/home/anakin/bin/etcd \\ --name=192.168.32.131 \\ --listen-client-urls=http://192.168.32.131:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=http://192.168.32.131:2379 \\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 然后执行以下命令： cp etcd.service /lib/systemd/system/ systemctl enable etcd.service mkdir -p /var/lib/etcd service etcd start 如果没有问题的话，etcd服务应该已经跑起来了。 APIServer部署 编写kube-apiserver.service文件，内容如下： [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] ExecStart=/home/anakin/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --insecure-bind-address=0.0.0.0 \\ --kubelet-https=false \\ --service-cluster-ip-range=10.68.0.0/16 \\ --service-node-port-range=20000-40000 \\ --etcd-servers=http://192.168.32.131:2379 \\ --enable-swagger-ui=true \\ --allow-privileged=true \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/lib/audit.log \\ --event-ttl=1h \\ --v=2 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target 然后执行和启动etcd类似的命令 cp kube-apiserver.service /lib/systemd/system/ systemctl enable kube-apiserver.service service kube-apiserver start ControllerManager部署 编写kube-controller-manager.service文件，内容如下： [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/home/anakin/bin/kube-controller-manager \\ --address=127.0.0.1 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.68.0.0/16 \\ --cluster-cidr=172.20.0.0/16 \\ --cluster-name=kubernetes \\ --leader-elect=true \\ --cluster-signing-cert-file= \\ --cluster-signing-key-file= \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 然后，同样的方式启动服务: cp kube-controller-manager.service /lib/systemd/system/ systemctl enable kube-controller-manager.service service kube-controller-manager start Scheduler部署 编写kube-scheduler.service文件，内容如下： [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/home/anakin/bin/kube-scheduler \\ --address=127.0.0.1 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 然后，继续上面的启动方式: cp kube-scheduler.service /lib/systemd/system/ systemctl enable kube-scheduler.service service kube-scheduler start CalicoNode部署 编写kube-calico.service文件，内容如下： [Unit] Description=calico node After=docker.service Requires=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run --net=host --privileged --name=calico-node \\ -e ETCD_ENDPOINTS=http://192.168.32.131:2379 \\ -e CALICO_LIBNETWORK_ENABLED=true \\ -e CALICO_NETWORKING_BACKEND=bird \\ -e CALICO_DISABLE_FILE_LOGGING=true \\ -e CALICO_IPV4POOL_CIDR=172.20.0.0/16 \\ -e CALICO_IPV4POOL_IPIP=off \\ -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \\ -e FELIX_IPV6SUPPORT=false \\ -e FELIX_LOGSEVERITYSCREEN=info \\ -e FELIX_IPINIPMTU=1440 \\ -e FELIX_HEALTHENABLED=true \\ -e IP=192.168.32.131 \\ -v /var/run/calico:/var/run/calico \\ -v /lib/modules:/lib/modules \\ -v /run/docker/plugins:/run/docker/plugins \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /var/log/calico:/var/log/calico \\ registry.anakin.sun.com/k8s/calico-node:v2.6.2 ExecStop=/usr/bin/docker rm -f calico-node Restart=always RestartSec=10 [Install] WantedBy=multi-user.target 同样的方式启动： cp kube-calico.service /lib/systemd/system/ systemctl enable kube-calico.service service kube-calico start kubectl命令配置 执行以下命令(root)： kubectl config set-cluster kubernetes --server=http://192.168.1.102:8080 kubectl config set-context kubernetes --cluster=kubernetes kubectl config use-context kubernetes 如果有问题，可以手动修改配置文件：~/.kube/config kube-proxy服务部署 编写kube-proxy.service文件，内容如下： [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/home/anakin/bin/kube-proxy \\ --bind-address=192.168.32.131 \\ --hostname-override=192.168.32.131 \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 编写kube-proxy.kubeconfig文件，内容如下： apiVersion: v1 clusters: - cluster: server: http://192.168.32.131:8080 name: kubernetes contexts: - context: cluster: kubernetes name: default current-context: default kind: Config preferences: {} users: [] 然后执行如下命令： mkdir -p /var/lib/kube-proxy cp kube-proxy.service /lib/systemd/system/ cp kube-proxy.kubeconfig /etc/kubernetes/ systemctl enable kube-proxy.service OK,至此，master节点应该已经配置完成了。 步骤3：worker环境部署 CalicoNode部署 参考master部分的内容 kube-proxy服务部署 参考master部分的内容 kubelet服务配置 编写kubelet.service文件，内容如下： [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/home/anakin/bin/kubelet \\ --address=192.168.32.131 \\ --hostname-override=192.168.32.131 \\ --pod-infra-container-image=registry.anakin.sun.com/imooc/pause-amd64:3.0 \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d \\ --cni-bin-dir=/home/anakin/bin \\ --cluster-dns=10.68.0.2 \\ --cluster-domain=cluster.local. \\ --allow-privileged=true \\ --fail-swap-on=false \\ --logtostderr=true \\ --v=2 ExecStartPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT ExecStartPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT ExecStartPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 编写kubelet.kubeconfig文件，内容如下： apiVersion: v1 clusters: - cluster: insecure-skip-tls-verify: true server: http://192.168.32.131:8080 name: kubernetes contexts: - context: cluster: kubernetes user: &quot;&quot; name: system:node:kube-master current-context: system:node:kube-master kind: Config preferences: {} users: [] 编写10-calico.conf文件，内容如下： { &quot;name&quot;: &quot;calico-k8s-network&quot;, &quot;cniVersion&quot;: &quot;0.1.0&quot;, &quot;type&quot;: &quot;calico&quot;, &quot;etcd_endpoints&quot;: &quot;http://192.168.32.131:2379&quot;, &quot;log_level&quot;: &quot;info&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;calico-ipam&quot; }, &quot;kubernetes&quot;: { &quot;k8s_api_root&quot;: &quot;http://192.168.32.131:8080&quot; } } 然后，执行如下命令： mkdir -p /var/lib/kubelet mkdir -p /etc/kubernetes mkdir -p /etc/cni/net.d cp kubelet.service /lib/systemd/system/ cp kubelet.kubeconfig /etc/kubernetes/ cp 10-calico.conf /etc/cni/net.d/ systemctl enable kubelet.service service kubelet start 至此，整个集群应该已经搭建好了，如果中间遇到什么问题，可以通过查看系统日志，或者google解决。","@type":"BlogPosting","url":"https://mlh.app/2019/05/20/787563.html","headline":"手动搭建kubernetes集群（二）","dateModified":"2019-05-20T00:00:00+08:00","datePublished":"2019-05-20T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/20/787563.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>手动搭建kubernetes集群（二）</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p>根据前文准备好的环境，我们现在来一步步的搭建一个基础的k8s集群<br><br> <code>注意，这里的配置信息都是按照我自己的虚拟环境来写的。</code><br><br> 把server01作为master节点，server02和server03作为worker节点<br><br> 各个节点需要配置的服务和命令如下：<br><br> master节点上需要部署的服务包括：<code>etcd服务</code>、<code>APIServer服务</code>、<code>Scheduler服务</code>、<code>ControllerManager服务</code>、<code>CalicoNode服务</code>、<code>kube-proxy服务</code>、<code>kubectl命令</code></p> 
  <p>worker节点上需要部署的服务包括：<code>CalicoNode服务</code>、<code>kubelet服务</code>、<code>kube-proxy服务</code></p> 
  <h2><a id="1_8"></a>步骤1：准备文件</h2> 
  <p>安装k8s集群有几种方式可以选择，比如容器化的方式，比如用kubeadmin的方式，这次我们打算尝试的是使用二进制文件的方式。<br></p> 
  <ol> 
   <li> <p>登录到master虚拟机上（server01），从github上下载安装文件的压缩包，我们使用的是1.13.6版本：</p> <pre><code> wget https://github.com/kubernetes/kubernetes/releases/download/v1.13.6/kubernetes.tar.gz
</code></pre> </li> 
   <li> <p>解压缩</p> <pre><code>tar zxvf kubernetes.tar.gz
</code></pre> </li> 
   <li> <p>下载文件，进入刚刚解压好的文件夹</p> <pre><code>cd kubernetes
./cluster/get-kube-binaries.sh
</code></pre> <p>这个步骤因为涉及到从官网下载文件，由于墙的原因会非常缓慢或者失败，请自行上网解决。</p> </li> 
  </ol> 
  <h2><a id="2master_27"></a>步骤2：master环境部署</h2> 
  <ol> 
   <li>etcd部署：<br><br> 编写etcd服务的启动配置文件etcd.service，内容如下：</li> 
  </ol> 
  <pre><code>[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/home/anakin/bin/etcd \
  --name=192.168.32.131 \
  --listen-client-urls=http://192.168.32.131:2379,http://127.0.0.1:2379 \
  --advertise-client-urls=http://192.168.32.131:2379 \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre> 
  <p>然后执行以下命令：</p> 
  <pre><code>cp etcd.service /lib/systemd/system/
systemctl enable etcd.service
mkdir -p /var/lib/etcd
service etcd start
</code></pre> 
  <p>如果没有问题的话，etcd服务应该已经跑起来了。</p> 
  <ol start="2"> 
   <li>APIServer部署<br><br> 编写kube-apiserver.service文件，内容如下：</li> 
  </ol> 
  <pre><code>[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
ExecStart=/home/anakin/bin/kube-apiserver \
  --admission-control=NamespaceLifecycle,LimitRanger,DefaultStorageClass,ResourceQuota,NodeRestriction \
  --insecure-bind-address=0.0.0.0 \
  --kubelet-https=false \
  --service-cluster-ip-range=10.68.0.0/16 \
  --service-node-port-range=20000-40000 \
  --etcd-servers=http://192.168.32.131:2379 \
  --enable-swagger-ui=true \
  --allow-privileged=true \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/lib/audit.log \
  --event-ttl=1h \
  --v=2
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
</code></pre> 
  <p>然后执行和启动etcd类似的命令</p> 
  <pre><code>cp kube-apiserver.service /lib/systemd/system/
systemctl enable kube-apiserver.service
service kube-apiserver start
</code></pre> 
  <ol start="3"> 
   <li>ControllerManager部署<br><br> 编写kube-controller-manager.service文件，内容如下：</li> 
  </ol> 
  <pre><code>[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
ExecStart=/home/anakin/bin/kube-controller-manager \
  --address=127.0.0.1 \
  --master=http://127.0.0.1:8080 \
  --allocate-node-cidrs=true \
  --service-cluster-ip-range=10.68.0.0/16 \
  --cluster-cidr=172.20.0.0/16 \
  --cluster-name=kubernetes \
  --leader-elect=true \
  --cluster-signing-cert-file= \
  --cluster-signing-key-file= \
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
</code></pre> 
  <p>然后，同样的方式启动服务:</p> 
  <pre><code>cp kube-controller-manager.service /lib/systemd/system/
systemctl enable kube-controller-manager.service
service kube-controller-manager start
</code></pre> 
  <ol start="4"> 
   <li>Scheduler部署<br><br> 编写kube-scheduler.service文件，内容如下：</li> 
  </ol> 
  <pre><code>[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/home/anakin/bin/kube-scheduler \
  --address=127.0.0.1 \
  --master=http://127.0.0.1:8080 \
  --leader-elect=true \
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
</code></pre> 
  <p>然后，继续上面的启动方式:</p> 
  <pre><code>cp kube-scheduler.service /lib/systemd/system/
systemctl enable kube-scheduler.service
service kube-scheduler start
</code></pre> 
  <ol start="5"> 
   <li>CalicoNode部署<br> 编写kube-calico.service文件，内容如下：</li> 
  </ol> 
  <pre><code>[Unit]
Description=calico node
After=docker.service
Requires=docker.service

[Service]
User=root
PermissionsStartOnly=true
ExecStart=/usr/bin/docker run --net=host --privileged --name=calico-node \
  -e ETCD_ENDPOINTS=http://192.168.32.131:2379 \
  -e CALICO_LIBNETWORK_ENABLED=true \
  -e CALICO_NETWORKING_BACKEND=bird \
  -e CALICO_DISABLE_FILE_LOGGING=true \
  -e CALICO_IPV4POOL_CIDR=172.20.0.0/16 \
  -e CALICO_IPV4POOL_IPIP=off \
  -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \
  -e FELIX_IPV6SUPPORT=false \
  -e FELIX_LOGSEVERITYSCREEN=info \
  -e FELIX_IPINIPMTU=1440 \
  -e FELIX_HEALTHENABLED=true \
  -e IP=192.168.32.131 \
  -v /var/run/calico:/var/run/calico \
  -v /lib/modules:/lib/modules \
  -v /run/docker/plugins:/run/docker/plugins \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v /var/log/calico:/var/log/calico \
  registry.anakin.sun.com/k8s/calico-node:v2.6.2
  ExecStop=/usr/bin/docker rm -f calico-node
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
</code></pre> 
  <p>同样的方式启动：</p> 
  <pre><code>cp kube-calico.service /lib/systemd/system/
systemctl enable kube-calico.service
service kube-calico start
</code></pre> 
  <ol start="6"> 
   <li>kubectl命令配置<br> 执行以下命令(root)：</li> 
  </ol> 
  <pre><code>kubectl config set-cluster kubernetes  --server=http://192.168.1.102:8080
kubectl config set-context kubernetes --cluster=kubernetes
kubectl config use-context kubernetes
</code></pre> 
  <p>如果有问题，可以手动修改配置文件：~/.kube/config</p> 
  <ol start="7"> 
   <li>kube-proxy服务部署<br> 编写kube-proxy.service文件，内容如下：</li> 
  </ol> 
  <pre><code>[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/home/anakin/bin/kube-proxy \
  --bind-address=192.168.32.131 \
  --hostname-override=192.168.32.131 \
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
  --logtostderr=true \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
</code></pre> 
  <p>编写kube-proxy.kubeconfig文件，内容如下：</p> 
  <pre><code>apiVersion: v1
clusters:
- cluster:
    server: http://192.168.32.131:8080
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
  name: default
current-context: default
kind: Config
preferences: {}
users: []
</code></pre> 
  <p>然后执行如下命令：</p> 
  <pre><code>mkdir -p /var/lib/kube-proxy
cp kube-proxy.service /lib/systemd/system/
cp kube-proxy.kubeconfig /etc/kubernetes/
systemctl enable kube-proxy.service
</code></pre> 
  <p>OK,至此，master节点应该已经配置完成了。</p> 
  <h2><a id="3worker_252"></a>步骤3：worker环境部署</h2> 
  <ol> 
   <li> <p>CalicoNode部署<br><br> 参考master部分的内容</p> </li> 
   <li> <p>kube-proxy服务部署<br><br> 参考master部分的内容</p> </li> 
   <li> <p>kubelet服务配置<br><br> 编写kubelet.service文件，内容如下：</p> </li> 
  </ol> 
  <pre><code>[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/home/anakin/bin/kubelet \
  --address=192.168.32.131 \
  --hostname-override=192.168.32.131 \
  --pod-infra-container-image=registry.anakin.sun.com/imooc/pause-amd64:3.0 \
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
  --network-plugin=cni \
  --cni-conf-dir=/etc/cni/net.d \
  --cni-bin-dir=/home/anakin/bin \
  --cluster-dns=10.68.0.2 \
  --cluster-domain=cluster.local. \
  --allow-privileged=true \
  --fail-swap-on=false \
  --logtostderr=true \
  --v=2
ExecStartPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
</code></pre> 
  <p>编写kubelet.kubeconfig文件，内容如下：</p> 
  <pre><code>apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: http://192.168.32.131:8080
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: ""
  name: system:node:kube-master
current-context: system:node:kube-master
kind: Config
preferences: {}
users: []
</code></pre> 
  <p>编写10-calico.conf文件，内容如下：</p> 
  <pre><code>{
    "name": "calico-k8s-network",
    "cniVersion": "0.1.0",
    "type": "calico",
    "etcd_endpoints": "http://192.168.32.131:2379",
    "log_level": "info",
    "ipam": {
        "type": "calico-ipam"
    },
    "kubernetes": {
        "k8s_api_root": "http://192.168.32.131:8080"
    }
}
</code></pre> 
  <p>然后，执行如下命令：</p> 
  <pre><code>mkdir -p /var/lib/kubelet
mkdir -p /etc/kubernetes
mkdir -p /etc/cni/net.d

cp kubelet.service /lib/systemd/system/
cp kubelet.kubeconfig /etc/kubernetes/
cp 10-calico.conf /etc/cni/net.d/

systemctl enable kubelet.service
service kubelet start
</code></pre> 
  <p>至此，整个集群应该已经搭建好了，如果中间遇到什么问题，可以通过查看系统日志，或者google解决。</p> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
