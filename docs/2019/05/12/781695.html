<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>机器学习与数据挖掘的学习路线图 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="机器学习与数据挖掘的学习路线图" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 应部分朋友要求，特奉上“机器学习与数据挖掘的学习路线图”，供有兴趣的读者研究。 说起机器学习和数据挖掘，当然两者并不完全等同。如果想简单的理清二者的关系，不妨这样来理解，机器学习应用在数据分析领域 = 数据挖掘。同理，如果将机器学习应用在图像处理领域 = 机器视觉。当然这只是一种比较直白的理解，并不能见得绝对准确或者全面。我们权且这样处理。而且在本文后面若提到这两个名词，我们所表示的意思是一致的。 但无论是机器学习，还是数据挖掘，你一定听说过很多很多，名字叼炸天的传说中的，“算法”，比如：SVM，神经网络，Logistic回归，决策树、EM、HMM、贝叶斯网络、随机森林、LDA... ....其实还是很多很多！无论你排十大算法还是二十大算法，总感觉只触及到了冰山一角！真是学海无涯啊- -!! 如果你想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作。教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 另外一种该类型的书，会把算法按照它们的实现的功能和目的，分成比如 Regression、Classification、Clustering等等等等的几类，然后各种讲可以实现聚类的算法有A、B、C，可以实现回归的有D、E、F。。。而且我们也知道，机器学习又可分为有监督、无监督以及半监督的，或者又可分为贝叶斯派和概率派两大阵营，所以按类别来介绍其中的算法也是一种很常见的思路。 这样的书代表作是Pang-Ning Tan, Michael Steinbach 和Vipin Kumar的那本《数据挖掘导论》，这样的书基本上对于构建一个大概的机器学习体系还是有裨益的。但是就初学者而言，其实这个体系还可以再优化。这也是我根据个人的一些经验想向各位介绍的一个基本的学习路线图，在我看来知识应该是有联系的，而不是孤立的， 找到这种内部隐藏的线索就如同获得了阿里巴巴的口诀，才能开启更大的宝藏。 当然，正式学习之前，你所需要的预备知识（主要是数学）应该包括：微积分（偏导数、梯度等等）、概率论与数理统计（例如极大似然估计、中央极限定理、大数法则等等）、最优化方法（比如梯度下降、牛顿-拉普什方法、变分法（欧拉-拉格朗日方程）、凸优化等等）——如果你对其中的某些名词感到陌生，那么就说明你尚不具备深入开展数据挖掘算法学习的能力。你会发现到处都是门槛，很难继续进行下去。 第一条线路： （基于普通最小二乘法的）简单线性回归 -&gt; 线性回归中的新进展（岭回归和LASSO回归）-&gt;(此处可以插入Bagging和AdaBoost的内容)-&gt; Logistic回归 -&gt;支持向量机（SVM）-&gt;感知机学习 -&gt; 神经网络（初学者可先主要关注BP算法）-&gt; 深度学习 之所以把它们归为一条线路，因为所有这些算法都是围绕着 y = Σxiβi，这样一条简单的公式展开的，如果你抓住这条线索，不断探索下去，就算是抓住它们之间的绳索了。其中蓝色部分主要是回归，绿色部分主要是有监督的分类学习法。 基于普通最小二乘的线性回归是统计中一种有着非常悠久历史的方法，它的使用甚至可以追溯到高斯的时代。但是它对数据有诸多要求，例如特征之间不能有多重共线性，而且岭回归和LASSO就是对这些问题的修正。 当沿着第一条路线学完的时候，其实你已经攻克机器学习的半壁江山了！当然，在这个过程中，你一定时刻问问自己后一个算法与前一个的联系在哪里?最初，人们从哪里出发，才会如此设计出它们的。 第二条路线： K-means &nbsp;-&gt;&nbsp;EM &nbsp;-&gt;&nbsp;朴素贝叶斯 -&gt; 贝叶斯网络 -&gt; 隐马尔科夫模型（基本模型，前向算法，维特比算法，前向-后向算法） （-&gt;卡尔曼滤波） 这条线路所涉及的基本都是那些各种画来画去的图模型，一个学术名词是 PGM 。这条线的思路和第一条是截然不同的！贝叶斯网络、HMM（隐马尔科夫模型），也就是绿色字体的部分是这个线路中的核心内容。而蓝色部分是为绿色内容做准备的部分。K-means 和 EM 具有与生俱来的联系，认识到这一点才能说明你真正读懂了它们。而EM算法要在HMM的模型训练中用到，所以你要先学EM才能深入学习HMM。所以尽管在EM中看不到那种画来画去的图模型，但我还把它放在了这条线路中，这也就是原因所在。朴素贝叶斯里面的很多内容在，贝叶斯网络和HMM里都会用到，类似贝叶斯定理，先验和后验概率，边缘分布等等（主要是概念性的）。最后，卡尔曼滤波可以作为HMM的一直深入或者后续扩展。尽管很多machine learning的书里没把它看做是一种机器学习算法（或许那些作者认为它应该是信号处理中的内容），但是它也确实可以被看成是一种机器学习技术。而且参考文献[4]中，作者也深刻地揭示了它与HMM之间的紧密联系，所以红色的部分可以作为HMM的后续扩展延伸内容&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 应部分朋友要求，特奉上“机器学习与数据挖掘的学习路线图”，供有兴趣的读者研究。 说起机器学习和数据挖掘，当然两者并不完全等同。如果想简单的理清二者的关系，不妨这样来理解，机器学习应用在数据分析领域 = 数据挖掘。同理，如果将机器学习应用在图像处理领域 = 机器视觉。当然这只是一种比较直白的理解，并不能见得绝对准确或者全面。我们权且这样处理。而且在本文后面若提到这两个名词，我们所表示的意思是一致的。 但无论是机器学习，还是数据挖掘，你一定听说过很多很多，名字叼炸天的传说中的，“算法”，比如：SVM，神经网络，Logistic回归，决策树、EM、HMM、贝叶斯网络、随机森林、LDA... ....其实还是很多很多！无论你排十大算法还是二十大算法，总感觉只触及到了冰山一角！真是学海无涯啊- -!! 如果你想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作。教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 另外一种该类型的书，会把算法按照它们的实现的功能和目的，分成比如 Regression、Classification、Clustering等等等等的几类，然后各种讲可以实现聚类的算法有A、B、C，可以实现回归的有D、E、F。。。而且我们也知道，机器学习又可分为有监督、无监督以及半监督的，或者又可分为贝叶斯派和概率派两大阵营，所以按类别来介绍其中的算法也是一种很常见的思路。 这样的书代表作是Pang-Ning Tan, Michael Steinbach 和Vipin Kumar的那本《数据挖掘导论》，这样的书基本上对于构建一个大概的机器学习体系还是有裨益的。但是就初学者而言，其实这个体系还可以再优化。这也是我根据个人的一些经验想向各位介绍的一个基本的学习路线图，在我看来知识应该是有联系的，而不是孤立的， 找到这种内部隐藏的线索就如同获得了阿里巴巴的口诀，才能开启更大的宝藏。 当然，正式学习之前，你所需要的预备知识（主要是数学）应该包括：微积分（偏导数、梯度等等）、概率论与数理统计（例如极大似然估计、中央极限定理、大数法则等等）、最优化方法（比如梯度下降、牛顿-拉普什方法、变分法（欧拉-拉格朗日方程）、凸优化等等）——如果你对其中的某些名词感到陌生，那么就说明你尚不具备深入开展数据挖掘算法学习的能力。你会发现到处都是门槛，很难继续进行下去。 第一条线路： （基于普通最小二乘法的）简单线性回归 -&gt; 线性回归中的新进展（岭回归和LASSO回归）-&gt;(此处可以插入Bagging和AdaBoost的内容)-&gt; Logistic回归 -&gt;支持向量机（SVM）-&gt;感知机学习 -&gt; 神经网络（初学者可先主要关注BP算法）-&gt; 深度学习 之所以把它们归为一条线路，因为所有这些算法都是围绕着 y = Σxiβi，这样一条简单的公式展开的，如果你抓住这条线索，不断探索下去，就算是抓住它们之间的绳索了。其中蓝色部分主要是回归，绿色部分主要是有监督的分类学习法。 基于普通最小二乘的线性回归是统计中一种有着非常悠久历史的方法，它的使用甚至可以追溯到高斯的时代。但是它对数据有诸多要求，例如特征之间不能有多重共线性，而且岭回归和LASSO就是对这些问题的修正。 当沿着第一条路线学完的时候，其实你已经攻克机器学习的半壁江山了！当然，在这个过程中，你一定时刻问问自己后一个算法与前一个的联系在哪里?最初，人们从哪里出发，才会如此设计出它们的。 第二条路线： K-means &nbsp;-&gt;&nbsp;EM &nbsp;-&gt;&nbsp;朴素贝叶斯 -&gt; 贝叶斯网络 -&gt; 隐马尔科夫模型（基本模型，前向算法，维特比算法，前向-后向算法） （-&gt;卡尔曼滤波） 这条线路所涉及的基本都是那些各种画来画去的图模型，一个学术名词是 PGM 。这条线的思路和第一条是截然不同的！贝叶斯网络、HMM（隐马尔科夫模型），也就是绿色字体的部分是这个线路中的核心内容。而蓝色部分是为绿色内容做准备的部分。K-means 和 EM 具有与生俱来的联系，认识到这一点才能说明你真正读懂了它们。而EM算法要在HMM的模型训练中用到，所以你要先学EM才能深入学习HMM。所以尽管在EM中看不到那种画来画去的图模型，但我还把它放在了这条线路中，这也就是原因所在。朴素贝叶斯里面的很多内容在，贝叶斯网络和HMM里都会用到，类似贝叶斯定理，先验和后验概率，边缘分布等等（主要是概念性的）。最后，卡尔曼滤波可以作为HMM的一直深入或者后续扩展。尽管很多machine learning的书里没把它看做是一种机器学习算法（或许那些作者认为它应该是信号处理中的内容），但是它也确实可以被看成是一种机器学习技术。而且参考文献[4]中，作者也深刻地揭示了它与HMM之间的紧密联系，所以红色的部分可以作为HMM的后续扩展延伸内容&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/12/781695.html" />
<meta property="og:url" content="https://mlh.app/2019/05/12/781695.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 应部分朋友要求，特奉上“机器学习与数据挖掘的学习路线图”，供有兴趣的读者研究。 说起机器学习和数据挖掘，当然两者并不完全等同。如果想简单的理清二者的关系，不妨这样来理解，机器学习应用在数据分析领域 = 数据挖掘。同理，如果将机器学习应用在图像处理领域 = 机器视觉。当然这只是一种比较直白的理解，并不能见得绝对准确或者全面。我们权且这样处理。而且在本文后面若提到这两个名词，我们所表示的意思是一致的。 但无论是机器学习，还是数据挖掘，你一定听说过很多很多，名字叼炸天的传说中的，“算法”，比如：SVM，神经网络，Logistic回归，决策树、EM、HMM、贝叶斯网络、随机森林、LDA... ....其实还是很多很多！无论你排十大算法还是二十大算法，总感觉只触及到了冰山一角！真是学海无涯啊- -!! 如果你想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作。教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 另外一种该类型的书，会把算法按照它们的实现的功能和目的，分成比如 Regression、Classification、Clustering等等等等的几类，然后各种讲可以实现聚类的算法有A、B、C，可以实现回归的有D、E、F。。。而且我们也知道，机器学习又可分为有监督、无监督以及半监督的，或者又可分为贝叶斯派和概率派两大阵营，所以按类别来介绍其中的算法也是一种很常见的思路。 这样的书代表作是Pang-Ning Tan, Michael Steinbach 和Vipin Kumar的那本《数据挖掘导论》，这样的书基本上对于构建一个大概的机器学习体系还是有裨益的。但是就初学者而言，其实这个体系还可以再优化。这也是我根据个人的一些经验想向各位介绍的一个基本的学习路线图，在我看来知识应该是有联系的，而不是孤立的， 找到这种内部隐藏的线索就如同获得了阿里巴巴的口诀，才能开启更大的宝藏。 当然，正式学习之前，你所需要的预备知识（主要是数学）应该包括：微积分（偏导数、梯度等等）、概率论与数理统计（例如极大似然估计、中央极限定理、大数法则等等）、最优化方法（比如梯度下降、牛顿-拉普什方法、变分法（欧拉-拉格朗日方程）、凸优化等等）——如果你对其中的某些名词感到陌生，那么就说明你尚不具备深入开展数据挖掘算法学习的能力。你会发现到处都是门槛，很难继续进行下去。 第一条线路： （基于普通最小二乘法的）简单线性回归 -&gt; 线性回归中的新进展（岭回归和LASSO回归）-&gt;(此处可以插入Bagging和AdaBoost的内容)-&gt; Logistic回归 -&gt;支持向量机（SVM）-&gt;感知机学习 -&gt; 神经网络（初学者可先主要关注BP算法）-&gt; 深度学习 之所以把它们归为一条线路，因为所有这些算法都是围绕着 y = Σxiβi，这样一条简单的公式展开的，如果你抓住这条线索，不断探索下去，就算是抓住它们之间的绳索了。其中蓝色部分主要是回归，绿色部分主要是有监督的分类学习法。 基于普通最小二乘的线性回归是统计中一种有着非常悠久历史的方法，它的使用甚至可以追溯到高斯的时代。但是它对数据有诸多要求，例如特征之间不能有多重共线性，而且岭回归和LASSO就是对这些问题的修正。 当沿着第一条路线学完的时候，其实你已经攻克机器学习的半壁江山了！当然，在这个过程中，你一定时刻问问自己后一个算法与前一个的联系在哪里?最初，人们从哪里出发，才会如此设计出它们的。 第二条路线： K-means &nbsp;-&gt;&nbsp;EM &nbsp;-&gt;&nbsp;朴素贝叶斯 -&gt; 贝叶斯网络 -&gt; 隐马尔科夫模型（基本模型，前向算法，维特比算法，前向-后向算法） （-&gt;卡尔曼滤波） 这条线路所涉及的基本都是那些各种画来画去的图模型，一个学术名词是 PGM 。这条线的思路和第一条是截然不同的！贝叶斯网络、HMM（隐马尔科夫模型），也就是绿色字体的部分是这个线路中的核心内容。而蓝色部分是为绿色内容做准备的部分。K-means 和 EM 具有与生俱来的联系，认识到这一点才能说明你真正读懂了它们。而EM算法要在HMM的模型训练中用到，所以你要先学EM才能深入学习HMM。所以尽管在EM中看不到那种画来画去的图模型，但我还把它放在了这条线路中，这也就是原因所在。朴素贝叶斯里面的很多内容在，贝叶斯网络和HMM里都会用到，类似贝叶斯定理，先验和后验概率，边缘分布等等（主要是概念性的）。最后，卡尔曼滤波可以作为HMM的一直深入或者后续扩展。尽管很多machine learning的书里没把它看做是一种机器学习算法（或许那些作者认为它应该是信号处理中的内容），但是它也确实可以被看成是一种机器学习技术。而且参考文献[4]中，作者也深刻地揭示了它与HMM之间的紧密联系，所以红色的部分可以作为HMM的后续扩展延伸内容&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/12/781695.html","headline":"机器学习与数据挖掘的学习路线图","dateModified":"2019-05-12T00:00:00+08:00","datePublished":"2019-05-12T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/12/781695.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>机器学习与数据挖掘的学习路线图</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div class="markdown_views prism-tomorrow-night" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <div class="htmledit_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <p><span>应部分朋友要求，特奉上“机器学习与数据挖掘的学习路线图”，供有兴趣的读者研究。</span></p>
    <p><span>说起机器学习和数据挖掘，当然两者并不完全等同。如果想简单的理清二者的关系，不妨这样来理解，机器学习应用在数据分析领域 = 数据挖掘。同理，如果将机器学习应用在图像处理领域 = 机器视觉。当然这只是一种比较直白的理解，并不能见得绝对准确或者全面。我们权且这样处理。而且在本文后面若提到这两个名词，我们所表示的意思是一致的。</span></p>
    <p><span>但无论是机器学习，还是数据挖掘，你一定听说过很多很多，名字叼炸天的传说中的，“算法”，比如：SVM，神经网络，Logistic回归，决策树、EM、HMM、贝叶斯网络、随机森林、LDA... ....其实还是很多很多！无论你排十大算法还是二十大算法，总感觉只触及到了冰山一角！真是学海无涯啊- -!!</span></p>
   </div>
   <p>如果你想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作。教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <p><span>另外一种该类型的书，会把算法按照它们的实现的功能和目的，分成比如 Regression、Classification、Clustering等等等等的几类，然后各种讲可以实现聚类的算法有A、B、C，可以实现回归的有D、E、F。。。而且我们也知道，机器学习又可分为有监督、无监督以及半监督的，或者又可分为贝叶斯派和概率派两大阵营，所以按类别来介绍其中的算法也是一种很常见的思路。</span></p>
   <p><span>这样的书代表作是Pang-Ning Tan, Michael Steinbach 和Vipin Kumar的那本《数据挖掘导论》，这样的书基本上对于构建一个大概的机器学习体系还是有裨益的。但是就初学者而言，其实这个体系还可以再优化。这也是我根据个人的一些经验想向各位介绍的一个基本的学习路线图，在我看来知识应该是有联系的，而不是孤立的， 找到这种内部隐藏的线索就如同获得了阿里巴巴的口诀，才能开启更大的宝藏。</span></p>
   <p><span>当然，正式学习之前，你所需要的预备知识（主要是数学）应该包括：微积分（偏导数、梯度等等）、概率论与数理统计（例如极大似然估计、中央极限定理、大数法则等等）、最优化方法（比如梯度下降、牛顿-拉普什方法、变分法（欧拉-拉格朗日方程）、凸优化等等）——如果你对其中的某些名词感到陌生，那么就说明你尚不具备深入开展数据挖掘算法学习的能力。你会发现到处都是门槛，很难继续进行下去。</span></p>
   <p><span>第一条线路：</span></p>
   <p><span><span>（基于普通最小二乘法的）简单线性回归 </span>-&gt;<span> 线性回归中的新进展（岭回归和LASSO回归）</span>-&gt;<span>(此处可以插入Bagging和AdaBoost的内容)</span>-&gt; <span>Logistic回归</span> -&gt;<span>支持向量机（SVM）</span>-&gt;<span>感知机学习</span> -&gt; <span>神经网络（初学者可先主要关注BP算法）</span>-&gt; <span>深度学习</span></span></p>
   <p><span>之所以把它们归为一条线路，因为所有这些算法都是围绕着 y = Σx</span><span>i</span><span>β</span><span>i</span><span>，这样一条简单的公式展开的，如果你抓住这条线索，不断探索下去，就算是抓住它们之间的绳索了。</span><span>其中蓝色部分主要是回归，绿色部分主要是有监督的分类学习法。</span></p>
   <p><span>基于普通最小二乘的线性回归是统计中一种有着非常悠久历史的方法，它的使用甚至可以追溯到高斯的时代。但是它对数据有诸多要求，例如特征之间不能有多重共线性，而且岭回归和LASSO就是对这些问题的修正。</span></p>
   <p><span>当沿着第一条路线学完的时候，其实你已经攻克机器学习的半壁江山了！当然，在这个过程中，你一定时刻问问自己后一个算法与前一个的联系在哪里?最初，人们从哪里出发，才会如此设计出它们的。</span></p>
   <p><span>第二条路线：</span></p>
   <p><span><span>K-means </span><span>&nbsp;</span><span>-&gt;</span><span>&nbsp;</span><span>EM </span><span><span>&nbsp;</span>-&gt;<span>&nbsp;</span></span><span>朴素贝叶斯</span> -&gt; <span>贝叶斯网络</span> -&gt; <span>隐马尔科夫模型</span>（基本模型，前向算法，维特比算法，前向-后向算法） （-&gt;<span>卡尔曼滤波</span>）</span></p>
   <p><span>这条线路所涉及的基本都是那些各种画来画去的图模型，一个学术名词是 PGM 。这条线的思路和第一条是截然不同的！贝叶斯网络、HMM（隐马尔科夫模型），也就是绿色字体的部分是这个线路中的核心内容。而蓝色部分是为绿色内容做准备的部分。K-means 和 EM 具有与生俱来的联系，认识到这一点才能说明你真正读懂了它们。而EM算法要在HMM的模型训练中用到，所以你要先学EM才能深入学习HMM。所以尽管在EM中看不到那种画来画去的图模型，但我还把它放在了这条线路中，这也就是原因所在。朴素贝叶斯里面的很多内容在，贝叶斯网络和HMM里都会用到，类似贝叶斯定理，先验和后验概率，边缘分布等等（主要是概念性的）。最后，卡尔曼滤波可以作为HMM的一直深入或者后续扩展。尽管很多machine learning的书里没把它看做是一种机器学习算法（或许那些作者认为它应该是信号处理中的内容），但是它也确实可以被看成是一种机器学习技术。而且参考文献[4]中，作者也深刻地揭示了它与HMM之间的紧密联系，所以红色的部分可以作为HMM的后续扩展延伸内容&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
