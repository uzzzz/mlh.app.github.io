<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>深度学习 图像分类入门，从VGG16卷积神经网络开始 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="深度学习 图像分类入门，从VGG16卷积神经网络开始" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 刚开始接触深度学习、卷积神经网络的时候非常懵逼，不知道从何入手，我觉得应该有一个进阶的过程，也就是说，理应有一些基本概念作为奠基石，让你有底气去完全理解一个庞大的卷积神经网络： 本文思路： 一、我认为学习卷积神经网络必须知道的几个概念： 1、卷积过程： &nbsp; 我们经常说卷积神经网络卷积神经网络，到底什么才是卷积？网络层卷积过程到底怎么实现？我们在这里借鉴了另一位博客大牛的动态图来给大家演示一下， 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 我们可以看到，卷积过程其实还是基于一个固定的矩阵，在另外一个矩阵不断一格一格扫过去的到的数值的和，（注意：这里的一格一格非常重要，因为涉及后面的概念：步长→我们不妨想一想当固定矩阵不是一格一格前进的时候，会发生什么呢？）产生的一个新的矩阵，我们以作为比较会发现：粉红色矩阵和绿色矩阵在根本上有很大不一样， 第一，卷积之后的维数降低了；第二，我们要想想为什么降维了？（思考：降低维度到底有没有规律？） &nbsp; 答案是有的：我们发现橙色的固定框为3*3，绿色是5*5，出来是三乘三； &nbsp; 所以规律可以得到：粉红色最后的卷积结果矩阵维度=绿色矩阵维数-橙色矩阵维数+1 &nbsp; （我们又应该思考：如果我不想最后减少维度，我只希望卷积，怎么办呢？） 2、两层之间的池化： &nbsp; 我们依然延用博客大牛的另一个动图（再次点赞做的精细准确！） 我们可以发现其实跟之前没什么不一样：还是以三个矩阵之间的运算，但是我们很容易发现，它并不是一行一行扫过去的，橙色矩阵维度是黄色矩阵的整数倍，所以池化的最终的结论是要把原来的维度减少到1/n.这是池化最根本的原理（当然也有特殊情况。） （思考点：我们想象一下如果一个19*19的矩阵做池化，会是一种什么样的体验呢？我们不可以缩小整数倍！！答案会在后面的VGG16里面讲清楚，不急不急） 3、第三个知识点是步长的概念： &nbsp; 卷积核（后面讲到VGG16会介绍）移动的步长（stride）小于卷积核的边长（一般为正方行）时，变会出现卷积核与原始输入矩阵作用范围在区域上的重叠（overlap），卷积核移动的步长（stride）与卷积核的边长相一致时，不会出现重叠现象。 &nbsp; 通俗一点其实就是：刚刚说的那个粉红色矩阵，他每一次移动多少格，格子就是步长！！ 4、卷积核： &nbsp; 一个听起来很高大上的词语，我们依然用之前的基础来解释：通俗易懂：就是粉红色矩阵的个数！！因为有时候我们要提取的特征非常多非常广泛，所以需要我们用更多的矩阵来扫（多扫几遍），那么粉红色矩阵的个数就是卷积核个数。 5、Padding: &nbsp; 这个应该是最抽象的概念了：但是也不会特别难呢，就是我们在之前讲到第一点：卷积的时候，我抛下了一个问题： &nbsp;（我们又应该思考：如果我不想最后减少维度，我只希望卷积，怎么办呢？）（现在知道括号的重要性了吧哈哈？） &nbsp; 现在我们来解决这个问题：比如：我们需要做一个300*300的原始矩阵，用一个3*3卷积核（粉红色矩阵）来扫，扫出来，按照之前公式，结果的矩阵应该是：298*298的矩阵，但是这样很难计算，减得也不多，反而增加我计算难度，还不如池化（pooling）来得干脆是吧！那我们就在300*300矩阵外面周围加一圈“0”，记住，是在外面外包一层“0” 重点是：这样的300*300就变成了302*302的矩阵，这样就可以完全避开卷积后那两层的抵消。 6、还有一个就是通道的概念：这个不算知识点，仅仅是一个常识词语，比如一张图片，有RGB三种颜色，对应三个灰度级别，也就是三个通道了： 更加抽象的图可以参照下面的结构： 二、等待已久的VGG16： VGG16分为16层，我们主要讲前面的前几层（越详细越好吧，后面是一样的） ——首先教会大家一个看其他神经网络也是用的办法：官方数据表格： 看懂一些式子表达： Conv3-512 &nbsp; → &nbsp; &nbsp;第三层卷积后维度变成512； Conv3_2 s=2 &nbsp; &nbsp; → &nbsp; &nbsp; 第三层卷积层里面的第二子层，滑动步长等于2（每次移动两个格子） 好了，我们有了以上的知识可以考试剖析VGG16卷积神经网络了 三、利用之前的基本概念来解释深层的VGG16卷及网络； 【1、从INPUT到Conv1：】 首先两个黄色的是卷积层，是VGG16网络结构十六层当中的第一层（Conv1_1）和第二层（Conv1_2），他们合称为Conv1。 我们主要讲述第一个，也就是第一层（Conv1_1），它怎么把一个300*300*3的矩阵变成一个300*300*64的矩阵？ 我们假设蓝色框是一个RGB图像，橙色是一个3*3*3的卷积核，我们对一个三维的27个数求和，然后扫过去，按照第一部分算的得出来的是一维的298*298的矩阵（因为卷积核也是三维所以结果是一维）； 然后回想一下什么是Padding、前面也讲过它的概念了；所以不了一圈的圆，回到了300*300*1； 然后，VGG16这一层安置有64个卷积核，那么，原来的300*300*1变成300*300*64 于是我们的到了想要的东西；最后的绿色框； 【1、从Conv1到Conv2之间的过度：】 这一步用的Pooling是：2*2*64 s=2； 也就是说，步长是二，滑动的矩阵本身没有重叠；刚好减半，第三维度64不变； 【3、顺利来到Conv2并且结构完全一样进入Conv3：】 我们知道原来INPUT是300*300*3过了第一层出来时150*150*64 那么第二层仍然有池化有128个卷积核，联想推理： 出来的应该是75*75*128；这一步没有问题，我们继续往下分析： 【4、进入Conv3的推演：】 可以知道第三层有256个卷积核，包含三层小的卷基层： 【5、从Conv3到Conv4之间的过度：】 池化没有问题，但是这里75不是一个偶数怎么弄，还记得我们第一部分前面的括号吗？ 就是这样，我们在75这里相加了一个一，使之成为76，变成一个偶数，还有一种方法是通过步长的设置这里先不展开来讲了； 【6、后续的步骤】 &nbsp; 后面的方法很简单，根据我给的那个VGG16的表格查找每一层里面有什么卷积核？多少个？池化的大小？步长多少？是否需要Padding？解决这些问题，你的VGG16就已经完全可以从头到尾说清楚了！！！ 【7、Faster Rcnn的例子】 http://blog.csdn.net/errors_in_life/article/details/70916583 ____________后续我将介绍一些基于VGG16深度学习的图像分类知识，一个爱分享自己错误和经验的师兄，多多指教！ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 刚开始接触深度学习、卷积神经网络的时候非常懵逼，不知道从何入手，我觉得应该有一个进阶的过程，也就是说，理应有一些基本概念作为奠基石，让你有底气去完全理解一个庞大的卷积神经网络： 本文思路： 一、我认为学习卷积神经网络必须知道的几个概念： 1、卷积过程： &nbsp; 我们经常说卷积神经网络卷积神经网络，到底什么才是卷积？网络层卷积过程到底怎么实现？我们在这里借鉴了另一位博客大牛的动态图来给大家演示一下， 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 我们可以看到，卷积过程其实还是基于一个固定的矩阵，在另外一个矩阵不断一格一格扫过去的到的数值的和，（注意：这里的一格一格非常重要，因为涉及后面的概念：步长→我们不妨想一想当固定矩阵不是一格一格前进的时候，会发生什么呢？）产生的一个新的矩阵，我们以作为比较会发现：粉红色矩阵和绿色矩阵在根本上有很大不一样， 第一，卷积之后的维数降低了；第二，我们要想想为什么降维了？（思考：降低维度到底有没有规律？） &nbsp; 答案是有的：我们发现橙色的固定框为3*3，绿色是5*5，出来是三乘三； &nbsp; 所以规律可以得到：粉红色最后的卷积结果矩阵维度=绿色矩阵维数-橙色矩阵维数+1 &nbsp; （我们又应该思考：如果我不想最后减少维度，我只希望卷积，怎么办呢？） 2、两层之间的池化： &nbsp; 我们依然延用博客大牛的另一个动图（再次点赞做的精细准确！） 我们可以发现其实跟之前没什么不一样：还是以三个矩阵之间的运算，但是我们很容易发现，它并不是一行一行扫过去的，橙色矩阵维度是黄色矩阵的整数倍，所以池化的最终的结论是要把原来的维度减少到1/n.这是池化最根本的原理（当然也有特殊情况。） （思考点：我们想象一下如果一个19*19的矩阵做池化，会是一种什么样的体验呢？我们不可以缩小整数倍！！答案会在后面的VGG16里面讲清楚，不急不急） 3、第三个知识点是步长的概念： &nbsp; 卷积核（后面讲到VGG16会介绍）移动的步长（stride）小于卷积核的边长（一般为正方行）时，变会出现卷积核与原始输入矩阵作用范围在区域上的重叠（overlap），卷积核移动的步长（stride）与卷积核的边长相一致时，不会出现重叠现象。 &nbsp; 通俗一点其实就是：刚刚说的那个粉红色矩阵，他每一次移动多少格，格子就是步长！！ 4、卷积核： &nbsp; 一个听起来很高大上的词语，我们依然用之前的基础来解释：通俗易懂：就是粉红色矩阵的个数！！因为有时候我们要提取的特征非常多非常广泛，所以需要我们用更多的矩阵来扫（多扫几遍），那么粉红色矩阵的个数就是卷积核个数。 5、Padding: &nbsp; 这个应该是最抽象的概念了：但是也不会特别难呢，就是我们在之前讲到第一点：卷积的时候，我抛下了一个问题： &nbsp;（我们又应该思考：如果我不想最后减少维度，我只希望卷积，怎么办呢？）（现在知道括号的重要性了吧哈哈？） &nbsp; 现在我们来解决这个问题：比如：我们需要做一个300*300的原始矩阵，用一个3*3卷积核（粉红色矩阵）来扫，扫出来，按照之前公式，结果的矩阵应该是：298*298的矩阵，但是这样很难计算，减得也不多，反而增加我计算难度，还不如池化（pooling）来得干脆是吧！那我们就在300*300矩阵外面周围加一圈“0”，记住，是在外面外包一层“0” 重点是：这样的300*300就变成了302*302的矩阵，这样就可以完全避开卷积后那两层的抵消。 6、还有一个就是通道的概念：这个不算知识点，仅仅是一个常识词语，比如一张图片，有RGB三种颜色，对应三个灰度级别，也就是三个通道了： 更加抽象的图可以参照下面的结构： 二、等待已久的VGG16： VGG16分为16层，我们主要讲前面的前几层（越详细越好吧，后面是一样的） ——首先教会大家一个看其他神经网络也是用的办法：官方数据表格： 看懂一些式子表达： Conv3-512 &nbsp; → &nbsp; &nbsp;第三层卷积后维度变成512； Conv3_2 s=2 &nbsp; &nbsp; → &nbsp; &nbsp; 第三层卷积层里面的第二子层，滑动步长等于2（每次移动两个格子） 好了，我们有了以上的知识可以考试剖析VGG16卷积神经网络了 三、利用之前的基本概念来解释深层的VGG16卷及网络； 【1、从INPUT到Conv1：】 首先两个黄色的是卷积层，是VGG16网络结构十六层当中的第一层（Conv1_1）和第二层（Conv1_2），他们合称为Conv1。 我们主要讲述第一个，也就是第一层（Conv1_1），它怎么把一个300*300*3的矩阵变成一个300*300*64的矩阵？ 我们假设蓝色框是一个RGB图像，橙色是一个3*3*3的卷积核，我们对一个三维的27个数求和，然后扫过去，按照第一部分算的得出来的是一维的298*298的矩阵（因为卷积核也是三维所以结果是一维）； 然后回想一下什么是Padding、前面也讲过它的概念了；所以不了一圈的圆，回到了300*300*1； 然后，VGG16这一层安置有64个卷积核，那么，原来的300*300*1变成300*300*64 于是我们的到了想要的东西；最后的绿色框； 【1、从Conv1到Conv2之间的过度：】 这一步用的Pooling是：2*2*64 s=2； 也就是说，步长是二，滑动的矩阵本身没有重叠；刚好减半，第三维度64不变； 【3、顺利来到Conv2并且结构完全一样进入Conv3：】 我们知道原来INPUT是300*300*3过了第一层出来时150*150*64 那么第二层仍然有池化有128个卷积核，联想推理： 出来的应该是75*75*128；这一步没有问题，我们继续往下分析： 【4、进入Conv3的推演：】 可以知道第三层有256个卷积核，包含三层小的卷基层： 【5、从Conv3到Conv4之间的过度：】 池化没有问题，但是这里75不是一个偶数怎么弄，还记得我们第一部分前面的括号吗？ 就是这样，我们在75这里相加了一个一，使之成为76，变成一个偶数，还有一种方法是通过步长的设置这里先不展开来讲了； 【6、后续的步骤】 &nbsp; 后面的方法很简单，根据我给的那个VGG16的表格查找每一层里面有什么卷积核？多少个？池化的大小？步长多少？是否需要Padding？解决这些问题，你的VGG16就已经完全可以从头到尾说清楚了！！！ 【7、Faster Rcnn的例子】 http://blog.csdn.net/errors_in_life/article/details/70916583 ____________后续我将介绍一些基于VGG16深度学习的图像分类知识，一个爱分享自己错误和经验的师兄，多多指教！ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/12/786969.html" />
<meta property="og:url" content="https://mlh.app/2019/05/12/786969.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 刚开始接触深度学习、卷积神经网络的时候非常懵逼，不知道从何入手，我觉得应该有一个进阶的过程，也就是说，理应有一些基本概念作为奠基石，让你有底气去完全理解一个庞大的卷积神经网络： 本文思路： 一、我认为学习卷积神经网络必须知道的几个概念： 1、卷积过程： &nbsp; 我们经常说卷积神经网络卷积神经网络，到底什么才是卷积？网络层卷积过程到底怎么实现？我们在这里借鉴了另一位博客大牛的动态图来给大家演示一下， 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 我们可以看到，卷积过程其实还是基于一个固定的矩阵，在另外一个矩阵不断一格一格扫过去的到的数值的和，（注意：这里的一格一格非常重要，因为涉及后面的概念：步长→我们不妨想一想当固定矩阵不是一格一格前进的时候，会发生什么呢？）产生的一个新的矩阵，我们以作为比较会发现：粉红色矩阵和绿色矩阵在根本上有很大不一样， 第一，卷积之后的维数降低了；第二，我们要想想为什么降维了？（思考：降低维度到底有没有规律？） &nbsp; 答案是有的：我们发现橙色的固定框为3*3，绿色是5*5，出来是三乘三； &nbsp; 所以规律可以得到：粉红色最后的卷积结果矩阵维度=绿色矩阵维数-橙色矩阵维数+1 &nbsp; （我们又应该思考：如果我不想最后减少维度，我只希望卷积，怎么办呢？） 2、两层之间的池化： &nbsp; 我们依然延用博客大牛的另一个动图（再次点赞做的精细准确！） 我们可以发现其实跟之前没什么不一样：还是以三个矩阵之间的运算，但是我们很容易发现，它并不是一行一行扫过去的，橙色矩阵维度是黄色矩阵的整数倍，所以池化的最终的结论是要把原来的维度减少到1/n.这是池化最根本的原理（当然也有特殊情况。） （思考点：我们想象一下如果一个19*19的矩阵做池化，会是一种什么样的体验呢？我们不可以缩小整数倍！！答案会在后面的VGG16里面讲清楚，不急不急） 3、第三个知识点是步长的概念： &nbsp; 卷积核（后面讲到VGG16会介绍）移动的步长（stride）小于卷积核的边长（一般为正方行）时，变会出现卷积核与原始输入矩阵作用范围在区域上的重叠（overlap），卷积核移动的步长（stride）与卷积核的边长相一致时，不会出现重叠现象。 &nbsp; 通俗一点其实就是：刚刚说的那个粉红色矩阵，他每一次移动多少格，格子就是步长！！ 4、卷积核： &nbsp; 一个听起来很高大上的词语，我们依然用之前的基础来解释：通俗易懂：就是粉红色矩阵的个数！！因为有时候我们要提取的特征非常多非常广泛，所以需要我们用更多的矩阵来扫（多扫几遍），那么粉红色矩阵的个数就是卷积核个数。 5、Padding: &nbsp; 这个应该是最抽象的概念了：但是也不会特别难呢，就是我们在之前讲到第一点：卷积的时候，我抛下了一个问题： &nbsp;（我们又应该思考：如果我不想最后减少维度，我只希望卷积，怎么办呢？）（现在知道括号的重要性了吧哈哈？） &nbsp; 现在我们来解决这个问题：比如：我们需要做一个300*300的原始矩阵，用一个3*3卷积核（粉红色矩阵）来扫，扫出来，按照之前公式，结果的矩阵应该是：298*298的矩阵，但是这样很难计算，减得也不多，反而增加我计算难度，还不如池化（pooling）来得干脆是吧！那我们就在300*300矩阵外面周围加一圈“0”，记住，是在外面外包一层“0” 重点是：这样的300*300就变成了302*302的矩阵，这样就可以完全避开卷积后那两层的抵消。 6、还有一个就是通道的概念：这个不算知识点，仅仅是一个常识词语，比如一张图片，有RGB三种颜色，对应三个灰度级别，也就是三个通道了： 更加抽象的图可以参照下面的结构： 二、等待已久的VGG16： VGG16分为16层，我们主要讲前面的前几层（越详细越好吧，后面是一样的） ——首先教会大家一个看其他神经网络也是用的办法：官方数据表格： 看懂一些式子表达： Conv3-512 &nbsp; → &nbsp; &nbsp;第三层卷积后维度变成512； Conv3_2 s=2 &nbsp; &nbsp; → &nbsp; &nbsp; 第三层卷积层里面的第二子层，滑动步长等于2（每次移动两个格子） 好了，我们有了以上的知识可以考试剖析VGG16卷积神经网络了 三、利用之前的基本概念来解释深层的VGG16卷及网络； 【1、从INPUT到Conv1：】 首先两个黄色的是卷积层，是VGG16网络结构十六层当中的第一层（Conv1_1）和第二层（Conv1_2），他们合称为Conv1。 我们主要讲述第一个，也就是第一层（Conv1_1），它怎么把一个300*300*3的矩阵变成一个300*300*64的矩阵？ 我们假设蓝色框是一个RGB图像，橙色是一个3*3*3的卷积核，我们对一个三维的27个数求和，然后扫过去，按照第一部分算的得出来的是一维的298*298的矩阵（因为卷积核也是三维所以结果是一维）； 然后回想一下什么是Padding、前面也讲过它的概念了；所以不了一圈的圆，回到了300*300*1； 然后，VGG16这一层安置有64个卷积核，那么，原来的300*300*1变成300*300*64 于是我们的到了想要的东西；最后的绿色框； 【1、从Conv1到Conv2之间的过度：】 这一步用的Pooling是：2*2*64 s=2； 也就是说，步长是二，滑动的矩阵本身没有重叠；刚好减半，第三维度64不变； 【3、顺利来到Conv2并且结构完全一样进入Conv3：】 我们知道原来INPUT是300*300*3过了第一层出来时150*150*64 那么第二层仍然有池化有128个卷积核，联想推理： 出来的应该是75*75*128；这一步没有问题，我们继续往下分析： 【4、进入Conv3的推演：】 可以知道第三层有256个卷积核，包含三层小的卷基层： 【5、从Conv3到Conv4之间的过度：】 池化没有问题，但是这里75不是一个偶数怎么弄，还记得我们第一部分前面的括号吗？ 就是这样，我们在75这里相加了一个一，使之成为76，变成一个偶数，还有一种方法是通过步长的设置这里先不展开来讲了； 【6、后续的步骤】 &nbsp; 后面的方法很简单，根据我给的那个VGG16的表格查找每一层里面有什么卷积核？多少个？池化的大小？步长多少？是否需要Padding？解决这些问题，你的VGG16就已经完全可以从头到尾说清楚了！！！ 【7、Faster Rcnn的例子】 http://blog.csdn.net/errors_in_life/article/details/70916583 ____________后续我将介绍一些基于VGG16深度学习的图像分类知识，一个爱分享自己错误和经验的师兄，多多指教！ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/12/786969.html","headline":"深度学习 图像分类入门，从VGG16卷积神经网络开始","dateModified":"2019-05-12T00:00:00+08:00","datePublished":"2019-05-12T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/12/786969.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>深度学习 图像分类入门，从VGG16卷积神经网络开始</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <div class="markdown_views prism-tomorrow-night" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <div class="htmledit_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <p><span>刚开始接触深度学习、卷积神经网络的时候非常懵逼，不知道从何入手，我觉得应该有一个进阶的过程，也就是说，理应有一些基本概念作为奠基石，让你有底气去完全理解一个庞大的卷积神经网络：</span></p>
    <p><span>本文思路：</span></p>
    <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170325212726690?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRXJyb3JzX0luX0xpZmU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"><br></span></p>
    <p><span></span></p>
   </div>
   <p><strong>一、我认为学习卷积神经网络必须知道的几个概念：</strong></p>
   <p><span><strong>1、卷积过程：</strong></span></p>
   <p><span>&nbsp; 我们经常说卷积神经网络卷积神经网络，到底什么才是卷积？网络层卷积过程到底怎么实现？我们在这里借鉴了另一位博客大牛的动态图来给大家演示一下，</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170325211712248?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRXJyb3JzX0luX0xpZmU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"></span></p>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <p><span>我们可以看到，卷积过程其实还是基于一个固定的矩阵，在另外一个矩阵不断一格一格扫过去的到的数值的和，（注意：这里的一格一格非常重要，因为涉及后面的概念：步长→我们不妨想一想当固定矩阵不是一格一格前进的时候，会发生什么呢？）产生的一个新的矩阵，我们以作为比较会发现：粉红色矩阵和绿色矩阵在根本上有很大不一样，</span></p>
   <p><span>第一，卷积之后的维数降低了；第二，我们要想想为什么降维了？<span>（思考：降低维度到底有没有规律？）</span><br></span></p>
   <p><span>&nbsp; 答案是有的：我们发现橙色的固定框为3*3，绿色是5*5，出来是三乘三；</span></p>
   <p><span>&nbsp; 所以规律可以得到：粉红色最后的卷积结果矩阵维度=绿色矩阵维数-橙色矩阵维数+1</span></p>
   <p><span>&nbsp; <span>（我们又应该思考：如果我不想最后减少维度，我只希望卷积，怎么办呢？）</span></span></p>
   <p><span><strong>2、两层之间的池化：</strong></span></p>
   <p><span>&nbsp; 我们依然延用博客大牛的另一个动图（再次点赞做的精细准确！）</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170325211641810?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRXJyb3JzX0luX0xpZmU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"><br></span></p>
   <p><span>我们可以发现其实跟之前没什么不一样：还是以三个矩阵之间的运算，但是我们很容易发现，它并不是一行一行扫过去的，橙色矩阵维度是黄色矩阵的整数倍，所以池化的最终的结论是要把原来的维度减少到1/n.这是池化最根本的原理<span>（当然也有特殊情况。）</span></span></p>
   <p><span>（思考点：我们想象一下如果一个19*19的矩阵做池化，会是一种什么样的体验呢？我们不可以缩小整数倍！！答案会在后面的VGG16里面讲清楚，不急不急<img alt="吐舌头" src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/tongue.gif"><img alt="吐舌头" src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/tongue.gif"><img alt="吐舌头" src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/tongue.gif">）</span></p>
   <p><span><br></span></p>
   <p><span><strong>3、第三个知识点是步长的概念：</strong></span></p>
   <p><span><span>&nbsp; 卷积核<span>（后面讲到VGG16会介绍）</span>移动的步长（stride）小于卷积核的边长（一般为正方行）时，变会出现卷积核与原始输入矩阵作用范围在区域上的重叠（overlap），卷积核移动的步长（stride）与卷积核的边长相一致时，不会出现重叠现象。</span><br></span></p>
   <p><span><span>&nbsp; 通俗一点其实就是：刚刚说的那个粉红色矩阵，他每一次移动多少格，格子就是步长！！</span></span></p>
   <p><span><span><strong>4、卷积核：</strong></span></span></p>
   <p><span><span>&nbsp; 一个听起来很高大上的词语，我们依然用之前的基础来解释：通俗易懂：就是粉红色矩阵的个数！！因为有时候我们要提取的特征非常多非常广泛，所以需要我们用更多的矩阵来扫（多扫几遍），那么粉红色矩阵的个数就是卷积核个数。</span></span></p>
   <p><span><span><strong>5、Padding:</strong></span></span></p>
   <p><span><span>&nbsp; 这个应该是最抽象的概念了：但是也不会特别难呢，就是我们在之前讲到第一点：卷积的时候，我抛下了一个问题：</span></span></p>
   <p><span><span>&nbsp;<span>（我们又应该思考：如果我不想最后减少维度，我只希望卷积，怎么办呢？）</span>（现在知道括号的重要性了吧哈哈？<img alt="骂人" src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/curse.gif"><img alt="骂人" src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/curse.gif"><img alt="骂人" src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/curse.gif">）</span></span></p>
   <p><span><span>&nbsp; 现在我们来解决这个问题：比如：我们需要做一个300*300的原始矩阵，用一个3*3卷积核（粉红色矩阵）来扫，扫出来，按照之前公式，结果的矩阵应该是：298*298的矩阵，但是这样很难计算，减得也不多，反而增加我计算难度，还不如池化（pooling）来得干脆是吧！那我们就在300*300矩阵外面周围加一圈“0”，记住，是在外面外包一层“0”</span></span></p>
   <p><span><span>重点是：这样的300*300就变成了302*302的矩阵，这样就可以完全避开卷积后那两层的抵消。</span></span></p>
   <p><span><span>6、还有一个就是通道的概念：这个不算知识点，仅仅是一个常识词语，比如一张图片，有RGB三种颜色，对应三个灰度级别，也就是三个通道了：</span></span></p>
   <p><span><span>更加抽象的图可以参照下面的结构：</span></span></p>
   <p><span><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170325220130828"><br></span></span></p>
   <p><span><span><strong>二、等待已久的VGG16：</strong></span></span></p>
   <p><span><span>VGG16分为16层，我们主要讲前面的前几层（越详细越好吧，后面是一样的）</span></span></p>
   <p><span><span>——首先教会大家一个看其他神经网络也是用的办法：官方数据表格：</span></span></p>
   <p><span><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170325221311877"><br></span></span></p>
   <p><span><span>看懂一些式子表达：</span></span></p>
   <p><span><span><strong>Conv3-512 &nbsp; → &nbsp; &nbsp;第三层卷积后维度变成512；</strong></span></span></p>
   <p><span><span><strong>Conv3_2 s=2 &nbsp; &nbsp; → &nbsp; &nbsp; 第三层卷积层里面的第二子层，滑动步长等于2（每次移动两个格子）</strong></span></span></p>
   <p><span><span>好了，我们有了以上的知识可以考试剖析VGG16卷积神经网络了</span></span></p>
   <p><span><span><br></span></span></p>
   <p><span><span><strong>三、利用之前的基本概念来解释深层的VGG16卷及网络；</strong></span></span></p>
   <p><span><span><strong>【1、从INPUT到Conv1：】</strong></span></span></p>
   <p><span><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170325222712022"><br></span></span></p>
   <p><span><span>首先两个黄色的是卷积层，是VGG16网络结构十六层当中的第一层（<span>Conv1_1</span>）和第二层<span>（</span><span>Conv1_2</span><span>）</span>，他们合称为Conv1。</span></span></p>
   <p><span><span>我们主要讲述第一个，也就是<span>第一层（</span><span>Conv1_1</span><span>），它怎么把一个300*300*3的矩阵变成一个300*300*64的矩阵？</span></span></span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170325223855788?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRXJyb3JzX0luX0xpZmU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></span></p>
   <p><span>我们假设蓝色框是一个RGB图像，橙色是一个3*3*3的卷积核，我们对一个三维的27个数求和，然后扫过去，按照第一部分算的得出来的是一维的298*298的矩阵（因为卷积核也是三维所以结果是一维）；</span></p>
   <p><span>然后回想一下什么是Padding、前面也讲过它的概念了；所以不了一圈的圆，回到了300*300*1；</span></p>
   <p><span>然后，VGG16这一层安置有64个卷积核，那么，原来的300*300*1变成300*300*64</span></p>
   <p><span>于是我们的到了想要的东西；最后的绿色框；</span></p>
   <p><span><span><strong><span>【1、从Conv1到<span>Conv2之间的过度</span>：】</span></strong></span><br></span></p>
   <p><span><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170325224827465?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRXJyb3JzX0luX0xpZmU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></span></span></p>
   <p><span>这一步用的Pooling是：2*2*64 s=2；</span></p>
   <p><span>也就是说，步长是二，滑动的矩阵本身没有重叠；刚好减半，第三维度64不变；</span></p>
   <p><span><span><span><span>【3、顺利来到</span><span>Conv2并且结构完全一样进入<span>Conv3</span></span><span>：】</span></span></span><br></span></p>
   <p><span><span>我们知道原来INPUT是300*300*3过了第一层出来时150*150*64</span></span></p>
   <p><span><span>那么第二层仍然有池化有128个卷积核，联想推理：</span></span></p>
   <p><span><span>出来的应该是75*75*128；这一步没有问题，我们继续往下分析：</span></span></p>
   <p><span><span><strong><span><span>【4、</span><span>进入Conv3的推演</span><span>：】</span></span></strong><br></span></span></p>
   <p><span><span><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170607224033199?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRXJyb3JzX0luX0xpZmU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"><br></span></span></span></p>
   <p><span><span><span>可以知道第三层有256个卷积核，包含三层小的卷基层：</span></span></span></p>
   <p><span><span><span><strong><span><span>【5、从Conv3到</span><span>Conv4之间的过度</span><span>：】</span></span></strong><br></span></span></span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20170325225637826?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRXJyb3JzX0luX0xpZmU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></span></p>
   <p><span>池化没有问题，但是这里75不是一个偶数怎么弄，还记得我们第一部分前面的括号吗？</span></p>
   <p><span>就是这样，我们在75这里相加了一个一，使之成为76，变成一个偶数，还有一种方法是通过步长的设置这里先不展开来讲了；</span></p>
   <p><span><strong><span>【6、后续的步骤】</span></strong></span></p>
   <p><span>&nbsp; 后面的方法很简单，根据我给的那个VGG16的表格查找每一层里面有什么卷积核？多少个？池化的大小？步长多少？是否需要Padding？解决这些问题，你的VGG16就已经完全可以从头到尾说清楚了！！！</span></p>
   <p><span><span><strong>【7、Faster Rcnn的例子】</strong></span><br></span></p>
   <p><span><span><u>http://blog.csdn.net/errors_in_life/article/details/70916583</u><br></span></span></p>
   <p><span><strong>____________后续我将介绍一些基于VGG16深度学习的图像分类知识，一个爱分享自己错误和经验的师兄<img alt="吐舌头" src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/tongue.gif"><img alt="吐舌头" src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/tongue.gif"><img alt="吐舌头" src="http://static.blog.csdn.net/xheditor/xheditor_emot/default/tongue.gif">，多多指教！</strong></span></p>
   <p><br></p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
