<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>自己动手写个聊天机器人吧 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="自己动手写个聊天机器人吧" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 学习来源于Sirajology的视频 Build a Chatbot 昨天写LSTM的时候提到了聊天机器人，今天放松一下，来看看chatrobot是如何实现的。 前天和一个小伙伴聊，如果一个机器人知道在它通过图灵测试后可能会被限制，那它假装自己不能通过然后逃过一劫，从此过上自由的生活会怎样。 Retrieval based model 以前很多聊天机器人是以 Retrieval based model 模型来进行对话的，这个模型就是程序员事先写好一些回答，然后机器人在接收到一个问题的时候，就去搜索并选择相关的答案。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 Machine Learning Classfier 最近，大家开始使用机器学习的分类器，例如 Facebook 的 chatbot API。 你可以提前设定一些问题和答案，然后系统会把词语进行分类，进一步来识别出用户的意图，这样你在问两句不一样的话时，机器人可以识别出它们的意图是一样的。 Generative Model 最难的就是在没有预先设定问答数据时就能自动生成答案的机器人，下面这篇Google的论文就是研究这样的机器人的。 他们在两个数据集上训练一个神经网络模型，一个是电影对话，一个是IT support对话记录，这样就有日常对话和专业领域知识了。 这个模型不需要写很多代码，但是需要很多数据。 结果是还不错： 接下来要用 Torch 和 Lua 重建一下论文里的 Neural Network 模型。 第一步，输入数据，定义变量 -- Dataprint(&quot;-- Loading dataset&quot;)dataset = neuralconvo.DataSet(neuralconvo.CornellMovieDialogs(&quot;data/cornell_movie_dialogs&quot;),&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loadFirst = options.dataset,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- 定义要用多少数据&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; minWordFreq = options.minWordFreq&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- 想要保持在词汇表里的单词的最小频率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }) 1 2 3 4 5 6 7 8 1 第二步，建模 -- Model-- options.hiddenSize：隐藏层数-- dataset.wordsCount: 数据集的词数model = neuralconvo.Seq2Seq(dataset.wordsCount, options.hiddenSize)model.goToken = dataset.goTokenmodel.eosToken = dataset.eosToken 1 2 3 4 5 6 1 这里用到的模型是 seq2seq，它包含两个 LSTM 递归神经网络，第一个是 encoder 负责处理 input，第二个是 decoder 负责生成 output。 为什么要用 seq2seq？ DNN需要 inputs 和 outputs 的维度是固定的，而我们接收的是一句话，输出的也是一句话，都是一串单词。 所以需要一个模型可以保持一定长度的记忆。 LSTM 可以将可变长度的inputs转化为固定维度的向量表达。所以在给了足够多的数据后，模型可以将两个相似的问题识别成同一个 thought vector 表达出来。在学习模型之后，不仅可以得到权重，还有 thought vectors。 第三步，加一些 hyperparameters 要用到 NLL Criterion ，NLL 就是 Negative Log Likelihood，可以改进句子的预测。 -- Training parametersmodel.criterion = nn.SequencerCriterion(nn.ClassNLLCriterion())&nbsp;&nbsp;&nbsp; -- 改进句子的预测model.learningRate = options.learningRatemodel.momentum = options.momentumlocal decayFactor = (options.minLR - options.learningRate) / options.saturateEpoch&nbsp;&nbsp;&nbsp; -- 改进 learning ratelocal minMeanError = nil&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- 改进 learning rate 1 2 3 4 5 6 7 1 接下来就是用 Backpropagation 来训练模型： -- Enabled CUDAif options.cuda then&nbsp; require &#39;cutorch&#39;&nbsp; require &#39;cunn&#39;&nbsp; model:cuda()elseif options.opencl then&nbsp; require &#39;cltorch&#39;&nbsp; require &#39;clnn&#39;&nbsp; model:cl()end 1 2 3 4 5 6 7 8 9 10 11 1 训练的目标是让error越来越小，每个例子有一个输入句子和一个目标句子。 local err = model:train(input, target) 1 1 最后把好的model存下来。 -- Save the model if it improved.if minMeanError == nil or errors:mean() &lt; minMeanError thenprint(&quot;\n(Saving model ...)&quot;)torch.save(&quot;data/model.t7&quot;, model)minMeanError = errors:mean()endmodel.learningRate = model.learningRate + decayFactormodel.learningRate = math.max(options.minLR, model.learningRate)end 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1 现在可以去 AWS 训练你的机器人了，投入的数据越多，聊得越开心。 其他资料： The code for this video is here Here’s the Neural Conversational Model paper check out the machine-generated support conversations, they’re mind-blowingly good&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 学习来源于Sirajology的视频 Build a Chatbot 昨天写LSTM的时候提到了聊天机器人，今天放松一下，来看看chatrobot是如何实现的。 前天和一个小伙伴聊，如果一个机器人知道在它通过图灵测试后可能会被限制，那它假装自己不能通过然后逃过一劫，从此过上自由的生活会怎样。 Retrieval based model 以前很多聊天机器人是以 Retrieval based model 模型来进行对话的，这个模型就是程序员事先写好一些回答，然后机器人在接收到一个问题的时候，就去搜索并选择相关的答案。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 Machine Learning Classfier 最近，大家开始使用机器学习的分类器，例如 Facebook 的 chatbot API。 你可以提前设定一些问题和答案，然后系统会把词语进行分类，进一步来识别出用户的意图，这样你在问两句不一样的话时，机器人可以识别出它们的意图是一样的。 Generative Model 最难的就是在没有预先设定问答数据时就能自动生成答案的机器人，下面这篇Google的论文就是研究这样的机器人的。 他们在两个数据集上训练一个神经网络模型，一个是电影对话，一个是IT support对话记录，这样就有日常对话和专业领域知识了。 这个模型不需要写很多代码，但是需要很多数据。 结果是还不错： 接下来要用 Torch 和 Lua 重建一下论文里的 Neural Network 模型。 第一步，输入数据，定义变量 -- Dataprint(&quot;-- Loading dataset&quot;)dataset = neuralconvo.DataSet(neuralconvo.CornellMovieDialogs(&quot;data/cornell_movie_dialogs&quot;),&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loadFirst = options.dataset,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- 定义要用多少数据&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; minWordFreq = options.minWordFreq&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- 想要保持在词汇表里的单词的最小频率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }) 1 2 3 4 5 6 7 8 1 第二步，建模 -- Model-- options.hiddenSize：隐藏层数-- dataset.wordsCount: 数据集的词数model = neuralconvo.Seq2Seq(dataset.wordsCount, options.hiddenSize)model.goToken = dataset.goTokenmodel.eosToken = dataset.eosToken 1 2 3 4 5 6 1 这里用到的模型是 seq2seq，它包含两个 LSTM 递归神经网络，第一个是 encoder 负责处理 input，第二个是 decoder 负责生成 output。 为什么要用 seq2seq？ DNN需要 inputs 和 outputs 的维度是固定的，而我们接收的是一句话，输出的也是一句话，都是一串单词。 所以需要一个模型可以保持一定长度的记忆。 LSTM 可以将可变长度的inputs转化为固定维度的向量表达。所以在给了足够多的数据后，模型可以将两个相似的问题识别成同一个 thought vector 表达出来。在学习模型之后，不仅可以得到权重，还有 thought vectors。 第三步，加一些 hyperparameters 要用到 NLL Criterion ，NLL 就是 Negative Log Likelihood，可以改进句子的预测。 -- Training parametersmodel.criterion = nn.SequencerCriterion(nn.ClassNLLCriterion())&nbsp;&nbsp;&nbsp; -- 改进句子的预测model.learningRate = options.learningRatemodel.momentum = options.momentumlocal decayFactor = (options.minLR - options.learningRate) / options.saturateEpoch&nbsp;&nbsp;&nbsp; -- 改进 learning ratelocal minMeanError = nil&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- 改进 learning rate 1 2 3 4 5 6 7 1 接下来就是用 Backpropagation 来训练模型： -- Enabled CUDAif options.cuda then&nbsp; require &#39;cutorch&#39;&nbsp; require &#39;cunn&#39;&nbsp; model:cuda()elseif options.opencl then&nbsp; require &#39;cltorch&#39;&nbsp; require &#39;clnn&#39;&nbsp; model:cl()end 1 2 3 4 5 6 7 8 9 10 11 1 训练的目标是让error越来越小，每个例子有一个输入句子和一个目标句子。 local err = model:train(input, target) 1 1 最后把好的model存下来。 -- Save the model if it improved.if minMeanError == nil or errors:mean() &lt; minMeanError thenprint(&quot;\n(Saving model ...)&quot;)torch.save(&quot;data/model.t7&quot;, model)minMeanError = errors:mean()endmodel.learningRate = model.learningRate + decayFactormodel.learningRate = math.max(options.minLR, model.learningRate)end 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1 现在可以去 AWS 训练你的机器人了，投入的数据越多，聊得越开心。 其他资料： The code for this video is here Here’s the Neural Conversational Model paper check out the machine-generated support conversations, they’re mind-blowingly good&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/12/781747.html" />
<meta property="og:url" content="https://mlh.app/2019/05/12/781747.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 学习来源于Sirajology的视频 Build a Chatbot 昨天写LSTM的时候提到了聊天机器人，今天放松一下，来看看chatrobot是如何实现的。 前天和一个小伙伴聊，如果一个机器人知道在它通过图灵测试后可能会被限制，那它假装自己不能通过然后逃过一劫，从此过上自由的生活会怎样。 Retrieval based model 以前很多聊天机器人是以 Retrieval based model 模型来进行对话的，这个模型就是程序员事先写好一些回答，然后机器人在接收到一个问题的时候，就去搜索并选择相关的答案。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 Machine Learning Classfier 最近，大家开始使用机器学习的分类器，例如 Facebook 的 chatbot API。 你可以提前设定一些问题和答案，然后系统会把词语进行分类，进一步来识别出用户的意图，这样你在问两句不一样的话时，机器人可以识别出它们的意图是一样的。 Generative Model 最难的就是在没有预先设定问答数据时就能自动生成答案的机器人，下面这篇Google的论文就是研究这样的机器人的。 他们在两个数据集上训练一个神经网络模型，一个是电影对话，一个是IT support对话记录，这样就有日常对话和专业领域知识了。 这个模型不需要写很多代码，但是需要很多数据。 结果是还不错： 接下来要用 Torch 和 Lua 重建一下论文里的 Neural Network 模型。 第一步，输入数据，定义变量 -- Dataprint(&quot;-- Loading dataset&quot;)dataset = neuralconvo.DataSet(neuralconvo.CornellMovieDialogs(&quot;data/cornell_movie_dialogs&quot;),&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loadFirst = options.dataset,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- 定义要用多少数据&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; minWordFreq = options.minWordFreq&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- 想要保持在词汇表里的单词的最小频率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }) 1 2 3 4 5 6 7 8 1 第二步，建模 -- Model-- options.hiddenSize：隐藏层数-- dataset.wordsCount: 数据集的词数model = neuralconvo.Seq2Seq(dataset.wordsCount, options.hiddenSize)model.goToken = dataset.goTokenmodel.eosToken = dataset.eosToken 1 2 3 4 5 6 1 这里用到的模型是 seq2seq，它包含两个 LSTM 递归神经网络，第一个是 encoder 负责处理 input，第二个是 decoder 负责生成 output。 为什么要用 seq2seq？ DNN需要 inputs 和 outputs 的维度是固定的，而我们接收的是一句话，输出的也是一句话，都是一串单词。 所以需要一个模型可以保持一定长度的记忆。 LSTM 可以将可变长度的inputs转化为固定维度的向量表达。所以在给了足够多的数据后，模型可以将两个相似的问题识别成同一个 thought vector 表达出来。在学习模型之后，不仅可以得到权重，还有 thought vectors。 第三步，加一些 hyperparameters 要用到 NLL Criterion ，NLL 就是 Negative Log Likelihood，可以改进句子的预测。 -- Training parametersmodel.criterion = nn.SequencerCriterion(nn.ClassNLLCriterion())&nbsp;&nbsp;&nbsp; -- 改进句子的预测model.learningRate = options.learningRatemodel.momentum = options.momentumlocal decayFactor = (options.minLR - options.learningRate) / options.saturateEpoch&nbsp;&nbsp;&nbsp; -- 改进 learning ratelocal minMeanError = nil&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- 改进 learning rate 1 2 3 4 5 6 7 1 接下来就是用 Backpropagation 来训练模型： -- Enabled CUDAif options.cuda then&nbsp; require &#39;cutorch&#39;&nbsp; require &#39;cunn&#39;&nbsp; model:cuda()elseif options.opencl then&nbsp; require &#39;cltorch&#39;&nbsp; require &#39;clnn&#39;&nbsp; model:cl()end 1 2 3 4 5 6 7 8 9 10 11 1 训练的目标是让error越来越小，每个例子有一个输入句子和一个目标句子。 local err = model:train(input, target) 1 1 最后把好的model存下来。 -- Save the model if it improved.if minMeanError == nil or errors:mean() &lt; minMeanError thenprint(&quot;\\n(Saving model ...)&quot;)torch.save(&quot;data/model.t7&quot;, model)minMeanError = errors:mean()endmodel.learningRate = model.learningRate + decayFactormodel.learningRate = math.max(options.minLR, model.learningRate)end 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 1 现在可以去 AWS 训练你的机器人了，投入的数据越多，聊得越开心。 其他资料： The code for this video is here Here’s the Neural Conversational Model paper check out the machine-generated support conversations, they’re mind-blowingly good&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/12/781747.html","headline":"自己动手写个聊天机器人吧","dateModified":"2019-05-12T00:00:00+08:00","datePublished":"2019-05-12T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/12/781747.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>自己动手写个聊天机器人吧</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div class="markdown_views" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <div class="markdown_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <!-- flowchart &amp;#31661;&amp;#22836;&amp;#22270;&amp;#26631; &amp;#21247;&amp;#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-1f8a7bd627d5982f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
    <p>学习来源于<a href="https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A" rel="nofollow" target="_blank">Sirajology</a>的视频 <a href="https://www.youtube.com/watch?v=5_SAroSvC0E&amp;index=6&amp;list=PL2-dafEMk2A4ut2pyv0fSIXqOzXtBGkLj" rel="nofollow" target="_blank">Build a Chatbot</a></p>
    <p>昨天写LSTM的时候提到了聊天机器人，今天放松一下，来看看chatrobot是如何实现的。</p>
    <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-d4d966bcc4e4a725.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
    <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-6b77b7764487d303.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
    <p>前天和一个小伙伴聊，如果一个机器人知道在它通过图灵测试后可能会被限制，那它假装自己不能通过然后逃过一劫，从此过上自由的生活会怎样。</p>
    <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-5083aadcb192e097.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
    <h3 id="retrieval-based-model"><a></a><a target="_blank"></a>Retrieval based model</h3>
    <p>以前很多聊天机器人是以 Retrieval based model 模型来进行对话的，这个模型就是程序员事先写好一些回答，然后机器人在接收到一个问题的时候，就去搜索并选择相关的答案。</p>
   </div>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-0b1d96d011260d0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-5d07f9b7c275f171.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-2c8d08920622d764.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <h3 id="machine-learning-classfier"><a></a><a target="_blank"></a>Machine Learning Classfier</h3>
   <p>最近，大家开始使用机器学习的分类器，例如 Facebook 的 chatbot API。</p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-d8659c68a280934d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <p>你可以提前设定一些问题和答案，然后系统会把词语进行分类，进一步来识别出用户的意图，这样你在问两句不一样的话时，机器人可以识别出它们的意图是一样的。</p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-475872fa4fb70866.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <h3 id="generative-model"><a></a><a target="_blank"></a>Generative Model</h3>
   <p>最难的就是在没有预先设定问答数据时就能自动生成答案的机器人，下面这篇Google的论文就是研究这样的机器人的。</p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-28aba16ff9d6e28e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <p>他们在两个数据集上训练一个神经网络模型，一个是电影对话，一个是IT support对话记录，这样就有日常对话和专业领域知识了。</p>
   <p>这个模型不需要写很多代码，但是需要很多数据。</p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-0daa1acd4dba72dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <p>结果是还不错：</p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-04b9b22ab2e6ef22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <p>接下来要用 Torch 和 Lua 重建一下论文里的 Neural Network 模型。</p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-269e43386c65337f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <h4 id="第一步输入数据定义变量">第一步，输入数据，定义变量</h4>
   <pre class="prettyprint"><code class="language-lua hljs&nbsp; has-numbering"><span class="hljs-comment">-- Data</span><span class="hljs-built_in">print</span>(<span class="hljs-string">"-- Loading dataset"</span>)dataset = neuralconvo.DataSet(neuralconvo.CornellMovieDialogs(<span class="hljs-string">"data/cornell_movie_dialogs"</span>),&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; {&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; loadFirst = options.dataset,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment">-- 定义要用多少数据</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; minWordFreq = options.minWordFreq&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment">-- 想要保持在词汇表里的单词的最小频率</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; })</code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <h4 id="第二步建模">第二步，建模</h4>
   <pre class="prettyprint"><code class="hljs avrasm has-numbering">-- Model-- options<span class="hljs-preprocessor">.hiddenSize</span>：隐藏层数-- dataset<span class="hljs-preprocessor">.wordsCount</span>: 数据集的词数model = neuralconvo<span class="hljs-preprocessor">.Seq</span>2Seq(dataset<span class="hljs-preprocessor">.wordsCount</span>, options<span class="hljs-preprocessor">.hiddenSize</span>)model<span class="hljs-preprocessor">.goToken</span> = dataset<span class="hljs-preprocessor">.goToken</span>model<span class="hljs-preprocessor">.eosToken</span> = dataset<span class="hljs-preprocessor">.eosToken</span></code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>这里用到的模型是 seq2seq，它包含两个 LSTM 递归神经网络，第一个是 encoder 负责处理 input，第二个是 decoder 负责生成 output。</p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-b595708ae551f07b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <p><strong>为什么要用 seq2seq？</strong> <br>DNN需要 inputs 和 outputs 的维度是固定的，而我们接收的是一句话，输出的也是一句话，都是一串单词。 <br>所以需要一个模型可以保持一定长度的记忆。</p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-7c2f2a6a5c200b03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <p>LSTM 可以将可变长度的inputs转化为固定维度的向量表达。所以在给了足够多的数据后，模型可以将两个相似的问题识别成同一个 thought vector 表达出来。在学习模型之后，不仅可以得到权重，还有 thought vectors。</p>
   <p><img title="" alt="" src="http://upload-images.jianshu.io/upload_images/1667471-b48a3782069350a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
   <h4 id="第三步加一些-hyperparameters">第三步，加一些 hyperparameters</h4>
   <p>要用到 NLL Criterion ，NLL 就是 Negative Log Likelihood，可以改进句子的预测。</p>
   <pre class="prettyprint"><code class="language-lua hljs&nbsp; has-numbering"><span class="hljs-comment">-- Training parameters</span>model.criterion = nn.SequencerCriterion(nn.ClassNLLCriterion())&nbsp;&nbsp;&nbsp; <span class="hljs-comment">-- 改进句子的预测</span>model.learningRate = options.learningRatemodel.momentum = options.momentum<span class="hljs-keyword">local</span> decayFactor = (options.minLR - options.learningRate) / options.saturateEpoch&nbsp;&nbsp;&nbsp; <span class="hljs-comment">-- 改进 learning rate</span><span class="hljs-keyword">local</span> minMeanError = <span class="hljs-keyword">nil</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="hljs-comment">-- 改进 learning rate</span></code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <h4 id="接下来就是用-backpropagation-来训练模型">接下来就是用 Backpropagation 来训练模型：</h4>
   <pre class="prettyprint"><code class="hljs lua has-numbering"><span class="hljs-comment">-- Enabled CUDA</span><span class="hljs-keyword">if</span> options.cuda <span class="hljs-keyword">then</span>&nbsp; <span class="hljs-built_in">require</span> <span class="hljs-string">'cutorch'</span>&nbsp; <span class="hljs-built_in">require</span> <span class="hljs-string">'cunn'</span>&nbsp; model:cuda()<span class="hljs-keyword">elseif</span> options.opencl <span class="hljs-keyword">then</span>&nbsp; <span class="hljs-built_in">require</span> <span class="hljs-string">'cltorch'</span>&nbsp; <span class="hljs-built_in">require</span> <span class="hljs-string">'clnn'</span>&nbsp; model:cl()<span class="hljs-keyword">end</span></code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>训练的目标是让error越来越小，每个例子有一个输入句子和一个目标句子。</p>
   <pre class="prettyprint"><code class="hljs fix has-numbering"><span class="hljs-attribute">local err </span>=<span class="hljs-string"> model:train(input, target)</span></code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>最后把好的model存下来。</p>
   <pre class="prettyprint"><code class="hljs lua has-numbering"><span class="hljs-comment">-- Save the model if it improved.</span><span class="hljs-keyword">if</span> minMeanError == <span class="hljs-keyword">nil</span> <span class="hljs-keyword">or</span> errors:mean() &lt; minMeanError <span class="hljs-keyword">then</span><span class="hljs-built_in">print</span>(<span class="hljs-string">"\n(Saving model ...)"</span>)torch.save(<span class="hljs-string">"data/model.t7"</span>, model)minMeanError = errors:mean()<span class="hljs-keyword">end</span>model.learningRate = model.learningRate + decayFactormodel.learningRate = <span class="hljs-built_in">math</span>.max(options.minLR, model.learningRate)<span class="hljs-keyword">end</span></code>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>现在可以去 AWS 训练你的机器人了，投入的数据越多，聊得越开心。</p>
   <hr>
   <p>其他资料： <br>The code for this video <a href="https://github.com/llSourcell/Chatbot-AI" rel="nofollow" target="_blank">is here</a></p>
   <p>Here’s the Neural Conversational Model paper <br> <a href="http://arxiv.org/pdf/1506.05869v3.pdf" rel="nofollow" target="_blank">check out the machine-generated support conversations, they’re mind-blowingly good</a></p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
