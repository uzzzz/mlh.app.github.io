<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>基于大数据技术构建数仓模型实践 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="基于大数据技术构建数仓模型实践" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="最近刚接触一个线上运行的数仓环境，是针对用户流量日志做点击量指标的多维度分析，维度表每天一个快照，经过数据统计分析发现有的维度表数据量很大，每天竟然有5亿多条的素材日志，并且这些维度数据是渐变维度，数据存储在亚马逊S3文件系统上面，严重浪费公司的存储成本，同时要是查询跨度一个周的数据则涉及到的维度数据就40亿条进行关联，这还不算其他维度的统计在内，个人观点，涉及到这些大维度数据的统计应该通过当前构建的数仓应该没有实际的应用价值。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 多维设计的整体思路要简化和加速查询，不要求必须满足3NF(消除数据冗余，规范化程度越高，需要关联的表也越多)，也就是说维度模型允许可控的数据冗余(数据更新异常，由于数据仓库中的数据很少有更新，主要是查询操作)，减少表和表间关系的数量，从而提高查询速度；例如，假设有100万订单，每个订单有10条明细，订单状态和订单明细状态各有10种，如果用户要查询某种状态特性的订单(状态是where过滤条件--&gt;维度，即看问题的角度)，按3NF模型，逻辑上需要关联100万与1000万的两个大表，然后过滤两个表的状态值得到所要的结果，其中100万是订单粒度，1000万是订单明细粒度；另一方面，事实表按最细数据粒度有1000万记录(订单明细)，3NF里的订单表属性在事实表里是冗余数据，状态维度有100条数据(10条订单状态 * 10条订单明细状态)，只需要关联1000万与100的两个表，再进行状态过滤即可。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过对现存数仓的调研提出下面的解决方案。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对维度表的处理：一、为提高查询性能，会优化表的存储，主要体现在1）存储类型采用压缩率比较高的ORC，降低公司存储成本，更重要的是该存储格式支持hive进行行级更新；（&nbsp;./hive –orcfiledump -j -p /hivedata/warehouse2/lxw1234_orc1/000000_0）2)通过水平拆分技术，采用分区+分桶，这些手段主要为减少扫描的数据量，分桶会根据下面的多维模型进行详细介绍；3)垂直拆分，因为有的表字段变化比较慢，有的表变化比较快，所以要对这些不同类型的字段区别对待，通过对各个维度表的数据统计分析，大部分字段变化的都很慢，凭借经验而谈，只有变化较慢的字段采用统计的价值，实际上90%以上的统计应该都落到变化较慢的字段上面，对应变化较慢的字段采用SCD2技术实现拉链表。二、为提供更丰富的查询类型，从事实表中抽取出维度数据，例如操作系统维度和设备品牌维度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最终设计出的多维数据模型如下所示： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; 图 1 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面构建的多维数据模型中的维度表字段没有全列出来，因为公司因为所限，但是不影响技术的分享；经过对维度表的分析，主要有三张维度表的数据量特别大，对应的数据量如下表所示： &nbsp; 维度名称 总条数 时间跨度(天) 平均每天的记录数 拉链表之后总条数 维度类型 备注 dim_adn_creative_list 123926985436 205 604521880 610904029 大维表 &nbsp; dim_adn_creative_source 38318221849 205 186918155 721969702 大维表 utime(更新时间戳) 和status(状态)两个字段变化相对较大 dim_adn_campain_list 23392624691 205 114110364 &nbsp; 大维表 已经定位到RDS数据库中个别字段中由\n导致，还没有做SCD2拉链表 dim_adn_advertiser_list 180283 197 915 943 小维表 &nbsp; &nbsp; &nbsp; dim_adn_campaign_list每天产生上亿量级记录,如图2所示： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;图2 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在构建数仓的整个过程中有可能会出现的问题以及对应的解决方案(原则上要保证在查询数据时能够快速查询)： 1.有的维度字段数据变化比较慢，尽管是采用SCD2技术，数据量依然较大，通过技术评估，分区技术不是太好解决，采用分桶技术； 2.有的维度表字段变化比较快，这种可以考虑采用分区技术，例如一个月或者一个季度一个分区，把夸分区的生效时间和失效时间在分区时间点进行切分； 3.数据量比较大的维度表不止一个，如果是一个分桶就创建在大维度表与事实表关联的字段上面，这样可以采用hive的优化机制，bucket_join来大大减少表关联的数据量(set hive.enforce.bucketing=true设置该参数(默认为false)，分桶才生效)；对这种多个维度表都比较大的，采用一种变通的方式(采用反向箭头)，让维度表的关联字段指向事实表的主键，而不是采用在事实表中打维度表的代理键；在事实表的主键上面创建分桶的关联字段，然后其他三个大维度表通过关联得到事实表上的主键来进行查询(对应的SQL如下)，在维度表上创建分桶，且分桶字段是指向事实表的主键字段，且维度表的分桶数量是事实表的倍数(因为事实表已经做过聚合操作，数据量大概在两千多万的量级）。 4.在广告维度进行开发过程中发现异常数据，把表的存储格式转换成text格式后，如何快速定位到这条异常数据所做的数据文件，通过在hiveSQL中添加虚列:1). &nbsp;INPUT__FILE__NAME显示map任务读入File的全路径;2). &nbsp;BLOCK__OFFSET__INSIDE__FILE，如果是RCFile或者是SequenceFile块压缩格式文件则显示Block file Offset，如果是TextFile，显示当前行的第一个字节在文件中的偏移量;3). &nbsp;ROW__OFFSET__INSIDE__BLOCK &nbsp; &nbsp; &nbsp; &nbsp;RCFile和SequenceFile显示row number, textfile显示为0，要显示ROW__OFFSET__INSIDE__BLOCK ，必须设置set hive.exec.rowoffset=true; &nbsp; SQL样例： fact_adn_tracking_click表有 两千万 的记录，把fact_adn_tracking_click 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。&nbsp; 解决思路通过mapjoin仅仅查询出需要的字段进行： &nbsp;insert overwrite table dim_adn_creative_list&nbsp; &nbsp; select &nbsp;/*+mapjoin(c)*/c.id,d.* &nbsp; &nbsp; &nbsp; from ( select distinct id,creative_id from fact_adn_tracking_click) c &nbsp; &nbsp; &nbsp; join dim_adn_creative_list_snapshot &nbsp;d &nbsp; &nbsp; &nbsp; on c.creative_id = d.creative_list_id &nbsp; 目标表建表语句： CREATE EXTERNAL TABLE `dim_adn_creative_list`( &nbsp; `fact_sk` int COMMENT &#39;surrogate key&#39;,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) CLUSTERED BY (&nbsp; &nbsp; creative_list_sk)&nbsp; INTO 10000 BUCKETS ROW FORMAT SERDE&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcSerde&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat&#39; LOCATION &nbsp; &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list&#39;; &nbsp; ---------------------------普通表，分桶普通表，分桶ORC格式------------------------------------------------------ CREATE &nbsp;EXTERNAL TABLE `dim_adn_creative_list_perfomance`( &nbsp; `creative_list_sk` int,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) ROW FORMAT SERDE&nbsp; &nbsp; &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.mapred.TextInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39; LOCATION &nbsp; &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_perfomance&#39;; &nbsp; CREATE EXTERNAL TABLE `dim_adn_creative_list_perfomance_bucket`( &nbsp; `creative_list_sk` int,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) CLUSTERED BY (&nbsp; &nbsp; creative_list_id)&nbsp; SORTED BY (&nbsp; &nbsp; creative_list_id ASC)&nbsp; INTO 50 BUCKETS ROW FORMAT DELIMITED&nbsp; &nbsp; FIELDS TERMINATED BY &#39;\t&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.mapred.TextInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39; LOCATION &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_perfomance_bucket&#39;; &nbsp; CREATE EXTERNAL TABLE `dim_adn_creative_list_orc_bucket`( &nbsp; `creative_list_sk` int COMMENT &#39;surrogate key&#39;,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) CLUSTERED BY (&nbsp; &nbsp; creative_list_sk)&nbsp; INTO 50 BUCKETS ROW FORMAT SERDE&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcSerde&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat&#39; LOCATION &nbsp; &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_orc_bucket&#39;;" />
<meta property="og:description" content="最近刚接触一个线上运行的数仓环境，是针对用户流量日志做点击量指标的多维度分析，维度表每天一个快照，经过数据统计分析发现有的维度表数据量很大，每天竟然有5亿多条的素材日志，并且这些维度数据是渐变维度，数据存储在亚马逊S3文件系统上面，严重浪费公司的存储成本，同时要是查询跨度一个周的数据则涉及到的维度数据就40亿条进行关联，这还不算其他维度的统计在内，个人观点，涉及到这些大维度数据的统计应该通过当前构建的数仓应该没有实际的应用价值。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 多维设计的整体思路要简化和加速查询，不要求必须满足3NF(消除数据冗余，规范化程度越高，需要关联的表也越多)，也就是说维度模型允许可控的数据冗余(数据更新异常，由于数据仓库中的数据很少有更新，主要是查询操作)，减少表和表间关系的数量，从而提高查询速度；例如，假设有100万订单，每个订单有10条明细，订单状态和订单明细状态各有10种，如果用户要查询某种状态特性的订单(状态是where过滤条件--&gt;维度，即看问题的角度)，按3NF模型，逻辑上需要关联100万与1000万的两个大表，然后过滤两个表的状态值得到所要的结果，其中100万是订单粒度，1000万是订单明细粒度；另一方面，事实表按最细数据粒度有1000万记录(订单明细)，3NF里的订单表属性在事实表里是冗余数据，状态维度有100条数据(10条订单状态 * 10条订单明细状态)，只需要关联1000万与100的两个表，再进行状态过滤即可。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过对现存数仓的调研提出下面的解决方案。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对维度表的处理：一、为提高查询性能，会优化表的存储，主要体现在1）存储类型采用压缩率比较高的ORC，降低公司存储成本，更重要的是该存储格式支持hive进行行级更新；（&nbsp;./hive –orcfiledump -j -p /hivedata/warehouse2/lxw1234_orc1/000000_0）2)通过水平拆分技术，采用分区+分桶，这些手段主要为减少扫描的数据量，分桶会根据下面的多维模型进行详细介绍；3)垂直拆分，因为有的表字段变化比较慢，有的表变化比较快，所以要对这些不同类型的字段区别对待，通过对各个维度表的数据统计分析，大部分字段变化的都很慢，凭借经验而谈，只有变化较慢的字段采用统计的价值，实际上90%以上的统计应该都落到变化较慢的字段上面，对应变化较慢的字段采用SCD2技术实现拉链表。二、为提供更丰富的查询类型，从事实表中抽取出维度数据，例如操作系统维度和设备品牌维度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最终设计出的多维数据模型如下所示： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; 图 1 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面构建的多维数据模型中的维度表字段没有全列出来，因为公司因为所限，但是不影响技术的分享；经过对维度表的分析，主要有三张维度表的数据量特别大，对应的数据量如下表所示： &nbsp; 维度名称 总条数 时间跨度(天) 平均每天的记录数 拉链表之后总条数 维度类型 备注 dim_adn_creative_list 123926985436 205 604521880 610904029 大维表 &nbsp; dim_adn_creative_source 38318221849 205 186918155 721969702 大维表 utime(更新时间戳) 和status(状态)两个字段变化相对较大 dim_adn_campain_list 23392624691 205 114110364 &nbsp; 大维表 已经定位到RDS数据库中个别字段中由\n导致，还没有做SCD2拉链表 dim_adn_advertiser_list 180283 197 915 943 小维表 &nbsp; &nbsp; &nbsp; dim_adn_campaign_list每天产生上亿量级记录,如图2所示： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;图2 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在构建数仓的整个过程中有可能会出现的问题以及对应的解决方案(原则上要保证在查询数据时能够快速查询)： 1.有的维度字段数据变化比较慢，尽管是采用SCD2技术，数据量依然较大，通过技术评估，分区技术不是太好解决，采用分桶技术； 2.有的维度表字段变化比较快，这种可以考虑采用分区技术，例如一个月或者一个季度一个分区，把夸分区的生效时间和失效时间在分区时间点进行切分； 3.数据量比较大的维度表不止一个，如果是一个分桶就创建在大维度表与事实表关联的字段上面，这样可以采用hive的优化机制，bucket_join来大大减少表关联的数据量(set hive.enforce.bucketing=true设置该参数(默认为false)，分桶才生效)；对这种多个维度表都比较大的，采用一种变通的方式(采用反向箭头)，让维度表的关联字段指向事实表的主键，而不是采用在事实表中打维度表的代理键；在事实表的主键上面创建分桶的关联字段，然后其他三个大维度表通过关联得到事实表上的主键来进行查询(对应的SQL如下)，在维度表上创建分桶，且分桶字段是指向事实表的主键字段，且维度表的分桶数量是事实表的倍数(因为事实表已经做过聚合操作，数据量大概在两千多万的量级）。 4.在广告维度进行开发过程中发现异常数据，把表的存储格式转换成text格式后，如何快速定位到这条异常数据所做的数据文件，通过在hiveSQL中添加虚列:1). &nbsp;INPUT__FILE__NAME显示map任务读入File的全路径;2). &nbsp;BLOCK__OFFSET__INSIDE__FILE，如果是RCFile或者是SequenceFile块压缩格式文件则显示Block file Offset，如果是TextFile，显示当前行的第一个字节在文件中的偏移量;3). &nbsp;ROW__OFFSET__INSIDE__BLOCK &nbsp; &nbsp; &nbsp; &nbsp;RCFile和SequenceFile显示row number, textfile显示为0，要显示ROW__OFFSET__INSIDE__BLOCK ，必须设置set hive.exec.rowoffset=true; &nbsp; SQL样例： fact_adn_tracking_click表有 两千万 的记录，把fact_adn_tracking_click 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。&nbsp; 解决思路通过mapjoin仅仅查询出需要的字段进行： &nbsp;insert overwrite table dim_adn_creative_list&nbsp; &nbsp; select &nbsp;/*+mapjoin(c)*/c.id,d.* &nbsp; &nbsp; &nbsp; from ( select distinct id,creative_id from fact_adn_tracking_click) c &nbsp; &nbsp; &nbsp; join dim_adn_creative_list_snapshot &nbsp;d &nbsp; &nbsp; &nbsp; on c.creative_id = d.creative_list_id &nbsp; 目标表建表语句： CREATE EXTERNAL TABLE `dim_adn_creative_list`( &nbsp; `fact_sk` int COMMENT &#39;surrogate key&#39;,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) CLUSTERED BY (&nbsp; &nbsp; creative_list_sk)&nbsp; INTO 10000 BUCKETS ROW FORMAT SERDE&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcSerde&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat&#39; LOCATION &nbsp; &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list&#39;; &nbsp; ---------------------------普通表，分桶普通表，分桶ORC格式------------------------------------------------------ CREATE &nbsp;EXTERNAL TABLE `dim_adn_creative_list_perfomance`( &nbsp; `creative_list_sk` int,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) ROW FORMAT SERDE&nbsp; &nbsp; &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.mapred.TextInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39; LOCATION &nbsp; &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_perfomance&#39;; &nbsp; CREATE EXTERNAL TABLE `dim_adn_creative_list_perfomance_bucket`( &nbsp; `creative_list_sk` int,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) CLUSTERED BY (&nbsp; &nbsp; creative_list_id)&nbsp; SORTED BY (&nbsp; &nbsp; creative_list_id ASC)&nbsp; INTO 50 BUCKETS ROW FORMAT DELIMITED&nbsp; &nbsp; FIELDS TERMINATED BY &#39;\t&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.mapred.TextInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39; LOCATION &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_perfomance_bucket&#39;; &nbsp; CREATE EXTERNAL TABLE `dim_adn_creative_list_orc_bucket`( &nbsp; `creative_list_sk` int COMMENT &#39;surrogate key&#39;,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) CLUSTERED BY (&nbsp; &nbsp; creative_list_sk)&nbsp; INTO 50 BUCKETS ROW FORMAT SERDE&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcSerde&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat&#39; LOCATION &nbsp; &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_orc_bucket&#39;;" />
<link rel="canonical" href="https://mlh.app/2019/05/12/787214.html" />
<meta property="og:url" content="https://mlh.app/2019/05/12/787214.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"最近刚接触一个线上运行的数仓环境，是针对用户流量日志做点击量指标的多维度分析，维度表每天一个快照，经过数据统计分析发现有的维度表数据量很大，每天竟然有5亿多条的素材日志，并且这些维度数据是渐变维度，数据存储在亚马逊S3文件系统上面，严重浪费公司的存储成本，同时要是查询跨度一个周的数据则涉及到的维度数据就40亿条进行关联，这还不算其他维度的统计在内，个人观点，涉及到这些大维度数据的统计应该通过当前构建的数仓应该没有实际的应用价值。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 多维设计的整体思路要简化和加速查询，不要求必须满足3NF(消除数据冗余，规范化程度越高，需要关联的表也越多)，也就是说维度模型允许可控的数据冗余(数据更新异常，由于数据仓库中的数据很少有更新，主要是查询操作)，减少表和表间关系的数量，从而提高查询速度；例如，假设有100万订单，每个订单有10条明细，订单状态和订单明细状态各有10种，如果用户要查询某种状态特性的订单(状态是where过滤条件--&gt;维度，即看问题的角度)，按3NF模型，逻辑上需要关联100万与1000万的两个大表，然后过滤两个表的状态值得到所要的结果，其中100万是订单粒度，1000万是订单明细粒度；另一方面，事实表按最细数据粒度有1000万记录(订单明细)，3NF里的订单表属性在事实表里是冗余数据，状态维度有100条数据(10条订单状态 * 10条订单明细状态)，只需要关联1000万与100的两个表，再进行状态过滤即可。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过对现存数仓的调研提出下面的解决方案。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对维度表的处理：一、为提高查询性能，会优化表的存储，主要体现在1）存储类型采用压缩率比较高的ORC，降低公司存储成本，更重要的是该存储格式支持hive进行行级更新；（&nbsp;./hive –orcfiledump -j -p /hivedata/warehouse2/lxw1234_orc1/000000_0）2)通过水平拆分技术，采用分区+分桶，这些手段主要为减少扫描的数据量，分桶会根据下面的多维模型进行详细介绍；3)垂直拆分，因为有的表字段变化比较慢，有的表变化比较快，所以要对这些不同类型的字段区别对待，通过对各个维度表的数据统计分析，大部分字段变化的都很慢，凭借经验而谈，只有变化较慢的字段采用统计的价值，实际上90%以上的统计应该都落到变化较慢的字段上面，对应变化较慢的字段采用SCD2技术实现拉链表。二、为提供更丰富的查询类型，从事实表中抽取出维度数据，例如操作系统维度和设备品牌维度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最终设计出的多维数据模型如下所示： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; 图 1 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面构建的多维数据模型中的维度表字段没有全列出来，因为公司因为所限，但是不影响技术的分享；经过对维度表的分析，主要有三张维度表的数据量特别大，对应的数据量如下表所示： &nbsp; 维度名称 总条数 时间跨度(天) 平均每天的记录数 拉链表之后总条数 维度类型 备注 dim_adn_creative_list 123926985436 205 604521880 610904029 大维表 &nbsp; dim_adn_creative_source 38318221849 205 186918155 721969702 大维表 utime(更新时间戳) 和status(状态)两个字段变化相对较大 dim_adn_campain_list 23392624691 205 114110364 &nbsp; 大维表 已经定位到RDS数据库中个别字段中由\\n导致，还没有做SCD2拉链表 dim_adn_advertiser_list 180283 197 915 943 小维表 &nbsp; &nbsp; &nbsp; dim_adn_campaign_list每天产生上亿量级记录,如图2所示： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;图2 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在构建数仓的整个过程中有可能会出现的问题以及对应的解决方案(原则上要保证在查询数据时能够快速查询)： 1.有的维度字段数据变化比较慢，尽管是采用SCD2技术，数据量依然较大，通过技术评估，分区技术不是太好解决，采用分桶技术； 2.有的维度表字段变化比较快，这种可以考虑采用分区技术，例如一个月或者一个季度一个分区，把夸分区的生效时间和失效时间在分区时间点进行切分； 3.数据量比较大的维度表不止一个，如果是一个分桶就创建在大维度表与事实表关联的字段上面，这样可以采用hive的优化机制，bucket_join来大大减少表关联的数据量(set hive.enforce.bucketing=true设置该参数(默认为false)，分桶才生效)；对这种多个维度表都比较大的，采用一种变通的方式(采用反向箭头)，让维度表的关联字段指向事实表的主键，而不是采用在事实表中打维度表的代理键；在事实表的主键上面创建分桶的关联字段，然后其他三个大维度表通过关联得到事实表上的主键来进行查询(对应的SQL如下)，在维度表上创建分桶，且分桶字段是指向事实表的主键字段，且维度表的分桶数量是事实表的倍数(因为事实表已经做过聚合操作，数据量大概在两千多万的量级）。 4.在广告维度进行开发过程中发现异常数据，把表的存储格式转换成text格式后，如何快速定位到这条异常数据所做的数据文件，通过在hiveSQL中添加虚列:1). &nbsp;INPUT__FILE__NAME显示map任务读入File的全路径;2). &nbsp;BLOCK__OFFSET__INSIDE__FILE，如果是RCFile或者是SequenceFile块压缩格式文件则显示Block file Offset，如果是TextFile，显示当前行的第一个字节在文件中的偏移量;3). &nbsp;ROW__OFFSET__INSIDE__BLOCK &nbsp; &nbsp; &nbsp; &nbsp;RCFile和SequenceFile显示row number, textfile显示为0，要显示ROW__OFFSET__INSIDE__BLOCK ，必须设置set hive.exec.rowoffset=true; &nbsp; SQL样例： fact_adn_tracking_click表有 两千万 的记录，把fact_adn_tracking_click 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。&nbsp; 解决思路通过mapjoin仅仅查询出需要的字段进行： &nbsp;insert overwrite table dim_adn_creative_list&nbsp; &nbsp; select &nbsp;/*+mapjoin(c)*/c.id,d.* &nbsp; &nbsp; &nbsp; from ( select distinct id,creative_id from fact_adn_tracking_click) c &nbsp; &nbsp; &nbsp; join dim_adn_creative_list_snapshot &nbsp;d &nbsp; &nbsp; &nbsp; on c.creative_id = d.creative_list_id &nbsp; 目标表建表语句： CREATE EXTERNAL TABLE `dim_adn_creative_list`( &nbsp; `fact_sk` int COMMENT &#39;surrogate key&#39;,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) CLUSTERED BY (&nbsp; &nbsp; creative_list_sk)&nbsp; INTO 10000 BUCKETS ROW FORMAT SERDE&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcSerde&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat&#39; LOCATION &nbsp; &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list&#39;; &nbsp; ---------------------------普通表，分桶普通表，分桶ORC格式------------------------------------------------------ CREATE &nbsp;EXTERNAL TABLE `dim_adn_creative_list_perfomance`( &nbsp; `creative_list_sk` int,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) ROW FORMAT SERDE&nbsp; &nbsp; &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.mapred.TextInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39; LOCATION &nbsp; &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_perfomance&#39;; &nbsp; CREATE EXTERNAL TABLE `dim_adn_creative_list_perfomance_bucket`( &nbsp; `creative_list_sk` int,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) CLUSTERED BY (&nbsp; &nbsp; creative_list_id)&nbsp; SORTED BY (&nbsp; &nbsp; creative_list_id ASC)&nbsp; INTO 50 BUCKETS ROW FORMAT DELIMITED&nbsp; &nbsp; FIELDS TERMINATED BY &#39;\\t&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.mapred.TextInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39; LOCATION &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_perfomance_bucket&#39;; &nbsp; CREATE EXTERNAL TABLE `dim_adn_creative_list_orc_bucket`( &nbsp; `creative_list_sk` int COMMENT &#39;surrogate key&#39;,&nbsp; &nbsp; `creative_list_id` bigint,&nbsp; &nbsp; `creative_name` string,&nbsp; &nbsp; `user_id` bigint,&nbsp; &nbsp; `campaign_id` bigint,&nbsp; &nbsp; `type` int,&nbsp; &nbsp; `lang` int,&nbsp; &nbsp; `height` int,&nbsp; &nbsp; `width` int,&nbsp; &nbsp; `image` string,&nbsp; &nbsp; `text` string,&nbsp; &nbsp; `comment` string,&nbsp; &nbsp; `stime` bigint,&nbsp; &nbsp; `etime` bigint,&nbsp; &nbsp; `status` int,&nbsp; &nbsp; `timestamp` bigint,&nbsp; &nbsp; `tag` int,&nbsp; &nbsp; `version` int,&nbsp; &nbsp; `effective_date` string,&nbsp; &nbsp; `expiry_date` string) CLUSTERED BY (&nbsp; &nbsp; creative_list_sk)&nbsp; INTO 50 BUCKETS ROW FORMAT SERDE&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcSerde&#39;&nbsp; STORED AS INPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat&#39;&nbsp; OUTPUTFORMAT&nbsp; &nbsp; &#39;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat&#39; LOCATION &nbsp; &#39;s3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_orc_bucket&#39;;","@type":"BlogPosting","url":"https://mlh.app/2019/05/12/787214.html","headline":"基于大数据技术构建数仓模型实践","dateModified":"2019-05-12T00:00:00+08:00","datePublished":"2019-05-12T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/12/787214.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>基于大数据技术构建数仓模型实践</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>最近刚接触一个线上运行的数仓环境，是针对用户流量日志做点击量指标的多维度分析，维度表每天一个快照，经过数据统计分析发现有的维度表数据量很大，每天竟然有5亿多条的素材日志，并且这些维度数据是渐变维度，数据存储在亚马逊S3文件系统上面，严重浪费公司的存储成本，同时要是查询跨度一个周的数据则涉及到的维度数据就40亿条进行关联，这还不算其他维度的统计在内，个人观点，涉及到这些大维度数据的统计应该通过当前构建的数仓应该没有实际的应用价值。</p> 
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p> 
  <p>多维设计的整体思路要简化和加速查询，不要求必须满足3NF(消除数据冗余，规范化程度越高，需要关联的表也越多)，也就是说维度模型允许可控的数据冗余(数据更新异常，由于数据仓库中的数据很少有更新，主要是查询操作)，减少表和表间关系的数量，从而提高查询速度；例如，假设有100万订单，每个订单有10条明细，订单状态和订单明细状态各有10种，如果用户要查询某种状态特性的订单(状态是where过滤条件--&gt;维度，即看问题的角度)，按3NF模型，逻辑上需要关联100万与1000万的两个大表，然后过滤两个表的状态值得到所要的结果，其中100万是订单粒度，1000万是订单明细粒度；另一方面，事实表按最细数据粒度有1000万记录(订单明细)，3NF里的订单表属性在事实表里是冗余数据，状态维度有100条数据(10条订单状态 * 10条订单明细状态)，只需要关联1000万与100的两个表，再进行状态过滤即可。</p> 
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过对现存数仓的调研提出下面的解决方案。</p> 
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对维度表的处理：一、为提高查询性能，会优化表的存储，主要体现在1）存储类型采用压缩率比较高的ORC，降低公司存储成本，更重要的是该存储格式支持hive进行行级更新；（&nbsp;./hive –orcfiledump -j -p /hivedata/warehouse2/lxw1234_orc1/000000_0）2)通过水平拆分技术，采用分区+分桶，这些手段主要为减少扫描的数据量，分桶会根据下面的多维模型进行详细介绍；3)垂直拆分，因为有的表字段变化比较慢，有的表变化比较快，所以要对这些不同类型的字段区别对待，通过对各个维度表的数据统计分析，大部分字段变化的都很慢，凭借经验而谈，只有变化较慢的字段采用统计的价值，实际上90%以上的统计应该都落到变化较慢的字段上面，对应变化较慢的字段采用SCD2技术实现拉链表。二、为提供更丰富的查询类型，从事实表中抽取出维度数据，例如操作系统维度和设备品牌维度。</p> 
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最终设计出的多维数据模型如下所示：</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180413112846413?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FuZHlsaXV6aGlp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p> 
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; 图 1</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面构建的多维数据模型中的维度表字段没有全列出来，因为公司因为所限，但是不影响技术的分享；经过对维度表的分析，主要有三张维度表的数据量特别大，对应的数据量如下表所示：</p> 
  <p>&nbsp;</p> 
  <table border="1" cellpadding="0" cellspacing="0">
   <tbody>
    <tr>
     <td style="vertical-align:top;"> <p>维度名称</p> </td> 
     <td style="vertical-align:top;"> <p>总条数</p> </td> 
     <td style="vertical-align:top;"> <p>时间跨度(天)</p> </td> 
     <td style="vertical-align:top;"> <p>平均每天的记录数</p> </td> 
     <td style="vertical-align:top;"> <p>拉链表之后总条数</p> </td> 
     <td style="vertical-align:top;"> <p>维度类型</p> </td> 
     <td style="vertical-align:top;"> <p>备注</p> </td> 
    </tr>
    <tr>
     <td style="vertical-align:top;"> <p>dim_adn_creative_list</p> </td> 
     <td style="vertical-align:top;"> <p>123926985436</p> </td> 
     <td style="vertical-align:top;"> <p>205</p> </td> 
     <td style="vertical-align:top;"> <p>604521880</p> </td> 
     <td style="vertical-align:top;"> <p>610904029</p> </td> 
     <td style="vertical-align:top;"> <p>大维表</p> </td> 
     <td style="vertical-align:top;"> <p>&nbsp;</p> </td> 
    </tr>
    <tr>
     <td style="vertical-align:top;"> <p>dim_adn_creative_source</p> </td> 
     <td style="vertical-align:top;"> <p>38318221849</p> </td> 
     <td style="vertical-align:top;"> <p>205</p> </td> 
     <td style="vertical-align:top;"> <p>186918155</p> </td> 
     <td style="vertical-align:top;"> <p>721969702</p> </td> 
     <td style="vertical-align:top;"> <p>大维表</p> </td> 
     <td style="vertical-align:top;"> <p>utime(更新时间戳) 和status(状态)两个字段变化相对较大</p> </td> 
    </tr>
    <tr>
     <td style="vertical-align:top;"> <p>dim_adn_campain_list</p> </td> 
     <td style="vertical-align:top;"> <p>23392624691</p> </td> 
     <td style="vertical-align:top;"> <p>205</p> </td> 
     <td style="vertical-align:top;"> <p>114110364</p> </td> 
     <td style="vertical-align:top;"> <p>&nbsp;</p> </td> 
     <td style="vertical-align:top;"> <p>大维表</p> </td> 
     <td style="vertical-align:top;"> <p>已经定位到RDS数据库中个别字段中由\n导致，还没有做SCD2拉链表</p> </td> 
    </tr>
    <tr>
     <td style="vertical-align:top;"> <p>dim_adn_advertiser_list</p> </td> 
     <td style="vertical-align:top;"> <p>180283</p> </td> 
     <td style="vertical-align:top;"> <p>197</p> </td> 
     <td style="vertical-align:top;"> <p>915</p> </td> 
     <td style="vertical-align:top;"> <p>943</p> </td> 
     <td style="vertical-align:top;"> <p>小维表</p> </td> 
     <td style="vertical-align:top;"> <p>&nbsp;</p> </td> 
    </tr>
   </tbody>
  </table>
  <p>&nbsp;</p> 
  <p>&nbsp;</p> 
  <p>dim_adn_campaign_list每天产生上亿量级记录,如图2所示：</p> 
  <p><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180413133831775?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FuZHlsaXV6aGlp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p> 
  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;图2</p> 
  <p>&nbsp;</p> 
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在构建数仓的整个过程中有可能会出现的问题以及对应的解决方案(原则上要保证在查询数据时能够快速查询)：</p> 
  <p>1.有的维度字段数据变化比较慢，尽管是采用SCD2技术，数据量依然较大，通过技术评估，分区技术不是太好解决，采用分桶技术；</p> 
  <p>2.有的维度表字段变化比较快，这种可以考虑采用分区技术，例如一个月或者一个季度一个分区，把夸分区的生效时间和失效时间在分区时间点进行切分；</p> 
  <p>3.数据量比较大的维度表不止一个，如果是一个分桶就创建在大维度表与事实表关联的字段上面，这样可以采用hive的优化机制，bucket_join来大大减少表关联的数据量(set hive.enforce.bucketing=true设置该参数(默认为false)，分桶才生效)；对这种多个维度表都比较大的，采用一种变通的方式(采用反向箭头)，让维度表的关联字段指向事实表的主键，而不是采用在事实表中打维度表的代理键；在事实表的主键上面创建分桶的关联字段，然后其他三个大维度表通过关联得到事实表上的主键来进行查询(对应的SQL如下)，在维度表上创建分桶，且分桶字段是指向事实表的主键字段，且维度表的分桶数量是事实表的倍数(因为事实表已经做过聚合操作，数据量大概在两千多万的量级）。</p> 
  <p>4.在广告维度进行开发过程中发现异常数据，把表的存储格式转换成text格式后，如何快速定位到这条异常数据所做的数据文件，通过在hiveSQL中添加虚列:1). &nbsp;INPUT__FILE__NAME显示map任务读入File的全路径;2). &nbsp;BLOCK__OFFSET__INSIDE__FILE，如果是RCFile或者是SequenceFile块压缩格式文件则显示Block file Offset，如果是TextFile，显示当前行的第一个字节在文件中的偏移量;3). &nbsp;ROW__OFFSET__INSIDE__BLOCK &nbsp; &nbsp; &nbsp; &nbsp;RCFile和SequenceFile显示row number, textfile显示为0，要显示ROW__OFFSET__INSIDE__BLOCK ，必须设置set hive.exec.rowoffset=true;</p> 
  <p>&nbsp;</p> 
  <p>SQL样例：</p> 
  <p>fact_adn_tracking_click表有 两千万 的记录，把fact_adn_tracking_click 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。&nbsp;</p> 
  <p>解决思路通过mapjoin仅仅查询出需要的字段进行：</p> 
  <p>&nbsp;insert overwrite table dim_adn_creative_list&nbsp;<br> &nbsp; select &nbsp;/*+mapjoin(c)*/c.id,d.*<br> &nbsp; &nbsp; &nbsp; from ( select distinct id,creative_id from fact_adn_tracking_click) c<br> &nbsp; &nbsp; &nbsp; join dim_adn_creative_list_snapshot &nbsp;d</p> 
  <p>&nbsp; &nbsp; &nbsp; on c.creative_id = d.creative_list_id</p> 
  <p>&nbsp;</p> 
  <p>目标表建表语句：</p> 
  <p>CREATE EXTERNAL TABLE `dim_adn_creative_list`(<br> &nbsp; `fact_sk` int COMMENT 'surrogate key',&nbsp;<br> &nbsp; `creative_list_id` bigint,&nbsp;<br> &nbsp; `creative_name` string,&nbsp;<br> &nbsp; `user_id` bigint,&nbsp;<br> &nbsp; `campaign_id` bigint,&nbsp;<br> &nbsp; `type` int,&nbsp;<br> &nbsp; `lang` int,&nbsp;<br> &nbsp; `height` int,&nbsp;<br> &nbsp; `width` int,&nbsp;<br> &nbsp; `image` string,&nbsp;<br> &nbsp; `text` string,&nbsp;<br> &nbsp; `comment` string,&nbsp;<br> &nbsp; `stime` bigint,&nbsp;<br> &nbsp; `etime` bigint,&nbsp;<br> &nbsp; `status` int,&nbsp;<br> &nbsp; `timestamp` bigint,&nbsp;<br> &nbsp; `tag` int,&nbsp;<br> &nbsp; `version` int,&nbsp;<br> &nbsp; `effective_date` string,&nbsp;<br> &nbsp; `expiry_date` string)<br> CLUSTERED BY (&nbsp;<br> &nbsp; creative_list_sk)&nbsp;<br> INTO 10000 BUCKETS<br> ROW FORMAT SERDE&nbsp;<br> &nbsp; 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'&nbsp;<br> STORED AS INPUTFORMAT&nbsp;<br> &nbsp; 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'&nbsp;<br> OUTPUTFORMAT&nbsp;<br> &nbsp; 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'<br> LOCATION<br> &nbsp; 's3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list';</p> 
  <p>&nbsp;</p> 
  <p>---------------------------普通表，分桶普通表，分桶ORC格式------------------------------------------------------</p> 
  <p>CREATE &nbsp;EXTERNAL TABLE `dim_adn_creative_list_perfomance`(<br> &nbsp; `creative_list_sk` int,&nbsp;<br> &nbsp; `creative_list_id` bigint,&nbsp;<br> &nbsp; `creative_name` string,&nbsp;<br> &nbsp; `user_id` bigint,&nbsp;<br> &nbsp; `campaign_id` bigint,&nbsp;<br> &nbsp; `type` int,&nbsp;<br> &nbsp; `lang` int,&nbsp;<br> &nbsp; `height` int,&nbsp;<br> &nbsp; `width` int,&nbsp;<br> &nbsp; `image` string,&nbsp;<br> &nbsp; `text` string,&nbsp;<br> &nbsp; `comment` string,&nbsp;<br> &nbsp; `stime` bigint,&nbsp;<br> &nbsp; `etime` bigint,&nbsp;<br> &nbsp; `status` int,&nbsp;<br> &nbsp; `timestamp` bigint,&nbsp;<br> &nbsp; `tag` int,&nbsp;<br> &nbsp; `version` int,&nbsp;<br> &nbsp; `effective_date` string,&nbsp;<br> &nbsp; `expiry_date` string)<br> ROW FORMAT SERDE&nbsp;<br> &nbsp; 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'&nbsp;<br> STORED AS INPUTFORMAT&nbsp;<br> &nbsp; 'org.apache.hadoop.mapred.TextInputFormat'&nbsp;<br> OUTPUTFORMAT&nbsp;<br> &nbsp; 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'<br> LOCATION<br> &nbsp; 's3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_perfomance';</p> 
  <p>&nbsp;</p> 
  <p>CREATE EXTERNAL TABLE `dim_adn_creative_list_perfomance_bucket`(<br> &nbsp; `creative_list_sk` int,&nbsp;<br> &nbsp; `creative_list_id` bigint,&nbsp;<br> &nbsp; `creative_name` string,&nbsp;<br> &nbsp; `user_id` bigint,&nbsp;<br> &nbsp; `campaign_id` bigint,&nbsp;<br> &nbsp; `type` int,&nbsp;<br> &nbsp; `lang` int,&nbsp;<br> &nbsp; `height` int,&nbsp;<br> &nbsp; `width` int,&nbsp;<br> &nbsp; `image` string,&nbsp;<br> &nbsp; `text` string,&nbsp;<br> &nbsp; `comment` string,&nbsp;<br> &nbsp; `stime` bigint,&nbsp;<br> &nbsp; `etime` bigint,&nbsp;<br> &nbsp; `status` int,&nbsp;<br> &nbsp; `timestamp` bigint,&nbsp;<br> &nbsp; `tag` int,&nbsp;<br> &nbsp; `version` int,&nbsp;<br> &nbsp; `effective_date` string,&nbsp;<br> &nbsp; `expiry_date` string)<br> CLUSTERED BY (&nbsp;<br> &nbsp; creative_list_id)&nbsp;<br> SORTED BY (&nbsp;<br> &nbsp; creative_list_id ASC)&nbsp;<br> INTO 50 BUCKETS<br> ROW FORMAT DELIMITED&nbsp;<br> &nbsp; FIELDS TERMINATED BY '\t'&nbsp;<br> STORED AS INPUTFORMAT&nbsp;<br> &nbsp; 'org.apache.hadoop.mapred.TextInputFormat'&nbsp;<br> OUTPUTFORMAT&nbsp;<br> &nbsp; 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'<br> LOCATION<br> 's3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_perfomance_bucket';</p> 
  <p>&nbsp;</p> 
  <p>CREATE EXTERNAL TABLE `dim_adn_creative_list_orc_bucket`(<br> &nbsp; `creative_list_sk` int COMMENT 'surrogate key',&nbsp;<br> &nbsp; `creative_list_id` bigint,&nbsp;<br> &nbsp; `creative_name` string,&nbsp;<br> &nbsp; `user_id` bigint,&nbsp;<br> &nbsp; `campaign_id` bigint,&nbsp;<br> &nbsp; `type` int,&nbsp;<br> &nbsp; `lang` int,&nbsp;<br> &nbsp; `height` int,&nbsp;<br> &nbsp; `width` int,&nbsp;<br> &nbsp; `image` string,&nbsp;<br> &nbsp; `text` string,&nbsp;<br> &nbsp; `comment` string,&nbsp;<br> &nbsp; `stime` bigint,&nbsp;<br> &nbsp; `etime` bigint,&nbsp;<br> &nbsp; `status` int,&nbsp;<br> &nbsp; `timestamp` bigint,&nbsp;<br> &nbsp; `tag` int,&nbsp;<br> &nbsp; `version` int,&nbsp;<br> &nbsp; `effective_date` string,&nbsp;<br> &nbsp; `expiry_date` string)<br> CLUSTERED BY (&nbsp;<br> &nbsp; creative_list_sk)&nbsp;<br> INTO 50 BUCKETS<br> ROW FORMAT SERDE&nbsp;<br> &nbsp; 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'&nbsp;<br> STORED AS INPUTFORMAT&nbsp;<br> &nbsp; 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'&nbsp;<br> OUTPUTFORMAT&nbsp;<br> &nbsp; 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'<br> LOCATION<br> &nbsp; 's3://mob-emr-test/dataplatform/DataWareHouse/mobvista_dwh/tables/perm/dim_adn_creative_list_orc_bucket';</p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
