<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>神经网络中的神经元和激活函数详解 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="神经网络中的神经元和激活函数详解" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在上一节，我们通过两个浅显易懂的例子表明，人工智能的根本目标就是在不同的数据集中找到他们的边界，依靠这条边界线，当有新的数据点到来时，只要判断这个点与边界线的相互位置就可以判断新数据点的归属。 上一节我们举得例子中，数据集可以使用一条直线区分开。但对很多问题而言，单一直线是无法把数据点区分开的，例如亦或运算， 当两数的值不同时，亦或结果为1，相同时亦或运算结果为0，我们用 oxr 标记亦或运算，那么输入是0和1时，有以下几种情况： 1 xor 0 = 10 xor 1 = 11 xor 1 = 00 xor 0 = 0 1 我们把输入的四种情况绘制到坐标轴上看看： 我们看到，两个绿色点属于同一集合，因为绿色点做亦或运算后结果都是1，红色点属于统一集合，因为他们做运算后结果都是0，然而面对这种情形，你根本无法用一根直线把两种集合点区分开来，你必须如上图所示，用两根直线才能区分，如果点在两根直线之间，那么他们属于同一集合，如果点处于两跟直线之外，那么他们属于另外一个集合。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 所谓神经网络就是由很多个神经元组成的集合，我们可以认为一个神经元用于绘制一段分割线，复杂的数据分布需要很多条形状各异的线条才能组成合理的分界线，数据分布的情况越复杂，我们就需要越多个神经元来进行运算。 深度学习的神经网络借助了生物学对脑神经系统的研究成果。一个生物大脑神经元如下所示： 最左边的部分’dendrite‘叫突触，它用来接收外界输入的电信号，中间部分axon叫轴突，它把突触接收的信号进行整合处理，右边部分terminals叫终端输出，它把轴突整合后的信号切分成多部分，分别传送给其他神经元。下图是脑神经学家从鸽子脑子获取的神经元图像： 人的大脑大概有一千亿个神经元组成一个庞大的计算网络。苍蝇大脑只有十万个神经元，尽管如此，苍蝇就能够控制飞行，寻找食物，识别和躲避危险，这些很看似简单的动作操控就连现在最强大的计算机都无法实现。生物大脑其运算能力远逊于计算机，为何生物能轻而易举做到的事情计算机却做不到呢？大脑的运行机制目前人类还没有完全搞懂，但有一点可以肯定的是，生物大脑的运算运行存在“模糊性”，而电子计算机不行。 我们看一个神经元是如何工作的。神经元接收的是电信号，然后输出另一种电信号。如果输入电信号的强度不够大，那么神经元就不会做出任何反应，如果电信号的强度大于某个界限，那么神经元就会做出反应，向其他神经元传递电信号： 想象你把手指深入水中，如果水的温度不高，你不会感到疼痛，如果水的温度不断升高，当温度超过某个度数时，你会神经反射般的把手指抽出来，然后才感觉到疼痛，这就是输入神经元的电信号强度超过预定阈值后，神经元做出反应的结果。为了模拟神经元这种根据输入信号强弱做出反应的行为，在深度学习算法中，运用了多种函数来模拟这种特性，最常用的分布是步调函数和sigmoid函数，我们先看看步调函数的特性，我们通过以下代码来绘制步调函数： import matplotlib.pyplot as pltx = [1,2,3,4]y = [0, 1, 2, 3]plt.step(x, y)plt.show() 1 2 3 4 5 1 上面代码运行后结果如下： 我们看到，这个函数的特点是，当输入的x小于1时，函数的输出一直都是零。当输入的x大于等于1时，输出一下子从零跃迁到1，当x输入处于1到2之间时，输出一直是1，当x增大到2以上时，输出一下子跃迁到2，以此类推。 第二种常用的模拟函数就是sigmoid,它的形状就像字母S,输入下面代码绘制sigmoid函数： from matplotlib import pylabimport pylab as pltimport numpy as npdef sigmoid(x):&nbsp;&nbsp;&nbsp; return (1 / (1 + np.exp(-x)))mySamples = []mySigmoid = []x = plt.linspace(-10, 10, 10)y = plt.linspace(-10, 10, 100)plt.plot(x, sigmoid(x), &#39;r&#39;, label = &#39;linspace(-10, 10, 10)&#39;)plt.plot(y, sigmoid(y), &#39;r&#39;, label=&#39;linspace(-10, 10, 1000)&#39;)plt.grid()plt.title(&#39;Sigmoid function&#39;)plt.suptitle(&#39;Sigmoid&#39;)plt.legend(loc=&#39;lower right&#39;)plt.text(4, 0.8, r&#39;$\sigma(x)=\frac{1}{1+e^(-x)}$&#39;, fontsize=15)plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.1))plt.xlabel(&#39;X Axis&#39;)plt.ylabel(&#39;Y Axis&#39;)plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 1 上面代码执行后结果如下： 从函数图我们看到，当输入小于0时，函数的输出增长很缓慢，当输入大于0时，输出便极具增长，等到x大到一定程度后，输出保持在固定水准。sigmoid函数的代数式子如下： 其中的字母e表示欧拉常数，它的值约为2.71828。以后面对更复杂的问题时，我们还得使用更复杂的模拟函数，所有这些模拟神经元对电信号进行反应的函数统称为激活函数。 一个神经元会同时接收多个电信号，把这些电信号统一起来，用激活函数处理后再输出新的电信号，如下图： 神经网络算法中设计的神经元会同时接收多个输入参数，它把这些参数加总求和，然后代入用激活函数，产生的结果就是神经元输出的电信号。如果输入参数加总的值不大，那么输出的信号值就会很小，如果输入信号中，有某一个值很大其他的都很小，那么加总后值很大，输出的信号值就会变大，如果每个输入参数都不算太大，但加总后结果很大，于是输出的信号值就会很大，这种情况就使得运算具备一定的模糊性,这样就跟生物大脑的神经元运转方式很相像。 神经元不是各自为战，而是连成一个网络，并对电信号的处理形成一种链式反应： 前一个神经元接收输入信号，处理后会把输出信号分别传送给下一层的多个神经元。在神经网络算法上也会模拟这种特性，在算法设计中，我们会构造如下的数据结构： 上面有三层节点，每层有三个节点，第一层的节点接收输入，进行运算后，把输出结果分别提交给下一层的三个节点，如此类推直到最后一层。现在问题是，这种结构如何像上一节我们举得例子那样，根据误差进行学习调整呢？事实上，在上一层的节点把处理后的电信号传达到下一层的节点时，输出信号会分成若干部分分别传给下一层的不同节点，每一部分都对应一个权值，如下图： 我们看到，第一层的节点1把输出信号传给第二层的节点1时，传递路径上有一个权值W(1,1)，也就是节点1输出的电信号值乘以这个权值W(1,1)后，所得的结果才会提交给第二层的节点1，同理第一层节点1输出的信号要乘以W(1,2)这个权值后，所得结果才会传递给第二层节点2. 这些参数就对应于我们上一节例子中用于调整的参数，整个网络对输入进行运算后，在最外层产生输出，输出会跟结果进行比对，获取误差，然后网络再根据误差反过来调整这些层与层之间的传递参数，只不过参数调整的算法比我们前一节的例子要复杂不少。接下来我们看看一个具体的两层网络信号传递运算过程。 上图是一个两层网络，每个网络有两个节点，假设从第一次开始输入两个信号，分别是1，0.5： 第一层神经元直接把输入加总后分发到第二层，第二层神经元使用的激活函数是sigmoid, 神经元之间的信号权值如下： W(1,1) = 0.9; W(1,2) = 0.2W(2,1) = 0.3; W(2,2) = 0.8 1 2 1 就如上一节例子描述的，一开始每层节点间的权值是随机获取的。于是第一层神经元处理信号后把输出传递给第二层： 主要的运算发生在第二层的神经元节点。第二层的神经元要把第一层传来的信号值加总然后在传给sigmoid激活函数 从第一层第一个节点传给第二层第一个节点的信号值是 1.0 * 0.9 = 0.9; 第一层第二个节点传给第二层第一个节点的信号值是 0.5 * 0.3 = 0.15。第二层第一个节点把这些信号值加总后得 X = 0.9 + 0.15 = 1.05; 再把这个值传给sigmoid函数: 1 / (1 + exp(-x))；也就是y = 1 / (1 + exp(-1.05)) = 0.7408; 第一层第一个节点传递给第二层第二个节点的信号值是 1.0 * 0.2 = 0.2; 第一层第二个节点传给第二层第二个节点的信号值是 0.5 * 0.8 = 0.4, 第二层第二个节点把这些信号值加总得 X = 0.2 + 0.4 = 0.6, 再将其传入激活函数得 y = 1 / (1 + exp(-0.6)) = 0.6457，最后我们得到神经网络的输出结果为： (0.7408, 0.6457)。 下一节我们将深入研究如何使用张量运算加快神经网络的运算，以及探讨如何通过误差调整网络中节点间的权值。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在上一节，我们通过两个浅显易懂的例子表明，人工智能的根本目标就是在不同的数据集中找到他们的边界，依靠这条边界线，当有新的数据点到来时，只要判断这个点与边界线的相互位置就可以判断新数据点的归属。 上一节我们举得例子中，数据集可以使用一条直线区分开。但对很多问题而言，单一直线是无法把数据点区分开的，例如亦或运算， 当两数的值不同时，亦或结果为1，相同时亦或运算结果为0，我们用 oxr 标记亦或运算，那么输入是0和1时，有以下几种情况： 1 xor 0 = 10 xor 1 = 11 xor 1 = 00 xor 0 = 0 1 我们把输入的四种情况绘制到坐标轴上看看： 我们看到，两个绿色点属于同一集合，因为绿色点做亦或运算后结果都是1，红色点属于统一集合，因为他们做运算后结果都是0，然而面对这种情形，你根本无法用一根直线把两种集合点区分开来，你必须如上图所示，用两根直线才能区分，如果点在两根直线之间，那么他们属于同一集合，如果点处于两跟直线之外，那么他们属于另外一个集合。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 所谓神经网络就是由很多个神经元组成的集合，我们可以认为一个神经元用于绘制一段分割线，复杂的数据分布需要很多条形状各异的线条才能组成合理的分界线，数据分布的情况越复杂，我们就需要越多个神经元来进行运算。 深度学习的神经网络借助了生物学对脑神经系统的研究成果。一个生物大脑神经元如下所示： 最左边的部分’dendrite‘叫突触，它用来接收外界输入的电信号，中间部分axon叫轴突，它把突触接收的信号进行整合处理，右边部分terminals叫终端输出，它把轴突整合后的信号切分成多部分，分别传送给其他神经元。下图是脑神经学家从鸽子脑子获取的神经元图像： 人的大脑大概有一千亿个神经元组成一个庞大的计算网络。苍蝇大脑只有十万个神经元，尽管如此，苍蝇就能够控制飞行，寻找食物，识别和躲避危险，这些很看似简单的动作操控就连现在最强大的计算机都无法实现。生物大脑其运算能力远逊于计算机，为何生物能轻而易举做到的事情计算机却做不到呢？大脑的运行机制目前人类还没有完全搞懂，但有一点可以肯定的是，生物大脑的运算运行存在“模糊性”，而电子计算机不行。 我们看一个神经元是如何工作的。神经元接收的是电信号，然后输出另一种电信号。如果输入电信号的强度不够大，那么神经元就不会做出任何反应，如果电信号的强度大于某个界限，那么神经元就会做出反应，向其他神经元传递电信号： 想象你把手指深入水中，如果水的温度不高，你不会感到疼痛，如果水的温度不断升高，当温度超过某个度数时，你会神经反射般的把手指抽出来，然后才感觉到疼痛，这就是输入神经元的电信号强度超过预定阈值后，神经元做出反应的结果。为了模拟神经元这种根据输入信号强弱做出反应的行为，在深度学习算法中，运用了多种函数来模拟这种特性，最常用的分布是步调函数和sigmoid函数，我们先看看步调函数的特性，我们通过以下代码来绘制步调函数： import matplotlib.pyplot as pltx = [1,2,3,4]y = [0, 1, 2, 3]plt.step(x, y)plt.show() 1 2 3 4 5 1 上面代码运行后结果如下： 我们看到，这个函数的特点是，当输入的x小于1时，函数的输出一直都是零。当输入的x大于等于1时，输出一下子从零跃迁到1，当x输入处于1到2之间时，输出一直是1，当x增大到2以上时，输出一下子跃迁到2，以此类推。 第二种常用的模拟函数就是sigmoid,它的形状就像字母S,输入下面代码绘制sigmoid函数： from matplotlib import pylabimport pylab as pltimport numpy as npdef sigmoid(x):&nbsp;&nbsp;&nbsp; return (1 / (1 + np.exp(-x)))mySamples = []mySigmoid = []x = plt.linspace(-10, 10, 10)y = plt.linspace(-10, 10, 100)plt.plot(x, sigmoid(x), &#39;r&#39;, label = &#39;linspace(-10, 10, 10)&#39;)plt.plot(y, sigmoid(y), &#39;r&#39;, label=&#39;linspace(-10, 10, 1000)&#39;)plt.grid()plt.title(&#39;Sigmoid function&#39;)plt.suptitle(&#39;Sigmoid&#39;)plt.legend(loc=&#39;lower right&#39;)plt.text(4, 0.8, r&#39;$\sigma(x)=\frac{1}{1+e^(-x)}$&#39;, fontsize=15)plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.1))plt.xlabel(&#39;X Axis&#39;)plt.ylabel(&#39;Y Axis&#39;)plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 1 上面代码执行后结果如下： 从函数图我们看到，当输入小于0时，函数的输出增长很缓慢，当输入大于0时，输出便极具增长，等到x大到一定程度后，输出保持在固定水准。sigmoid函数的代数式子如下： 其中的字母e表示欧拉常数，它的值约为2.71828。以后面对更复杂的问题时，我们还得使用更复杂的模拟函数，所有这些模拟神经元对电信号进行反应的函数统称为激活函数。 一个神经元会同时接收多个电信号，把这些电信号统一起来，用激活函数处理后再输出新的电信号，如下图： 神经网络算法中设计的神经元会同时接收多个输入参数，它把这些参数加总求和，然后代入用激活函数，产生的结果就是神经元输出的电信号。如果输入参数加总的值不大，那么输出的信号值就会很小，如果输入信号中，有某一个值很大其他的都很小，那么加总后值很大，输出的信号值就会变大，如果每个输入参数都不算太大，但加总后结果很大，于是输出的信号值就会很大，这种情况就使得运算具备一定的模糊性,这样就跟生物大脑的神经元运转方式很相像。 神经元不是各自为战，而是连成一个网络，并对电信号的处理形成一种链式反应： 前一个神经元接收输入信号，处理后会把输出信号分别传送给下一层的多个神经元。在神经网络算法上也会模拟这种特性，在算法设计中，我们会构造如下的数据结构： 上面有三层节点，每层有三个节点，第一层的节点接收输入，进行运算后，把输出结果分别提交给下一层的三个节点，如此类推直到最后一层。现在问题是，这种结构如何像上一节我们举得例子那样，根据误差进行学习调整呢？事实上，在上一层的节点把处理后的电信号传达到下一层的节点时，输出信号会分成若干部分分别传给下一层的不同节点，每一部分都对应一个权值，如下图： 我们看到，第一层的节点1把输出信号传给第二层的节点1时，传递路径上有一个权值W(1,1)，也就是节点1输出的电信号值乘以这个权值W(1,1)后，所得的结果才会提交给第二层的节点1，同理第一层节点1输出的信号要乘以W(1,2)这个权值后，所得结果才会传递给第二层节点2. 这些参数就对应于我们上一节例子中用于调整的参数，整个网络对输入进行运算后，在最外层产生输出，输出会跟结果进行比对，获取误差，然后网络再根据误差反过来调整这些层与层之间的传递参数，只不过参数调整的算法比我们前一节的例子要复杂不少。接下来我们看看一个具体的两层网络信号传递运算过程。 上图是一个两层网络，每个网络有两个节点，假设从第一次开始输入两个信号，分别是1，0.5： 第一层神经元直接把输入加总后分发到第二层，第二层神经元使用的激活函数是sigmoid, 神经元之间的信号权值如下： W(1,1) = 0.9; W(1,2) = 0.2W(2,1) = 0.3; W(2,2) = 0.8 1 2 1 就如上一节例子描述的，一开始每层节点间的权值是随机获取的。于是第一层神经元处理信号后把输出传递给第二层： 主要的运算发生在第二层的神经元节点。第二层的神经元要把第一层传来的信号值加总然后在传给sigmoid激活函数 从第一层第一个节点传给第二层第一个节点的信号值是 1.0 * 0.9 = 0.9; 第一层第二个节点传给第二层第一个节点的信号值是 0.5 * 0.3 = 0.15。第二层第一个节点把这些信号值加总后得 X = 0.9 + 0.15 = 1.05; 再把这个值传给sigmoid函数: 1 / (1 + exp(-x))；也就是y = 1 / (1 + exp(-1.05)) = 0.7408; 第一层第一个节点传递给第二层第二个节点的信号值是 1.0 * 0.2 = 0.2; 第一层第二个节点传给第二层第二个节点的信号值是 0.5 * 0.8 = 0.4, 第二层第二个节点把这些信号值加总得 X = 0.2 + 0.4 = 0.6, 再将其传入激活函数得 y = 1 / (1 + exp(-0.6)) = 0.6457，最后我们得到神经网络的输出结果为： (0.7408, 0.6457)。 下一节我们将深入研究如何使用张量运算加快神经网络的运算，以及探讨如何通过误差调整网络中节点间的权值。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/12/781696.html" />
<meta property="og:url" content="https://mlh.app/2019/05/12/781696.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 在上一节，我们通过两个浅显易懂的例子表明，人工智能的根本目标就是在不同的数据集中找到他们的边界，依靠这条边界线，当有新的数据点到来时，只要判断这个点与边界线的相互位置就可以判断新数据点的归属。 上一节我们举得例子中，数据集可以使用一条直线区分开。但对很多问题而言，单一直线是无法把数据点区分开的，例如亦或运算， 当两数的值不同时，亦或结果为1，相同时亦或运算结果为0，我们用 oxr 标记亦或运算，那么输入是0和1时，有以下几种情况： 1 xor 0 = 10 xor 1 = 11 xor 1 = 00 xor 0 = 0 1 我们把输入的四种情况绘制到坐标轴上看看： 我们看到，两个绿色点属于同一集合，因为绿色点做亦或运算后结果都是1，红色点属于统一集合，因为他们做运算后结果都是0，然而面对这种情形，你根本无法用一根直线把两种集合点区分开来，你必须如上图所示，用两根直线才能区分，如果点在两根直线之间，那么他们属于同一集合，如果点处于两跟直线之外，那么他们属于另外一个集合。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 所谓神经网络就是由很多个神经元组成的集合，我们可以认为一个神经元用于绘制一段分割线，复杂的数据分布需要很多条形状各异的线条才能组成合理的分界线，数据分布的情况越复杂，我们就需要越多个神经元来进行运算。 深度学习的神经网络借助了生物学对脑神经系统的研究成果。一个生物大脑神经元如下所示： 最左边的部分’dendrite‘叫突触，它用来接收外界输入的电信号，中间部分axon叫轴突，它把突触接收的信号进行整合处理，右边部分terminals叫终端输出，它把轴突整合后的信号切分成多部分，分别传送给其他神经元。下图是脑神经学家从鸽子脑子获取的神经元图像： 人的大脑大概有一千亿个神经元组成一个庞大的计算网络。苍蝇大脑只有十万个神经元，尽管如此，苍蝇就能够控制飞行，寻找食物，识别和躲避危险，这些很看似简单的动作操控就连现在最强大的计算机都无法实现。生物大脑其运算能力远逊于计算机，为何生物能轻而易举做到的事情计算机却做不到呢？大脑的运行机制目前人类还没有完全搞懂，但有一点可以肯定的是，生物大脑的运算运行存在“模糊性”，而电子计算机不行。 我们看一个神经元是如何工作的。神经元接收的是电信号，然后输出另一种电信号。如果输入电信号的强度不够大，那么神经元就不会做出任何反应，如果电信号的强度大于某个界限，那么神经元就会做出反应，向其他神经元传递电信号： 想象你把手指深入水中，如果水的温度不高，你不会感到疼痛，如果水的温度不断升高，当温度超过某个度数时，你会神经反射般的把手指抽出来，然后才感觉到疼痛，这就是输入神经元的电信号强度超过预定阈值后，神经元做出反应的结果。为了模拟神经元这种根据输入信号强弱做出反应的行为，在深度学习算法中，运用了多种函数来模拟这种特性，最常用的分布是步调函数和sigmoid函数，我们先看看步调函数的特性，我们通过以下代码来绘制步调函数： import matplotlib.pyplot as pltx = [1,2,3,4]y = [0, 1, 2, 3]plt.step(x, y)plt.show() 1 2 3 4 5 1 上面代码运行后结果如下： 我们看到，这个函数的特点是，当输入的x小于1时，函数的输出一直都是零。当输入的x大于等于1时，输出一下子从零跃迁到1，当x输入处于1到2之间时，输出一直是1，当x增大到2以上时，输出一下子跃迁到2，以此类推。 第二种常用的模拟函数就是sigmoid,它的形状就像字母S,输入下面代码绘制sigmoid函数： from matplotlib import pylabimport pylab as pltimport numpy as npdef sigmoid(x):&nbsp;&nbsp;&nbsp; return (1 / (1 + np.exp(-x)))mySamples = []mySigmoid = []x = plt.linspace(-10, 10, 10)y = plt.linspace(-10, 10, 100)plt.plot(x, sigmoid(x), &#39;r&#39;, label = &#39;linspace(-10, 10, 10)&#39;)plt.plot(y, sigmoid(y), &#39;r&#39;, label=&#39;linspace(-10, 10, 1000)&#39;)plt.grid()plt.title(&#39;Sigmoid function&#39;)plt.suptitle(&#39;Sigmoid&#39;)plt.legend(loc=&#39;lower right&#39;)plt.text(4, 0.8, r&#39;$\\sigma(x)=\\frac{1}{1+e^(-x)}$&#39;, fontsize=15)plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.1))plt.xlabel(&#39;X Axis&#39;)plt.ylabel(&#39;Y Axis&#39;)plt.show() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 1 上面代码执行后结果如下： 从函数图我们看到，当输入小于0时，函数的输出增长很缓慢，当输入大于0时，输出便极具增长，等到x大到一定程度后，输出保持在固定水准。sigmoid函数的代数式子如下： 其中的字母e表示欧拉常数，它的值约为2.71828。以后面对更复杂的问题时，我们还得使用更复杂的模拟函数，所有这些模拟神经元对电信号进行反应的函数统称为激活函数。 一个神经元会同时接收多个电信号，把这些电信号统一起来，用激活函数处理后再输出新的电信号，如下图： 神经网络算法中设计的神经元会同时接收多个输入参数，它把这些参数加总求和，然后代入用激活函数，产生的结果就是神经元输出的电信号。如果输入参数加总的值不大，那么输出的信号值就会很小，如果输入信号中，有某一个值很大其他的都很小，那么加总后值很大，输出的信号值就会变大，如果每个输入参数都不算太大，但加总后结果很大，于是输出的信号值就会很大，这种情况就使得运算具备一定的模糊性,这样就跟生物大脑的神经元运转方式很相像。 神经元不是各自为战，而是连成一个网络，并对电信号的处理形成一种链式反应： 前一个神经元接收输入信号，处理后会把输出信号分别传送给下一层的多个神经元。在神经网络算法上也会模拟这种特性，在算法设计中，我们会构造如下的数据结构： 上面有三层节点，每层有三个节点，第一层的节点接收输入，进行运算后，把输出结果分别提交给下一层的三个节点，如此类推直到最后一层。现在问题是，这种结构如何像上一节我们举得例子那样，根据误差进行学习调整呢？事实上，在上一层的节点把处理后的电信号传达到下一层的节点时，输出信号会分成若干部分分别传给下一层的不同节点，每一部分都对应一个权值，如下图： 我们看到，第一层的节点1把输出信号传给第二层的节点1时，传递路径上有一个权值W(1,1)，也就是节点1输出的电信号值乘以这个权值W(1,1)后，所得的结果才会提交给第二层的节点1，同理第一层节点1输出的信号要乘以W(1,2)这个权值后，所得结果才会传递给第二层节点2. 这些参数就对应于我们上一节例子中用于调整的参数，整个网络对输入进行运算后，在最外层产生输出，输出会跟结果进行比对，获取误差，然后网络再根据误差反过来调整这些层与层之间的传递参数，只不过参数调整的算法比我们前一节的例子要复杂不少。接下来我们看看一个具体的两层网络信号传递运算过程。 上图是一个两层网络，每个网络有两个节点，假设从第一次开始输入两个信号，分别是1，0.5： 第一层神经元直接把输入加总后分发到第二层，第二层神经元使用的激活函数是sigmoid, 神经元之间的信号权值如下： W(1,1) = 0.9; W(1,2) = 0.2W(2,1) = 0.3; W(2,2) = 0.8 1 2 1 就如上一节例子描述的，一开始每层节点间的权值是随机获取的。于是第一层神经元处理信号后把输出传递给第二层： 主要的运算发生在第二层的神经元节点。第二层的神经元要把第一层传来的信号值加总然后在传给sigmoid激活函数 从第一层第一个节点传给第二层第一个节点的信号值是 1.0 * 0.9 = 0.9; 第一层第二个节点传给第二层第一个节点的信号值是 0.5 * 0.3 = 0.15。第二层第一个节点把这些信号值加总后得 X = 0.9 + 0.15 = 1.05; 再把这个值传给sigmoid函数: 1 / (1 + exp(-x))；也就是y = 1 / (1 + exp(-1.05)) = 0.7408; 第一层第一个节点传递给第二层第二个节点的信号值是 1.0 * 0.2 = 0.2; 第一层第二个节点传给第二层第二个节点的信号值是 0.5 * 0.8 = 0.4, 第二层第二个节点把这些信号值加总得 X = 0.2 + 0.4 = 0.6, 再将其传入激活函数得 y = 1 / (1 + exp(-0.6)) = 0.6457，最后我们得到神经网络的输出结果为： (0.7408, 0.6457)。 下一节我们将深入研究如何使用张量运算加快神经网络的运算，以及探讨如何通过误差调整网络中节点间的权值。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/12/781696.html","headline":"神经网络中的神经元和激活函数详解","dateModified":"2019-05-12T00:00:00+08:00","datePublished":"2019-05-12T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/12/781696.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>神经网络中的神经元和激活函数详解</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div class="markdown_views" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <div class="markdown_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <!-- flowchart &amp;#31661;&amp;#22836;&amp;#22270;&amp;#26631; &amp;#21247;&amp;#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <p>在上一节，我们通过两个浅显易懂的例子表明，人工智能的根本目标就是在不同的数据集中找到他们的边界，依靠这条边界线，当有新的数据点到来时，只要判断这个点与边界线的相互位置就可以判断新数据点的归属。</p>
    <p><img title="" alt="这里写图片描述" src="http://upload-images.jianshu.io/upload_images/2849961-92bf182dd096b854?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
    <p>上一节我们举得例子中，数据集可以使用一条直线区分开。但对很多问题而言，单一直线是无法把数据点区分开的，例如亦或运算， 当两数的值不同时，亦或结果为1，相同时亦或运算结果为0，我们用 oxr 标记亦或运算，那么输入是0和1时，有以下几种情况：</p>
    <pre class="prettyprint"><code class="hljs erlang has-numbering"><span class="hljs-number">1</span> <span class="hljs-keyword">xor</span> <span class="hljs-number">0</span> = <span class="hljs-number">1</span><span class="hljs-number">0</span> <span class="hljs-keyword">xor</span> <span class="hljs-number">1</span> = <span class="hljs-number">1</span><span class="hljs-number">1</span> <span class="hljs-keyword">xor</span> <span class="hljs-number">1</span> = <span class="hljs-number">0</span><span class="hljs-number">0</span> <span class="hljs-keyword">xor</span> <span class="hljs-number">0</span> = <span class="hljs-number">0</span></code>
     <div class="hljs-button signin"></div>
     <ul class="pre-numbering"></ul>
     <ul class="pre-numbering">
      <li>1</li>
     </ul></pre>
    <p>我们把输入的四种情况绘制到坐标轴上看看：</p>
    <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209161732426?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
    <p>我们看到，两个绿色点属于同一集合，因为绿色点做亦或运算后结果都是1，红色点属于统一集合，因为他们做运算后结果都是0，然而面对这种情形，你根本无法用一根直线把两种集合点区分开来，你必须如上图所示，用两根直线才能区分，如果点在两根直线之间，那么他们属于同一集合，如果点处于两跟直线之外，那么他们属于另外一个集合。</p>
   </div>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <p>所谓神经网络就是由很多个神经元组成的集合，我们可以认为一个神经元用于绘制一段分割线，复杂的数据分布需要很多条形状各异的线条才能组成合理的分界线，数据分布的情况越复杂，我们就需要越多个神经元来进行运算。</p>
   <p>深度学习的神经网络借助了生物学对脑神经系统的研究成果。一个生物大脑神经元如下所示：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209162644343?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>最左边的部分’dendrite‘叫突触，它用来接收外界输入的电信号，中间部分axon叫轴突，它把突触接收的信号进行整合处理，右边部分terminals叫终端输出，它把轴突整合后的信号切分成多部分，分别传送给其他神经元。下图是脑神经学家从鸽子脑子获取的神经元图像：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209162957635?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>人的大脑大概有一千亿个神经元组成一个庞大的计算网络。苍蝇大脑只有十万个神经元，尽管如此，苍蝇就能够控制飞行，寻找食物，识别和躲避危险，这些很看似简单的动作操控就连现在最强大的计算机都无法实现。生物大脑其运算能力远逊于计算机，为何生物能轻而易举做到的事情计算机却做不到呢？大脑的运行机制目前人类还没有完全搞懂，但有一点可以肯定的是，生物大脑的运算运行存在“模糊性”，而电子计算机不行。</p>
   <p>我们看一个神经元是如何工作的。神经元接收的是电信号，然后输出另一种电信号。如果输入电信号的强度不够大，那么神经元就不会做出任何反应，如果电信号的强度大于某个界限，那么神经元就会做出反应，向其他神经元传递电信号：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209163641726?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>想象你把手指深入水中，如果水的温度不高，你不会感到疼痛，如果水的温度不断升高，当温度超过某个度数时，你会神经反射般的把手指抽出来，然后才感觉到疼痛，这就是输入神经元的电信号强度超过预定阈值后，神经元做出反应的结果。为了模拟神经元这种根据输入信号强弱做出反应的行为，在深度学习算法中，运用了多种函数来模拟这种特性，最常用的分布是步调函数和sigmoid函数，我们先看看步调函数的特性，我们通过以下代码来绘制步调函数：</p>
   <pre class="prettyprint"><code class="hljs avrasm has-numbering">import matplotlib<span class="hljs-preprocessor">.pyplot</span> as plt<span class="hljs-built_in">x</span> = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]<span class="hljs-built_in">y</span> = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]plt<span class="hljs-preprocessor">.step</span>(<span class="hljs-built_in">x</span>, <span class="hljs-built_in">y</span>)plt<span class="hljs-preprocessor">.show</span>()</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>上面代码运行后结果如下：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209164248398?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>我们看到，这个函数的特点是，当输入的x小于1时，函数的输出一直都是零。当输入的x大于等于1时，输出一下子从零跃迁到1，当x输入处于1到2之间时，输出一直是1，当x增大到2以上时，输出一下子跃迁到2，以此类推。</p>
   <p>第二种常用的模拟函数就是sigmoid,它的形状就像字母S,输入下面代码绘制sigmoid函数：</p>
   <pre class="prettyprint"><code class="hljs python has-numbering"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pylab<span class="hljs-keyword">import</span> pylab <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(x)</span>:</span>&nbsp;&nbsp;&nbsp; <span class="hljs-keyword">return</span> (<span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x)))mySamples = []mySigmoid = []x = plt.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>)y = plt.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>)plt.plot(x, sigmoid(x), <span class="hljs-string">'r'</span>, label = <span class="hljs-string">'linspace(-10, 10, 10)'</span>)plt.plot(y, sigmoid(y), <span class="hljs-string">'r'</span>, label=<span class="hljs-string">'linspace(-10, 10, 1000)'</span>)plt.grid()plt.title(<span class="hljs-string">'Sigmoid function'</span>)plt.suptitle(<span class="hljs-string">'Sigmoid'</span>)plt.legend(loc=<span class="hljs-string">'lower right'</span>)plt.text(<span class="hljs-number">4</span>, <span class="hljs-number">0.8</span>, <span class="hljs-string">r'$\sigma(x)=\frac{1}{1+e^(-x)}$'</span>, fontsize=<span class="hljs-number">15</span>)plt.gca().xaxis.set_major_locator(plt.MultipleLocator(<span class="hljs-number">1</span>))plt.gca().yaxis.set_major_locator(plt.MultipleLocator(<span class="hljs-number">0.1</span>))plt.xlabel(<span class="hljs-string">'X Axis'</span>)plt.ylabel(<span class="hljs-string">'Y Axis'</span>)plt.show()</code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
     <li>3</li>
     <li>4</li>
     <li>5</li>
     <li>6</li>
     <li>7</li>
     <li>8</li>
     <li>9</li>
     <li>10</li>
     <li>11</li>
     <li>12</li>
     <li>13</li>
     <li>14</li>
     <li>15</li>
     <li>16</li>
     <li>17</li>
     <li>18</li>
     <li>19</li>
     <li>20</li>
     <li>21</li>
     <li>22</li>
     <li>23</li>
     <li>24</li>
     <li>25</li>
     <li>26</li>
     <li>27</li>
     <li>28</li>
     <li>29</li>
     <li>30</li>
     <li>31</li>
     <li>32</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>上面代码执行后结果如下：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209164713667?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>从函数图我们看到，当输入小于0时，函数的输出增长很缓慢，当输入大于0时，输出便极具增长，等到x大到一定程度后，输出保持在固定水准。sigmoid函数的代数式子如下：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209164904517?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>其中的字母e表示欧拉常数，它的值约为2.71828。以后面对更复杂的问题时，我们还得使用更复杂的模拟函数，所有这些模拟神经元对电信号进行反应的函数统称为<strong><em>激活函数</em></strong>。</p>
   <p>一个神经元会同时接收多个电信号，把这些电信号统一起来，用激活函数处理后再输出新的电信号，如下图：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209165320424?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>神经网络算法中设计的神经元会同时接收多个输入参数，它把这些参数加总求和，然后代入用激活函数，产生的结果就是神经元输出的电信号。如果输入参数加总的值不大，那么输出的信号值就会很小，如果输入信号中，有某一个值很大其他的都很小，那么加总后值很大，输出的信号值就会变大，如果每个输入参数都不算太大，但加总后结果很大，于是输出的信号值就会很大，这种情况就使得运算具备一定的<strong><em>模糊性</em></strong>,这样就跟生物大脑的神经元运转方式很相像。</p>
   <p>神经元不是各自为战，而是连成一个网络，并对电信号的处理形成一种链式反应：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209165844533?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>前一个神经元接收输入信号，处理后会把输出信号分别传送给下一层的多个神经元。在神经网络算法上也会模拟这种特性，在算法设计中，我们会构造如下的数据结构：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209170033172?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>上面有三层节点，每层有三个节点，第一层的节点接收输入，进行运算后，把输出结果分别提交给下一层的三个节点，如此类推直到最后一层。现在问题是，这种结构如何像上一节我们举得例子那样，根据误差进行学习调整呢？事实上，在上一层的节点把处理后的电信号传达到下一层的节点时，输出信号会分成若干部分分别传给下一层的不同节点，每一部分都对应一个权值，如下图：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209170352549?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>我们看到，第一层的节点1把输出信号传给第二层的节点1时，传递路径上有一个权值W(1,1)，也就是节点1输出的电信号值乘以这个权值W(1,1)后，所得的结果才会提交给第二层的节点1，同理第一层节点1输出的信号要乘以W(1,2)这个权值后，所得结果才会传递给第二层节点2.</p>
   <p>这些参数就对应于我们上一节例子中用于调整的参数，整个网络对输入进行运算后，在最外层产生输出，输出会跟结果进行比对，获取误差，然后网络再根据误差反过来调整这些层与层之间的传递参数，只不过参数调整的算法比我们前一节的例子要复杂不少。接下来我们看看一个具体的两层网络信号传递运算过程。</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209174442510?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>上图是一个两层网络，每个网络有两个节点，假设从第一次开始输入两个信号，分别是1，0.5：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209174539898?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>第一层神经元直接把输入加总后分发到第二层，第二层神经元使用的激活函数是sigmoid, 神经元之间的信号权值如下：</p>
   <pre class="prettyprint"><code class="hljs fix has-numbering"><span class="hljs-attribute">W(1,1) </span>=<span class="hljs-string"> 0.9; W(1,2) = 0.2W(2,1) = 0.3; W(2,2) = 0.8</span></code>
    <div class="hljs-button signin"></div>
    <ul class="pre-numbering">
     <li>1</li>
     <li>2</li>
    </ul>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>就如上一节例子描述的，一开始每层节点间的权值是随机获取的。于是第一层神经元处理信号后把输出传递给第二层：</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209174933916?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>主要的运算发生在第二层的神经元节点。第二层的神经元要把第一层传来的信号值加总然后在传给sigmoid激活函数</p>
   <p><img title="" alt="这里写图片描述" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180209175316154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHlsZXJfZG93bmxvYWQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
   <p>从第一层第一个节点传给第二层第一个节点的信号值是 1.0 * 0.9 = 0.9; 第一层第二个节点传给第二层第一个节点的信号值是 0.5 * 0.3 = 0.15。第二层第一个节点把这些信号值加总后得 X = 0.9 + 0.15 = 1.05; 再把这个值传给sigmoid函数: 1 / (1 + exp(-x))；也就是y = 1 / (1 + exp(-1.05)) = 0.7408;</p>
   <p>第一层第一个节点传递给第二层第二个节点的信号值是 1.0 * 0.2 = 0.2; 第一层第二个节点传给第二层第二个节点的信号值是 0.5 * 0.8 = 0.4, 第二层第二个节点把这些信号值加总得 X = 0.2 + 0.4 = 0.6, 再将其传入激活函数得 y = 1 / (1 + exp(-0.6)) = 0.6457，最后我们得到神经网络的输出结果为： (0.7408, 0.6457)。</p>
   <p>下一节我们将深入研究如何使用张量运算加快神经网络的运算，以及探讨如何通过误差调整网络中节点间的权值。</p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
