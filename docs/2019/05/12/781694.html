<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>机器学习–Logistic回归计算过程的推导 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="机器学习–Logistic回归计算过程的推导" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (很多讲逻辑回归的文章都没有给出详细的推导，只是列出最后的计算公式，今天在网上看到一篇解释得非常详细的文章，赶紧转载一下： 【机器学习笔记1】Logistic回归总结(http://blog.csdn.net/dongtingzhizi/article/details/15962797) &nbsp;作者说&quot;未经允许，不得转载&quot;，我这里先冒犯了，如果觉得不合适，请告知。 ) &nbsp;Logistic回归总结 1.引言 首先说一下我的感受，《机器学习实战》一书在介绍原理的同时将全部的算法用源代码实现，非常具有操作性，可以加深对算法的理解，但是美中不足的是在原理上介绍的比较粗略，很多细节没有具体介绍。所以，对于没有基础的朋友（包括我）某些地方可能看的一头雾水，需要查阅相关资料进行了解。所以说，该书还是比较适合有基础的朋友。 本文主要介绍以下三个方面的内容： （1）Logistic Regression的基本原理，分布在第二章中； （2）Logistic Regression的具体过程，包括：选取预测函数，求解Cost函数和J(θ)，梯度下降法求J(θ)的最小值，以及递归下降过程的向量化（vectorization），分布在第三章中； （3）对《机器学习实战》中给出的实现代码进行了分析，对阅读该书LogisticRegression部分遇到的疑惑进行了解释。没有基础的朋友在阅读该书的Logistic Regression部分时可能会觉得一头雾水，书中给出的代码很简单，但是怎么也跟书中介绍的理论联系不起来。也会有很多的疑问，比如：一般都是用梯度下降法求损失函数的最小值，为何这里用梯度上升法呢？书中说用梯度上升发，为何代码实现时没见到求梯度的代码呢？这些问题在第三章和第四章中都会得到解答。 文中参考或引用内容的出处列在最后的“参考文献”中。文中所阐述的内容仅仅是我个人的理解，如有错误或疏漏，欢迎大家批评指正。下面进入正题。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 2. 基本原理 Logistic Regression和Linear Regression的原理是相似的，按照我自己的理解，可以简单的描述为这样的过程： （1）找一个合适的预测函数（Andrew Ng的公开课中称为hypothesis），一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。 （2）构造一个Cost函数（损失函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。 （3）显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。 3. 具体过程 3.1&nbsp; 构造预测函数 Logistic Regression虽然名字里带“回归”，但是它实际上是一种分类方法，用于两分类问题（即输出只有两种）。根据第二章中的步骤，需要先找到一个预测函数（h），显然，该函数的输出必须是两个值（分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 对应的函数图像是一个取值在0和1之间的S型曲线（图1）。 图1 接下来需要确定数据划分的边界类型，对于图2和图3中的两种数据分布，显然图2需要一个线性的边界，而图3需要一个非线性的边界。接下来我们只讨论线性边界的情况。 图2 图3 对于线性边界的情况，边界形式如下： 构造预测函数为： hθ(x)函数的值有特殊的含义，它表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为： 3.2&nbsp; 构造Cost函数 Andrew Ng在课程中直接给出了Cost函数及J(θ)函数如式（5）和（6），但是并没有给出具体的解释，只是说明了这个函数来衡量h函数预测的好坏是合理的。 实际上这里的Cost函数和J(θ)函数是基于最大似然估计推导得到的。下面详细说明推导的过程。（4）式综合起来可以写成： 取似然函数为： 对数似然函数为： 最大似然估计就是要求得使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将J(θ)取为（6）式，即： 因为乘了一个负的系数-1/m，所以J(θ)取最小值时的θ为要求的最佳参数。 3.3&nbsp; 梯度下降法求J(θ)的最小值 求J(θ)的最小值可以使用梯度下降法，根据梯度下降法可得θ的更新过程： &nbsp; &nbsp;&nbsp; 式中为α学习步长，下面来求偏导： 上式求解过程中用到如下的公式： 因此，（11）式的更新过程可以写成： &nbsp; 因为式中α本来为一常量，所以1/m一般将省略，所以最终的θ更新过程为： 另外，补充一下，3.2节中提到求得l(θ)取最大值时的θ也是一样的，用梯度上升法求（9）式的最大值，可得： &nbsp;&nbsp; 观察上式发现跟（14）是一样的，所以，采用梯度上升发和梯度下降法是完全一样的，这也是《机器学习实战》中采用梯度上升法的原因。 3.4&nbsp; 梯度下降过程向量化 关于θ更新过程的vectorization，Andrew Ng的课程中只是一带而过，没有具体的讲解。 《机器学习实战》连Cost函数及求梯度等都没有说明，所以更不可能说明vectorization了。但是，其中给出的实现代码确是实现了vectorization的，图4所示代码的32行中weights（也就是θ）的更新只用了一行代码，直接通过矩阵或者向量计算更新，没有用for循环，说明确实实现了vectorization，具体代码下一章分析。 文献[3]中也提到了vectorization，但是也是比较粗略，很简单的给出vectorization的结果为： &nbsp; &nbsp;&nbsp; 且不论该更新公式正确与否，这里的Σ(...)是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization，不像《机器学习实战》的代码中一条语句就可以完成θ的更新。 下面说明一下我理解《机器学习实战》中代码实现的vectorization过程。 约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值： 约定待求的参数θ的矩阵形式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 先求x.θ并记为A： 求hθ(x)-y并记为E： &nbsp; g(A)的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知hθ(x)-y可以由g(A)-y一次计算求得。 再来看一下（15）式的θ更新过程，当j=0时： 同样的可以写出θj， 综合起来就是： 综上所述，vectorization后θ更新的步骤如下： （1）求A=x.θ； （2）求E=g(A)-y； （3）求θ:=θ-α.x&#39;.E,x&#39;表示矩阵x的转置。 也可以综合起来写成： 前面已经提到过：1/m是可以省略的。 4. 代码分析 图4中是《机器学习实战》中给出的部分实现代码。 图4 sigmoid函数就是前文中的g(z)函数，参数inX可以是向量，因为程序中使用了Python的numpy。 gradAscent函数是梯度上升的实现函数，参数dataMatin和classLabels为训练数据，23和24行对训练数据做了处理，转换成numpy的矩阵类型，同时将横向量的classlabels转换成列向量labelMat，此时的dataMatrix和labelMat就是（18）式中的x和y。alpha为学习步长，maxCycles为迭代次数。weights为n维（等于x的列数）列向量，就是（19）式中的θ。 29行的for循环将更新θ的过程迭代maxCycles次，每循环一次更新一次。对比3.4节最后总结的向量化的θ更新步骤，30行相当于求了A=x.θ和g(A)，31行相当于求了E=g(A)-y，32行相当于求θ:=θ-α.x&#39;.E。所以这三行代码实际上与向量化的θ更新步骤是完全一致的。 总结一下，从上面代码分析可以看出，虽然只有十多行的代码，但是里面却隐含了太多的细节，如果没有相关基础确实是非常难以理解的。相信完整的阅读了本文，就应该没有问题了！^_^。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (很多讲逻辑回归的文章都没有给出详细的推导，只是列出最后的计算公式，今天在网上看到一篇解释得非常详细的文章，赶紧转载一下： 【机器学习笔记1】Logistic回归总结(http://blog.csdn.net/dongtingzhizi/article/details/15962797) &nbsp;作者说&quot;未经允许，不得转载&quot;，我这里先冒犯了，如果觉得不合适，请告知。 ) &nbsp;Logistic回归总结 1.引言 首先说一下我的感受，《机器学习实战》一书在介绍原理的同时将全部的算法用源代码实现，非常具有操作性，可以加深对算法的理解，但是美中不足的是在原理上介绍的比较粗略，很多细节没有具体介绍。所以，对于没有基础的朋友（包括我）某些地方可能看的一头雾水，需要查阅相关资料进行了解。所以说，该书还是比较适合有基础的朋友。 本文主要介绍以下三个方面的内容： （1）Logistic Regression的基本原理，分布在第二章中； （2）Logistic Regression的具体过程，包括：选取预测函数，求解Cost函数和J(θ)，梯度下降法求J(θ)的最小值，以及递归下降过程的向量化（vectorization），分布在第三章中； （3）对《机器学习实战》中给出的实现代码进行了分析，对阅读该书LogisticRegression部分遇到的疑惑进行了解释。没有基础的朋友在阅读该书的Logistic Regression部分时可能会觉得一头雾水，书中给出的代码很简单，但是怎么也跟书中介绍的理论联系不起来。也会有很多的疑问，比如：一般都是用梯度下降法求损失函数的最小值，为何这里用梯度上升法呢？书中说用梯度上升发，为何代码实现时没见到求梯度的代码呢？这些问题在第三章和第四章中都会得到解答。 文中参考或引用内容的出处列在最后的“参考文献”中。文中所阐述的内容仅仅是我个人的理解，如有错误或疏漏，欢迎大家批评指正。下面进入正题。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 2. 基本原理 Logistic Regression和Linear Regression的原理是相似的，按照我自己的理解，可以简单的描述为这样的过程： （1）找一个合适的预测函数（Andrew Ng的公开课中称为hypothesis），一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。 （2）构造一个Cost函数（损失函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。 （3）显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。 3. 具体过程 3.1&nbsp; 构造预测函数 Logistic Regression虽然名字里带“回归”，但是它实际上是一种分类方法，用于两分类问题（即输出只有两种）。根据第二章中的步骤，需要先找到一个预测函数（h），显然，该函数的输出必须是两个值（分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 对应的函数图像是一个取值在0和1之间的S型曲线（图1）。 图1 接下来需要确定数据划分的边界类型，对于图2和图3中的两种数据分布，显然图2需要一个线性的边界，而图3需要一个非线性的边界。接下来我们只讨论线性边界的情况。 图2 图3 对于线性边界的情况，边界形式如下： 构造预测函数为： hθ(x)函数的值有特殊的含义，它表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为： 3.2&nbsp; 构造Cost函数 Andrew Ng在课程中直接给出了Cost函数及J(θ)函数如式（5）和（6），但是并没有给出具体的解释，只是说明了这个函数来衡量h函数预测的好坏是合理的。 实际上这里的Cost函数和J(θ)函数是基于最大似然估计推导得到的。下面详细说明推导的过程。（4）式综合起来可以写成： 取似然函数为： 对数似然函数为： 最大似然估计就是要求得使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将J(θ)取为（6）式，即： 因为乘了一个负的系数-1/m，所以J(θ)取最小值时的θ为要求的最佳参数。 3.3&nbsp; 梯度下降法求J(θ)的最小值 求J(θ)的最小值可以使用梯度下降法，根据梯度下降法可得θ的更新过程： &nbsp; &nbsp;&nbsp; 式中为α学习步长，下面来求偏导： 上式求解过程中用到如下的公式： 因此，（11）式的更新过程可以写成： &nbsp; 因为式中α本来为一常量，所以1/m一般将省略，所以最终的θ更新过程为： 另外，补充一下，3.2节中提到求得l(θ)取最大值时的θ也是一样的，用梯度上升法求（9）式的最大值，可得： &nbsp;&nbsp; 观察上式发现跟（14）是一样的，所以，采用梯度上升发和梯度下降法是完全一样的，这也是《机器学习实战》中采用梯度上升法的原因。 3.4&nbsp; 梯度下降过程向量化 关于θ更新过程的vectorization，Andrew Ng的课程中只是一带而过，没有具体的讲解。 《机器学习实战》连Cost函数及求梯度等都没有说明，所以更不可能说明vectorization了。但是，其中给出的实现代码确是实现了vectorization的，图4所示代码的32行中weights（也就是θ）的更新只用了一行代码，直接通过矩阵或者向量计算更新，没有用for循环，说明确实实现了vectorization，具体代码下一章分析。 文献[3]中也提到了vectorization，但是也是比较粗略，很简单的给出vectorization的结果为： &nbsp; &nbsp;&nbsp; 且不论该更新公式正确与否，这里的Σ(...)是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization，不像《机器学习实战》的代码中一条语句就可以完成θ的更新。 下面说明一下我理解《机器学习实战》中代码实现的vectorization过程。 约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值： 约定待求的参数θ的矩阵形式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 先求x.θ并记为A： 求hθ(x)-y并记为E： &nbsp; g(A)的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知hθ(x)-y可以由g(A)-y一次计算求得。 再来看一下（15）式的θ更新过程，当j=0时： 同样的可以写出θj， 综合起来就是： 综上所述，vectorization后θ更新的步骤如下： （1）求A=x.θ； （2）求E=g(A)-y； （3）求θ:=θ-α.x&#39;.E,x&#39;表示矩阵x的转置。 也可以综合起来写成： 前面已经提到过：1/m是可以省略的。 4. 代码分析 图4中是《机器学习实战》中给出的部分实现代码。 图4 sigmoid函数就是前文中的g(z)函数，参数inX可以是向量，因为程序中使用了Python的numpy。 gradAscent函数是梯度上升的实现函数，参数dataMatin和classLabels为训练数据，23和24行对训练数据做了处理，转换成numpy的矩阵类型，同时将横向量的classlabels转换成列向量labelMat，此时的dataMatrix和labelMat就是（18）式中的x和y。alpha为学习步长，maxCycles为迭代次数。weights为n维（等于x的列数）列向量，就是（19）式中的θ。 29行的for循环将更新θ的过程迭代maxCycles次，每循环一次更新一次。对比3.4节最后总结的向量化的θ更新步骤，30行相当于求了A=x.θ和g(A)，31行相当于求了E=g(A)-y，32行相当于求θ:=θ-α.x&#39;.E。所以这三行代码实际上与向量化的θ更新步骤是完全一致的。 总结一下，从上面代码分析可以看出，虽然只有十多行的代码，但是里面却隐含了太多的细节，如果没有相关基础确实是非常难以理解的。相信完整的阅读了本文，就应该没有问题了！^_^。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/12/781694.html" />
<meta property="og:url" content="https://mlh.app/2019/05/12/781694.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-12T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (很多讲逻辑回归的文章都没有给出详细的推导，只是列出最后的计算公式，今天在网上看到一篇解释得非常详细的文章，赶紧转载一下： 【机器学习笔记1】Logistic回归总结(http://blog.csdn.net/dongtingzhizi/article/details/15962797) &nbsp;作者说&quot;未经允许，不得转载&quot;，我这里先冒犯了，如果觉得不合适，请告知。 ) &nbsp;Logistic回归总结 1.引言 首先说一下我的感受，《机器学习实战》一书在介绍原理的同时将全部的算法用源代码实现，非常具有操作性，可以加深对算法的理解，但是美中不足的是在原理上介绍的比较粗略，很多细节没有具体介绍。所以，对于没有基础的朋友（包括我）某些地方可能看的一头雾水，需要查阅相关资料进行了解。所以说，该书还是比较适合有基础的朋友。 本文主要介绍以下三个方面的内容： （1）Logistic Regression的基本原理，分布在第二章中； （2）Logistic Regression的具体过程，包括：选取预测函数，求解Cost函数和J(θ)，梯度下降法求J(θ)的最小值，以及递归下降过程的向量化（vectorization），分布在第三章中； （3）对《机器学习实战》中给出的实现代码进行了分析，对阅读该书LogisticRegression部分遇到的疑惑进行了解释。没有基础的朋友在阅读该书的Logistic Regression部分时可能会觉得一头雾水，书中给出的代码很简单，但是怎么也跟书中介绍的理论联系不起来。也会有很多的疑问，比如：一般都是用梯度下降法求损失函数的最小值，为何这里用梯度上升法呢？书中说用梯度上升发，为何代码实现时没见到求梯度的代码呢？这些问题在第三章和第四章中都会得到解答。 文中参考或引用内容的出处列在最后的“参考文献”中。文中所阐述的内容仅仅是我个人的理解，如有错误或疏漏，欢迎大家批评指正。下面进入正题。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 2. 基本原理 Logistic Regression和Linear Regression的原理是相似的，按照我自己的理解，可以简单的描述为这样的过程： （1）找一个合适的预测函数（Andrew Ng的公开课中称为hypothesis），一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。 （2）构造一个Cost函数（损失函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。 （3）显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。 3. 具体过程 3.1&nbsp; 构造预测函数 Logistic Regression虽然名字里带“回归”，但是它实际上是一种分类方法，用于两分类问题（即输出只有两种）。根据第二章中的步骤，需要先找到一个预测函数（h），显然，该函数的输出必须是两个值（分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 对应的函数图像是一个取值在0和1之间的S型曲线（图1）。 图1 接下来需要确定数据划分的边界类型，对于图2和图3中的两种数据分布，显然图2需要一个线性的边界，而图3需要一个非线性的边界。接下来我们只讨论线性边界的情况。 图2 图3 对于线性边界的情况，边界形式如下： 构造预测函数为： hθ(x)函数的值有特殊的含义，它表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为： 3.2&nbsp; 构造Cost函数 Andrew Ng在课程中直接给出了Cost函数及J(θ)函数如式（5）和（6），但是并没有给出具体的解释，只是说明了这个函数来衡量h函数预测的好坏是合理的。 实际上这里的Cost函数和J(θ)函数是基于最大似然估计推导得到的。下面详细说明推导的过程。（4）式综合起来可以写成： 取似然函数为： 对数似然函数为： 最大似然估计就是要求得使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将J(θ)取为（6）式，即： 因为乘了一个负的系数-1/m，所以J(θ)取最小值时的θ为要求的最佳参数。 3.3&nbsp; 梯度下降法求J(θ)的最小值 求J(θ)的最小值可以使用梯度下降法，根据梯度下降法可得θ的更新过程： &nbsp; &nbsp;&nbsp; 式中为α学习步长，下面来求偏导： 上式求解过程中用到如下的公式： 因此，（11）式的更新过程可以写成： &nbsp; 因为式中α本来为一常量，所以1/m一般将省略，所以最终的θ更新过程为： 另外，补充一下，3.2节中提到求得l(θ)取最大值时的θ也是一样的，用梯度上升法求（9）式的最大值，可得： &nbsp;&nbsp; 观察上式发现跟（14）是一样的，所以，采用梯度上升发和梯度下降法是完全一样的，这也是《机器学习实战》中采用梯度上升法的原因。 3.4&nbsp; 梯度下降过程向量化 关于θ更新过程的vectorization，Andrew Ng的课程中只是一带而过，没有具体的讲解。 《机器学习实战》连Cost函数及求梯度等都没有说明，所以更不可能说明vectorization了。但是，其中给出的实现代码确是实现了vectorization的，图4所示代码的32行中weights（也就是θ）的更新只用了一行代码，直接通过矩阵或者向量计算更新，没有用for循环，说明确实实现了vectorization，具体代码下一章分析。 文献[3]中也提到了vectorization，但是也是比较粗略，很简单的给出vectorization的结果为： &nbsp; &nbsp;&nbsp; 且不论该更新公式正确与否，这里的Σ(...)是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization，不像《机器学习实战》的代码中一条语句就可以完成θ的更新。 下面说明一下我理解《机器学习实战》中代码实现的vectorization过程。 约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值： 约定待求的参数θ的矩阵形式为： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 先求x.θ并记为A： 求hθ(x)-y并记为E： &nbsp; g(A)的参数A为一列向量，所以实现g函数时要支持列向量作为参数，并返回列向量。由上式可知hθ(x)-y可以由g(A)-y一次计算求得。 再来看一下（15）式的θ更新过程，当j=0时： 同样的可以写出θj， 综合起来就是： 综上所述，vectorization后θ更新的步骤如下： （1）求A=x.θ； （2）求E=g(A)-y； （3）求θ:=θ-α.x&#39;.E,x&#39;表示矩阵x的转置。 也可以综合起来写成： 前面已经提到过：1/m是可以省略的。 4. 代码分析 图4中是《机器学习实战》中给出的部分实现代码。 图4 sigmoid函数就是前文中的g(z)函数，参数inX可以是向量，因为程序中使用了Python的numpy。 gradAscent函数是梯度上升的实现函数，参数dataMatin和classLabels为训练数据，23和24行对训练数据做了处理，转换成numpy的矩阵类型，同时将横向量的classlabels转换成列向量labelMat，此时的dataMatrix和labelMat就是（18）式中的x和y。alpha为学习步长，maxCycles为迭代次数。weights为n维（等于x的列数）列向量，就是（19）式中的θ。 29行的for循环将更新θ的过程迭代maxCycles次，每循环一次更新一次。对比3.4节最后总结的向量化的θ更新步骤，30行相当于求了A=x.θ和g(A)，31行相当于求了E=g(A)-y，32行相当于求θ:=θ-α.x&#39;.E。所以这三行代码实际上与向量化的θ更新步骤是完全一致的。 总结一下，从上面代码分析可以看出，虽然只有十多行的代码，但是里面却隐含了太多的细节，如果没有相关基础确实是非常难以理解的。相信完整的阅读了本文，就应该没有问题了！^_^。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/12/781694.html","headline":"机器学习–Logistic回归计算过程的推导","dateModified":"2019-05-12T00:00:00+08:00","datePublished":"2019-05-12T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/12/781694.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>机器学习--Logistic回归计算过程的推导</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> 
  <div class="markdown_views prism-tomorrow-night" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
   <div class="htmledit_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <p>(很多讲逻辑回归的文章都没有给出详细的推导，只是列出最后的计算公式，今天在网上看到一篇解释得非常详细的文章，赶紧转载一下：</p>
    <p><a href="http://blog.csdn.net/dongtingzhizi/article/details/15962797" rel="nofollow" target="_blank">【机器学习笔记1】Logistic回归总结</a>(http://blog.csdn.net/dongtingzhizi/article/details/15962797)</p>
    <p>&nbsp;作者说"未经允许，不得转载"，我这里先冒犯了，如果觉得不合适，请告知。</p>
    <p>)</p>
    <p><br></p>
    <p></p>
    <h1><a></a><a target="_blank"></a><strong>&nbsp;Logistic回归总结</strong></h1>
    <p></p>
    <a target="_blank"></a>
    <a target="_blank"></a>1.引言
    <span>首先说一下我的感受，《机器学习实战》一书在介绍原理的同时将全部的<a title="算法与数据结构知识库" class="replace_word" href="http://lib.csdn.net/base/datastructure" rel="nofollow" target="_blank">算法</a>用源代码实现，非常具有操作性，可以加深对算法的理解，但是美中不足的是在原理上介绍的比较粗略，很多细节没有具体介绍。所以，对于没有基础的朋友（包括我）某些地方可能看的一头雾水，需要查阅相关资料进行了解。所以说，该书还是比较适合有基础的朋友。</span>
    <p><span>本文主要介绍以下三个方面的内容：</span></p>
    <p><span>（1）Logistic Regression的基本原理，分布在第二章中；</span></p>
    <p><span>（2）Logistic Regression的具体过程，包括：选取预测函数，求解Cost函数和<strong><em>J(θ)</em></strong>，梯度下降法求<strong><em>J(θ)</em></strong>的最小值，以及递归下降过程的向量化（vectorization），分布在第三章中；</span></p>
    <p><span>（3）对《机器学习实战》中给出的实现代码进行了分析，对阅读该书LogisticRegression部分遇到的疑惑进行了解释。没有基础的朋友在阅读该书的Logistic Regression部分时可能会觉得一头雾水，书中给出的代码很简单，但是怎么也跟书中介绍的理论联系不起来。也会有很多的疑问，比如：一般都是用梯度下降法求损失函数的最小值，为何这里用梯度上升法呢？书中说用梯度上升发，为何代码实现时没见到求梯度的代码呢？这些问题在第三章和第四章中都会得到解答。</span></p>
    <p><span>文中参考或引用内容的出处列在最后的“参考文献”中。文中所阐述的内容仅仅是我个人的理解，如有错误或疏漏，欢迎大家批评指正。下面进入正题。</span></p>
   </div>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <h1><a></a><a target="_blank"></a><a target="_blank"></a>2. 基本原理</h1>
   <p><span>Logistic Regression和Linear Regression的原理是相似的，按照我自己的理解，可以简单的描述为这样的过程：</span></p>
   <p><span>（1）找一个合适的预测函数（Andrew Ng的公开课中称为hypothesis），一般表示为<strong><em>h</em></strong>函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。</span></p>
   <p><span>（2）构造一个Cost函数（损失函数），该函数表示预测的输出（<strong><em>h</em></strong>）与训练数据类别（<strong><em>y</em></strong>）之间的偏差，可以是二者之间的差（<strong><em>h-y</em></strong>）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为<strong><em>J(θ)</em></strong>函数，表示所有训练数据预测值与实际类别的偏差。</span></p>
   <p><span>（3）显然，<strong><em>J(θ)</em></strong>函数的值越小表示预测函数越准确（即<strong><em>h</em></strong>函数越准确），所以这一步需要做的是找到<span>J(θ)</span>函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。</span></p>
   <p><span><br></span></p>
   <h1><a></a><a target="_blank"></a><a target="_blank"></a>3. 具体过程</h1>
   <h2><a></a><a target="_blank"></a><a target="_blank"></a>3.1&nbsp; 构造预测函数</h2>
   <p><span>Logistic Regression虽然名字里带“回归”，但是它实际上是一种分类方法，用于两分类问题（即输出只有两种）。根据第二章中的步骤，需要先找到一个预测函数（<strong><em>h</em></strong>），显然，该函数的输出必须是两个值（分别代表两个类别），所以利用了Logistic函数（或称为Sigmoid函数），函数形式为：</span></p>
   <p align="right"><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203307078?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span></p>
   <p><span>对应的函数图像是一个取值在0和1之间的S型曲线（图1）。</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113202319171?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></span></p>
   <p align="center"></p>
   <p><span>图1</span></p>
   <p><span>接下来需要确定数据划分的边界类型，对于图2和图3中的两种数据分布，显然图2需要一个线性的边界，而图3需要一个非线性的边界。接下来我们只讨论线性边界的情况。</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113202358203?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></span></p>
   <p align="center"></p>
   <p><span>图2</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113202421671?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></span></p>
   <p align="center"></p>
   <p><span>图3</span></p>
   <p><span>对于线性边界的情况，边界形式如下：</span></p>
   <p align="right"><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203346500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>构造预测函数为：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203423234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span><strong><em>hθ(x)</em></strong>函数的值有特殊的含义，它表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203441312?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p align="right"><br></p>
   <h2><a></a><a target="_blank"></a><a target="_blank"></a>3.2&nbsp; 构造Cost函数</h2>
   <p><span>Andrew Ng在课程中直接给出了Cost函数及<strong><em>J(θ)</em></strong>函数如式（5）和（6），但是并没有给出具体的解释，只是说明了这个函数来衡量<strong><em>h</em></strong>函数预测的好坏是合理的。</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203457937?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203517875?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>实际上这里的Cost函数和<strong><em>J(θ)</em></strong>函数是基于<a href="http://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1" rel="nofollow" target="_blank">最大似然估计</a>推导得到的。下面详细说明推导的过程。（4）式综合起来可以写成：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203546390?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>取似然函数为：</span></p>
   <p align="right"><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203558968?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>对数似然函数为：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203626296?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>最大似然估计就是要求得使<strong><em>l(θ</em></strong>)取最大值时的<em><strong>θ</strong></em>，其实这里可以使用梯度上升法求解，求得的<em><strong>θ</strong></em>就是要求的最佳参数。但是，在Andrew Ng的课程中将<em><strong>J(θ)</strong></em>取为（6）式，即：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203644500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>因为乘了一个负的系数<strong><em>-1/m</em></strong>，所以<em><strong>J(θ)</strong></em>取最小值时的<em><strong>θ</strong></em>为要求的最佳参数。</span></p>
   <p><span><br></span></p>
   <h2><a></a><a target="_blank"></a><a target="_blank"></a>3.3&nbsp; 梯度下降法求<em>J(θ)</em>的最小值</h2>
   <p><span>求<em><strong>J(θ)</strong></em>的最小值可以使用梯度下降法，根据梯度下降法可得<em><strong>θ</strong></em>的更新过程：</span></p>
   <p align="right"><span>&nbsp; &nbsp;&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203704250?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>式中为<strong><em>α</em></strong>学习步长，下面来求偏导：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203723187?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>上式求解过程中用到如下的公式：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203741453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>因此，（11）式的更新过程可以写成：</span></p>
   <p align="right"><span>&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203807453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>因为式中<em><strong>α</strong></em>本来为一常量，所以<strong><em>1/m</em></strong>一般将省略，所以最终的<em><strong>θ</strong></em>更新过程为：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113205240203?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>另外，补充一下，3.2节中提到求得<strong><em>l(θ</em>)</strong>取最大值时的<em><strong>θ</strong></em>也是一样的，用梯度上升法求（9）式的最大值，可得：</span></p>
   <p align="right"><span>&nbsp;&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203849390?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>观察上式发现跟（14）是一样的，所以，采用梯度上升发和梯度下降法是完全一样的，这也是《机器学习实战》中采用梯度上升法的原因。</span></p>
   <p><span><br></span></p>
   <h2><a></a><a target="_blank"></a><a target="_blank"></a>3.4&nbsp; 梯度下降过程向量化</h2>
   <p><span>关于<em><strong>θ</strong></em>更新过程的vectorization，Andrew Ng的课程中只是一带而过，没有具体的讲解。</span></p>
   <p><span>《机器学习实战》连Cost函数及求梯度等都没有说明，所以更不可能说明vectorization了。但是，其中给出的实现代码确是实现了vectorization的，图4所示代码的32行中weights（也就是<em><strong>θ</strong></em>）的更新只用了一行代码，直接通过矩阵或者向量计算更新，没有用for循环，说明确实实现了vectorization，具体代码下一章分析。</span></p>
   <p><span>文献[3]中也提到了vectorization，但是也是比较粗略，很简单的给出vectorization的结果为：</span></p>
   <p align="right"><span>&nbsp; &nbsp;&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203911984?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>且不论该更新公式正确与否，这里的<strong>Σ(...)</strong>是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization，不像《机器学习实战》的代码中一条语句就可以完成<em><strong>θ</strong></em>的更新。</span></p>
   <p><span>下面说明一下我理解《机器学习实战》中代码实现的vectorization过程。</span></p>
   <p><span>约定训练数据的矩阵形式如下，<strong><em>x</em></strong>的每一行为一条训练样本，而每一列为不同的特称取值：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203934875?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>约定待求的参数<em><strong>θ</strong></em>的矩阵形式为：</span></p>
   <p align="right"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;<img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113203954453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></span></p>
   <p><span>先求<strong><em>x.θ</em></strong>并记为<strong><em>A</em></strong>：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113204012546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>求<strong><em>hθ(x)-y</em></strong>并记为<em><strong>E</strong></em>：</span></p>
   <p align="right"><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113204103593?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">&nbsp;</span></p>
   <p><span><strong><em>g(A)</em></strong>的参数<strong><em>A</em></strong>为一列向量，所以实现<strong><em>g</em></strong>函数时要支持列向量作为参数，并返回列向量。由上式可知<strong><em>hθ(x)-y</em></strong>可以由<em><strong>g(A)-y</strong></em>一次计算求得。</span></p>
   <p><span>再来看一下（15）式的<em><strong>θ</strong></em>更新过程，当<strong><em>j=0</em></strong>时：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113204122000?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>同样的可以写出<em><strong>θj</strong></em>，</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113204138093?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>综合起来就是：</span></p>
   <p align="right"><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113204152062?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></p>
   <p><span>综上所述，vectorization后<em><strong>θ</strong></em>更新的步骤如下：</span></p>
   <p><span>（1）求<strong><em>A=x.θ</em></strong>；</span></p>
   <p><span>（2）求<strong><em>E=g(A)-y</em></strong>；</span></p>
   <p><span>（3）求<strong>θ:=θ-α.x'.E,</strong>x'表示矩阵x的转置。</span></p>
   <p><span>也可以综合起来写成：</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131213085438093"><br></span></p>
   <p><span>前面已经提到过：1/m是可以省略的。</span></p>
   <h1><a></a><a target="_blank"></a><a target="_blank"></a>4. 代码分析</h1>
   <p><span>图4中是《机器学习实战》中给出的部分实现代码。</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20131113202512453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br></span></p>
   <p align="center"></p>
   <p><span>图4</span></p>
   <p><span>sigmoid函数就是前文中的<strong><em>g(z)</em></strong>函数，参数<strong>inX</strong>可以是向量，因为程序中使用了<a title="Python知识库" class="replace_word" href="http://lib.csdn.net/base/python" rel="nofollow" target="_blank">Python</a>的numpy。</span></p>
   <p><span>gradAscent函数是梯度上升的实现函数，参数dataMatin和classLabels为训练数据，23和24行对训练数据做了处理，转换成numpy的矩阵类型，同时将横向量的classlabels转换成列向量labelMat，此时的dataMatrix和labelMat就是（18）式中的<strong>x</strong>和<strong>y</strong>。alpha为学习步长，maxCycles为迭代次数。weights为n维（等于<strong><em>x</em></strong>的列数）列向量，就是（19）式中的<em><strong>θ</strong></em>。</span></p>
   <p><span>29行的for循环将更新<em><strong>θ</strong></em>的过程迭代maxCycles次，每循环一次更新一次。对比3.4节最后总结的向量化的<em><strong>θ</strong></em>更新步骤，30行相当于求了</span><span><strong><em>A=x.θ</em></strong>和</span><span><strong><em>g(A)</em></strong></span><span>，31行相当于求了<em><strong>E=g(A)-y</strong></em>，32行相当于求<em><strong>θ:=θ-α.x'.E</strong></em>。所以这三行代码实际上与向量化的<em><strong>θ</strong></em>更新步骤是完全一致的。</span></p>
   <p><span>总结一下，从上面代码分析可以看出，虽然只有十多行的代码，但是里面却隐含了太多的细节，如果没有相关基础确实是非常难以理解的。相信完整的阅读了本文，就应该没有问题了！^_^。</span></p>
   <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
