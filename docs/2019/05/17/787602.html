<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>关于新手入门：Spark 部署实战入门 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="关于新手入门：Spark 部署实战入门" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Spark简介 整体认识 Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架。最初在2009年由加州大学伯克利分校的AMPLab开发，并于2010年成为Apache的开源项目之一。 Spark在整个大数据系统中处于中间偏上层的地位，如下图，对hadoop起到了补充作用： 基本概念 Fork/Join框架是Java7提供了的一个用于并行执行任务的框架， 是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架。 &nbsp; 第一步分割任务。首先我们需要有一个fork类来把大任务分割成子任务，有可能子任务还是很大，所以还需要不停的分割，直到分割出的子任务足够小。 第二步执行任务并合并结果。分割的子任务分别放在双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都统一放在一个队列里，启动一个线程从队列里拿数据，然后合并这些数据。 核心概念 RDD(Resilient Distributed Dataset) 弹性分布数据集介绍 弹性分布式数据集(基于Matei的研究论文)或RDD是Spark框架中的核心概念。可以将RDD视作数据库中的一张表。其中可以保存任何类型的数据。Spark将数据存储在不同分区上的RDD之中。 RDD可以帮助重新安排计算并优化数据处理过程。 此外，它还具有容错性，因为RDD知道如何重新创建和重新计算数据集。 RDD是不可变的。你可以用变换(Transformation)修改RDD，但是这个变换所返回的是一个全新的RDD，而原有的RDD仍然保持不变。 RDD支持两种类型的操作： 变换(Transformation) 行动(Action) 变换：变换的返回值是一个新的RDD集合，而不是单个值。调用一个变换方法，不会有任何求值计算，它只获取一个RDD作为参数，然后返回一个新的RDD。变换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，pipe和coalesce。 行动：行动操作计算并返回一个新的值。当在一个RDD对象上调用行动函数时，会在这一时刻计算全部的数据处理查询并返回结果值。 行动操作包括：reduce，collect，count，first，take，countByKey以及foreach。 共享变量(Shared varialbes)： 广播变量(Broadcast variables) 累加器(Accumulators) Master/Worker/Driver/Executor &nbsp; &nbsp; o Master：1. 接受Worker的注册请求，统筹记录所有Worker的CPU、Memory等资源，并跟踪Worker结点的活动状态;2. 接受Driver中App的注册请求(这个请求由Driver端的Client发出)，为App在Worker上分配CPU、Memory资源，生成后台Executor进程;之后跟踪Executor和App的活动状态。 o Worker：负责接收Master的指示，为App创建Executor进程。Worker在Master和Executor之间起着桥梁作用，实际不会参与计算工作。 o Driver：负责用户侧逻辑处理。 o Executor：负责计算，接受并执行由App划分的Task任务，并将结果缓存在本地内存或磁盘。 Spark部署 关于Spark的部署网上相关资料很多，这里进行归纳整理 部署环境 Ubuntu 14.04LTS Hadoop:2.7.0 Java JDK 1.8 Spark 1.6.1 Scala 2.11.8 Hadoop安装 由于Spark会利用HDFS和YARN，所以需要提前配置Hadoop，配置教程可以参考： Setting up aApache Hadoop 2.7 single node on Ubuntu 14.04 Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0/Ubuntu14.04 Spark安装 在安装好Hadoop的基础上，搭建Spark，配置教程参考： Spark快速入门指南 – Spark安装与基础使用 scala安装 Scala作为编写Spark的源生语言，更新速度和支持情况肯定是最好的，而另一方面Scala本身语言中对于面向对象和函数式编程两种思想的糅合，使得该语言具有很多炫酷的语法糖，所以在使用Spark的过程中我采用了Scala语言进行开发。 Scala最终编译成字节码需要运行在JVM中，所以需要依托于jdk，需要部署jdk Eclipse作为一款开发Java的IDE神器，在Scala中当然也可以使用，有两种方式: o Eclipse-&gt;Help-&gt;Install New Software安装Scala Plugins o 下载官网已经提供的集成好的Scala IDE 基于以上两步已经可以进行Scala开发，需要用到Scala自带的SBT编译的同学可以装下Scala官网下载地址，本人一直使用Maven进行包管理就延续Maven的使用 简单示例：WordCount(Spark Scala) 开发IDE：Eclipse Scala 包管理：Maven 开发语言：Scala 创建Maven项目 &nbsp; &nbsp; 1. 跳过archetype项目模板的选择 2. 下载模板pom.xml 3. 对maven项目添加Scala属性：Right click on project -&gt; configure - &gt; Add Scala Nature. 4. 调整下Scala编译器的版本，与Spark版本对应： Right click on project- &gt; Go to properties -&gt; Scala compiler -&gt; update Scala installation version to 2.10.5 5. 从Build Path中移除Scala Library(由于在Maven中添加了Spark Core的依赖项，而Spark是依赖于Scala的，Scala的jar包已经存在于Maven Dependency中)： Right click on the project -&gt; Build path -&gt; Configure build path and remove Scala Library Container. 6. 添加package包com.spark.sample &nbsp; &nbsp; 7. 创建Object WordCount和SimpleCount，用来作为Spark的两个简单示例 Spark Sample SimpleCount.scala package com.spark.sample import org.apache.spark.SparkConf import org.apache.spark.SparkContext object SimpleCount { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&quot;TrySparkStreaming&quot;).setMaster(&quot;local[2]&quot;) // Create spark context val sc = new SparkContext(conf) // val ssc = new StreamingContext(conf, Seconds(1)) // create streaming context val txtFile = &quot;test&quot; val txtData = sc.textFile(txtFile) txtData.cache() txtData.count() val wcData = txtData.flatMap { line =&gt; line.split(&quot;,&quot;) }.map { word =&gt; (word, 1) }.reduceByKey(_ + _) wcData.collect().foreach(println) sc.stop } } WordCount.scala package com.spark.sample import org.apache.spark.SparkConf import org.apache.spark.SparkContext import org.apache.spark.rdd.RDD.rddToPairRDDFunctions object WordCount { def main(args: Array[String]) = { //Start the Spark context val conf = new SparkConf() .setAppName(&quot;WordCount&quot;) .setMaster(&quot;local&quot;) val sc = new SparkContext(conf) //Read some example file to a test RDD val test = sc.textFile(&quot;input.txt&quot;) test.flatMap { line =&gt; //for each line line.split(&quot; &quot;) //split the line in word by word. }.map { word =&gt; //for each word (word, 1) //Return a key/value tuple, with the word as key and 1 as value }.reduceByKey(_ + _) //Sum all of the value with same key .saveAsTextFile(&quot;output.txt&quot;) //Save to a text file //Stop the Spark context sc.stop } } 原理如下图： &nbsp; 推荐阅读文章 大数据工程师在阿里面试流程是什么？ 学习大数据需要具备怎么样基础？ 年薪30K的大数据开发工程师的工作经验总结？ &nbsp;" />
<meta property="og:description" content="Spark简介 整体认识 Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架。最初在2009年由加州大学伯克利分校的AMPLab开发，并于2010年成为Apache的开源项目之一。 Spark在整个大数据系统中处于中间偏上层的地位，如下图，对hadoop起到了补充作用： 基本概念 Fork/Join框架是Java7提供了的一个用于并行执行任务的框架， 是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架。 &nbsp; 第一步分割任务。首先我们需要有一个fork类来把大任务分割成子任务，有可能子任务还是很大，所以还需要不停的分割，直到分割出的子任务足够小。 第二步执行任务并合并结果。分割的子任务分别放在双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都统一放在一个队列里，启动一个线程从队列里拿数据，然后合并这些数据。 核心概念 RDD(Resilient Distributed Dataset) 弹性分布数据集介绍 弹性分布式数据集(基于Matei的研究论文)或RDD是Spark框架中的核心概念。可以将RDD视作数据库中的一张表。其中可以保存任何类型的数据。Spark将数据存储在不同分区上的RDD之中。 RDD可以帮助重新安排计算并优化数据处理过程。 此外，它还具有容错性，因为RDD知道如何重新创建和重新计算数据集。 RDD是不可变的。你可以用变换(Transformation)修改RDD，但是这个变换所返回的是一个全新的RDD，而原有的RDD仍然保持不变。 RDD支持两种类型的操作： 变换(Transformation) 行动(Action) 变换：变换的返回值是一个新的RDD集合，而不是单个值。调用一个变换方法，不会有任何求值计算，它只获取一个RDD作为参数，然后返回一个新的RDD。变换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，pipe和coalesce。 行动：行动操作计算并返回一个新的值。当在一个RDD对象上调用行动函数时，会在这一时刻计算全部的数据处理查询并返回结果值。 行动操作包括：reduce，collect，count，first，take，countByKey以及foreach。 共享变量(Shared varialbes)： 广播变量(Broadcast variables) 累加器(Accumulators) Master/Worker/Driver/Executor &nbsp; &nbsp; o Master：1. 接受Worker的注册请求，统筹记录所有Worker的CPU、Memory等资源，并跟踪Worker结点的活动状态;2. 接受Driver中App的注册请求(这个请求由Driver端的Client发出)，为App在Worker上分配CPU、Memory资源，生成后台Executor进程;之后跟踪Executor和App的活动状态。 o Worker：负责接收Master的指示，为App创建Executor进程。Worker在Master和Executor之间起着桥梁作用，实际不会参与计算工作。 o Driver：负责用户侧逻辑处理。 o Executor：负责计算，接受并执行由App划分的Task任务，并将结果缓存在本地内存或磁盘。 Spark部署 关于Spark的部署网上相关资料很多，这里进行归纳整理 部署环境 Ubuntu 14.04LTS Hadoop:2.7.0 Java JDK 1.8 Spark 1.6.1 Scala 2.11.8 Hadoop安装 由于Spark会利用HDFS和YARN，所以需要提前配置Hadoop，配置教程可以参考： Setting up aApache Hadoop 2.7 single node on Ubuntu 14.04 Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0/Ubuntu14.04 Spark安装 在安装好Hadoop的基础上，搭建Spark，配置教程参考： Spark快速入门指南 – Spark安装与基础使用 scala安装 Scala作为编写Spark的源生语言，更新速度和支持情况肯定是最好的，而另一方面Scala本身语言中对于面向对象和函数式编程两种思想的糅合，使得该语言具有很多炫酷的语法糖，所以在使用Spark的过程中我采用了Scala语言进行开发。 Scala最终编译成字节码需要运行在JVM中，所以需要依托于jdk，需要部署jdk Eclipse作为一款开发Java的IDE神器，在Scala中当然也可以使用，有两种方式: o Eclipse-&gt;Help-&gt;Install New Software安装Scala Plugins o 下载官网已经提供的集成好的Scala IDE 基于以上两步已经可以进行Scala开发，需要用到Scala自带的SBT编译的同学可以装下Scala官网下载地址，本人一直使用Maven进行包管理就延续Maven的使用 简单示例：WordCount(Spark Scala) 开发IDE：Eclipse Scala 包管理：Maven 开发语言：Scala 创建Maven项目 &nbsp; &nbsp; 1. 跳过archetype项目模板的选择 2. 下载模板pom.xml 3. 对maven项目添加Scala属性：Right click on project -&gt; configure - &gt; Add Scala Nature. 4. 调整下Scala编译器的版本，与Spark版本对应： Right click on project- &gt; Go to properties -&gt; Scala compiler -&gt; update Scala installation version to 2.10.5 5. 从Build Path中移除Scala Library(由于在Maven中添加了Spark Core的依赖项，而Spark是依赖于Scala的，Scala的jar包已经存在于Maven Dependency中)： Right click on the project -&gt; Build path -&gt; Configure build path and remove Scala Library Container. 6. 添加package包com.spark.sample &nbsp; &nbsp; 7. 创建Object WordCount和SimpleCount，用来作为Spark的两个简单示例 Spark Sample SimpleCount.scala package com.spark.sample import org.apache.spark.SparkConf import org.apache.spark.SparkContext object SimpleCount { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&quot;TrySparkStreaming&quot;).setMaster(&quot;local[2]&quot;) // Create spark context val sc = new SparkContext(conf) // val ssc = new StreamingContext(conf, Seconds(1)) // create streaming context val txtFile = &quot;test&quot; val txtData = sc.textFile(txtFile) txtData.cache() txtData.count() val wcData = txtData.flatMap { line =&gt; line.split(&quot;,&quot;) }.map { word =&gt; (word, 1) }.reduceByKey(_ + _) wcData.collect().foreach(println) sc.stop } } WordCount.scala package com.spark.sample import org.apache.spark.SparkConf import org.apache.spark.SparkContext import org.apache.spark.rdd.RDD.rddToPairRDDFunctions object WordCount { def main(args: Array[String]) = { //Start the Spark context val conf = new SparkConf() .setAppName(&quot;WordCount&quot;) .setMaster(&quot;local&quot;) val sc = new SparkContext(conf) //Read some example file to a test RDD val test = sc.textFile(&quot;input.txt&quot;) test.flatMap { line =&gt; //for each line line.split(&quot; &quot;) //split the line in word by word. }.map { word =&gt; //for each word (word, 1) //Return a key/value tuple, with the word as key and 1 as value }.reduceByKey(_ + _) //Sum all of the value with same key .saveAsTextFile(&quot;output.txt&quot;) //Save to a text file //Stop the Spark context sc.stop } } 原理如下图： &nbsp; 推荐阅读文章 大数据工程师在阿里面试流程是什么？ 学习大数据需要具备怎么样基础？ 年薪30K的大数据开发工程师的工作经验总结？ &nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/17/787602.html" />
<meta property="og:url" content="https://mlh.app/2019/05/17/787602.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-17T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"Spark简介 整体认识 Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架。最初在2009年由加州大学伯克利分校的AMPLab开发，并于2010年成为Apache的开源项目之一。 Spark在整个大数据系统中处于中间偏上层的地位，如下图，对hadoop起到了补充作用： 基本概念 Fork/Join框架是Java7提供了的一个用于并行执行任务的框架， 是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架。 &nbsp; 第一步分割任务。首先我们需要有一个fork类来把大任务分割成子任务，有可能子任务还是很大，所以还需要不停的分割，直到分割出的子任务足够小。 第二步执行任务并合并结果。分割的子任务分别放在双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都统一放在一个队列里，启动一个线程从队列里拿数据，然后合并这些数据。 核心概念 RDD(Resilient Distributed Dataset) 弹性分布数据集介绍 弹性分布式数据集(基于Matei的研究论文)或RDD是Spark框架中的核心概念。可以将RDD视作数据库中的一张表。其中可以保存任何类型的数据。Spark将数据存储在不同分区上的RDD之中。 RDD可以帮助重新安排计算并优化数据处理过程。 此外，它还具有容错性，因为RDD知道如何重新创建和重新计算数据集。 RDD是不可变的。你可以用变换(Transformation)修改RDD，但是这个变换所返回的是一个全新的RDD，而原有的RDD仍然保持不变。 RDD支持两种类型的操作： 变换(Transformation) 行动(Action) 变换：变换的返回值是一个新的RDD集合，而不是单个值。调用一个变换方法，不会有任何求值计算，它只获取一个RDD作为参数，然后返回一个新的RDD。变换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，pipe和coalesce。 行动：行动操作计算并返回一个新的值。当在一个RDD对象上调用行动函数时，会在这一时刻计算全部的数据处理查询并返回结果值。 行动操作包括：reduce，collect，count，first，take，countByKey以及foreach。 共享变量(Shared varialbes)： 广播变量(Broadcast variables) 累加器(Accumulators) Master/Worker/Driver/Executor &nbsp; &nbsp; o Master：1. 接受Worker的注册请求，统筹记录所有Worker的CPU、Memory等资源，并跟踪Worker结点的活动状态;2. 接受Driver中App的注册请求(这个请求由Driver端的Client发出)，为App在Worker上分配CPU、Memory资源，生成后台Executor进程;之后跟踪Executor和App的活动状态。 o Worker：负责接收Master的指示，为App创建Executor进程。Worker在Master和Executor之间起着桥梁作用，实际不会参与计算工作。 o Driver：负责用户侧逻辑处理。 o Executor：负责计算，接受并执行由App划分的Task任务，并将结果缓存在本地内存或磁盘。 Spark部署 关于Spark的部署网上相关资料很多，这里进行归纳整理 部署环境 Ubuntu 14.04LTS Hadoop:2.7.0 Java JDK 1.8 Spark 1.6.1 Scala 2.11.8 Hadoop安装 由于Spark会利用HDFS和YARN，所以需要提前配置Hadoop，配置教程可以参考： Setting up aApache Hadoop 2.7 single node on Ubuntu 14.04 Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0/Ubuntu14.04 Spark安装 在安装好Hadoop的基础上，搭建Spark，配置教程参考： Spark快速入门指南 – Spark安装与基础使用 scala安装 Scala作为编写Spark的源生语言，更新速度和支持情况肯定是最好的，而另一方面Scala本身语言中对于面向对象和函数式编程两种思想的糅合，使得该语言具有很多炫酷的语法糖，所以在使用Spark的过程中我采用了Scala语言进行开发。 Scala最终编译成字节码需要运行在JVM中，所以需要依托于jdk，需要部署jdk Eclipse作为一款开发Java的IDE神器，在Scala中当然也可以使用，有两种方式: o Eclipse-&gt;Help-&gt;Install New Software安装Scala Plugins o 下载官网已经提供的集成好的Scala IDE 基于以上两步已经可以进行Scala开发，需要用到Scala自带的SBT编译的同学可以装下Scala官网下载地址，本人一直使用Maven进行包管理就延续Maven的使用 简单示例：WordCount(Spark Scala) 开发IDE：Eclipse Scala 包管理：Maven 开发语言：Scala 创建Maven项目 &nbsp; &nbsp; 1. 跳过archetype项目模板的选择 2. 下载模板pom.xml 3. 对maven项目添加Scala属性：Right click on project -&gt; configure - &gt; Add Scala Nature. 4. 调整下Scala编译器的版本，与Spark版本对应： Right click on project- &gt; Go to properties -&gt; Scala compiler -&gt; update Scala installation version to 2.10.5 5. 从Build Path中移除Scala Library(由于在Maven中添加了Spark Core的依赖项，而Spark是依赖于Scala的，Scala的jar包已经存在于Maven Dependency中)： Right click on the project -&gt; Build path -&gt; Configure build path and remove Scala Library Container. 6. 添加package包com.spark.sample &nbsp; &nbsp; 7. 创建Object WordCount和SimpleCount，用来作为Spark的两个简单示例 Spark Sample SimpleCount.scala package com.spark.sample import org.apache.spark.SparkConf import org.apache.spark.SparkContext object SimpleCount { def main(args: Array[String]) { val conf = new SparkConf().setAppName(&quot;TrySparkStreaming&quot;).setMaster(&quot;local[2]&quot;) // Create spark context val sc = new SparkContext(conf) // val ssc = new StreamingContext(conf, Seconds(1)) // create streaming context val txtFile = &quot;test&quot; val txtData = sc.textFile(txtFile) txtData.cache() txtData.count() val wcData = txtData.flatMap { line =&gt; line.split(&quot;,&quot;) }.map { word =&gt; (word, 1) }.reduceByKey(_ + _) wcData.collect().foreach(println) sc.stop } } WordCount.scala package com.spark.sample import org.apache.spark.SparkConf import org.apache.spark.SparkContext import org.apache.spark.rdd.RDD.rddToPairRDDFunctions object WordCount { def main(args: Array[String]) = { //Start the Spark context val conf = new SparkConf() .setAppName(&quot;WordCount&quot;) .setMaster(&quot;local&quot;) val sc = new SparkContext(conf) //Read some example file to a test RDD val test = sc.textFile(&quot;input.txt&quot;) test.flatMap { line =&gt; //for each line line.split(&quot; &quot;) //split the line in word by word. }.map { word =&gt; //for each word (word, 1) //Return a key/value tuple, with the word as key and 1 as value }.reduceByKey(_ + _) //Sum all of the value with same key .saveAsTextFile(&quot;output.txt&quot;) //Save to a text file //Stop the Spark context sc.stop } } 原理如下图： &nbsp; 推荐阅读文章 大数据工程师在阿里面试流程是什么？ 学习大数据需要具备怎么样基础？ 年薪30K的大数据开发工程师的工作经验总结？ &nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/17/787602.html","headline":"关于新手入门：Spark 部署实战入门","dateModified":"2019-05-17T00:00:00+08:00","datePublished":"2019-05-17T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/17/787602.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>关于新手入门：Spark 部署实战入门</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div class="htmledit_views" id="content_views"> 
  <p>Spark简介</p> 
  <p>整体认识</p> 
  <p>Apache Spark是一个围绕速度、易用性和复杂分析构建的大数据处理框架。最初在2009年由加州大学伯克利分校的AMPLab开发，并于2010年成为Apache的开源项目之一。</p> 
  <p>Spark在整个大数据系统中处于中间偏上层的地位，如下图，对hadoop起到了补充作用：</p> 
  <p>基本概念</p> 
  <p>Fork/Join框架是Java7提供了的一个用于并行执行任务的框架， 是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架。</p> 
  <p><img alt="" class="has" height="311" src="http://www.raincent.com/uploadfile/2016/0713/20160713040412938.png" width="533"></p> 
  <p>&nbsp;</p> 
  <p>第一步分割任务。首先我们需要有一个fork类来把大任务分割成子任务，有可能子任务还是很大，所以还需要不停的分割，直到分割出的子任务足够小。</p> 
  <p>第二步执行任务并合并结果。分割的子任务分别放在双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都统一放在一个队列里，启动一个线程从队列里拿数据，然后合并这些数据。</p> 
  <p><strong>核心概念</strong></p> 
  <p><strong>RDD(Resilient Distributed Dataset) 弹性分布数据集介绍</strong></p> 
  <p>弹性分布式数据集(基于Matei的研究论文)或RDD是Spark框架中的核心概念。可以将RDD视作数据库中的一张表。其中可以保存任何类型的数据。Spark将数据存储在不同分区上的RDD之中。</p> 
  <p>RDD可以帮助重新安排计算并优化数据处理过程。</p> 
  <p>此外，它还具有容错性，因为RDD知道如何重新创建和重新计算数据集。</p> 
  <p>RDD是不可变的。你可以用变换(Transformation)修改RDD，但是这个变换所返回的是一个全新的RDD，而原有的RDD仍然保持不变。</p> 
  <p>RDD支持两种类型的操作：</p> 
  <p>变换(Transformation)<br> 行动(Action)</p> 
  <p><strong>变换：</strong>变换的返回值是一个新的RDD集合，而不是单个值。调用一个变换方法，不会有任何求值计算，它只获取一个RDD作为参数，然后返回一个新的RDD。变换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，pipe和coalesce。</p> 
  <p><strong>行动：</strong>行动操作计算并返回一个新的值。当在一个RDD对象上调用行动函数时，会在这一时刻计算全部的数据处理查询并返回结果值。</p> 
  <p>行动操作包括：reduce，collect，count，first，take，countByKey以及foreach。</p> 
  <p><strong>共享变量(Shared varialbes)：</strong></p> 
  <p>广播变量(Broadcast variables)<br> 累加器(Accumulators)</p> 
  <p><strong>Master/Worker/Driver/Executor</strong></p> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" height="625" src="http://www.raincent.com/uploadfile/2016/0713/20160713040412962.png" width="627"></p> 
  <p>&nbsp;</p> 
  <p><strong>o Master：1</strong>. 接受Worker的注册请求，统筹记录所有Worker的CPU、Memory等资源，并跟踪Worker结点的活动状态;2. 接受Driver中App的注册请求(这个请求由Driver端的Client发出)，为App在Worker上分配CPU、Memory资源，生成后台Executor进程;之后跟踪Executor和App的活动状态。</p> 
  <p><strong>o Worker：</strong>负责接收Master的指示，为App创建Executor进程。Worker在Master和Executor之间起着桥梁作用，实际不会参与计算工作。</p> 
  <p><strong>o Driver</strong>：负责用户侧逻辑处理。</p> 
  <p><strong>o Executor：</strong>负责计算，接受并执行由App划分的Task任务，并将结果缓存在本地内存或磁盘。</p> 
  <p><strong>Spark部署</strong></p> 
  <p>关于Spark的部署网上相关资料很多，这里进行归纳整理</p> 
  <p>部署环境</p> 
  <p>Ubuntu 14.04LTS<br> Hadoop:2.7.0<br> Java JDK 1.8<br> Spark 1.6.1<br> Scala 2.11.8</p> 
  <p><strong>Hadoop安装</strong></p> 
  <p>由于Spark会利用HDFS和YARN，所以需要提前配置Hadoop，配置教程可以参考：</p> 
  <p><a href="http://www.powerxing.com/install-hadoop/" rel="nofollow">Setting up aApache Hadoop 2.7 single node on Ubuntu 14.04</a></p> 
  <p><a href="http://www.powerxing.com/install-hadoop/" rel="nofollow">Hadoop安装教程_单机/伪分布式配置_Hadoop2.6.0/Ubuntu14.04</a></p> 
  <p><strong>Spark安装</strong></p> 
  <p>在安装好Hadoop的基础上，搭建Spark，配置教程参考：</p> 
  <p><a href="http://www.powerxing.com/spark-quick-start-guide/" rel="nofollow">Spark快速入门指南 – Spark安装与基础使用</a></p> 
  <p><strong>scala安装</strong></p> 
  <p>Scala作为编写Spark的源生语言，更新速度和支持情况肯定是最好的，而另一方面Scala本身语言中对于面向对象和函数式编程两种思想的糅合，使得该语言具有很多炫酷的语法糖，所以在使用Spark的过程中我采用了Scala语言进行开发。</p> 
  <p><strong>Scala</strong>最终编译成字节码需要运行在JVM中，所以需要依托于jdk，需要部署jdk</p> 
  <p><strong>Eclips</strong>e作为一款开发Java的IDE神器，在Scala中当然也可以使用，有两种方式:</p> 
  <p>o Eclipse-&gt;Help-&gt;Install New Software安装Scala Plugins<br> o 下载官网已经提供的集成好的Scala IDE</p> 
  <p>基于以上两步已经可以进行Scala开发，需要用到Scala自带的SBT编译的同学可以装下Scala官网下载地址，本人一直使用Maven进行包管理就延续Maven的使用</p> 
  <p>简单示例：WordCount(Spark Scala)</p> 
  <p>开发IDE：Eclipse Scala<br> 包管理：Maven<br> 开发语言：Scala</p> 
  <p><strong>创建Maven项目</strong></p> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" height="256" src="http://www.raincent.com/uploadfile/2016/0713/20160713040412911.png" width="557"></p> 
  <p>&nbsp;</p> 
  <p>1. 跳过archetype项目模板的选择</p> 
  <p>2. 下载模板pom.xml</p> 
  <p>3. 对maven项目添加Scala属性：Right click on project -&gt; configure - &gt; Add Scala Nature.</p> 
  <p>4. 调整下Scala编译器的版本，与Spark版本对应：<br> Right click on project- &gt; Go to properties -&gt; Scala compiler -&gt; update Scala installation version to 2.10.5</p> 
  <p>5. 从Build Path中移除Scala Library(由于在Maven中添加了Spark Core的依赖项，而Spark是依赖于Scala的，Scala的jar包已经存在于Maven Dependency中)：<br> Right click on the project -&gt; Build path -&gt; Configure build path and remove Scala Library Container.</p> 
  <p>6. 添加package包com.spark.sample</p> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" height="493" src="http://www.raincent.com/uploadfile/2016/0713/20160713040413622.png" width="693"></p> 
  <p>&nbsp;</p> 
  <p>7. 创建Object WordCount和SimpleCount，用来作为Spark的两个简单示例</p> 
  <p>Spark Sample<br> SimpleCount.scala<br> package com.spark.sample<br> import org.apache.spark.SparkConf<br> import org.apache.spark.SparkContext<br> object SimpleCount {<br> def main(args: Array[String]) {<br> val conf = new<br> SparkConf().setAppName("TrySparkStreaming").setMaster("local[2]")<br> // Create spark context<br> val sc = new<br> SparkContext(conf)<br> // val ssc = new StreamingContext(conf, Seconds(1)) //<br> create streaming context<br> val txtFile =<br> "test"<br> val txtData =<br> sc.textFile(txtFile)<br> txtData.cache()<br> txtData.count()<br> val wcData =<br> txtData.flatMap { line =&gt; line.split(",") }.map { word =&gt;<br> (word, 1) }.reduceByKey(_ + _)<br> wcData.collect().foreach(println)<br> sc.stop<br> }<br> }<br> WordCount.scala<br> package com.spark.sample<br> import org.apache.spark.SparkConf<br> import org.apache.spark.SparkContext<br> import org.apache.spark.rdd.RDD.rddToPairRDDFunctions<br> object WordCount {<br> def main(args: Array[String]) = {<br> //Start the Spark context<br> val conf = new SparkConf()<br> .setAppName("WordCount")<br> .setMaster("local")<br> val sc = new<br> SparkContext(conf)<br> //Read some example file<br> to a test RDD<br> val test =<br> sc.textFile("input.txt")<br> test.flatMap { line =&gt;<br> //for each line<br> line.split("<br> ") //split the line in word by word.<br> }.map { word =&gt; //for<br> each word<br> (word, 1) //Return<br> a key/value tuple, with the word as key and 1 as value<br> }.reduceByKey(_ + _) //Sum<br> all of the value with same key<br> .saveAsTextFile("output.txt")<br> //Save to a text file<br> //Stop the Spark context<br> sc.stop<br> }<br> }</p> 
  <p>原理如下图：</p> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" height="320" src="http://www.raincent.com/uploadfile/2016/0713/20160713040413814.png" width="571"></p> 
  <p><strong>推荐阅读文章</strong></p> 
  <p><strong><a href="https://mp.weixin.qq.com/s?__biz=Mzg4MTE5MTUwMw==&amp;mid=100000042&amp;idx=5&amp;sn=6fee18778be5646423278ab174fd1f46&amp;chksm=4f68fe0a781f771c093f541c875dae43183fe49c1eb3a482a06dc0f7aaccc241309863c5fee4&amp;xtrack=1&amp;scene=0&amp;subscene=90&amp;sessionid=1557816542&amp;clicktime=1557816723&amp;ascene=7&amp;devicetype=android-26&amp;version=27000481&amp;nettype=ctnet&amp;abtest_cookie=BQABAAgACgALABIAEwAGAJ%2BGHgBWmR4Aw5keANyZHgDimR4A85keAAAA&amp;lang=zh_CN&amp;pass_ticket=8W5MIKb%2FtX1WSbZTwoN%2FlGj0D9eK3p%2FWwM3V4hSialiV1TSZj%2FcTL2BgLpbZxEPI&amp;wx_header=1" rel="nofollow">大数据工程师在阿里面试流程是什么？</a></strong></p> 
  <p><strong><a href="https://mp.weixin.qq.com/s?__biz=Mzg4MTE5MTUwMw==&amp;mid=100000042&amp;idx=1&amp;sn=73ae2337351cb064c531f1e0dba58456&amp;chksm=4f68fe0a781f771cf74c282db1600032fdc494b489e6708f989906c41fd05c32181403669dd0&amp;xtrack=1&amp;scene=0&amp;subscene=90&amp;sessionid=1557816440&amp;clicktime=1557816502&amp;ascene=7&amp;devicetype=android-26&amp;version=27000481&amp;nettype=ctnet&amp;abtest_cookie=BQABAAgACgALABIAEwAGAJ%2BGHgBWmR4Aw5keANyZHgDimR4A85keAAAA&amp;lang=zh_CN&amp;pass_ticket=8W5MIKb%2FtX1WSbZTwoN%2FlGj0D9eK3p%2FWwM3V4hSialiV1TSZj%2FcTL2BgLpbZxEPI&amp;wx_header=1" rel="nofollow">学习大数据需要具备怎么样基础？</a></strong></p> 
  <p><strong><a href="https://mp.weixin.qq.com/s?__biz=Mzg4MTE5MTUwMw==&amp;mid=100000045&amp;idx=1&amp;sn=4ce11a6ff4bad76c4457a3c0110efd0c&amp;chksm=4f68fe0d781f771b9c8e5f86860a81f3dbe567ae2bdfc7cb67ae37c77ab9a4310711f53ba4ca&amp;xtrack=1&amp;scene=0&amp;subscene=90&amp;sessionid=1557816440&amp;clicktime=1557816449&amp;ascene=7&amp;devicetype=android-26&amp;version=27000481&amp;nettype=ctnet&amp;abtest_cookie=BQABAAgACgALABIAEwAGAJ%2BGHgBWmR4Aw5keANyZHgDimR4A85keAAAA&amp;lang=zh_CN&amp;pass_ticket=8W5MIKb%2FtX1WSbZTwoN%2FlGj0D9eK3p%2FWwM3V4hSialiV1TSZj%2FcTL2BgLpbZxEPI&amp;wx_header=1" rel="nofollow">年薪30K的大数据开发工程师的工作经验总结？</a></strong></p> 
  <p>&nbsp;</p> 
  <p><img alt="" class="has" src="https://upload-images.jianshu.io/upload_images/5471126-d9a978035ee4ad9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p> 
 </div> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
