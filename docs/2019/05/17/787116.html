<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>日志分析系统(zookeeper+flume+kafka)之实时接收数据 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="日志分析系统(zookeeper+flume+kafka)之实时接收数据" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="日志实时分析系统（单机） 本次教程的主要目的是利用flume采集Apache的access.log数据通过kafka消息订阅服务转发至日志分析程序，其中需要的环境包括：linux系统，Java JDK,zookeeper,flume,kafka,接收数据的程序java程序。 一准备环境 软件下载 软件的下载 wget http://www.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz wget http://www.apache.org/dist/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz wget http://mirror.bit.edu.cn/apache/kafka/2.2.0/kafka_2.11-2.2.0.tgz 结果 -rw-r--r--. 1 root root 58688757 Oct 27 2018 apache-flume-1.8.0-bin.tar.gz -rw-r--r--. 1 root root 286821827 Apr 4 14:13 flink-1.8.0-bin-scala_2.12.tgz -rw-r--r--. 1 root root 63999924 Mar 23 08:57 kafka_2.11-2.2.0.tgz -rw-r--r--. 1 root root 28678231 Mar 4 2016 scala-2.11.8.tgz -rw-r--r--. 1 root root 37676320 Apr 1 22:44 zookeeper-3.4.14.tar.gz 环境安装 Java JDK 安装 yum -y remove java-1.8.0-openjdk* 安装结果： java -version openjdk version &quot;1.8.0_212&quot; OpenJDK Runtime Environment (build 1.8.0_212-b04) OpenJDK 64-Bit Server VM (build 25.212-b04, mixed mode) 安装zookeeper+flume+kafka zookeeper安装配置 cd /usr/local tar -zxvf zookeeper-3.4.14.tar.gz mv ./zookeeper-3.4.14 ./zookeeper vim ./zookeeper/conf/zoo.cfg 修改配置 tickTime=2000 initLimit=10 syncLimit=5 dataDir=/tmp/zookeeper clientPort=2181 flume安装配置 cd /usr/local tar -zxvf apache-flume-1.8.0-bin.tar.gz mv ./apache-flume-1.8.0 ./flume vim ./flume/conf/flume-conf.properties # 配置修改 agent.sources = s1 agent.channels = c1 agent.sinks = k1 # 从指定文件读取数据 agent.sources.s1.type = exec agent.sources.s1.command = tail -f /home/wwwlogs/access.log agent.sources.s1.channels = c1 # 配置传输通道 agent.channels.c1.type = memory agent.channels.c1.capacity = 10000 agent.channels.c1.transactionCapacity = 100 # 配置kafka接收数据 agent.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink agent.sinks.k1.brokerList = 10.1.1.35:9092 agent.sinks.k1.topic = test agent.sinks.k1.serializer.class = kafka.serializer.StringEncoder agent.sinks.k1.channel = c1 kafka安装配置 cd /usr/local tar -zxvf kafka_2.11-2.2.0.tgz mv ./kafka_2.11-2.2.0 ./kafka vim ./kafka/conf/server.properties 修改配置 broker.id=1 advertised.listeners=PLAINTEXT://192.168.56.101:9092 zookeeper.connect=127.0.0.1:2181 程序启动 注意启动需要按照顺序来 启动zookeeper ：/usr/local/zookeeper/bin/zkServer.sh start &amp; 启动kafka ：/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &gt;/dev/null 2&gt;&amp;1 &amp; 启动flume ：/usr/local/flume/bin/flume-ng agent --conf-file /usr/local/flume/conf/flume-conf.properties -c conf/ --name agent -Dflume.root.logger=DEBUG,console &amp; 接收程序开发 IDEA 开发程序 新建maven 工程 pom.xml &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com&lt;/groupId&gt; &lt;artifactId&gt;yang.flink&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;description&gt;flink 测试&lt;/description&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;yang.flink&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-clients --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;!-- &lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/log4j/log4j --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt; target/classes/lib &lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;mainClass&gt; com.tonytaotao.flink.FlinkKafka &lt;/mainClass&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;/manifest&gt; &lt;manifestEntries&gt; &lt;Class-Path&gt;.&lt;/Class-Path&gt; &lt;/manifestEntries&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; FlinkKafka.java package com.yang.flink; import org.apache.flink.api.common.serialization.SimpleStringSchema; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks; import org.apache.flink.streaming.api.watermark.Watermark; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import org.apache.kafka.clients.consumer.OffsetAndMetadata; import org.apache.kafka.common.TopicPartition; import javax.annotation.Nullable; import java.util.ArrayList; import java.util.Collections; import java.util.List; import java.util.Properties; import java.util.regex.Matcher; import java.util.regex.Pattern; public class FlinkKafka { public static void main(String[] args) throws Exception{ // 引入Flink StreamExecutionEnvironment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置监控数据流时间间隔（官方叫状态与检查点） env.enableCheckpointing(1000); // 配置kafka和zookeeper的ip和端口 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;10.1.1.35:9092&quot;); properties.setProperty(&quot;zookeeper.connect&quot;, &quot;10.1.1.35:2181&quot;); properties.setProperty(&quot;auto.commit.interval.ms&quot;, &quot;false&quot;); properties.setProperty(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); properties.setProperty(&quot;session.timeout.ms&quot;, &quot;30000&quot;); properties.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); properties.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); properties.setProperty(&quot;group.id&quot;, &quot;test&quot;); System.out.printf(&quot; Start listener: \n&quot;); KafkaConsumer&lt;String, String&gt; consumer = new org.apache.kafka.clients.consumer.KafkaConsumer&lt;String, String&gt;(properties); List&lt;String&gt; subscribedTopics = new ArrayList&lt;String&gt;(); //如果需要订阅多个Topic，则在这里add进去即可 //subscribedTopics.add(topic1); subscribedTopics.add(&quot;test&quot;); consumer.subscribe(subscribedTopics); try { int index = 0; while(true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (TopicPartition partition : records.partitions()) { List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition); for (ConsumerRecord&lt;String, String&gt; record : partitionRecords) { //System.out.println(record.value().toString()+&quot; --&gt; Content &quot;); Pattern p3 = Pattern.compile( &quot;^([\\d.]+) (\\S+) (\\S+) \\[([\\w\\d:/]+\\s[+\\-]\\d{4})\\] \&quot;(.+?)\&quot; (\\d{3}) ([\\d\\-]+) \&quot;([^\&quot;]+)\&quot; \&quot;([^\&quot;]+)\&quot;.*&quot;); Matcher matcher = p3.matcher(record.value().toString()); if(matcher.find()){ System.out.println(&quot;访问的源IP --&gt; &quot;+matcher.group(1)); System.out.println(&quot;访问时间 --&gt; &quot;+matcher.group(4)); System.out.println(&quot;请求方法+地址 --&gt; &quot;+matcher.group(5)); System.out.println(&quot;HTTP相应状态 --&gt; &quot;+matcher.group(6)); System.out.println(&quot;完全URL --&gt; &quot;+matcher.group(8)); System.out.println(&quot;HTTP请求头信息 --&gt; &quot;+matcher.group(9)); } index++; } long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset(); //数据库操作操作 //.................... consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset - 1))); System.out.println(&quot;此次接收到数据 --&gt; 累计数 【&quot; + index +&quot;】&quot;); } } } finally { consumer.close(); } } public static final class LineSplitter implements AssignerWithPunctuatedWatermarks&lt;String&gt; { private static final long serialVersionUID = 1L; @Nullable public Watermark checkAndGetNextWatermark(String arg0, long arg1) { if (null != arg0 &amp;&amp; arg0.contains(&quot;,&quot;)) { String parts[] = arg0.split(&quot;,&quot;); return new Watermark(Long.parseLong(parts[0])); } return null; } public long extractTimestamp(String arg0, long arg1) { if (null != arg0 &amp;&amp; arg0.contains(&quot;,&quot;)) { String parts[] = arg0.split(&quot;,&quot;); return Long.parseLong(parts[0]); } return 0; } } } 运行项目查看结果 访问的源IP --&gt; 10.1.3.165 访问时间 --&gt; 17/May/2019:14:11:52 +0800 请求方法+地址 --&gt; GET /doc/list HTTP/1.1 HTTP相应状态 --&gt; 200 完全URL --&gt; http://10.1.1.35/doc HTTP请求头信息 --&gt; Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36 OPR/58.0.3135.132" />
<meta property="og:description" content="日志实时分析系统（单机） 本次教程的主要目的是利用flume采集Apache的access.log数据通过kafka消息订阅服务转发至日志分析程序，其中需要的环境包括：linux系统，Java JDK,zookeeper,flume,kafka,接收数据的程序java程序。 一准备环境 软件下载 软件的下载 wget http://www.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz wget http://www.apache.org/dist/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz wget http://mirror.bit.edu.cn/apache/kafka/2.2.0/kafka_2.11-2.2.0.tgz 结果 -rw-r--r--. 1 root root 58688757 Oct 27 2018 apache-flume-1.8.0-bin.tar.gz -rw-r--r--. 1 root root 286821827 Apr 4 14:13 flink-1.8.0-bin-scala_2.12.tgz -rw-r--r--. 1 root root 63999924 Mar 23 08:57 kafka_2.11-2.2.0.tgz -rw-r--r--. 1 root root 28678231 Mar 4 2016 scala-2.11.8.tgz -rw-r--r--. 1 root root 37676320 Apr 1 22:44 zookeeper-3.4.14.tar.gz 环境安装 Java JDK 安装 yum -y remove java-1.8.0-openjdk* 安装结果： java -version openjdk version &quot;1.8.0_212&quot; OpenJDK Runtime Environment (build 1.8.0_212-b04) OpenJDK 64-Bit Server VM (build 25.212-b04, mixed mode) 安装zookeeper+flume+kafka zookeeper安装配置 cd /usr/local tar -zxvf zookeeper-3.4.14.tar.gz mv ./zookeeper-3.4.14 ./zookeeper vim ./zookeeper/conf/zoo.cfg 修改配置 tickTime=2000 initLimit=10 syncLimit=5 dataDir=/tmp/zookeeper clientPort=2181 flume安装配置 cd /usr/local tar -zxvf apache-flume-1.8.0-bin.tar.gz mv ./apache-flume-1.8.0 ./flume vim ./flume/conf/flume-conf.properties # 配置修改 agent.sources = s1 agent.channels = c1 agent.sinks = k1 # 从指定文件读取数据 agent.sources.s1.type = exec agent.sources.s1.command = tail -f /home/wwwlogs/access.log agent.sources.s1.channels = c1 # 配置传输通道 agent.channels.c1.type = memory agent.channels.c1.capacity = 10000 agent.channels.c1.transactionCapacity = 100 # 配置kafka接收数据 agent.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink agent.sinks.k1.brokerList = 10.1.1.35:9092 agent.sinks.k1.topic = test agent.sinks.k1.serializer.class = kafka.serializer.StringEncoder agent.sinks.k1.channel = c1 kafka安装配置 cd /usr/local tar -zxvf kafka_2.11-2.2.0.tgz mv ./kafka_2.11-2.2.0 ./kafka vim ./kafka/conf/server.properties 修改配置 broker.id=1 advertised.listeners=PLAINTEXT://192.168.56.101:9092 zookeeper.connect=127.0.0.1:2181 程序启动 注意启动需要按照顺序来 启动zookeeper ：/usr/local/zookeeper/bin/zkServer.sh start &amp; 启动kafka ：/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &gt;/dev/null 2&gt;&amp;1 &amp; 启动flume ：/usr/local/flume/bin/flume-ng agent --conf-file /usr/local/flume/conf/flume-conf.properties -c conf/ --name agent -Dflume.root.logger=DEBUG,console &amp; 接收程序开发 IDEA 开发程序 新建maven 工程 pom.xml &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com&lt;/groupId&gt; &lt;artifactId&gt;yang.flink&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;description&gt;flink 测试&lt;/description&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;yang.flink&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-clients --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;!-- &lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/log4j/log4j --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt; target/classes/lib &lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;mainClass&gt; com.tonytaotao.flink.FlinkKafka &lt;/mainClass&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;/manifest&gt; &lt;manifestEntries&gt; &lt;Class-Path&gt;.&lt;/Class-Path&gt; &lt;/manifestEntries&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; FlinkKafka.java package com.yang.flink; import org.apache.flink.api.common.serialization.SimpleStringSchema; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks; import org.apache.flink.streaming.api.watermark.Watermark; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import org.apache.kafka.clients.consumer.OffsetAndMetadata; import org.apache.kafka.common.TopicPartition; import javax.annotation.Nullable; import java.util.ArrayList; import java.util.Collections; import java.util.List; import java.util.Properties; import java.util.regex.Matcher; import java.util.regex.Pattern; public class FlinkKafka { public static void main(String[] args) throws Exception{ // 引入Flink StreamExecutionEnvironment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置监控数据流时间间隔（官方叫状态与检查点） env.enableCheckpointing(1000); // 配置kafka和zookeeper的ip和端口 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;10.1.1.35:9092&quot;); properties.setProperty(&quot;zookeeper.connect&quot;, &quot;10.1.1.35:2181&quot;); properties.setProperty(&quot;auto.commit.interval.ms&quot;, &quot;false&quot;); properties.setProperty(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); properties.setProperty(&quot;session.timeout.ms&quot;, &quot;30000&quot;); properties.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); properties.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); properties.setProperty(&quot;group.id&quot;, &quot;test&quot;); System.out.printf(&quot; Start listener: \n&quot;); KafkaConsumer&lt;String, String&gt; consumer = new org.apache.kafka.clients.consumer.KafkaConsumer&lt;String, String&gt;(properties); List&lt;String&gt; subscribedTopics = new ArrayList&lt;String&gt;(); //如果需要订阅多个Topic，则在这里add进去即可 //subscribedTopics.add(topic1); subscribedTopics.add(&quot;test&quot;); consumer.subscribe(subscribedTopics); try { int index = 0; while(true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (TopicPartition partition : records.partitions()) { List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition); for (ConsumerRecord&lt;String, String&gt; record : partitionRecords) { //System.out.println(record.value().toString()+&quot; --&gt; Content &quot;); Pattern p3 = Pattern.compile( &quot;^([\\d.]+) (\\S+) (\\S+) \\[([\\w\\d:/]+\\s[+\\-]\\d{4})\\] \&quot;(.+?)\&quot; (\\d{3}) ([\\d\\-]+) \&quot;([^\&quot;]+)\&quot; \&quot;([^\&quot;]+)\&quot;.*&quot;); Matcher matcher = p3.matcher(record.value().toString()); if(matcher.find()){ System.out.println(&quot;访问的源IP --&gt; &quot;+matcher.group(1)); System.out.println(&quot;访问时间 --&gt; &quot;+matcher.group(4)); System.out.println(&quot;请求方法+地址 --&gt; &quot;+matcher.group(5)); System.out.println(&quot;HTTP相应状态 --&gt; &quot;+matcher.group(6)); System.out.println(&quot;完全URL --&gt; &quot;+matcher.group(8)); System.out.println(&quot;HTTP请求头信息 --&gt; &quot;+matcher.group(9)); } index++; } long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset(); //数据库操作操作 //.................... consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset - 1))); System.out.println(&quot;此次接收到数据 --&gt; 累计数 【&quot; + index +&quot;】&quot;); } } } finally { consumer.close(); } } public static final class LineSplitter implements AssignerWithPunctuatedWatermarks&lt;String&gt; { private static final long serialVersionUID = 1L; @Nullable public Watermark checkAndGetNextWatermark(String arg0, long arg1) { if (null != arg0 &amp;&amp; arg0.contains(&quot;,&quot;)) { String parts[] = arg0.split(&quot;,&quot;); return new Watermark(Long.parseLong(parts[0])); } return null; } public long extractTimestamp(String arg0, long arg1) { if (null != arg0 &amp;&amp; arg0.contains(&quot;,&quot;)) { String parts[] = arg0.split(&quot;,&quot;); return Long.parseLong(parts[0]); } return 0; } } } 运行项目查看结果 访问的源IP --&gt; 10.1.3.165 访问时间 --&gt; 17/May/2019:14:11:52 +0800 请求方法+地址 --&gt; GET /doc/list HTTP/1.1 HTTP相应状态 --&gt; 200 完全URL --&gt; http://10.1.1.35/doc HTTP请求头信息 --&gt; Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36 OPR/58.0.3135.132" />
<link rel="canonical" href="https://mlh.app/2019/05/17/787116.html" />
<meta property="og:url" content="https://mlh.app/2019/05/17/787116.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-17T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"日志实时分析系统（单机） 本次教程的主要目的是利用flume采集Apache的access.log数据通过kafka消息订阅服务转发至日志分析程序，其中需要的环境包括：linux系统，Java JDK,zookeeper,flume,kafka,接收数据的程序java程序。 一准备环境 软件下载 软件的下载 wget http://www.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz wget http://www.apache.org/dist/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz wget http://mirror.bit.edu.cn/apache/kafka/2.2.0/kafka_2.11-2.2.0.tgz 结果 -rw-r--r--. 1 root root 58688757 Oct 27 2018 apache-flume-1.8.0-bin.tar.gz -rw-r--r--. 1 root root 286821827 Apr 4 14:13 flink-1.8.0-bin-scala_2.12.tgz -rw-r--r--. 1 root root 63999924 Mar 23 08:57 kafka_2.11-2.2.0.tgz -rw-r--r--. 1 root root 28678231 Mar 4 2016 scala-2.11.8.tgz -rw-r--r--. 1 root root 37676320 Apr 1 22:44 zookeeper-3.4.14.tar.gz 环境安装 Java JDK 安装 yum -y remove java-1.8.0-openjdk* 安装结果： java -version openjdk version &quot;1.8.0_212&quot; OpenJDK Runtime Environment (build 1.8.0_212-b04) OpenJDK 64-Bit Server VM (build 25.212-b04, mixed mode) 安装zookeeper+flume+kafka zookeeper安装配置 cd /usr/local tar -zxvf zookeeper-3.4.14.tar.gz mv ./zookeeper-3.4.14 ./zookeeper vim ./zookeeper/conf/zoo.cfg 修改配置 tickTime=2000 initLimit=10 syncLimit=5 dataDir=/tmp/zookeeper clientPort=2181 flume安装配置 cd /usr/local tar -zxvf apache-flume-1.8.0-bin.tar.gz mv ./apache-flume-1.8.0 ./flume vim ./flume/conf/flume-conf.properties # 配置修改 agent.sources = s1 agent.channels = c1 agent.sinks = k1 # 从指定文件读取数据 agent.sources.s1.type = exec agent.sources.s1.command = tail -f /home/wwwlogs/access.log agent.sources.s1.channels = c1 # 配置传输通道 agent.channels.c1.type = memory agent.channels.c1.capacity = 10000 agent.channels.c1.transactionCapacity = 100 # 配置kafka接收数据 agent.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink agent.sinks.k1.brokerList = 10.1.1.35:9092 agent.sinks.k1.topic = test agent.sinks.k1.serializer.class = kafka.serializer.StringEncoder agent.sinks.k1.channel = c1 kafka安装配置 cd /usr/local tar -zxvf kafka_2.11-2.2.0.tgz mv ./kafka_2.11-2.2.0 ./kafka vim ./kafka/conf/server.properties 修改配置 broker.id=1 advertised.listeners=PLAINTEXT://192.168.56.101:9092 zookeeper.connect=127.0.0.1:2181 程序启动 注意启动需要按照顺序来 启动zookeeper ：/usr/local/zookeeper/bin/zkServer.sh start &amp; 启动kafka ：/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &gt;/dev/null 2&gt;&amp;1 &amp; 启动flume ：/usr/local/flume/bin/flume-ng agent --conf-file /usr/local/flume/conf/flume-conf.properties -c conf/ --name agent -Dflume.root.logger=DEBUG,console &amp; 接收程序开发 IDEA 开发程序 新建maven 工程 pom.xml &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com&lt;/groupId&gt; &lt;artifactId&gt;yang.flink&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;description&gt;flink 测试&lt;/description&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;yang.flink&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-clients --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-java --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;!-- &lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/log4j/log4j --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;test&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt; target/classes/lib &lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;mainClass&gt; com.tonytaotao.flink.FlinkKafka &lt;/mainClass&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;/manifest&gt; &lt;manifestEntries&gt; &lt;Class-Path&gt;.&lt;/Class-Path&gt; &lt;/manifestEntries&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; FlinkKafka.java package com.yang.flink; import org.apache.flink.api.common.serialization.SimpleStringSchema; import org.apache.flink.streaming.api.datastream.DataStream; import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment; import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks; import org.apache.flink.streaming.api.watermark.Watermark; import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import org.apache.kafka.clients.consumer.OffsetAndMetadata; import org.apache.kafka.common.TopicPartition; import javax.annotation.Nullable; import java.util.ArrayList; import java.util.Collections; import java.util.List; import java.util.Properties; import java.util.regex.Matcher; import java.util.regex.Pattern; public class FlinkKafka { public static void main(String[] args) throws Exception{ // 引入Flink StreamExecutionEnvironment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置监控数据流时间间隔（官方叫状态与检查点） env.enableCheckpointing(1000); // 配置kafka和zookeeper的ip和端口 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;10.1.1.35:9092&quot;); properties.setProperty(&quot;zookeeper.connect&quot;, &quot;10.1.1.35:2181&quot;); properties.setProperty(&quot;auto.commit.interval.ms&quot;, &quot;false&quot;); properties.setProperty(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); properties.setProperty(&quot;session.timeout.ms&quot;, &quot;30000&quot;); properties.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); properties.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); properties.setProperty(&quot;group.id&quot;, &quot;test&quot;); System.out.printf(&quot; Start listener: \\n&quot;); KafkaConsumer&lt;String, String&gt; consumer = new org.apache.kafka.clients.consumer.KafkaConsumer&lt;String, String&gt;(properties); List&lt;String&gt; subscribedTopics = new ArrayList&lt;String&gt;(); //如果需要订阅多个Topic，则在这里add进去即可 //subscribedTopics.add(topic1); subscribedTopics.add(&quot;test&quot;); consumer.subscribe(subscribedTopics); try { int index = 0; while(true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (TopicPartition partition : records.partitions()) { List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition); for (ConsumerRecord&lt;String, String&gt; record : partitionRecords) { //System.out.println(record.value().toString()+&quot; --&gt; Content &quot;); Pattern p3 = Pattern.compile( &quot;^([\\\\d.]+) (\\\\S+) (\\\\S+) \\\\[([\\\\w\\\\d:/]+\\\\s[+\\\\-]\\\\d{4})\\\\] \\&quot;(.+?)\\&quot; (\\\\d{3}) ([\\\\d\\\\-]+) \\&quot;([^\\&quot;]+)\\&quot; \\&quot;([^\\&quot;]+)\\&quot;.*&quot;); Matcher matcher = p3.matcher(record.value().toString()); if(matcher.find()){ System.out.println(&quot;访问的源IP --&gt; &quot;+matcher.group(1)); System.out.println(&quot;访问时间 --&gt; &quot;+matcher.group(4)); System.out.println(&quot;请求方法+地址 --&gt; &quot;+matcher.group(5)); System.out.println(&quot;HTTP相应状态 --&gt; &quot;+matcher.group(6)); System.out.println(&quot;完全URL --&gt; &quot;+matcher.group(8)); System.out.println(&quot;HTTP请求头信息 --&gt; &quot;+matcher.group(9)); } index++; } long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset(); //数据库操作操作 //.................... consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset - 1))); System.out.println(&quot;此次接收到数据 --&gt; 累计数 【&quot; + index +&quot;】&quot;); } } } finally { consumer.close(); } } public static final class LineSplitter implements AssignerWithPunctuatedWatermarks&lt;String&gt; { private static final long serialVersionUID = 1L; @Nullable public Watermark checkAndGetNextWatermark(String arg0, long arg1) { if (null != arg0 &amp;&amp; arg0.contains(&quot;,&quot;)) { String parts[] = arg0.split(&quot;,&quot;); return new Watermark(Long.parseLong(parts[0])); } return null; } public long extractTimestamp(String arg0, long arg1) { if (null != arg0 &amp;&amp; arg0.contains(&quot;,&quot;)) { String parts[] = arg0.split(&quot;,&quot;); return Long.parseLong(parts[0]); } return 0; } } } 运行项目查看结果 访问的源IP --&gt; 10.1.3.165 访问时间 --&gt; 17/May/2019:14:11:52 +0800 请求方法+地址 --&gt; GET /doc/list HTTP/1.1 HTTP相应状态 --&gt; 200 完全URL --&gt; http://10.1.1.35/doc HTTP请求头信息 --&gt; Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36 OPR/58.0.3135.132","@type":"BlogPosting","url":"https://mlh.app/2019/05/17/787116.html","headline":"日志分析系统(zookeeper+flume+kafka)之实时接收数据","dateModified":"2019-05-17T00:00:00+08:00","datePublished":"2019-05-17T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/17/787116.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>日志分析系统(zookeeper+flume+kafka)之实时接收数据</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-dark"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <h1><a id="_1"></a>日志实时分析系统（单机）</h1> 
  <p>本次教程的主要目的是利用flume采集Apache的access.log数据通过kafka消息订阅服务转发至日志分析程序，其中需要的环境包括：linux系统，Java JDK,zookeeper,flume,kafka,接收数据的程序java程序。</p> 
  <!-- more --> 
  <h1><a id="_4"></a>一准备环境</h1> 
  <h2><a id="_5"></a>软件下载</h2> 
  <pre><code>软件的下载
  wget http://www.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz	
  wget http://www.apache.org/dist/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz
  wget http://mirror.bit.edu.cn/apache/kafka/2.2.0/kafka_2.11-2.2.0.tgz 
</code></pre> 
  <p>结果</p> 
  <pre><code>-rw-r--r--. 1 root root  58688757 Oct 27  2018 apache-flume-1.8.0-bin.tar.gz
-rw-r--r--. 1 root root 286821827 Apr  4 14:13 flink-1.8.0-bin-scala_2.12.tgz
-rw-r--r--. 1 root root  63999924 Mar 23 08:57 kafka_2.11-2.2.0.tgz
-rw-r--r--. 1 root root  28678231 Mar  4  2016 scala-2.11.8.tgz
-rw-r--r--. 1 root root  37676320 Apr  1 22:44 zookeeper-3.4.14.tar.gz
</code></pre> 
  <h1><a id="_20"></a>环境安装</h1> 
  <h2><a id="Java_JDK__21"></a>Java JDK 安装</h2> 
  <pre><code>yum -y remove java-1.8.0-openjdk*
安装结果： 
java -version 
openjdk version "1.8.0_212"
OpenJDK Runtime Environment (build 1.8.0_212-b04)
OpenJDK 64-Bit Server VM (build 25.212-b04, mixed mode)
</code></pre> 
  <h2><a id="zookeeperflumekafka_30"></a>安装zookeeper+flume+kafka</h2> 
  <h3><a id="zookeeper_31"></a>zookeeper安装配置</h3> 
  <pre><code>cd /usr/local
tar -zxvf zookeeper-3.4.14.tar.gz
mv ./zookeeper-3.4.14  ./zookeeper
vim ./zookeeper/conf/zoo.cfg 
修改配置

tickTime=2000 initLimit=10
syncLimit=5
dataDir=/tmp/zookeeper
clientPort=2181

</code></pre> 
  <h3><a id="flume_45"></a>flume安装配置</h3> 
  <pre><code>cd /usr/local
tar -zxvf apache-flume-1.8.0-bin.tar.gz
mv ./apache-flume-1.8.0  ./flume
vim ./flume/conf/flume-conf.properties  

# 配置修改

agent.sources = s1
agent.channels = c1
agent.sinks = k1

# 从指定文件读取数据
agent.sources.s1.type = exec
agent.sources.s1.command = tail -f /home/wwwlogs/access.log
agent.sources.s1.channels = c1

# 配置传输通道
agent.channels.c1.type = memory
agent.channels.c1.capacity = 10000
agent.channels.c1.transactionCapacity = 100

# 配置kafka接收数据
agent.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
agent.sinks.k1.brokerList = 10.1.1.35:9092
agent.sinks.k1.topic = test
agent.sinks.k1.serializer.class = kafka.serializer.StringEncoder
agent.sinks.k1.channel = c1

</code></pre> 
  <h3><a id="kafka_76"></a>kafka安装配置</h3> 
  <pre><code>cd /usr/local
tar -zxvf kafka_2.11-2.2.0.tgz
mv ./kafka_2.11-2.2.0  ./kafka
vim ./kafka/conf/server.properties 
修改配置

broker.id=1
advertised.listeners=PLAINTEXT://192.168.56.101:9092
zookeeper.connect=127.0.0.1:2181

</code></pre> 
  <h1><a id="_89"></a>程序启动</h1> 
  <pre><code>注意启动需要按照顺序来
</code></pre> 
  <ul> 
   <li>启动zookeeper ：/usr/local/zookeeper/bin/zkServer.sh start &amp;</li> 
   <li>启动kafka ：/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties &gt;/dev/null 2&gt;&amp;1 &amp;</li> 
   <li>启动flume ：/usr/local/flume/bin/flume-ng agent --conf-file /usr/local/flume/conf/flume-conf.properties -c conf/ --name agent -Dflume.root.logger=DEBUG,console &amp;</li> 
  </ul> 
  <h1><a id="_95"></a>接收程序开发</h1> 
  <h2><a id="IDEA__maven__96"></a>IDEA 开发程序 新建maven 工程</h2> 
  <p>pom.xml</p> 
  <pre><code>&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

  &lt;groupId&gt;com&lt;/groupId&gt;
  &lt;artifactId&gt;yang.flink&lt;/artifactId&gt;
  &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
  &lt;description&gt;flink 测试&lt;/description&gt;
  &lt;packaging&gt;jar&lt;/packaging&gt;

  &lt;name&gt;yang.flink&lt;/name&gt;
  &lt;url&gt;http://maven.apache.org&lt;/url&gt;

  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
  &lt;/properties&gt;

  &lt;dependencies&gt;

        &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-java --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
            &lt;artifactId&gt;flink-java&lt;/artifactId&gt;
            &lt;version&gt;1.6.1&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-clients --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
            &lt;artifactId&gt;flink-clients_2.11&lt;/artifactId&gt;
            &lt;version&gt;1.6.1&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-java --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
            &lt;artifactId&gt;flink-streaming-java_2.11&lt;/artifactId&gt;
            &lt;version&gt;1.6.1&lt;/version&gt;
           &lt;!-- &lt;scope&gt;provided&lt;/scope&gt;--&gt;
        &lt;/dependency&gt;

        &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
            &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt;
            &lt;version&gt;1.6.1&lt;/version&gt;
        &lt;/dependency&gt;


        &lt;!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12 --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
            &lt;version&gt;1.7.25&lt;/version&gt;
        &lt;/dependency&gt;

        &lt;!-- https://mvnrepository.com/artifact/log4j/log4j --&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j&lt;/artifactId&gt;
            &lt;version&gt;1.2.17&lt;/version&gt;
        &lt;/dependency&gt;

    &lt;/dependencies&gt;

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                &lt;version&gt;3.2&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;source&gt;1.8&lt;/source&gt;
                    &lt;target&gt;1.8&lt;/target&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;


            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;copy-dependencies&lt;/id&gt;
                        &lt;phase&gt;test&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;
                        &lt;/goals&gt;
                        &lt;configuration&gt;
                            &lt;outputDirectory&gt;
                                target/classes/lib
                            &lt;/outputDirectory&gt;
                        &lt;/configuration&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
            &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;archive&gt;
                        &lt;manifest&gt;
                            &lt;addClasspath&gt;true&lt;/addClasspath&gt;
                            &lt;mainClass&gt;
                                com.tonytaotao.flink.FlinkKafka
                            &lt;/mainClass&gt;
                            &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt;
                        &lt;/manifest&gt;
                        &lt;manifestEntries&gt;
                            &lt;Class-Path&gt;.&lt;/Class-Path&gt;
                        &lt;/manifestEntries&gt;
                    &lt;/archive&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;
&lt;/project&gt;
</code></pre> 
  <p>FlinkKafka.java</p> 
  <pre><code>package com.yang.flink;

import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.AssignerWithPunctuatedWatermarks;
import org.apache.flink.streaming.api.watermark.Watermark;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.OffsetAndMetadata;
import org.apache.kafka.common.TopicPartition;

import javax.annotation.Nullable;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Properties;
import java.util.regex.Matcher;
import java.util.regex.Pattern;


public class FlinkKafka {
	public static void main(String[] args) throws Exception{

        // 引入Flink StreamExecutionEnvironment
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 设置监控数据流时间间隔（官方叫状态与检查点）
        env.enableCheckpointing(1000);

        // 配置kafka和zookeeper的ip和端口
        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "10.1.1.35:9092");
        properties.setProperty("zookeeper.connect", "10.1.1.35:2181");
        properties.setProperty("auto.commit.interval.ms", "false");
        properties.setProperty("auto.commit.interval.ms", "1000");
        properties.setProperty("session.timeout.ms", "30000");
        properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        properties.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        properties.setProperty("group.id", "test");
        System.out.printf(" Start listener: \n");

        	KafkaConsumer&lt;String, String&gt; consumer = new org.apache.kafka.clients.consumer.KafkaConsumer&lt;String, String&gt;(properties);
            List&lt;String&gt; subscribedTopics =  new ArrayList&lt;String&gt;();
            //如果需要订阅多个Topic，则在这里add进去即可
            //subscribedTopics.add(topic1);
            subscribedTopics.add("test");
            consumer.subscribe(subscribedTopics);
            try {
                int index = 0;
                while(true) {
                    ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);
                    for (TopicPartition partition : records.partitions()) {
                        List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(partition);
                        for (ConsumerRecord&lt;String, String&gt; record : partitionRecords) {
                        	//System.out.println(record.value().toString()+" --&gt; Content ");
                        	Pattern p3 = Pattern.compile( "^([\\d.]+) (\\S+) (\\S+) \\[([\\w\\d:/]+\\s[+\\-]\\d{4})\\] \"(.+?)\" (\\d{3}) ([\\d\\-]+) \"([^\"]+)\" \"([^\"]+)\".*"); 
                        	Matcher matcher = p3.matcher(record.value().toString());
                        	if(matcher.find()){
                        		System.out.println("访问的源IP --&gt; "+matcher.group(1));
                        		System.out.println("访问时间  --&gt; "+matcher.group(4));
                        		System.out.println("请求方法+地址  --&gt; "+matcher.group(5));
                        		System.out.println("HTTP相应状态  --&gt; "+matcher.group(6));
                        		System.out.println("完全URL --&gt; "+matcher.group(8));
                        		System.out.println("HTTP请求头信息  --&gt;  "+matcher.group(9));
                        	}
                        	
                            index++;
                        }
                        long lastOffset = partitionRecords.get(partitionRecords.size() - 1).offset();
                        //数据库操作操作
                        //....................
                        consumer.commitSync(Collections.singletonMap(partition, new OffsetAndMetadata(lastOffset - 1)));
                        System.out.println("此次接收到数据  --&gt; 累计数 【" + index +"】");
                    }
                }
            } finally {
              consumer.close();
            }
   }
	
	public static final class LineSplitter implements AssignerWithPunctuatedWatermarks&lt;String&gt; {

        private static final long serialVersionUID = 1L;


        @Nullable
        public Watermark checkAndGetNextWatermark(String arg0, long arg1) {
            if (null != arg0 &amp;&amp; arg0.contains(",")) {
                String parts[] = arg0.split(",");
                return new Watermark(Long.parseLong(parts[0]));
            }
            return null;
        }

        public long extractTimestamp(String arg0, long arg1) {
            if (null != arg0 &amp;&amp; arg0.contains(",")) {
                String parts[] = arg0.split(",");
                return Long.parseLong(parts[0]);
            }
            return 0;
        }
    }

}
</code></pre> 
  <h1><a id="_328"></a>运行项目查看结果</h1> 
  <pre><code>访问的源IP --&gt; 10.1.3.165
访问时间  --&gt; 17/May/2019:14:11:52 +0800
请求方法+地址  --&gt; GET /doc/list HTTP/1.1
HTTP相应状态  --&gt; 200
完全URL --&gt; http://10.1.1.35/doc
HTTP请求头信息  --&gt;  Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36 OPR/58.0.3135.132
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
