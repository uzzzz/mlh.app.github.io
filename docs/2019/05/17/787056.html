<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>TensorFlow框架做实时人脸识别小项目 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="TensorFlow框架做实时人脸识别小项目" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 人脸识别是深度学习最有价值也是最成熟的的应用之一。在研究环境下，人脸识别已经赶上甚至超过了人工识别的精度。一般来说，一个完整的人脸识别项目会包括两大部分：人脸检测与人脸识别。下面就我近期自己练习写的一个“粗糙”的人脸识别小项目讲起，也算是做一个学习记录。 首先 ，整个项目的框架包括四个主要的部分：（1）利用opencv从图像传感器处（比如电脑摄像头）实时的读入视频帧；（2）使用mtcnn网络做人脸检测和对齐；（3）利用facenet网络计算人脸特征，也就是embedding；（4）knn算法进行具体的人脸识别。如下图所示： 其中的mtcnn的人脸检测是很关键的一步，它检测定位的人脸准确与否直接影响到后面的特征计算与识别；facenet实际是一个对人脸进行特征编码的网络，具体的实现后面会讨论；knn的分类算法在用于真正的识别前要经过训练，训练的样本的质量好坏与数量也会对识别的 结果产生很大的影响。今天在这只讨论mtcnn网络的人脸检测对齐部分。 mtcnn网络全称为multi-task convolutinal neural network，意为多任务卷积神经网络。mtcnn由三个神经网络组成，分别是P-Net, R-Net, O-Net。在使用这些网络之前，首先要将原始图片缩放到不同尺寸，形成一个图像金字塔，接着会对每个尺寸的图片通过神经网络计算一遍。其目的在于兼顾图片中的不同大小的人脸，在统一的尺度下检测人脸。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 第一个网络P-Net的结构如下图所示： P-Net的输入是一个12x12的3通道RGB图像，它的作用是要判断这个网络中是否有人脸，并且给出人脸框和关键点位置。第一个部分face classification输出的是判断是人脸的概率和不是人脸的概率，两个值加起来严格等于1；第二个部分输出的是框的精确位置，4个值分别是框的左上角二维坐标和框的高度与宽度；第三个部分输出的是人脸5个关键点：左眼，右眼，鼻子，左嘴角，右嘴角的位置的二维坐标。 第二个网络R-Net的网络结构与P-Net差别不大，如下图： 除了输入大小与中间层大小不同，R-Net的结构与P-Net非常相似，只是在最后的输出层前多加入了一个全连接层。R-Net的输出完全与P-Net一样，同样由人脸判别，框回归，关键点位置预测三部分组成。 第三个网络O-Net结构如下： O-Net相比R-Net在结构上又多出一个中间层，但是输出结果还是一样的。 从P-Net到R-Net，再到O-Net，网络输入的图片越来越大，中间层的通道数越来越多，识别人脸的准确度也越来越高。实际上mtcnn的工作原理就是P-Net先做一遍过滤，将过滤后的结果交给R-Net进行过滤，最后再将过滤的结果交给O-Net进行判别。它是层层递进的一个筛查机制。 mtcnn中每个网络都有三部分输出，因此训练时损失的定义也由三部分组成。针对人脸判别face classification，使用交叉熵损失，针对框和回归点的判定，直接使用L2损失。最后这三部分损失各自乘以自身的权重再加起来，形成最后的总损失。P-Net和R-Net网络关心框位置的准确性，O-Net关心关键点判定，它们三部分的各自权重是不一样的。 mtcnn网络需要大量的人脸数据进行训练，才能得到合适的网络参数，达到较好的检测效果。在github上有已经训练好的mtcnn模型可以直接使用，点这里可以直达。其中的src/align/detect_face.py与det1.npy，det2.npy，det3.npy就是mtcnn网络的结构与训练模型，可以直接使用。 测试mtcnn的部分代码： def test(): &nbsp;&nbsp;&nbsp; video = cv2.VideoCapture( 0) &nbsp;&nbsp;&nbsp; print( &#39;Creating networks and loading parameters&#39;) &nbsp;&nbsp;&nbsp; with tf.Graph().as_default(): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction= 1.0) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement= False)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; with sess.as_default(): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pnet, rnet, onet = detect_face.create_mtcnn(sess, None) &nbsp;&nbsp;&nbsp; minsize = 20&nbsp; &nbsp;&nbsp;&nbsp; threshold = [ 0.6, 0.7, 0.7]&nbsp; &nbsp;&nbsp;&nbsp; factor = 0.709 &nbsp;&nbsp;&nbsp; while True: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret, frame = video.read() &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bounding_boxes, _ = detect_face.detect_face(frame, minsize, pnet, rnet, onet, threshold, factor) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nrof_faces = bounding_boxes.shape[ 0] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print( &#39;找到人脸数目为:{}&#39;.format(nrof_faces)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for face_position in bounding_boxes: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; face_position = face_position.astype(int) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cv2.rectangle(frame, (face_position[ 0], face_position[ 1]),(face_position[ 2], face_position[ 3]), ( 0, 255, 0), 2) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cv2.imshow( &#39;show&#39;, frame) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if cv2.waitKey( 5) &amp; 0xFF == ord( &#39;q&#39;): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break &nbsp;&nbsp;&nbsp; video.release() &nbsp;&nbsp;&nbsp; cv2.destroyAllWindows() 1 经过测试，mtcnn的实时性和准确性都非常好。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 人脸识别是深度学习最有价值也是最成熟的的应用之一。在研究环境下，人脸识别已经赶上甚至超过了人工识别的精度。一般来说，一个完整的人脸识别项目会包括两大部分：人脸检测与人脸识别。下面就我近期自己练习写的一个“粗糙”的人脸识别小项目讲起，也算是做一个学习记录。 首先 ，整个项目的框架包括四个主要的部分：（1）利用opencv从图像传感器处（比如电脑摄像头）实时的读入视频帧；（2）使用mtcnn网络做人脸检测和对齐；（3）利用facenet网络计算人脸特征，也就是embedding；（4）knn算法进行具体的人脸识别。如下图所示： 其中的mtcnn的人脸检测是很关键的一步，它检测定位的人脸准确与否直接影响到后面的特征计算与识别；facenet实际是一个对人脸进行特征编码的网络，具体的实现后面会讨论；knn的分类算法在用于真正的识别前要经过训练，训练的样本的质量好坏与数量也会对识别的 结果产生很大的影响。今天在这只讨论mtcnn网络的人脸检测对齐部分。 mtcnn网络全称为multi-task convolutinal neural network，意为多任务卷积神经网络。mtcnn由三个神经网络组成，分别是P-Net, R-Net, O-Net。在使用这些网络之前，首先要将原始图片缩放到不同尺寸，形成一个图像金字塔，接着会对每个尺寸的图片通过神经网络计算一遍。其目的在于兼顾图片中的不同大小的人脸，在统一的尺度下检测人脸。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 第一个网络P-Net的结构如下图所示： P-Net的输入是一个12x12的3通道RGB图像，它的作用是要判断这个网络中是否有人脸，并且给出人脸框和关键点位置。第一个部分face classification输出的是判断是人脸的概率和不是人脸的概率，两个值加起来严格等于1；第二个部分输出的是框的精确位置，4个值分别是框的左上角二维坐标和框的高度与宽度；第三个部分输出的是人脸5个关键点：左眼，右眼，鼻子，左嘴角，右嘴角的位置的二维坐标。 第二个网络R-Net的网络结构与P-Net差别不大，如下图： 除了输入大小与中间层大小不同，R-Net的结构与P-Net非常相似，只是在最后的输出层前多加入了一个全连接层。R-Net的输出完全与P-Net一样，同样由人脸判别，框回归，关键点位置预测三部分组成。 第三个网络O-Net结构如下： O-Net相比R-Net在结构上又多出一个中间层，但是输出结果还是一样的。 从P-Net到R-Net，再到O-Net，网络输入的图片越来越大，中间层的通道数越来越多，识别人脸的准确度也越来越高。实际上mtcnn的工作原理就是P-Net先做一遍过滤，将过滤后的结果交给R-Net进行过滤，最后再将过滤的结果交给O-Net进行判别。它是层层递进的一个筛查机制。 mtcnn中每个网络都有三部分输出，因此训练时损失的定义也由三部分组成。针对人脸判别face classification，使用交叉熵损失，针对框和回归点的判定，直接使用L2损失。最后这三部分损失各自乘以自身的权重再加起来，形成最后的总损失。P-Net和R-Net网络关心框位置的准确性，O-Net关心关键点判定，它们三部分的各自权重是不一样的。 mtcnn网络需要大量的人脸数据进行训练，才能得到合适的网络参数，达到较好的检测效果。在github上有已经训练好的mtcnn模型可以直接使用，点这里可以直达。其中的src/align/detect_face.py与det1.npy，det2.npy，det3.npy就是mtcnn网络的结构与训练模型，可以直接使用。 测试mtcnn的部分代码： def test(): &nbsp;&nbsp;&nbsp; video = cv2.VideoCapture( 0) &nbsp;&nbsp;&nbsp; print( &#39;Creating networks and loading parameters&#39;) &nbsp;&nbsp;&nbsp; with tf.Graph().as_default(): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction= 1.0) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement= False)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; with sess.as_default(): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pnet, rnet, onet = detect_face.create_mtcnn(sess, None) &nbsp;&nbsp;&nbsp; minsize = 20&nbsp; &nbsp;&nbsp;&nbsp; threshold = [ 0.6, 0.7, 0.7]&nbsp; &nbsp;&nbsp;&nbsp; factor = 0.709 &nbsp;&nbsp;&nbsp; while True: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret, frame = video.read() &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bounding_boxes, _ = detect_face.detect_face(frame, minsize, pnet, rnet, onet, threshold, factor) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nrof_faces = bounding_boxes.shape[ 0] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print( &#39;找到人脸数目为:{}&#39;.format(nrof_faces)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for face_position in bounding_boxes: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; face_position = face_position.astype(int) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cv2.rectangle(frame, (face_position[ 0], face_position[ 1]),(face_position[ 2], face_position[ 3]), ( 0, 255, 0), 2) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cv2.imshow( &#39;show&#39;, frame) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if cv2.waitKey( 5) &amp; 0xFF == ord( &#39;q&#39;): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break &nbsp;&nbsp;&nbsp; video.release() &nbsp;&nbsp;&nbsp; cv2.destroyAllWindows() 1 经过测试，mtcnn的实时性和准确性都非常好。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;" />
<link rel="canonical" href="https://mlh.app/2019/05/17/787056.html" />
<meta property="og:url" content="https://mlh.app/2019/05/17/787056.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-17T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 人脸识别是深度学习最有价值也是最成熟的的应用之一。在研究环境下，人脸识别已经赶上甚至超过了人工识别的精度。一般来说，一个完整的人脸识别项目会包括两大部分：人脸检测与人脸识别。下面就我近期自己练习写的一个“粗糙”的人脸识别小项目讲起，也算是做一个学习记录。 首先 ，整个项目的框架包括四个主要的部分：（1）利用opencv从图像传感器处（比如电脑摄像头）实时的读入视频帧；（2）使用mtcnn网络做人脸检测和对齐；（3）利用facenet网络计算人脸特征，也就是embedding；（4）knn算法进行具体的人脸识别。如下图所示： 其中的mtcnn的人脸检测是很关键的一步，它检测定位的人脸准确与否直接影响到后面的特征计算与识别；facenet实际是一个对人脸进行特征编码的网络，具体的实现后面会讨论；knn的分类算法在用于真正的识别前要经过训练，训练的样本的质量好坏与数量也会对识别的 结果产生很大的影响。今天在这只讨论mtcnn网络的人脸检测对齐部分。 mtcnn网络全称为multi-task convolutinal neural network，意为多任务卷积神经网络。mtcnn由三个神经网络组成，分别是P-Net, R-Net, O-Net。在使用这些网络之前，首先要将原始图片缩放到不同尺寸，形成一个图像金字塔，接着会对每个尺寸的图片通过神经网络计算一遍。其目的在于兼顾图片中的不同大小的人脸，在统一的尺度下检测人脸。 如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击这里可以查看教程。 第一个网络P-Net的结构如下图所示： P-Net的输入是一个12x12的3通道RGB图像，它的作用是要判断这个网络中是否有人脸，并且给出人脸框和关键点位置。第一个部分face classification输出的是判断是人脸的概率和不是人脸的概率，两个值加起来严格等于1；第二个部分输出的是框的精确位置，4个值分别是框的左上角二维坐标和框的高度与宽度；第三个部分输出的是人脸5个关键点：左眼，右眼，鼻子，左嘴角，右嘴角的位置的二维坐标。 第二个网络R-Net的网络结构与P-Net差别不大，如下图： 除了输入大小与中间层大小不同，R-Net的结构与P-Net非常相似，只是在最后的输出层前多加入了一个全连接层。R-Net的输出完全与P-Net一样，同样由人脸判别，框回归，关键点位置预测三部分组成。 第三个网络O-Net结构如下： O-Net相比R-Net在结构上又多出一个中间层，但是输出结果还是一样的。 从P-Net到R-Net，再到O-Net，网络输入的图片越来越大，中间层的通道数越来越多，识别人脸的准确度也越来越高。实际上mtcnn的工作原理就是P-Net先做一遍过滤，将过滤后的结果交给R-Net进行过滤，最后再将过滤的结果交给O-Net进行判别。它是层层递进的一个筛查机制。 mtcnn中每个网络都有三部分输出，因此训练时损失的定义也由三部分组成。针对人脸判别face classification，使用交叉熵损失，针对框和回归点的判定，直接使用L2损失。最后这三部分损失各自乘以自身的权重再加起来，形成最后的总损失。P-Net和R-Net网络关心框位置的准确性，O-Net关心关键点判定，它们三部分的各自权重是不一样的。 mtcnn网络需要大量的人脸数据进行训练，才能得到合适的网络参数，达到较好的检测效果。在github上有已经训练好的mtcnn模型可以直接使用，点这里可以直达。其中的src/align/detect_face.py与det1.npy，det2.npy，det3.npy就是mtcnn网络的结构与训练模型，可以直接使用。 测试mtcnn的部分代码： def test(): &nbsp;&nbsp;&nbsp; video = cv2.VideoCapture( 0) &nbsp;&nbsp;&nbsp; print( &#39;Creating networks and loading parameters&#39;) &nbsp;&nbsp;&nbsp; with tf.Graph().as_default(): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction= 1.0) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement= False)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; with sess.as_default(): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pnet, rnet, onet = detect_face.create_mtcnn(sess, None) &nbsp;&nbsp;&nbsp; minsize = 20&nbsp; &nbsp;&nbsp;&nbsp; threshold = [ 0.6, 0.7, 0.7]&nbsp; &nbsp;&nbsp;&nbsp; factor = 0.709 &nbsp;&nbsp;&nbsp; while True: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret, frame = video.read() &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bounding_boxes, _ = detect_face.detect_face(frame, minsize, pnet, rnet, onet, threshold, factor) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nrof_faces = bounding_boxes.shape[ 0] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print( &#39;找到人脸数目为:{}&#39;.format(nrof_faces)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for face_position in bounding_boxes: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; face_position = face_position.astype(int) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cv2.rectangle(frame, (face_position[ 0], face_position[ 1]),(face_position[ 2], face_position[ 3]), ( 0, 255, 0), 2) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cv2.imshow( &#39;show&#39;, frame) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; if cv2.waitKey( 5) &amp; 0xFF == ord( &#39;q&#39;): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; break &nbsp;&nbsp;&nbsp; video.release() &nbsp;&nbsp;&nbsp; cv2.destroyAllWindows() 1 经过测试，mtcnn的实时性和准确性都非常好。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;","@type":"BlogPosting","url":"https://mlh.app/2019/05/17/787056.html","headline":"TensorFlow框架做实时人脸识别小项目","dateModified":"2019-05-17T00:00:00+08:00","datePublished":"2019-05-17T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/17/787056.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>TensorFlow框架做实时人脸识别小项目</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <div class="markdown_views prism-tomorrow-night" id="content_views">
   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
   <!-- flowchart &#31661;&#22836;&#22270;&#26631; &#21247;&#21024; -->&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
   <div class="htmledit_views" id="content_views">
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
    <p>人脸识别是深度学习最有价值也是最成熟的的应用之一。<span>在研究环境下，人脸识别已经赶上甚至超过了人工识别的精度。一般来说，一个完整的人脸识别项目会包括两大部分：人脸检测与人脸识别。下面就我近期自己练习写的一个“粗糙”的人脸识别小项目讲起，也算是做一个学习记录。</span></p>
    <p><span>首先 ，整个项目的框架包括四个主要的部分：（1）利用opencv从图像传感器处（<span>比如电脑摄像头</span>）实时的读入视频帧；（2）使用mtcnn网络做人脸检测和对齐；（3）利用facenet网络计算人脸特征，也就是embedding；（4）knn算法进行具体的人脸识别。如下图所示：</span></p>
    <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180505185647191"></span></p>
    <p><span>其中的mtcnn的人脸检测是很关键的一步，它检测定位的人脸准确与否直接影响到后面的特征计算与识别；facenet实际是一个对人脸进行特征编码的网络，具体的实现后面会讨论；knn的分类算法在用于真正的识别前要经过训练，训练的样本的质量好坏与数量也会对识别的 结果产生很大的影响。今天在这只讨论mtcnn网络的人脸检测对齐部分。<br></span></p>
   </div>
   <p><span>mtcnn网络全称为multi-task convolutinal neural network，意为多任务卷积神经网络。mtcnn由三个神经网络组成，分别是P-Net, R-Net, O-Net。在使用这些网络之前，首先要将原始图片缩放到不同尺寸，形成一个图像金字塔，接着会对每个尺寸的图片通过神经网络计算一遍。其目的在于兼顾图片中的不同大小的人脸，在统一的尺度下检测人脸。</span></p>
   <p>如果你觉得这篇文章看起来稍微还有些吃力，或者想要系统地学习人工智能，那么推荐你去看床长人工智能教程。非常棒的大神之作，教程不仅通俗易懂，而且很风趣幽默。点击<a href="http://www.captainbed.net/csdn" rel="nofollow" target="_blank">这里</a>可以查看教程。</p>
   <p><span>第一个网络P-Net的结构如下图所示：</span></p>
   <p><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180505194230861"><br></span></p>
   <p><span><span>P-Net的输入是一个12x12的3通道RGB图像，它的作用是要判断这个网络中是否有人脸，并且给出人脸框和关键点位置。第一个部分face classification输出的是判断是人脸的概率和不是人脸的概率，两个值加起来严格等于1；第二个部分输出的是框的精确位置，4个值分别是框的左上角二维坐标和框的高度与宽度；第三个部分输出的是人脸5个关键点：左眼，右眼，鼻子，左嘴角，右嘴角的位置的二维坐标。</span></span></p>
   <p><span><span>第二个网络R-Net的网络结构与P-Net差别不大，如下图：</span></span></p>
   <p><span><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180505201723716"><br></span></span></p>
   <p><span><span>除了输入大小与中间层大小不同，R-Net的结构与P-Net非常相似，只是在最后的输出层前多加入了一个全连接层。R-Net的输出完全与P-Net一样，同样由人脸判别，框回归，关键点位置预测三部分组成。</span></span></p>
   <p><span><span>第三个网络O-Net结构如下：</span></span></p>
   <p><span><span><img alt="" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180505213952865"><br></span></span></p>
   <p><span><span>O-Net相比R-Net在结构上又多出一个中间层，但是输出结果还是一样的。</span></span></p>
   <p><span><span>从P-Net到R-Net，再到O-Net，网络输入的图片越来越大，中间层的通道数越来越多，识别人脸的准确度也越来越高。实际上mtcnn的工作原理就是P-Net先做一遍过滤，将过滤后的结果交给R-Net进行过滤，最后再将过滤的结果交给O-Net进行判别。它是层层递进的一个筛查机制。</span></span></p>
   <p><span><span>mtcnn中每个网络都有三部分输出，因此训练时损失的定义也由三部分组成。针对人脸判别face classification，使用交叉熵损失，针对框和回归点的判定，直接使用L2损失。最后这三部分损失各自乘以自身的权重再加起来，形成最后的总损失。P-Net和R-Net网络关心框位置的准确性，O-Net关心关键点判定，它们三部分的各自权重是不一样的。</span></span></p>
   <p><span><span><span>mtcnn网络需要大量的人脸数据进行训练，才能得到合适的网络参数，达到较好的检测效果。在github上有已经训练好的mtcnn模型可以直接使用，</span><a href="https://github.com/davidsandberg/facenet" rel="nofollow" target="_blank"><span>点这里可以直达</span></a><span>。其中的src/align/detect_face.py与det1.npy，det2.npy，det3.npy就是mtcnn网络的结构与训练模型，可以直接使用。</span></span></span></p>
   <p><span><span>测试mtcnn的部分代码：</span></span></p>
   <pre class="prettyprint"><code class="language-python hljs has-numbering"></code>
    <ol class="hljs-ln">
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">()</span>:</span>
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp; video = cv2.VideoCapture(
        <span class="hljs-number">0</span>)
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line"> 
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp; print(
        <span class="hljs-string">'Creating networks and loading parameters'</span>)
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line"> 
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp; 
        <span class="hljs-keyword">with</span> tf.Graph().as_default():
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=
        <span class="hljs-number">1.0</span>)
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=
        <span class="hljs-keyword">False</span>))
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
        <span class="hljs-keyword">with</span> sess.as_default():
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pnet, rnet, onet = detect_face.create_mtcnn(sess, 
        <span class="hljs-keyword">None</span>)
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp; minsize = 
        <span class="hljs-number">20</span>&nbsp; 
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp; threshold = [
        <span class="hljs-number">0.6</span>, 
        <span class="hljs-number">0.7</span>, 
        <span class="hljs-number">0.7</span>]&nbsp; 
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp; factor = 
        <span class="hljs-number">0.709</span> 
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp; 
        <span class="hljs-keyword">while</span> 
        <span class="hljs-keyword">True</span>:
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ret, frame = video.read()
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; bounding_boxes, _ = detect_face.detect_face(frame, minsize, pnet, rnet, onet, threshold, factor)
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nrof_faces = bounding_boxes.shape[
        <span class="hljs-number">0</span>]
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; print(
        <span class="hljs-string">'找到人脸数目为:{}'</span>.format(nrof_faces))
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
        <span class="hljs-keyword">for</span> face_position 
        <span class="hljs-keyword">in</span> bounding_boxes:
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; face_position = face_position.astype(int)
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cv2.rectangle(frame, (face_position[
        <span class="hljs-number">0</span>], face_position[
        <span class="hljs-number">1</span>]),(face_position[
        <span class="hljs-number">2</span>], face_position[
        <span class="hljs-number">3</span>]), (
        <span class="hljs-number">0</span>, 
        <span class="hljs-number">255</span>, 
        <span class="hljs-number">0</span>), 
        <span class="hljs-number">2</span>)
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cv2.imshow(
        <span class="hljs-string">'show'</span>, frame)
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
        <span class="hljs-keyword">if</span> cv2.waitKey(
        <span class="hljs-number">5</span>) &amp; 
        <span class="hljs-number">0xFF</span> == ord(
        <span class="hljs-string">'q'</span>):
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
        <span class="hljs-keyword">break</span>
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp; video.release()
       </div>
      </div></li>
     <li>
      <div class="hljs-ln-numbers">
       <div class="hljs-ln-line hljs-ln-n"></div>
      </div>
      <div class="hljs-ln-code">
       <div class="hljs-ln-line">
        &nbsp;&nbsp;&nbsp; cv2.destroyAllWindows()
       </div>
      </div></li>
    </ol>
    <div class="hljs-button {2}"></div>
    <ul class="pre-numbering">
     <li>1</li>
    </ul></pre>
   <p>经过测试，mtcnn的实时性和准确性都非常好。</p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
  </div> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
