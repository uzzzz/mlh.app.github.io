<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>大数据学习笔记之Hive（六）：Hive综合练习 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="大数据学习笔记之Hive（六）：Hive综合练习" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="文章目录 Hive综合练习： 1、上传文件到HDFS 1.1 文件/目录准备工作 2、清洗日志 2.1 编写清洗日志的MR代码： 2.2 导出Jar包 3、执行创建表操作 3.1 创建track_log表，在syllabus.db目录下 4、关联表操作 4.1 4.2 5、分析数据 5.1、 5.2、 6、导出到Mysql 6.1、 6.2、 auto.sh linux补充 Hive综合练习： 1、写一个自动化脚本，自动拷贝/home/admin/WebLog/目录（服务器产生日志的目录）下的两个日志文件到指定目录/opt/modules/weblog。 2、crontab -e定时执行日志文件的拷贝迁移。 3、我们把日志上传到HDFS，删除该目录下的文件（有可能是只保存7天以内的日志） 4、数据清洗 5、数据分析，产生最终的结果表 6、将结果导入到Mysql中（就1条数据） 文档： 需求分析 日期 UV PV 登录人数 游客人数 平均访问时长 二跳率 独立IP 20150828 38985 131668.0 18548 21902 750.7895179233622 0.5166089521610876 29668 逻辑图 数据字典 解决方案： 预定义变量： HIVE_DIR=/opt/modules/cdh/hive-0.13.1-cdh5.3.6 HADOOP_DIR=/opt/modules/cdh/hadoop-2.5.0-cdh5.3.6 HQL_DIR=/opt/modules/cdh/clean（SHELL脚本和HIVE脚本的存放路径） 1、上传文件到HDFS 1.1 文件/目录准备工作 bin/hdfs dfs -put /日志在Linux本地存放的绝对路径 /HDFS路径 例如：bin/hdfs dfs -put /opt/modules/weblog/20170725/2015082818 /weblog/ 脚本： YESTERDAY=$(date --date=&quot;1 day ago&quot; +%Y%m%d) 创建目录： $HADOOP_DIR/bin/hdfs dfs -mkdir /weblog/$YESTERDAY(以上两步不要放在循环中执行) 循环上传日志文件： $HADOOP_DIR/bin/hdfs dfs -put /opt/modules/weblog/$YESTERDAY/$i 2、清洗日志 2.1 编写清洗日志的MR代码： bin/yarn jar /home/admin/Desktop/cleanlog.jar com.z.etl.CleanLogMapReduce /weblog/20170725/2015082818 /user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18 *** Tools.java：执行删除本次任务中可能涉及到的输出路径，代码如下： public static void deleteFileInHDFS(String directory, String exist) throws IOException{ // 获取文件系统管理对象 FileSystem fileSystem = FileSystem.get(URI.create(directory), conf); FileStatus[] fileList = fileSystem.listStatus(new Path(directory)); for(int i = 0; i &lt; fileList.length; i++){ FileStatus fileStatus = fileList[i]; if(fileStatus.getPath().getName().startsWith(exist)){ fileSystem.delete(fileStatus.getPath(), true); } } } *** 截取字符串： ///user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18 Tools.deleteFileInHDFS(args[1].substring(0, 57), args[1].substring(57)); 代码：见清洗代码包 2.2 导出Jar包 注意： 1如果在导包时指定了MainClass，则调用： bin/yarn jar xxxxx.jar /input/ /output/ 2如果在导包时没有指定MainClass，则调用： bin/yarn jar xxxxx.jar MainClass /input/ /output/ JAR包的路径是：/home/admin/Desktop/clearlog.jar 先测试： $ /opt/modules/cdh/hadoop-2.5.0-cdh5.3.6/bin/yarn jar \ /home/admin/Desktop/clearlog.jar com.z.hive.etl.LogCleanMapReduce \ /weblog/20170725/2015082818 \ /user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18 ///weblog/20170725/2015082818 输入路径 ///user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18输出路径 //com.z.hive.etl.LogCleanMapReduce 主函数的名字，包含包名，LogCleanMapReduce是包含main的类名 //date=20150828和hour=18是文件的名称，只不过教程中问加你的名称取的比较特殊 尖叫提示：要测试两遍，在没有目录的时候测试一次，在有目录的时候再测试一次。 3、执行创建表操作 3.1 创建track_log表，在syllabus.db目录下 建表语句： create database if not exists syllabus; create table if not exists syllabus.track_log( id string, url string, referer string, keyword string, type string, guid string, pageId string, moduleId string, linkId string, attachedInfo string, sessionId string, trackerU string, trackerType string, ip string, trackerSrc string, cookie string, orderCode string, trackTime string, endUserId string, firstLink string, sessionViewNo string, productId string, curMerchantId string, provinceId string, cityId string, fee string, edmActivity string, edmEmail string, edmJobId string, ieVersion string, platform string, internalKeyword string, resultSum string, currentPage string, linkPosition string, buttonPosition string ) partitioned by (date string,hour string) row format delimited fields terminated by &#39;\t&#39;; 将如上代码保存在create_table_track_log.hql文件中，执行单个测试，命令如下： $ /opt/modules/cdh/hive-0.13.1-cdh5.3.6/bin/hive -f /opt/modules/cdh/clean/create_table_track_log.hql 4、关联表操作 4.1 如下代码有BUG，Hive-0.14以及Hive-0.15版本已经修复该BUG 尖叫提示：解决方案，先use表所在的库，然后再alter，而且在alter的时候，表名前边不能有库名。 alter table track_log add partition(date=‘20150828’, hour=‘19’) location “/user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18”; 另一种方案： load data inpath &quot;/user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18/part-r-00000&quot; into table syllabus.track_log partition(date=&#39;20150828&#39;, hour=&#39;18&#39;); alter 关联代码： //在执行alter语句之前，输入use syllabus语句，就不会报错了 use syllabus;alter table track_log add partition(date=&#39;${hiveconf:DATE_NEW}&#39;, hour=&#39;${hiveconf:HOUR_NEW}&#39;) location &quot;${hiveconf:LOCATION_NEW}&quot;; 4.2 该功能对应shell脚本如下： echo &quot;======关联清洗后的数据==&quot; $HIVE_DIR/bin/hive \ --hiveconf LOCATION_NEW=/user/hive/warehouse/syllabus.db/track_log/date=$DATE/hour=$HOUR \ --hiveconf DATE_NEW=$DATE \ --hiveconf HOUR_NEW=$HOUR \ -f $HQL_DIR/alter_table_track_log.hql 测试查询数据： hive&gt; select * from track_log limit 1; 5、分析数据 5.1、 创建一系列分析语句 -rw-rw-r-- 1 admin admin 209 Jul 26 16:15 create_result_info.hql create table syllabus.result_info( date string, uv string, pv string, login_users string, visit_users string, avg_time string, sec_hop string, ip_count string ) -rw-rw-r-- 1 admin admin 319 Jul 26 16:12 create_session_info.hql create table if not exists syllabus.session_info( session_id string, guid string, tracker_u string, landing_url string, landing_url_ref string, user_id string, pv string, stay_time string, min_tracktime string, ip string, province_id string) partitioned by (date string) row format delimited fields terminated by &#39;\t&#39;; -rw-rw-r-- 1 admin admin 235 Jul 26 16:13 create_session_info_temp1.hql insert overwrite table syllabus.session_info_temp1 select sessionId, max(guid), max(endUserId), count(url), max(unix_timestamp(trackTime)) - min(unix_timestamp(trackTime)), from_unixtime(min(unix_timestamp(trackTime))), max(ip), max(provinceId) from syllabus.track_log where date=&#39;20150828&#39; group by sessionId; -rw-rw-r-- 1 admin admin 190 Jul 26 16:14 create_session_info_temp2.hql create table syllabus.session_info_temp2( session_id string, tracktime string, tracker_u string, landing_url string, landing_url_ref string ) row format delimited fields terminated by &#39;\t&#39;; -rw-rw-r-- 1 admin admin 388 Jul 26 16:15 insert_result_info.hql insert overwrite table syllabus.result_info select date, count(distinct guid), sum(pv), count(case when user_id != &#39;&#39; then user_id else null end), count(case when user_id = &#39;&#39; then user_id else null end), avg(stay_time), count(distinct (case when pv &gt;= 2 then guid else null end))/count(distinct guid), count(distinct ip) from syllabus.session_info where date=&#39;20170725&#39; group by date; -rw-rw-r-- 1 admin admin 368 Jul 26 16:14 insert_session_info.hql insert overwrite table syllabus.session_info partition(date=&#39;20150828&#39;) select p1.session_id, p1.guid, p2.tracker_u, p2.landing_url, p2.landing_url_ref, p1.user_id, p1.pv, p1.stay_time, p1.min_tracktime, p1.ip, p1.province_id from syllabus.session_info_temp1 p1 join syllabus.session_info_temp2 p2 on p1.session_id=p2.session_id and p1.min_tracktime=p2.tracktime; -rw-rw-r-- 1 admin admin 314 Jul 26 16:13 insert_session_info_temp1.hql create table if not exists syllabus.session_info_temp1( session_id string, guid string, user_id string, pv string, stay_time string, min_tracktime string, ip string, province_id string ) row format delimited fields terminated by &#39;\t&#39;; -rw-rw-r-- 1 admin admin 153 Jul 26 16:14 insert_session_info_temp2.hql insert overwrite table syllabus.session_info_temp2 select sessionId, trackTime, trackerU, url, referer from syllabus.track_log where date=&#39;20150828&#39;; 5.2、 编写脚本依次执行 echo &quot;==开始分析网站流量数据==&quot; echo &quot;==检查所有表结构========&quot; $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp1.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp2.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_result_info.hql echo &quot;==开始插入数据==========&quot; $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info_temp1.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info_temp2.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_result_info.hql 6、导出到Mysql 6.1、 创建sqoop可执行的opt文件 $ vi /opt/modules/cdh/clean/weblog_hive_2_mysql.opt 内容如下： export --connect jdbc:mysql://hadoop-senior01.itguigu.com:3306/syllabus_weblog --username root --password 123456 --table result_web_log --num-mappers 1 --export-dir /user/hive/warehouse/syllabus.db/result_info --input-fields-terminated-by &quot;\t&quot; 6.2、 进入Mysql创建对应数据库以及数据表 auto.sh 完整脚本 #!/bin/bash #执行系统环境变量脚本，初始化一些变量信息 . /etc/profile #定义Hive目录 HIVE_DIR=/opt/modules/cdh/hive-0.13.1-cdh5.3.6 HADOOP_DIR=/opt/modules/cdh/hadoop-2.5.0-cdh5.3.6 HQL_DIR=/opt/modules/cdh/clean SQOOP_DIR=/opt/modules/cdh/sqoop-1.4.5-cdh5.3.6 echo $HIVE_DIR echo $HADOOP_DIR #定义日志的存储路径 WEB_LOG=/opt/modules/weblog #昨天的日期，用于访问目录 YESTERDAY=$(date --date=&quot;1 day ago&quot; +%Y%m%d) #在HDFS上创建指定文件夹 echo &quot;======正在创建目录======&quot; #/weblog/20170725 $HADOOP_DIR/bin/hdfs dfs -mkdir /weblog/$YESTERDAY echo &quot;======正在检查数据表====&quot; $HIVE_DIR/bin/hive -f $HQL_DIR/create_table_track_log.hql #遍历目录 for i in `ls $WEB_LOG/$YESTERDAY` do #20150828 DATE=${i:0:8} #18 HOUR=${i:8:2} #上传文件到HDFS echo &quot;======正在上传日志======&quot; #bin/hdfs /opt/modules/weblog/2015082818 /weblog/20170726 $HADOOP_DIR/bin/hdfs dfs -put $WEB_LOG/$YESTERDAY/$i /weblog/$YESTERDAY #清洗日志 echo &quot;======开始清洗==========&quot; # bin/yarn jar /home/admin/Desktop/clearlog.jar com.z.hive.etl.LogCleanMapReduce /weblog/20170726/2015082818 /user/hive/warehouse/syllabus.db/track_log/date=20170725/hour=18 $HADOOP_DIR/bin/yarn jar /home/admin/Desktop/clearlog.jar com.z.hive.etl.LogCleanMapReduce /weblog/$YESTERDAY/$i /user/hive/warehouse/syllabus.db/track_log/date=$DATE/hour=$HOUR echo &quot;======关联清洗后的数据==&quot; $HIVE_DIR/bin/hive \ --hiveconf LOCATION_NEW=/user/hive/warehouse/syllabus.db/track_log/date=$DATE/hour=$HOUR \ --hiveconf DATE_NEW=$DATE \ --hiveconf HOUR_NEW=$HOUR \ -f $HQL_DIR/alter_table_track_log.hql done echo &quot;==开始分析网站流量数据==&quot; echo &quot;==检查所有表结构========&quot; $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp1.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp2.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_result_info.hql echo &quot;==开始插入数据==========&quot; $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info_temp1.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info_temp2.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_result_info.hql echo &quot;==开始导出数据到Mysql==&quot; $SQOOP_DIR/bin/sqoop --options-file $HQL_DIR/weblog_hive_2_mysql.opt echo &quot;==任务完成=============&quot; linux补充 if [ file -r ] -eq 等于则为真 -ne 不等于则为真 -gt 大于则为真 -ge 大于等于则为真 -lt 小于则为真 -le 小于等于则为真 字串测试 = 等于则为真 != 不相等则为真 -z字串 字串长度伪则为真 -n字串 字串长度不伪则为真 文件测试 -e 文件名 如果文件存在则为真 -r 文件名 果文件存在且可读则为真 -w 文件名 如果文件存在且可写则为真 -x 文件名 如果文件存在且可执行则为真 文件测试 -s 文件名 如果文件存在且至少有一个字符则为真 -d 文件名 如果文件存在且为目录则为真 -f 文件名 如果文件存在且为普通文件则为真 -c 文件名 如果文件存在且为字符型特殊文件则为真 -b 文件名 如果文件存在且为块特殊文件则为真 Linux还提供了非（！）、或（-o）、与（-a）三个逻辑操作符，用于将测试条件连接起来，其优先顺序为：！最高，-a次之，-o最低" />
<meta property="og:description" content="文章目录 Hive综合练习： 1、上传文件到HDFS 1.1 文件/目录准备工作 2、清洗日志 2.1 编写清洗日志的MR代码： 2.2 导出Jar包 3、执行创建表操作 3.1 创建track_log表，在syllabus.db目录下 4、关联表操作 4.1 4.2 5、分析数据 5.1、 5.2、 6、导出到Mysql 6.1、 6.2、 auto.sh linux补充 Hive综合练习： 1、写一个自动化脚本，自动拷贝/home/admin/WebLog/目录（服务器产生日志的目录）下的两个日志文件到指定目录/opt/modules/weblog。 2、crontab -e定时执行日志文件的拷贝迁移。 3、我们把日志上传到HDFS，删除该目录下的文件（有可能是只保存7天以内的日志） 4、数据清洗 5、数据分析，产生最终的结果表 6、将结果导入到Mysql中（就1条数据） 文档： 需求分析 日期 UV PV 登录人数 游客人数 平均访问时长 二跳率 独立IP 20150828 38985 131668.0 18548 21902 750.7895179233622 0.5166089521610876 29668 逻辑图 数据字典 解决方案： 预定义变量： HIVE_DIR=/opt/modules/cdh/hive-0.13.1-cdh5.3.6 HADOOP_DIR=/opt/modules/cdh/hadoop-2.5.0-cdh5.3.6 HQL_DIR=/opt/modules/cdh/clean（SHELL脚本和HIVE脚本的存放路径） 1、上传文件到HDFS 1.1 文件/目录准备工作 bin/hdfs dfs -put /日志在Linux本地存放的绝对路径 /HDFS路径 例如：bin/hdfs dfs -put /opt/modules/weblog/20170725/2015082818 /weblog/ 脚本： YESTERDAY=$(date --date=&quot;1 day ago&quot; +%Y%m%d) 创建目录： $HADOOP_DIR/bin/hdfs dfs -mkdir /weblog/$YESTERDAY(以上两步不要放在循环中执行) 循环上传日志文件： $HADOOP_DIR/bin/hdfs dfs -put /opt/modules/weblog/$YESTERDAY/$i 2、清洗日志 2.1 编写清洗日志的MR代码： bin/yarn jar /home/admin/Desktop/cleanlog.jar com.z.etl.CleanLogMapReduce /weblog/20170725/2015082818 /user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18 *** Tools.java：执行删除本次任务中可能涉及到的输出路径，代码如下： public static void deleteFileInHDFS(String directory, String exist) throws IOException{ // 获取文件系统管理对象 FileSystem fileSystem = FileSystem.get(URI.create(directory), conf); FileStatus[] fileList = fileSystem.listStatus(new Path(directory)); for(int i = 0; i &lt; fileList.length; i++){ FileStatus fileStatus = fileList[i]; if(fileStatus.getPath().getName().startsWith(exist)){ fileSystem.delete(fileStatus.getPath(), true); } } } *** 截取字符串： ///user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18 Tools.deleteFileInHDFS(args[1].substring(0, 57), args[1].substring(57)); 代码：见清洗代码包 2.2 导出Jar包 注意： 1如果在导包时指定了MainClass，则调用： bin/yarn jar xxxxx.jar /input/ /output/ 2如果在导包时没有指定MainClass，则调用： bin/yarn jar xxxxx.jar MainClass /input/ /output/ JAR包的路径是：/home/admin/Desktop/clearlog.jar 先测试： $ /opt/modules/cdh/hadoop-2.5.0-cdh5.3.6/bin/yarn jar \ /home/admin/Desktop/clearlog.jar com.z.hive.etl.LogCleanMapReduce \ /weblog/20170725/2015082818 \ /user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18 ///weblog/20170725/2015082818 输入路径 ///user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18输出路径 //com.z.hive.etl.LogCleanMapReduce 主函数的名字，包含包名，LogCleanMapReduce是包含main的类名 //date=20150828和hour=18是文件的名称，只不过教程中问加你的名称取的比较特殊 尖叫提示：要测试两遍，在没有目录的时候测试一次，在有目录的时候再测试一次。 3、执行创建表操作 3.1 创建track_log表，在syllabus.db目录下 建表语句： create database if not exists syllabus; create table if not exists syllabus.track_log( id string, url string, referer string, keyword string, type string, guid string, pageId string, moduleId string, linkId string, attachedInfo string, sessionId string, trackerU string, trackerType string, ip string, trackerSrc string, cookie string, orderCode string, trackTime string, endUserId string, firstLink string, sessionViewNo string, productId string, curMerchantId string, provinceId string, cityId string, fee string, edmActivity string, edmEmail string, edmJobId string, ieVersion string, platform string, internalKeyword string, resultSum string, currentPage string, linkPosition string, buttonPosition string ) partitioned by (date string,hour string) row format delimited fields terminated by &#39;\t&#39;; 将如上代码保存在create_table_track_log.hql文件中，执行单个测试，命令如下： $ /opt/modules/cdh/hive-0.13.1-cdh5.3.6/bin/hive -f /opt/modules/cdh/clean/create_table_track_log.hql 4、关联表操作 4.1 如下代码有BUG，Hive-0.14以及Hive-0.15版本已经修复该BUG 尖叫提示：解决方案，先use表所在的库，然后再alter，而且在alter的时候，表名前边不能有库名。 alter table track_log add partition(date=‘20150828’, hour=‘19’) location “/user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18”; 另一种方案： load data inpath &quot;/user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18/part-r-00000&quot; into table syllabus.track_log partition(date=&#39;20150828&#39;, hour=&#39;18&#39;); alter 关联代码： //在执行alter语句之前，输入use syllabus语句，就不会报错了 use syllabus;alter table track_log add partition(date=&#39;${hiveconf:DATE_NEW}&#39;, hour=&#39;${hiveconf:HOUR_NEW}&#39;) location &quot;${hiveconf:LOCATION_NEW}&quot;; 4.2 该功能对应shell脚本如下： echo &quot;======关联清洗后的数据==&quot; $HIVE_DIR/bin/hive \ --hiveconf LOCATION_NEW=/user/hive/warehouse/syllabus.db/track_log/date=$DATE/hour=$HOUR \ --hiveconf DATE_NEW=$DATE \ --hiveconf HOUR_NEW=$HOUR \ -f $HQL_DIR/alter_table_track_log.hql 测试查询数据： hive&gt; select * from track_log limit 1; 5、分析数据 5.1、 创建一系列分析语句 -rw-rw-r-- 1 admin admin 209 Jul 26 16:15 create_result_info.hql create table syllabus.result_info( date string, uv string, pv string, login_users string, visit_users string, avg_time string, sec_hop string, ip_count string ) -rw-rw-r-- 1 admin admin 319 Jul 26 16:12 create_session_info.hql create table if not exists syllabus.session_info( session_id string, guid string, tracker_u string, landing_url string, landing_url_ref string, user_id string, pv string, stay_time string, min_tracktime string, ip string, province_id string) partitioned by (date string) row format delimited fields terminated by &#39;\t&#39;; -rw-rw-r-- 1 admin admin 235 Jul 26 16:13 create_session_info_temp1.hql insert overwrite table syllabus.session_info_temp1 select sessionId, max(guid), max(endUserId), count(url), max(unix_timestamp(trackTime)) - min(unix_timestamp(trackTime)), from_unixtime(min(unix_timestamp(trackTime))), max(ip), max(provinceId) from syllabus.track_log where date=&#39;20150828&#39; group by sessionId; -rw-rw-r-- 1 admin admin 190 Jul 26 16:14 create_session_info_temp2.hql create table syllabus.session_info_temp2( session_id string, tracktime string, tracker_u string, landing_url string, landing_url_ref string ) row format delimited fields terminated by &#39;\t&#39;; -rw-rw-r-- 1 admin admin 388 Jul 26 16:15 insert_result_info.hql insert overwrite table syllabus.result_info select date, count(distinct guid), sum(pv), count(case when user_id != &#39;&#39; then user_id else null end), count(case when user_id = &#39;&#39; then user_id else null end), avg(stay_time), count(distinct (case when pv &gt;= 2 then guid else null end))/count(distinct guid), count(distinct ip) from syllabus.session_info where date=&#39;20170725&#39; group by date; -rw-rw-r-- 1 admin admin 368 Jul 26 16:14 insert_session_info.hql insert overwrite table syllabus.session_info partition(date=&#39;20150828&#39;) select p1.session_id, p1.guid, p2.tracker_u, p2.landing_url, p2.landing_url_ref, p1.user_id, p1.pv, p1.stay_time, p1.min_tracktime, p1.ip, p1.province_id from syllabus.session_info_temp1 p1 join syllabus.session_info_temp2 p2 on p1.session_id=p2.session_id and p1.min_tracktime=p2.tracktime; -rw-rw-r-- 1 admin admin 314 Jul 26 16:13 insert_session_info_temp1.hql create table if not exists syllabus.session_info_temp1( session_id string, guid string, user_id string, pv string, stay_time string, min_tracktime string, ip string, province_id string ) row format delimited fields terminated by &#39;\t&#39;; -rw-rw-r-- 1 admin admin 153 Jul 26 16:14 insert_session_info_temp2.hql insert overwrite table syllabus.session_info_temp2 select sessionId, trackTime, trackerU, url, referer from syllabus.track_log where date=&#39;20150828&#39;; 5.2、 编写脚本依次执行 echo &quot;==开始分析网站流量数据==&quot; echo &quot;==检查所有表结构========&quot; $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp1.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp2.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_result_info.hql echo &quot;==开始插入数据==========&quot; $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info_temp1.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info_temp2.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_result_info.hql 6、导出到Mysql 6.1、 创建sqoop可执行的opt文件 $ vi /opt/modules/cdh/clean/weblog_hive_2_mysql.opt 内容如下： export --connect jdbc:mysql://hadoop-senior01.itguigu.com:3306/syllabus_weblog --username root --password 123456 --table result_web_log --num-mappers 1 --export-dir /user/hive/warehouse/syllabus.db/result_info --input-fields-terminated-by &quot;\t&quot; 6.2、 进入Mysql创建对应数据库以及数据表 auto.sh 完整脚本 #!/bin/bash #执行系统环境变量脚本，初始化一些变量信息 . /etc/profile #定义Hive目录 HIVE_DIR=/opt/modules/cdh/hive-0.13.1-cdh5.3.6 HADOOP_DIR=/opt/modules/cdh/hadoop-2.5.0-cdh5.3.6 HQL_DIR=/opt/modules/cdh/clean SQOOP_DIR=/opt/modules/cdh/sqoop-1.4.5-cdh5.3.6 echo $HIVE_DIR echo $HADOOP_DIR #定义日志的存储路径 WEB_LOG=/opt/modules/weblog #昨天的日期，用于访问目录 YESTERDAY=$(date --date=&quot;1 day ago&quot; +%Y%m%d) #在HDFS上创建指定文件夹 echo &quot;======正在创建目录======&quot; #/weblog/20170725 $HADOOP_DIR/bin/hdfs dfs -mkdir /weblog/$YESTERDAY echo &quot;======正在检查数据表====&quot; $HIVE_DIR/bin/hive -f $HQL_DIR/create_table_track_log.hql #遍历目录 for i in `ls $WEB_LOG/$YESTERDAY` do #20150828 DATE=${i:0:8} #18 HOUR=${i:8:2} #上传文件到HDFS echo &quot;======正在上传日志======&quot; #bin/hdfs /opt/modules/weblog/2015082818 /weblog/20170726 $HADOOP_DIR/bin/hdfs dfs -put $WEB_LOG/$YESTERDAY/$i /weblog/$YESTERDAY #清洗日志 echo &quot;======开始清洗==========&quot; # bin/yarn jar /home/admin/Desktop/clearlog.jar com.z.hive.etl.LogCleanMapReduce /weblog/20170726/2015082818 /user/hive/warehouse/syllabus.db/track_log/date=20170725/hour=18 $HADOOP_DIR/bin/yarn jar /home/admin/Desktop/clearlog.jar com.z.hive.etl.LogCleanMapReduce /weblog/$YESTERDAY/$i /user/hive/warehouse/syllabus.db/track_log/date=$DATE/hour=$HOUR echo &quot;======关联清洗后的数据==&quot; $HIVE_DIR/bin/hive \ --hiveconf LOCATION_NEW=/user/hive/warehouse/syllabus.db/track_log/date=$DATE/hour=$HOUR \ --hiveconf DATE_NEW=$DATE \ --hiveconf HOUR_NEW=$HOUR \ -f $HQL_DIR/alter_table_track_log.hql done echo &quot;==开始分析网站流量数据==&quot; echo &quot;==检查所有表结构========&quot; $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp1.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp2.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_result_info.hql echo &quot;==开始插入数据==========&quot; $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info_temp1.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info_temp2.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_session_info.hql $HIVE_DIR/bin/hive \ --hiveconf DATE_NEW=$YESTERDAY \ -f $HQL_DIR/insert_result_info.hql echo &quot;==开始导出数据到Mysql==&quot; $SQOOP_DIR/bin/sqoop --options-file $HQL_DIR/weblog_hive_2_mysql.opt echo &quot;==任务完成=============&quot; linux补充 if [ file -r ] -eq 等于则为真 -ne 不等于则为真 -gt 大于则为真 -ge 大于等于则为真 -lt 小于则为真 -le 小于等于则为真 字串测试 = 等于则为真 != 不相等则为真 -z字串 字串长度伪则为真 -n字串 字串长度不伪则为真 文件测试 -e 文件名 如果文件存在则为真 -r 文件名 果文件存在且可读则为真 -w 文件名 如果文件存在且可写则为真 -x 文件名 如果文件存在且可执行则为真 文件测试 -s 文件名 如果文件存在且至少有一个字符则为真 -d 文件名 如果文件存在且为目录则为真 -f 文件名 如果文件存在且为普通文件则为真 -c 文件名 如果文件存在且为字符型特殊文件则为真 -b 文件名 如果文件存在且为块特殊文件则为真 Linux还提供了非（！）、或（-o）、与（-a）三个逻辑操作符，用于将测试条件连接起来，其优先顺序为：！最高，-a次之，-o最低" />
<link rel="canonical" href="https://mlh.app/2019/05/17/787734.html" />
<meta property="og:url" content="https://mlh.app/2019/05/17/787734.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-17T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"文章目录 Hive综合练习： 1、上传文件到HDFS 1.1 文件/目录准备工作 2、清洗日志 2.1 编写清洗日志的MR代码： 2.2 导出Jar包 3、执行创建表操作 3.1 创建track_log表，在syllabus.db目录下 4、关联表操作 4.1 4.2 5、分析数据 5.1、 5.2、 6、导出到Mysql 6.1、 6.2、 auto.sh linux补充 Hive综合练习： 1、写一个自动化脚本，自动拷贝/home/admin/WebLog/目录（服务器产生日志的目录）下的两个日志文件到指定目录/opt/modules/weblog。 2、crontab -e定时执行日志文件的拷贝迁移。 3、我们把日志上传到HDFS，删除该目录下的文件（有可能是只保存7天以内的日志） 4、数据清洗 5、数据分析，产生最终的结果表 6、将结果导入到Mysql中（就1条数据） 文档： 需求分析 日期 UV PV 登录人数 游客人数 平均访问时长 二跳率 独立IP 20150828 38985 131668.0 18548 21902 750.7895179233622 0.5166089521610876 29668 逻辑图 数据字典 解决方案： 预定义变量： HIVE_DIR=/opt/modules/cdh/hive-0.13.1-cdh5.3.6 HADOOP_DIR=/opt/modules/cdh/hadoop-2.5.0-cdh5.3.6 HQL_DIR=/opt/modules/cdh/clean（SHELL脚本和HIVE脚本的存放路径） 1、上传文件到HDFS 1.1 文件/目录准备工作 bin/hdfs dfs -put /日志在Linux本地存放的绝对路径 /HDFS路径 例如：bin/hdfs dfs -put /opt/modules/weblog/20170725/2015082818 /weblog/ 脚本： YESTERDAY=$(date --date=&quot;1 day ago&quot; +%Y%m%d) 创建目录： $HADOOP_DIR/bin/hdfs dfs -mkdir /weblog/$YESTERDAY(以上两步不要放在循环中执行) 循环上传日志文件： $HADOOP_DIR/bin/hdfs dfs -put /opt/modules/weblog/$YESTERDAY/$i 2、清洗日志 2.1 编写清洗日志的MR代码： bin/yarn jar /home/admin/Desktop/cleanlog.jar com.z.etl.CleanLogMapReduce /weblog/20170725/2015082818 /user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18 *** Tools.java：执行删除本次任务中可能涉及到的输出路径，代码如下： public static void deleteFileInHDFS(String directory, String exist) throws IOException{ // 获取文件系统管理对象 FileSystem fileSystem = FileSystem.get(URI.create(directory), conf); FileStatus[] fileList = fileSystem.listStatus(new Path(directory)); for(int i = 0; i &lt; fileList.length; i++){ FileStatus fileStatus = fileList[i]; if(fileStatus.getPath().getName().startsWith(exist)){ fileSystem.delete(fileStatus.getPath(), true); } } } *** 截取字符串： ///user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18 Tools.deleteFileInHDFS(args[1].substring(0, 57), args[1].substring(57)); 代码：见清洗代码包 2.2 导出Jar包 注意： 1如果在导包时指定了MainClass，则调用： bin/yarn jar xxxxx.jar /input/ /output/ 2如果在导包时没有指定MainClass，则调用： bin/yarn jar xxxxx.jar MainClass /input/ /output/ JAR包的路径是：/home/admin/Desktop/clearlog.jar 先测试： $ /opt/modules/cdh/hadoop-2.5.0-cdh5.3.6/bin/yarn jar \\ /home/admin/Desktop/clearlog.jar com.z.hive.etl.LogCleanMapReduce \\ /weblog/20170725/2015082818 \\ /user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18 ///weblog/20170725/2015082818 输入路径 ///user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18输出路径 //com.z.hive.etl.LogCleanMapReduce 主函数的名字，包含包名，LogCleanMapReduce是包含main的类名 //date=20150828和hour=18是文件的名称，只不过教程中问加你的名称取的比较特殊 尖叫提示：要测试两遍，在没有目录的时候测试一次，在有目录的时候再测试一次。 3、执行创建表操作 3.1 创建track_log表，在syllabus.db目录下 建表语句： create database if not exists syllabus; create table if not exists syllabus.track_log( id string, url string, referer string, keyword string, type string, guid string, pageId string, moduleId string, linkId string, attachedInfo string, sessionId string, trackerU string, trackerType string, ip string, trackerSrc string, cookie string, orderCode string, trackTime string, endUserId string, firstLink string, sessionViewNo string, productId string, curMerchantId string, provinceId string, cityId string, fee string, edmActivity string, edmEmail string, edmJobId string, ieVersion string, platform string, internalKeyword string, resultSum string, currentPage string, linkPosition string, buttonPosition string ) partitioned by (date string,hour string) row format delimited fields terminated by &#39;\\t&#39;; 将如上代码保存在create_table_track_log.hql文件中，执行单个测试，命令如下： $ /opt/modules/cdh/hive-0.13.1-cdh5.3.6/bin/hive -f /opt/modules/cdh/clean/create_table_track_log.hql 4、关联表操作 4.1 如下代码有BUG，Hive-0.14以及Hive-0.15版本已经修复该BUG 尖叫提示：解决方案，先use表所在的库，然后再alter，而且在alter的时候，表名前边不能有库名。 alter table track_log add partition(date=‘20150828’, hour=‘19’) location “/user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18”; 另一种方案： load data inpath &quot;/user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18/part-r-00000&quot; into table syllabus.track_log partition(date=&#39;20150828&#39;, hour=&#39;18&#39;); alter 关联代码： //在执行alter语句之前，输入use syllabus语句，就不会报错了 use syllabus;alter table track_log add partition(date=&#39;${hiveconf:DATE_NEW}&#39;, hour=&#39;${hiveconf:HOUR_NEW}&#39;) location &quot;${hiveconf:LOCATION_NEW}&quot;; 4.2 该功能对应shell脚本如下： echo &quot;======关联清洗后的数据==&quot; $HIVE_DIR/bin/hive \\ --hiveconf LOCATION_NEW=/user/hive/warehouse/syllabus.db/track_log/date=$DATE/hour=$HOUR \\ --hiveconf DATE_NEW=$DATE \\ --hiveconf HOUR_NEW=$HOUR \\ -f $HQL_DIR/alter_table_track_log.hql 测试查询数据： hive&gt; select * from track_log limit 1; 5、分析数据 5.1、 创建一系列分析语句 -rw-rw-r-- 1 admin admin 209 Jul 26 16:15 create_result_info.hql create table syllabus.result_info( date string, uv string, pv string, login_users string, visit_users string, avg_time string, sec_hop string, ip_count string ) -rw-rw-r-- 1 admin admin 319 Jul 26 16:12 create_session_info.hql create table if not exists syllabus.session_info( session_id string, guid string, tracker_u string, landing_url string, landing_url_ref string, user_id string, pv string, stay_time string, min_tracktime string, ip string, province_id string) partitioned by (date string) row format delimited fields terminated by &#39;\\t&#39;; -rw-rw-r-- 1 admin admin 235 Jul 26 16:13 create_session_info_temp1.hql insert overwrite table syllabus.session_info_temp1 select sessionId, max(guid), max(endUserId), count(url), max(unix_timestamp(trackTime)) - min(unix_timestamp(trackTime)), from_unixtime(min(unix_timestamp(trackTime))), max(ip), max(provinceId) from syllabus.track_log where date=&#39;20150828&#39; group by sessionId; -rw-rw-r-- 1 admin admin 190 Jul 26 16:14 create_session_info_temp2.hql create table syllabus.session_info_temp2( session_id string, tracktime string, tracker_u string, landing_url string, landing_url_ref string ) row format delimited fields terminated by &#39;\\t&#39;; -rw-rw-r-- 1 admin admin 388 Jul 26 16:15 insert_result_info.hql insert overwrite table syllabus.result_info select date, count(distinct guid), sum(pv), count(case when user_id != &#39;&#39; then user_id else null end), count(case when user_id = &#39;&#39; then user_id else null end), avg(stay_time), count(distinct (case when pv &gt;= 2 then guid else null end))/count(distinct guid), count(distinct ip) from syllabus.session_info where date=&#39;20170725&#39; group by date; -rw-rw-r-- 1 admin admin 368 Jul 26 16:14 insert_session_info.hql insert overwrite table syllabus.session_info partition(date=&#39;20150828&#39;) select p1.session_id, p1.guid, p2.tracker_u, p2.landing_url, p2.landing_url_ref, p1.user_id, p1.pv, p1.stay_time, p1.min_tracktime, p1.ip, p1.province_id from syllabus.session_info_temp1 p1 join syllabus.session_info_temp2 p2 on p1.session_id=p2.session_id and p1.min_tracktime=p2.tracktime; -rw-rw-r-- 1 admin admin 314 Jul 26 16:13 insert_session_info_temp1.hql create table if not exists syllabus.session_info_temp1( session_id string, guid string, user_id string, pv string, stay_time string, min_tracktime string, ip string, province_id string ) row format delimited fields terminated by &#39;\\t&#39;; -rw-rw-r-- 1 admin admin 153 Jul 26 16:14 insert_session_info_temp2.hql insert overwrite table syllabus.session_info_temp2 select sessionId, trackTime, trackerU, url, referer from syllabus.track_log where date=&#39;20150828&#39;; 5.2、 编写脚本依次执行 echo &quot;==开始分析网站流量数据==&quot; echo &quot;==检查所有表结构========&quot; $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp1.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp2.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_result_info.hql echo &quot;==开始插入数据==========&quot; $HIVE_DIR/bin/hive \\ --hiveconf DATE_NEW=$YESTERDAY \\ -f $HQL_DIR/insert_session_info_temp1.hql $HIVE_DIR/bin/hive \\ --hiveconf DATE_NEW=$YESTERDAY \\ -f $HQL_DIR/insert_session_info_temp2.hql $HIVE_DIR/bin/hive \\ --hiveconf DATE_NEW=$YESTERDAY \\ -f $HQL_DIR/insert_session_info.hql $HIVE_DIR/bin/hive \\ --hiveconf DATE_NEW=$YESTERDAY \\ -f $HQL_DIR/insert_result_info.hql 6、导出到Mysql 6.1、 创建sqoop可执行的opt文件 $ vi /opt/modules/cdh/clean/weblog_hive_2_mysql.opt 内容如下： export --connect jdbc:mysql://hadoop-senior01.itguigu.com:3306/syllabus_weblog --username root --password 123456 --table result_web_log --num-mappers 1 --export-dir /user/hive/warehouse/syllabus.db/result_info --input-fields-terminated-by &quot;\\t&quot; 6.2、 进入Mysql创建对应数据库以及数据表 auto.sh 完整脚本 #!/bin/bash #执行系统环境变量脚本，初始化一些变量信息 . /etc/profile #定义Hive目录 HIVE_DIR=/opt/modules/cdh/hive-0.13.1-cdh5.3.6 HADOOP_DIR=/opt/modules/cdh/hadoop-2.5.0-cdh5.3.6 HQL_DIR=/opt/modules/cdh/clean SQOOP_DIR=/opt/modules/cdh/sqoop-1.4.5-cdh5.3.6 echo $HIVE_DIR echo $HADOOP_DIR #定义日志的存储路径 WEB_LOG=/opt/modules/weblog #昨天的日期，用于访问目录 YESTERDAY=$(date --date=&quot;1 day ago&quot; +%Y%m%d) #在HDFS上创建指定文件夹 echo &quot;======正在创建目录======&quot; #/weblog/20170725 $HADOOP_DIR/bin/hdfs dfs -mkdir /weblog/$YESTERDAY echo &quot;======正在检查数据表====&quot; $HIVE_DIR/bin/hive -f $HQL_DIR/create_table_track_log.hql #遍历目录 for i in `ls $WEB_LOG/$YESTERDAY` do #20150828 DATE=${i:0:8} #18 HOUR=${i:8:2} #上传文件到HDFS echo &quot;======正在上传日志======&quot; #bin/hdfs /opt/modules/weblog/2015082818 /weblog/20170726 $HADOOP_DIR/bin/hdfs dfs -put $WEB_LOG/$YESTERDAY/$i /weblog/$YESTERDAY #清洗日志 echo &quot;======开始清洗==========&quot; # bin/yarn jar /home/admin/Desktop/clearlog.jar com.z.hive.etl.LogCleanMapReduce /weblog/20170726/2015082818 /user/hive/warehouse/syllabus.db/track_log/date=20170725/hour=18 $HADOOP_DIR/bin/yarn jar /home/admin/Desktop/clearlog.jar com.z.hive.etl.LogCleanMapReduce /weblog/$YESTERDAY/$i /user/hive/warehouse/syllabus.db/track_log/date=$DATE/hour=$HOUR echo &quot;======关联清洗后的数据==&quot; $HIVE_DIR/bin/hive \\ --hiveconf LOCATION_NEW=/user/hive/warehouse/syllabus.db/track_log/date=$DATE/hour=$HOUR \\ --hiveconf DATE_NEW=$DATE \\ --hiveconf HOUR_NEW=$HOUR \\ -f $HQL_DIR/alter_table_track_log.hql done echo &quot;==开始分析网站流量数据==&quot; echo &quot;==检查所有表结构========&quot; $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp1.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_session_info_temp2.hql $HIVE_DIR/bin/hive -f $HQL_DIR/create_result_info.hql echo &quot;==开始插入数据==========&quot; $HIVE_DIR/bin/hive \\ --hiveconf DATE_NEW=$YESTERDAY \\ -f $HQL_DIR/insert_session_info_temp1.hql $HIVE_DIR/bin/hive \\ --hiveconf DATE_NEW=$YESTERDAY \\ -f $HQL_DIR/insert_session_info_temp2.hql $HIVE_DIR/bin/hive \\ --hiveconf DATE_NEW=$YESTERDAY \\ -f $HQL_DIR/insert_session_info.hql $HIVE_DIR/bin/hive \\ --hiveconf DATE_NEW=$YESTERDAY \\ -f $HQL_DIR/insert_result_info.hql echo &quot;==开始导出数据到Mysql==&quot; $SQOOP_DIR/bin/sqoop --options-file $HQL_DIR/weblog_hive_2_mysql.opt echo &quot;==任务完成=============&quot; linux补充 if [ file -r ] -eq 等于则为真 -ne 不等于则为真 -gt 大于则为真 -ge 大于等于则为真 -lt 小于则为真 -le 小于等于则为真 字串测试 = 等于则为真 != 不相等则为真 -z字串 字串长度伪则为真 -n字串 字串长度不伪则为真 文件测试 -e 文件名 如果文件存在则为真 -r 文件名 果文件存在且可读则为真 -w 文件名 如果文件存在且可写则为真 -x 文件名 如果文件存在且可执行则为真 文件测试 -s 文件名 如果文件存在且至少有一个字符则为真 -d 文件名 如果文件存在且为目录则为真 -f 文件名 如果文件存在且为普通文件则为真 -c 文件名 如果文件存在且为字符型特殊文件则为真 -b 文件名 如果文件存在且为块特殊文件则为真 Linux还提供了非（！）、或（-o）、与（-a）三个逻辑操作符，用于将测试条件连接起来，其优先顺序为：！最高，-a次之，-o最低","@type":"BlogPosting","url":"https://mlh.app/2019/05/17/787734.html","headline":"大数据学习笔记之Hive（六）：Hive综合练习","dateModified":"2019-05-17T00:00:00+08:00","datePublished":"2019-05-17T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2019/05/17/787734.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>大数据学习笔记之Hive（六）：Hive综合练习</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post">  
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-f57960eb32.css"> 
 <div id="content_views" class="markdown_views prism-atom-one-light"> 
  <!-- flowchart 箭头图标 勿删 --> 
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> 
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path> 
  </svg> 
  <p></p>
  <div class="toc">
   <h3>文章目录</h3>
   <ul>
    <li><a href="#Hive_2" rel="nofollow">Hive综合练习：</a></li>
    <li><a href="#1HDFS_25" rel="nofollow">1、上传文件到HDFS</a></li>
    <ul>
     <li><a href="#11__26" rel="nofollow">1.1 文件/目录准备工作</a></li>
    </ul>
    <li><a href="#2_46" rel="nofollow">2、清洗日志</a></li>
    <ul>
     <li><a href="#21_MR_47" rel="nofollow">2.1 编写清洗日志的MR代码：</a></li>
     <li><a href="#22_Jar_76" rel="nofollow">2.2 导出Jar包</a></li>
    </ul>
    <li><a href="#3_101" rel="nofollow">3、执行创建表操作</a></li>
    <ul>
     <li><a href="#31_track_logsyllabusdb_102" rel="nofollow">3.1 创建track_log表，在syllabus.db目录下</a></li>
    </ul>
    <li><a href="#4_156" rel="nofollow">4、关联表操作</a></li>
    <ul>
     <li><a href="#41_157" rel="nofollow">4.1</a></li>
     <li><a href="#42_176" rel="nofollow">4.2</a></li>
    </ul>
    <li><a href="#5_193" rel="nofollow">5、分析数据</a></li>
    <ul>
     <li><a href="#51_194" rel="nofollow">5.1、</a></li>
     <li><a href="#52_327" rel="nofollow">5.2、</a></li>
    </ul>
    <li><a href="#6Mysql_355" rel="nofollow">6、导出到Mysql</a></li>
    <ul>
     <li><a href="#61_356" rel="nofollow">6.1、</a></li>
     <li><a href="#62_379" rel="nofollow">6.2、</a></li>
    </ul>
    <li><a href="#autosh_383" rel="nofollow">auto.sh</a></li>
    <li><a href="#linux_465" rel="nofollow">linux补充</a></li>
   </ul>
  </div>
  <p></p> 
  <h1><a id="Hive_2"></a>Hive综合练习：</h1> 
  <pre><code>1、写一个自动化脚本，自动拷贝/home/admin/WebLog/目录（服务器产生日志的目录）下的两个日志文件到指定目录/opt/modules/weblog。
2、crontab -e定时执行日志文件的拷贝迁移。
3、我们把日志上传到HDFS，删除该目录下的文件（有可能是只保存7天以内的日志）
4、数据清洗
5、数据分析，产生最终的结果表
6、将结果导入到Mysql中（就1条数据）
</code></pre> 
  <p>文档：<br> 需求分析<br> 日期 UV PV 登录人数 游客人数 平均访问时长 二跳率 独立IP<br> 20150828 38985 131668.0 18548 21902 750.7895179233622 0.5166089521610876 29668<br> 逻辑图<br> 数据字典</p> 
  <p>解决方案：<br> 预定义变量：</p> 
  <p>HIVE_DIR=/opt/modules/cdh/hive-0.13.1-cdh5.3.6<br> HADOOP_DIR=/opt/modules/cdh/hadoop-2.5.0-cdh5.3.6<br> HQL_DIR=/opt/modules/cdh/clean（SHELL脚本和HIVE脚本的存放路径）</p> 
  <h1><a id="1HDFS_25"></a>1、上传文件到HDFS</h1> 
  <h2><a id="11__26"></a>1.1 文件/目录准备工作</h2> 
  <p>bin/hdfs dfs -put /日志在Linux本地存放的绝对路径 /HDFS路径<br> 例如：bin/hdfs dfs -put /opt/modules/weblog/20170725/2015082818 /weblog/<br> 脚本：</p> 
  <pre><code class="prism language-js"><span class="token constant">YESTERDAY</span><span class="token operator">=</span><span class="token function">$</span><span class="token punctuation">(</span>date <span class="token operator">--</span>date<span class="token operator">=</span><span class="token string">"1 day ago"</span> <span class="token operator">+</span><span class="token operator">%</span><span class="token constant">Y</span><span class="token operator">%</span>m<span class="token operator">%</span>d<span class="token punctuation">)</span>
</code></pre> 
  <p>创建目录：</p> 
  <pre><code class="prism language-js">$<span class="token constant">HADOOP_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hdfs dfs <span class="token operator">-</span>mkdir <span class="token operator">/</span>weblog<span class="token operator">/</span>$<span class="token constant">YESTERDAY</span><span class="token punctuation">(</span>以上两步不要放在循环中执行<span class="token punctuation">)</span>
</code></pre> 
  <p>循环上传日志文件：</p> 
  <pre><code class="prism language-js">$<span class="token constant">HADOOP_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hdfs dfs <span class="token operator">-</span>put <span class="token operator">/</span>opt<span class="token operator">/</span>modules<span class="token operator">/</span>weblog<span class="token operator">/</span>$<span class="token constant">YESTERDAY</span><span class="token operator">/</span>$i 
</code></pre> 
  <h1><a id="2_46"></a>2、清洗日志</h1> 
  <h2><a id="21_MR_47"></a>2.1 编写清洗日志的MR代码：</h2> 
  <p>bin/yarn jar /home/admin/Desktop/cleanlog.jar com.z.etl.CleanLogMapReduce /weblog/20170725/2015082818<br> /user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18</p> 
  <p>*** Tools.java：执行删除本次任务中可能涉及到的输出路径，代码如下：</p> 
  <pre><code class="prism language-js"><span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">deleteFileInHDFS</span><span class="token punctuation">(</span>String directory<span class="token punctuation">,</span> String exist<span class="token punctuation">)</span> throws IOException<span class="token punctuation">{</span>
			<span class="token comment">// 获取文件系统管理对象</span>
			FileSystem fileSystem <span class="token operator">=</span> FileSystem<span class="token punctuation">.</span><span class="token keyword">get</span><span class="token punctuation">(</span><span class="token constant">URI</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span>directory<span class="token punctuation">)</span><span class="token punctuation">,</span> conf<span class="token punctuation">)</span><span class="token punctuation">;</span>
			FileStatus<span class="token punctuation">[</span><span class="token punctuation">]</span> fileList <span class="token operator">=</span> fileSystem<span class="token punctuation">.</span><span class="token function">listStatus</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">Path</span><span class="token punctuation">(</span>directory<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
			<span class="token keyword">for</span><span class="token punctuation">(</span>int i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> fileList<span class="token punctuation">.</span>length<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>	
				FileStatus fileStatus <span class="token operator">=</span> fileList<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>
				<span class="token keyword">if</span><span class="token punctuation">(</span>fileStatus<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">startsWith</span><span class="token punctuation">(</span>exist<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
					fileSystem<span class="token punctuation">.</span><span class="token keyword">delete</span><span class="token punctuation">(</span>fileStatus<span class="token punctuation">.</span><span class="token function">getPath</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
				<span class="token punctuation">}</span>
			
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>
</code></pre> 
  <p>*** 截取字符串：</p> 
  <pre><code class="prism language-js"><span class="token comment">///user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18</span>
Tools<span class="token punctuation">.</span><span class="token function">deleteFileInHDFS</span><span class="token punctuation">(</span>args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">57</span><span class="token punctuation">)</span><span class="token punctuation">,</span> args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token function">substring</span><span class="token punctuation">(</span><span class="token number">57</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
  <p>代码：见清洗代码包</p> 
  <h2><a id="22_Jar_76"></a>2.2 导出Jar包</h2> 
  <p>注意：</p> 
  <p>1如果在导包时指定了MainClass，则调用：<br> bin/yarn jar xxxxx.jar /input/ /output/<br> 2如果在导包时没有指定MainClass，则调用：<br> bin/yarn jar xxxxx.jar MainClass /input/ /output/</p> 
  <p>JAR包的路径是：/home/admin/Desktop/clearlog.jar</p> 
  <p>先测试：</p> 
  <pre><code class="prism language-js">$ <span class="token operator">/</span>opt<span class="token operator">/</span>modules<span class="token operator">/</span>cdh<span class="token operator">/</span>hadoop<span class="token operator">-</span><span class="token number">2.5</span><span class="token number">.0</span><span class="token operator">-</span>cdh5<span class="token punctuation">.</span><span class="token number">3.6</span><span class="token operator">/</span>bin<span class="token operator">/</span>yarn jar \
				<span class="token operator">/</span>home<span class="token operator">/</span>admin<span class="token operator">/</span>Desktop<span class="token operator">/</span>clearlog<span class="token punctuation">.</span>jar 
				com<span class="token punctuation">.</span>z<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>etl<span class="token punctuation">.</span>LogCleanMapReduce \
				<span class="token operator">/</span>weblog<span class="token operator">/</span><span class="token number">20170725</span><span class="token operator">/</span><span class="token number">2015082818</span> \
				<span class="token operator">/</span>user<span class="token operator">/</span>hive<span class="token operator">/</span>warehouse<span class="token operator">/</span>syllabus<span class="token punctuation">.</span>db<span class="token operator">/</span>track_log<span class="token operator">/</span>date<span class="token operator">=</span><span class="token number">20150828</span><span class="token operator">/</span>hour<span class="token operator">=</span><span class="token number">18</span>
<span class="token comment">///weblog/20170725/2015082818 输入路径</span>
<span class="token comment">///user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18输出路径</span>
<span class="token comment">//com.z.hive.etl.LogCleanMapReduce 主函数的名字，包含包名，LogCleanMapReduce是包含main的类名</span>
<span class="token comment">//date=20150828和hour=18是文件的名称，只不过教程中问加你的名称取的比较特殊</span>
</code></pre> 
  <p>尖叫提示：要测试两遍，在没有目录的时候测试一次，在有目录的时候再测试一次。</p> 
  <h1><a id="3_101"></a>3、执行创建表操作</h1> 
  <h2><a id="31_track_logsyllabusdb_102"></a>3.1 创建track_log表，在syllabus.db目录下</h2> 
  <pre><code class="prism language-js">建表语句：
			create database <span class="token keyword">if</span> not exists syllabus<span class="token punctuation">;</span>
			create table <span class="token keyword">if</span> not exists syllabus<span class="token punctuation">.</span><span class="token function">track_log</span><span class="token punctuation">(</span>
			id string<span class="token punctuation">,</span>
			url string<span class="token punctuation">,</span>
			referer string<span class="token punctuation">,</span>
			keyword string<span class="token punctuation">,</span>
			type string<span class="token punctuation">,</span>
			guid string<span class="token punctuation">,</span>
			pageId string<span class="token punctuation">,</span>
			moduleId string<span class="token punctuation">,</span>
			linkId string<span class="token punctuation">,</span>
			attachedInfo string<span class="token punctuation">,</span>
			sessionId string<span class="token punctuation">,</span>
			trackerU string<span class="token punctuation">,</span>
			trackerType string<span class="token punctuation">,</span>
			ip string<span class="token punctuation">,</span>
			trackerSrc string<span class="token punctuation">,</span>
			cookie string<span class="token punctuation">,</span>
			orderCode string<span class="token punctuation">,</span>
			trackTime string<span class="token punctuation">,</span>
			endUserId string<span class="token punctuation">,</span>
			firstLink string<span class="token punctuation">,</span>
			sessionViewNo string<span class="token punctuation">,</span>
			productId string<span class="token punctuation">,</span>
			curMerchantId string<span class="token punctuation">,</span>
			provinceId string<span class="token punctuation">,</span>
			cityId string<span class="token punctuation">,</span>
			fee string<span class="token punctuation">,</span>
			edmActivity string<span class="token punctuation">,</span>
			edmEmail string<span class="token punctuation">,</span>
			edmJobId string<span class="token punctuation">,</span>
			ieVersion string<span class="token punctuation">,</span>
			platform string<span class="token punctuation">,</span>
			internalKeyword string<span class="token punctuation">,</span>
			resultSum string<span class="token punctuation">,</span>
			currentPage string<span class="token punctuation">,</span>
			linkPosition string<span class="token punctuation">,</span>
			buttonPosition string
			<span class="token punctuation">)</span>
			partitioned <span class="token function">by</span> <span class="token punctuation">(</span>date string<span class="token punctuation">,</span>hour string<span class="token punctuation">)</span>
			row format delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
</code></pre> 
  <pre><code>将如上代码保存在create_table_track_log.hql文件中，执行单个测试，命令如下：
</code></pre> 
  <pre><code class="prism language-js">$ <span class="token operator">/</span>opt<span class="token operator">/</span>modules<span class="token operator">/</span>cdh<span class="token operator">/</span>hive<span class="token operator">-</span><span class="token number">0.13</span><span class="token number">.1</span><span class="token operator">-</span>cdh5<span class="token punctuation">.</span><span class="token number">3.6</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive <span class="token operator">-</span>f <span class="token operator">/</span>opt<span class="token operator">/</span>modules<span class="token operator">/</span>cdh<span class="token operator">/</span>clean<span class="token operator">/</span>create_table_track_log<span class="token punctuation">.</span>hql
</code></pre> 
  <h1><a id="4_156"></a>4、关联表操作</h1> 
  <h2><a id="41_157"></a>4.1</h2> 
  <p>如下代码有BUG，Hive-0.14以及Hive-0.15版本已经修复该BUG<br> 尖叫提示：解决方案，先use表所在的库，然后再alter，而且在alter的时候，表名前边不能有库名。<br> alter table track_log add partition(date=‘20150828’, hour=‘19’) location “/user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18”;</p> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019052621012733.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2RhdGFpeWFuZ3U=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> 另一种方案：</p> 
  <pre><code class="prism language-js">load data inpath <span class="token string">"/user/hive/warehouse/syllabus.db/track_log/date=20150828/hour=18/part-r-00000"</span> into table syllabus<span class="token punctuation">.</span>track_log <span class="token function">partition</span><span class="token punctuation">(</span>date<span class="token operator">=</span><span class="token string">'20150828'</span><span class="token punctuation">,</span> hour<span class="token operator">=</span><span class="token string">'18'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
  <p>alter 关联代码：</p> 
  <pre><code class="prism language-js"><span class="token comment">//在执行alter语句之前，输入use syllabus语句，就不会报错了</span>
use syllabus<span class="token punctuation">;</span>alter table track_log add <span class="token function">partition</span><span class="token punctuation">(</span>date<span class="token operator">=</span><span class="token string">'${hiveconf:DATE_NEW}'</span><span class="token punctuation">,</span> hour<span class="token operator">=</span><span class="token string">'${hiveconf:HOUR_NEW}'</span><span class="token punctuation">)</span> location <span class="token string">"${hiveconf:LOCATION_NEW}"</span><span class="token punctuation">;</span>
</code></pre> 
  <p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190526210507700.png" alt="在这里插入图片描述"></p> 
  <h2><a id="42_176"></a>4.2</h2> 
  <p>该功能对应shell脚本如下：</p> 
  <pre><code class="prism language-js">echo <span class="token string">"======关联清洗后的数据=="</span>
			$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive \
			<span class="token operator">--</span>hiveconf <span class="token constant">LOCATION_NEW</span><span class="token operator">=</span><span class="token operator">/</span>user<span class="token operator">/</span>hive<span class="token operator">/</span>warehouse<span class="token operator">/</span>syllabus<span class="token punctuation">.</span>db<span class="token operator">/</span>track_log<span class="token operator">/</span>date<span class="token operator">=</span>$<span class="token constant">DATE</span><span class="token operator">/</span>hour<span class="token operator">=</span>$<span class="token constant">HOUR</span> \
			<span class="token operator">--</span>hiveconf <span class="token constant">DATE_NEW</span><span class="token operator">=</span>$<span class="token constant">DATE</span> \
			<span class="token operator">--</span>hiveconf <span class="token constant">HOUR_NEW</span><span class="token operator">=</span>$<span class="token constant">HOUR</span> \
			<span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>alter_table_track_log<span class="token punctuation">.</span>hql
</code></pre> 
  <p>测试查询数据：</p> 
  <pre><code class="prism language-js">hive<span class="token operator">&gt;</span> select <span class="token operator">*</span> <span class="token keyword">from</span> track_log limit <span class="token number">1</span><span class="token punctuation">;</span>
</code></pre> 
  <h1><a id="5_193"></a>5、分析数据</h1> 
  <h2><a id="51_194"></a>5.1、</h2> 
  <p>创建一系列分析语句<br> -rw-rw-r-- 1 admin admin 209 Jul 26 16:15 create_result_info.hql</p> 
  <pre><code class="prism language-js">create table syllabus<span class="token punctuation">.</span><span class="token function">result_info</span><span class="token punctuation">(</span>
date string<span class="token punctuation">,</span>
uv string<span class="token punctuation">,</span>
pv string<span class="token punctuation">,</span>
login_users string<span class="token punctuation">,</span>
visit_users string<span class="token punctuation">,</span>
avg_time string<span class="token punctuation">,</span>
sec_hop string<span class="token punctuation">,</span>
ip_count string
<span class="token punctuation">)</span>
</code></pre> 
  <p>-rw-rw-r-- 1 admin admin 319 Jul 26 16:12 create_session_info.hql</p> 
  <pre><code class="prism language-js">create table <span class="token keyword">if</span> not exists syllabus<span class="token punctuation">.</span><span class="token function">session_info</span><span class="token punctuation">(</span>
session_id string<span class="token punctuation">,</span>
guid string<span class="token punctuation">,</span>
tracker_u string<span class="token punctuation">,</span>
landing_url string<span class="token punctuation">,</span>
landing_url_ref string<span class="token punctuation">,</span>
user_id string<span class="token punctuation">,</span>
pv string<span class="token punctuation">,</span>
stay_time string<span class="token punctuation">,</span>
min_tracktime string<span class="token punctuation">,</span>
ip string<span class="token punctuation">,</span>
province_id string<span class="token punctuation">)</span>
partitioned <span class="token function">by</span> <span class="token punctuation">(</span>date string<span class="token punctuation">)</span>
row format delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
</code></pre> 
  <p>-rw-rw-r-- 1 admin admin 235 Jul 26 16:13 create_session_info_temp1.hql</p> 
  <pre><code class="prism language-js">insert overwrite table syllabus<span class="token punctuation">.</span>session_info_temp1
select
sessionId<span class="token punctuation">,</span>
<span class="token function">max</span><span class="token punctuation">(</span>guid<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">max</span><span class="token punctuation">(</span>endUserId<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">count</span><span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">max</span><span class="token punctuation">(</span><span class="token function">unix_timestamp</span><span class="token punctuation">(</span>trackTime<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token function">min</span><span class="token punctuation">(</span><span class="token function">unix_timestamp</span><span class="token punctuation">(</span>trackTime<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">from_unixtime</span><span class="token punctuation">(</span><span class="token function">min</span><span class="token punctuation">(</span><span class="token function">unix_timestamp</span><span class="token punctuation">(</span>trackTime<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">max</span><span class="token punctuation">(</span>ip<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">max</span><span class="token punctuation">(</span>provinceId<span class="token punctuation">)</span>
<span class="token keyword">from</span> syllabus<span class="token punctuation">.</span>track_log where date<span class="token operator">=</span><span class="token string">'20150828'</span>
group by
sessionId<span class="token punctuation">;</span>
</code></pre> 
  <p>-rw-rw-r-- 1 admin admin 190 Jul 26 16:14 create_session_info_temp2.hql</p> 
  <pre><code class="prism language-js">create table syllabus<span class="token punctuation">.</span><span class="token function">session_info_temp2</span><span class="token punctuation">(</span>
session_id string<span class="token punctuation">,</span>
tracktime string<span class="token punctuation">,</span>
tracker_u string<span class="token punctuation">,</span>
landing_url string<span class="token punctuation">,</span>
landing_url_ref string
<span class="token punctuation">)</span>
row format delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
</code></pre> 
  <p>-rw-rw-r-- 1 admin admin 388 Jul 26 16:15 insert_result_info.hql</p> 
  <pre><code class="prism language-js">insert overwrite table syllabus<span class="token punctuation">.</span>result_info
select
date<span class="token punctuation">,</span>
<span class="token function">count</span><span class="token punctuation">(</span>distinct guid<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">sum</span><span class="token punctuation">(</span>pv<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">case</span> when user_id <span class="token operator">!=</span> <span class="token string">''</span> then user_id <span class="token keyword">else</span> <span class="token keyword">null</span> end<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">count</span><span class="token punctuation">(</span><span class="token keyword">case</span> when user_id <span class="token operator">=</span> <span class="token string">''</span> then user_id <span class="token keyword">else</span> <span class="token keyword">null</span> end<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">avg</span><span class="token punctuation">(</span>stay_time<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">count</span><span class="token punctuation">(</span><span class="token function">distinct</span> <span class="token punctuation">(</span><span class="token keyword">case</span> when pv <span class="token operator">&gt;=</span> <span class="token number">2</span> then guid <span class="token keyword">else</span> <span class="token keyword">null</span> end<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span><span class="token function">count</span><span class="token punctuation">(</span>distinct guid<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token function">count</span><span class="token punctuation">(</span>distinct ip<span class="token punctuation">)</span>
<span class="token keyword">from</span> syllabus<span class="token punctuation">.</span>session_info where date<span class="token operator">=</span><span class="token string">'20170725'</span>
group by
date<span class="token punctuation">;</span>
</code></pre> 
  <p>-rw-rw-r-- 1 admin admin 368 Jul 26 16:14 insert_session_info.hql</p> 
  <pre><code class="prism language-js">insert overwrite table syllabus<span class="token punctuation">.</span>session_info <span class="token function">partition</span><span class="token punctuation">(</span>date<span class="token operator">=</span><span class="token string">'20150828'</span><span class="token punctuation">)</span>
select
p1<span class="token punctuation">.</span>session_id<span class="token punctuation">,</span>
p1<span class="token punctuation">.</span>guid<span class="token punctuation">,</span>
p2<span class="token punctuation">.</span>tracker_u<span class="token punctuation">,</span>
p2<span class="token punctuation">.</span>landing_url<span class="token punctuation">,</span>
p2<span class="token punctuation">.</span>landing_url_ref<span class="token punctuation">,</span>
p1<span class="token punctuation">.</span>user_id<span class="token punctuation">,</span>
p1<span class="token punctuation">.</span>pv<span class="token punctuation">,</span>
p1<span class="token punctuation">.</span>stay_time<span class="token punctuation">,</span>
p1<span class="token punctuation">.</span>min_tracktime<span class="token punctuation">,</span>
p1<span class="token punctuation">.</span>ip<span class="token punctuation">,</span>
p1<span class="token punctuation">.</span>province_id
<span class="token keyword">from</span> syllabus<span class="token punctuation">.</span>session_info_temp1 p1 join syllabus<span class="token punctuation">.</span>session_info_temp2 p2
on p1<span class="token punctuation">.</span>session_id<span class="token operator">=</span>p2<span class="token punctuation">.</span>session_id and p1<span class="token punctuation">.</span>min_tracktime<span class="token operator">=</span>p2<span class="token punctuation">.</span>tracktime<span class="token punctuation">;</span>
</code></pre> 
  <p>-rw-rw-r-- 1 admin admin 314 Jul 26 16:13 insert_session_info_temp1.hql</p> 
  <pre><code class="prism language-js">create table <span class="token keyword">if</span> not exists syllabus<span class="token punctuation">.</span><span class="token function">session_info_temp1</span><span class="token punctuation">(</span>
session_id string<span class="token punctuation">,</span>
guid string<span class="token punctuation">,</span>
user_id string<span class="token punctuation">,</span>
pv string<span class="token punctuation">,</span>
stay_time string<span class="token punctuation">,</span>
min_tracktime string<span class="token punctuation">,</span>
ip string<span class="token punctuation">,</span>
province_id string
<span class="token punctuation">)</span>
row format delimited fields terminated by <span class="token string">'\t'</span><span class="token punctuation">;</span>
</code></pre> 
  <p>-rw-rw-r-- 1 admin admin 153 Jul 26 16:14 insert_session_info_temp2.hql</p> 
  <pre><code class="prism language-js">insert overwrite table syllabus<span class="token punctuation">.</span>session_info_temp2
select
sessionId<span class="token punctuation">,</span>
trackTime<span class="token punctuation">,</span>
trackerU<span class="token punctuation">,</span>
url<span class="token punctuation">,</span>
referer
<span class="token keyword">from</span> syllabus<span class="token punctuation">.</span>track_log where date<span class="token operator">=</span><span class="token string">'20150828'</span><span class="token punctuation">;</span>
</code></pre> 
  <h2><a id="52_327"></a>5.2、</h2> 
  <p>编写脚本依次执行</p> 
  <pre><code class="prism language-js">echo <span class="token string">"==开始分析网站流量数据=="</span>
			echo <span class="token string">"==检查所有表结构========"</span>
			$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive <span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>create_session_info<span class="token punctuation">.</span>hql
			$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive <span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>create_session_info_temp1<span class="token punctuation">.</span>hql
			$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive <span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>create_session_info_temp2<span class="token punctuation">.</span>hql
			$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive <span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>create_result_info<span class="token punctuation">.</span>hql
			echo <span class="token string">"==开始插入数据=========="</span>
			$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive \
			<span class="token operator">--</span>hiveconf <span class="token constant">DATE_NEW</span><span class="token operator">=</span>$<span class="token constant">YESTERDAY</span> \
			<span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>insert_session_info_temp1<span class="token punctuation">.</span>hql

			$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive \
			<span class="token operator">--</span>hiveconf <span class="token constant">DATE_NEW</span><span class="token operator">=</span>$<span class="token constant">YESTERDAY</span> \
			<span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>insert_session_info_temp2<span class="token punctuation">.</span>hql

			$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive \
			<span class="token operator">--</span>hiveconf <span class="token constant">DATE_NEW</span><span class="token operator">=</span>$<span class="token constant">YESTERDAY</span> \
			<span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>insert_session_info<span class="token punctuation">.</span>hql

			$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive \
			<span class="token operator">--</span>hiveconf <span class="token constant">DATE_NEW</span><span class="token operator">=</span>$<span class="token constant">YESTERDAY</span> \
			<span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>insert_result_info<span class="token punctuation">.</span>hql
</code></pre> 
  <h1><a id="6Mysql_355"></a>6、导出到Mysql</h1> 
  <h2><a id="61_356"></a>6.1、</h2> 
  <p>创建sqoop可执行的opt文件</p> 
  <pre><code class="prism language-js">$ vi <span class="token operator">/</span>opt<span class="token operator">/</span>modules<span class="token operator">/</span>cdh<span class="token operator">/</span>clean<span class="token operator">/</span>weblog_hive_2_mysql<span class="token punctuation">.</span>opt
			内容如下：
			<span class="token keyword">export</span>
			<span class="token operator">--</span>connect
			jdbc<span class="token punctuation">:</span>mysql<span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span>hadoop<span class="token operator">-</span>senior01<span class="token punctuation">.</span>itguigu<span class="token punctuation">.</span>com<span class="token punctuation">:</span><span class="token number">3306</span><span class="token operator">/</span>syllabus_weblog
			<span class="token operator">--</span>username
			root
			<span class="token operator">--</span>password
			<span class="token number">123456</span>
			<span class="token operator">--</span>table
			result_web_log
			<span class="token operator">--</span>num<span class="token operator">-</span>mappers
			<span class="token number">1</span>
			<span class="token operator">--</span><span class="token keyword">export</span><span class="token operator">-</span>dir
			<span class="token operator">/</span>user<span class="token operator">/</span>hive<span class="token operator">/</span>warehouse<span class="token operator">/</span>syllabus<span class="token punctuation">.</span>db<span class="token operator">/</span>result_info
			<span class="token operator">--</span>input<span class="token operator">-</span>fields<span class="token operator">-</span>terminated<span class="token operator">-</span>by
			<span class="token string">"\t"</span> 
</code></pre> 
  <h2><a id="62_379"></a>6.2、</h2> 
  <p>进入Mysql创建对应数据库以及数据表</p> 
  <h1><a id="autosh_383"></a><a href="http://auto.sh" rel="nofollow">auto.sh</a></h1> 
  <p>完整脚本</p> 
  <pre><code class="prism language-js">#<span class="token operator">!</span><span class="token operator">/</span>bin<span class="token operator">/</span>bash
#执行系统环境变量脚本，初始化一些变量信息
<span class="token punctuation">.</span> <span class="token operator">/</span>etc<span class="token operator">/</span>profile

#定义Hive目录
<span class="token constant">HIVE_DIR</span><span class="token operator">=</span><span class="token operator">/</span>opt<span class="token operator">/</span>modules<span class="token operator">/</span>cdh<span class="token operator">/</span>hive<span class="token operator">-</span><span class="token number">0.13</span><span class="token number">.1</span><span class="token operator">-</span>cdh5<span class="token punctuation">.</span><span class="token number">3.6</span>
<span class="token constant">HADOOP_DIR</span><span class="token operator">=</span><span class="token operator">/</span>opt<span class="token operator">/</span>modules<span class="token operator">/</span>cdh<span class="token operator">/</span>hadoop<span class="token operator">-</span><span class="token number">2.5</span><span class="token number">.0</span><span class="token operator">-</span>cdh5<span class="token punctuation">.</span><span class="token number">3.6</span>
<span class="token constant">HQL_DIR</span><span class="token operator">=</span><span class="token operator">/</span>opt<span class="token operator">/</span>modules<span class="token operator">/</span>cdh<span class="token operator">/</span>clean
<span class="token constant">SQOOP_DIR</span><span class="token operator">=</span><span class="token operator">/</span>opt<span class="token operator">/</span>modules<span class="token operator">/</span>cdh<span class="token operator">/</span>sqoop<span class="token operator">-</span><span class="token number">1.4</span><span class="token number">.5</span><span class="token operator">-</span>cdh5<span class="token punctuation">.</span><span class="token number">3.6</span>

echo $<span class="token constant">HIVE_DIR</span>
echo $<span class="token constant">HADOOP_DIR</span>

#定义日志的存储路径
<span class="token constant">WEB_LOG</span><span class="token operator">=</span><span class="token operator">/</span>opt<span class="token operator">/</span>modules<span class="token operator">/</span>weblog

#昨天的日期，用于访问目录
<span class="token constant">YESTERDAY</span><span class="token operator">=</span><span class="token function">$</span><span class="token punctuation">(</span>date <span class="token operator">--</span>date<span class="token operator">=</span><span class="token string">"1 day ago"</span> <span class="token operator">+</span><span class="token operator">%</span><span class="token constant">Y</span><span class="token operator">%</span>m<span class="token operator">%</span>d<span class="token punctuation">)</span>
#在<span class="token constant">HDFS</span>上创建指定文件夹
echo <span class="token string">"======正在创建目录======"</span>
#<span class="token operator">/</span>weblog<span class="token operator">/</span><span class="token number">20170725</span>
$<span class="token constant">HADOOP_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hdfs dfs <span class="token operator">-</span>mkdir <span class="token operator">/</span>weblog<span class="token operator">/</span>$<span class="token constant">YESTERDAY</span>

echo <span class="token string">"======正在检查数据表===="</span>
$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive <span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>create_table_track_log<span class="token punctuation">.</span>hql

#遍历目录
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token template-string"><span class="token string">`ls $WEB_LOG/$YESTERDAY`</span></span>
<span class="token keyword">do</span>
	#<span class="token number">20150828</span>
	<span class="token constant">DATE</span><span class="token operator">=</span>$<span class="token punctuation">{</span>i<span class="token punctuation">:</span><span class="token number">0</span><span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">}</span>
	#<span class="token number">18</span>
    <span class="token constant">HOUR</span><span class="token operator">=</span>$<span class="token punctuation">{</span>i<span class="token punctuation">:</span><span class="token number">8</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">}</span>
  	#上传文件到<span class="token constant">HDFS</span>
  	echo <span class="token string">"======正在上传日志======"</span>
  	#bin<span class="token operator">/</span>hdfs <span class="token operator">/</span>opt<span class="token operator">/</span>modules<span class="token operator">/</span>weblog<span class="token operator">/</span><span class="token number">2015082818</span> <span class="token operator">/</span>weblog<span class="token operator">/</span><span class="token number">20170726</span>
  	$<span class="token constant">HADOOP_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hdfs dfs <span class="token operator">-</span>put $<span class="token constant">WEB_LOG</span><span class="token operator">/</span>$<span class="token constant">YESTERDAY</span><span class="token operator">/</span>$i <span class="token operator">/</span>weblog<span class="token operator">/</span>$<span class="token constant">YESTERDAY</span>
	#清洗日志
	echo <span class="token string">"======开始清洗=========="</span>
	# bin<span class="token operator">/</span>yarn jar <span class="token operator">/</span>home<span class="token operator">/</span>admin<span class="token operator">/</span>Desktop<span class="token operator">/</span>clearlog<span class="token punctuation">.</span>jar com<span class="token punctuation">.</span>z<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>etl<span class="token punctuation">.</span>LogCleanMapReduce <span class="token operator">/</span>weblog<span class="token operator">/</span><span class="token number">20170726</span><span class="token operator">/</span><span class="token number">2015082818</span> <span class="token operator">/</span>user<span class="token operator">/</span>hive<span class="token operator">/</span>warehouse<span class="token operator">/</span>syllabus<span class="token punctuation">.</span>db<span class="token operator">/</span>track_log<span class="token operator">/</span>date<span class="token operator">=</span><span class="token number">20170725</span><span class="token operator">/</span>hour<span class="token operator">=</span><span class="token number">18</span>
	$<span class="token constant">HADOOP_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>yarn jar <span class="token operator">/</span>home<span class="token operator">/</span>admin<span class="token operator">/</span>Desktop<span class="token operator">/</span>clearlog<span class="token punctuation">.</span>jar com<span class="token punctuation">.</span>z<span class="token punctuation">.</span>hive<span class="token punctuation">.</span>etl<span class="token punctuation">.</span>LogCleanMapReduce <span class="token operator">/</span>weblog<span class="token operator">/</span>$<span class="token constant">YESTERDAY</span><span class="token operator">/</span>$i <span class="token operator">/</span>user<span class="token operator">/</span>hive<span class="token operator">/</span>warehouse<span class="token operator">/</span>syllabus<span class="token punctuation">.</span>db<span class="token operator">/</span>track_log<span class="token operator">/</span>date<span class="token operator">=</span>$<span class="token constant">DATE</span><span class="token operator">/</span>hour<span class="token operator">=</span>$<span class="token constant">HOUR</span>
	echo <span class="token string">"======关联清洗后的数据=="</span>
	$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive \
	<span class="token operator">--</span>hiveconf <span class="token constant">LOCATION_NEW</span><span class="token operator">=</span><span class="token operator">/</span>user<span class="token operator">/</span>hive<span class="token operator">/</span>warehouse<span class="token operator">/</span>syllabus<span class="token punctuation">.</span>db<span class="token operator">/</span>track_log<span class="token operator">/</span>date<span class="token operator">=</span>$<span class="token constant">DATE</span><span class="token operator">/</span>hour<span class="token operator">=</span>$<span class="token constant">HOUR</span> \
	<span class="token operator">--</span>hiveconf <span class="token constant">DATE_NEW</span><span class="token operator">=</span>$<span class="token constant">DATE</span> \
	<span class="token operator">--</span>hiveconf <span class="token constant">HOUR_NEW</span><span class="token operator">=</span>$<span class="token constant">HOUR</span> \
	<span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>alter_table_track_log<span class="token punctuation">.</span>hql
done
	echo <span class="token string">"==开始分析网站流量数据=="</span>
	echo <span class="token string">"==检查所有表结构========"</span>
	$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive <span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>create_session_info<span class="token punctuation">.</span>hql
	$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive <span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>create_session_info_temp1<span class="token punctuation">.</span>hql
	$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive <span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>create_session_info_temp2<span class="token punctuation">.</span>hql
	$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive <span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>create_result_info<span class="token punctuation">.</span>hql
	echo <span class="token string">"==开始插入数据=========="</span>
	$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive \
	<span class="token operator">--</span>hiveconf <span class="token constant">DATE_NEW</span><span class="token operator">=</span>$<span class="token constant">YESTERDAY</span> \
	<span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>insert_session_info_temp1<span class="token punctuation">.</span>hql

	$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive \
	<span class="token operator">--</span>hiveconf <span class="token constant">DATE_NEW</span><span class="token operator">=</span>$<span class="token constant">YESTERDAY</span> \
	<span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>insert_session_info_temp2<span class="token punctuation">.</span>hql

	$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive \
	<span class="token operator">--</span>hiveconf <span class="token constant">DATE_NEW</span><span class="token operator">=</span>$<span class="token constant">YESTERDAY</span> \
	<span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>insert_session_info<span class="token punctuation">.</span>hql

	$<span class="token constant">HIVE_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>hive \
	<span class="token operator">--</span>hiveconf <span class="token constant">DATE_NEW</span><span class="token operator">=</span>$<span class="token constant">YESTERDAY</span> \
	<span class="token operator">-</span>f $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>insert_result_info<span class="token punctuation">.</span>hql

	echo <span class="token string">"==开始导出数据到Mysql=="</span>
	$<span class="token constant">SQOOP_DIR</span><span class="token operator">/</span>bin<span class="token operator">/</span>sqoop <span class="token operator">--</span>options<span class="token operator">-</span>file $<span class="token constant">HQL_DIR</span><span class="token operator">/</span>weblog_hive_2_mysql<span class="token punctuation">.</span>opt
	echo <span class="token string">"==任务完成============="</span>



</code></pre> 
  <h1><a id="linux_465"></a>linux补充</h1> 
  <pre><code class="prism language-js"><span class="token keyword">if</span> <span class="token punctuation">[</span> file <span class="token operator">-</span>r <span class="token punctuation">]</span>

<span class="token operator">-</span>eq 等于则为真
<span class="token operator">-</span>ne 不等于则为真
<span class="token operator">-</span>gt 大于则为真
<span class="token operator">-</span>ge 大于等于则为真
<span class="token operator">-</span>lt 小于则为真
<span class="token operator">-</span>le 小于等于则为真


字串测试

<span class="token operator">=</span> 等于则为真
<span class="token operator">!=</span> 不相等则为真
<span class="token operator">-</span>z字串 	字串长度伪则为真
<span class="token operator">-</span>n字串 	字串长度不伪则为真

文件测试
<span class="token operator">-</span>e	文件名 如果文件存在则为真
<span class="token operator">-</span>r	文件名 果文件存在且可读则为真
<span class="token operator">-</span>w	文件名 如果文件存在且可写则为真
<span class="token operator">-</span>x	文件名 如果文件存在且可执行则为真


文件测试
<span class="token operator">-</span>s 文件名 如果文件存在且至少有一个字符则为真
<span class="token operator">-</span>d 文件名 如果文件存在且为目录则为真
<span class="token operator">-</span>f 文件名 如果文件存在且为普通文件则为真
<span class="token operator">-</span>c 文件名 如果文件存在且为字符型特殊文件则为真
<span class="token operator">-</span>b 文件名 如果文件存在且为块特殊文件则为真

Linux还提供了非（！）、或（<span class="token operator">-</span>o）、与（<span class="token operator">-</span>a）三个逻辑操作符，用于将测试条件连接起来，其优先顺序为：！最高，<span class="token operator">-</span>a次之，<span class="token operator">-</span>o最低
</code></pre> 
 </div> 
 <link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-258a4616f7.css" rel="stylesheet"> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
