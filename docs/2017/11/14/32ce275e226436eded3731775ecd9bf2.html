<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Object Detection–的一个好的对比总结文章 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Object Detection–的一个好的对比总结文章" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html Jump to... Leaderboard Papers R-CNN Fast R-CNN Faster R-CNN MultiBox SPP-Net DeepID-Net NoC DeepBox MR-CNN YOLO YOLOv2 AttentionNet DenseBox SSD DSSD Inside-Outside Net (ION) G-CNN HyperNet MultiPathNet CRAFT OHEM R-FCN MS-CNN PVANET GBD-Net StuffNet Feature Pyramid Network (FPN) CC-Net DSOD NMS Weakly Supervised Object Detection Detection From Video T-CNN Object Detection in 3D Object Detection on RGB-D Salient Object Detection Saliency Detection in Video Visual Relationship Detection Specific Object Deteciton Face Deteciton UnitBox MTCNN Facial Point / Landmark Detection People Detection Person Head Detection Pedestrian Detection Vehicle Detection Traffic-Sign Detection Boundary / Edge / Contour Detection Skeleton Detection Fruit Detection Part Detection Object Proposal Localization Tutorials / Talks Projects Tools Blogs Method VOC2007 VOC2010 VOC2012 ILSVRC 2013 MSCOCO 2015 Speed OverFeat &nbsp; &nbsp; &nbsp; 24.3% &nbsp; &nbsp; R-CNN (AlexNet) 58.5% 53.7% 53.3% 31.4% &nbsp; &nbsp; R-CNN (VGG16) 66.0% &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; SPP_net(ZF-5) 54.2%(1-model), 60.9%(2-model) &nbsp; &nbsp; 31.84%(1-model), 35.11%(6-model) &nbsp; &nbsp; DeepID-Net 64.1% &nbsp; &nbsp; 50.3% &nbsp; &nbsp; NoC 73.3% &nbsp; 68.8% &nbsp; &nbsp; &nbsp; Fast-RCNN (VGG16) 70.0% 68.8% 68.4% &nbsp; 19.7%(@[0.5-0.95]), 35.9%(@0.5) &nbsp; MR-CNN 78.2% &nbsp; 73.9% &nbsp; &nbsp; &nbsp; Faster-RCNN (VGG16) 78.8% &nbsp; 75.9% &nbsp; 21.9%(@[0.5-0.95]), 42.7%(@0.5) 198ms Faster-RCNN (ResNet-101) 85.6% &nbsp; 83.8% &nbsp; 37.4%(@[0.5-0.95]), 59.0%(@0.5) &nbsp; YOLO 63.4% &nbsp; 57.9% &nbsp; &nbsp; 45 fps YOLO VGG-16 66.4% &nbsp; &nbsp; &nbsp; &nbsp; 21 fps YOLOv2 544 × 544 78.6% &nbsp; 73.4% &nbsp; 21.6%(@[0.5-0.95]), 44.0%(@0.5) 40 fps SSD300 (VGG16) 77.2% &nbsp; 75.8% &nbsp; 25.1%(@[0.5-0.95]), 43.1%(@0.5) 46 fps SSD512 (VGG16) 79.8% &nbsp; 78.5% &nbsp; 28.8%(@[0.5-0.95]), 48.5%(@0.5) 19 fps ION 79.2% &nbsp; 76.4% &nbsp; &nbsp; &nbsp; CRAFT 75.7% &nbsp; 71.3% 48.5% &nbsp; &nbsp; OHEM 78.9% &nbsp; 76.3% &nbsp; 25.5%(@[0.5-0.95]), 45.9%(@0.5) &nbsp; R-FCN (ResNet-50) 77.4% &nbsp; &nbsp; &nbsp; &nbsp; 0.12sec(K40), 0.09sec(TitianX) R-FCN (ResNet-101) 79.5% &nbsp; &nbsp; &nbsp; &nbsp; 0.17sec(K40), 0.12sec(TitianX) R-FCN (ResNet-101),multi sc train 83.6% &nbsp; 82.0% &nbsp; 31.5%(@[0.5-0.95]), 53.2%(@0.5) &nbsp; PVANet 9.0 89.8% &nbsp; 84.2% &nbsp; &nbsp; 750ms(CPU), 46ms(TitianX) Leaderboard Detection Results: VOC2012 intro: Competition “comp4” (train on additional data) homepage:&nbsp;http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4 Papers Deep Neural Networks for Object Detection paper:&nbsp;http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks arxiv:&nbsp;http://arxiv.org/abs/1312.6229 github:&nbsp;https://github.com/sermanet/OverFeat code:&nbsp;http://cilvr.nyu.edu/doku.php?id=software:overfeat:start R-CNN Rich feature hierarchies for accurate object detection and semantic segmentation intro: R-CNN arxiv:&nbsp;http://arxiv.org/abs/1311.2524 supp:&nbsp;http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf slides:&nbsp;http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf slides:&nbsp;http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf github:&nbsp;https://github.com/rbgirshick/rcnn notes:&nbsp;http://zhangliliang.com/2014/07/23/paper-note-rcnn/ caffe-pr(“Make R-CNN the Caffe detection example”):&nbsp;https://github.com/BVLC/caffe/pull/482 Fast R-CNN Fast R-CNN arxiv:&nbsp;http://arxiv.org/abs/1504.08083 slides:&nbsp;http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf github:&nbsp;https://github.com/rbgirshick/fast-rcnn github(COCO-branch):&nbsp;https://github.com/rbgirshick/fast-rcnn/tree/coco webcam demo:&nbsp;https://github.com/rbgirshick/fast-rcnn/pull/29 notes:&nbsp;http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/ notes:&nbsp;http://blog.csdn.net/linj_m/article/details/48930179 github(“Fast R-CNN in MXNet”):&nbsp;https://github.com/precedenceguo/mx-rcnn github:&nbsp;https://github.com/mahyarnajibi/fast-rcnn-torch github:&nbsp;https://github.com/apple2373/chainer-simple-fast-rnn github:&nbsp;https://github.com/zplizzi/tensorflow-fast-rcnn A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1704.03414 paper:&nbsp;http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf github(Caffe):&nbsp;https://github.com/xiaolonw/adversarial-frcnn Faster R-CNN Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks intro: NIPS 2015 arxiv:&nbsp;http://arxiv.org/abs/1506.01497 gitxiv:&nbsp;http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region slides:&nbsp;http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf github(official, Matlab):&nbsp;https://github.com/ShaoqingRen/faster_rcnn github:&nbsp;https://github.com/rbgirshick/py-faster-rcnn github:&nbsp;https://github.com/mitmul/chainer-faster-rcnn github:&nbsp;https://github.com/andreaskoepf/faster-rcnn.torch github:&nbsp;https://github.com/ruotianluo/Faster-RCNN-Densecap-torch github:&nbsp;https://github.com/smallcorgi/Faster-RCNN_TF github:&nbsp;https://github.com/CharlesShang/TFFRCNN github(C++ demo):&nbsp;https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus github:&nbsp;https://github.com/yhenon/keras-frcnn Faster R-CNN in MXNet with distributed implementation and data parallelization github:&nbsp;https://github.com/dmlc/mxnet/tree/master/example/rcnn Contextual Priming and Feedback for Faster R-CNN intro: ECCV 2016. Carnegie Mellon University paper:&nbsp;http://abhinavsh.info/context_priming_feedback.pdf poster:&nbsp;http://www.eccv2016.org/files/posters/P-1A-20.pdf An Implementation of Faster RCNN with Study for Region Sampling intro: Technical Report, 3 pages. CMU arxiv:&nbsp;https://arxiv.org/abs/1702.02138 github:&nbsp;https://github.com/endernewton/tf-faster-rcnn MultiBox Scalable Object Detection using Deep Neural Networks intro: first MultiBox. Train a CNN to predict Region of Interest. arxiv:&nbsp;http://arxiv.org/abs/1312.2249 github:&nbsp;https://github.com/google/multibox blog:&nbsp;https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html Scalable, High-Quality Object Detection intro: second MultiBox arxiv:&nbsp;http://arxiv.org/abs/1412.1441 github:&nbsp;https://github.com/google/multibox SPP-Net Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition intro: ECCV 2014 / TPAMI 2015 arxiv:&nbsp;http://arxiv.org/abs/1406.4729 github:&nbsp;https://github.com/ShaoqingRen/SPP_net notes:&nbsp;http://zhangliliang.com/2014/09/13/paper-note-sppnet/ DeepID-Net DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection intro: PAMI 2016 intro: an extension of R-CNN. box pre-training, cascade on region proposals, deformation layers and context representations project page:&nbsp;http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html arxiv:&nbsp;http://arxiv.org/abs/1412.5661 Object Detectors Emerge in Deep Scene CNNs intro: ICLR 2015 arxiv:&nbsp;http://arxiv.org/abs/1412.6856 paper:&nbsp;https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf paper:&nbsp;https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf slides:&nbsp;http://places.csail.mit.edu/slide_iclr2015.pdf segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection intro: CVPR 2015 project(code+data):&nbsp;https://www.cs.toronto.edu/~yukun/segdeepm.html arxiv:&nbsp;https://arxiv.org/abs/1502.04275 github:&nbsp;https://github.com/YknZhu/segDeepM NoC Object Detection Networks on Convolutional Feature Maps intro: TPAMI 2015 arxiv:&nbsp;http://arxiv.org/abs/1504.06066 Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction arxiv:&nbsp;http://arxiv.org/abs/1504.03293 slides:&nbsp;http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf github:&nbsp;https://github.com/YutingZhang/fgs-obj DeepBox DeepBox: Learning Objectness with Convolutional Networks arxiv:&nbsp;http://arxiv.org/abs/1505.02146 github:&nbsp;https://github.com/weichengkuo/DeepBox MR-CNN Object detection via a multi-region &amp; semantic segmentation-aware CNN model intro: ICCV 2015. MR-CNN arxiv:&nbsp;http://arxiv.org/abs/1505.01749 github:&nbsp;https://github.com/gidariss/mrcnn-object-detection notes:&nbsp;http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/ notes:&nbsp;http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/ YOLO You Only Look Once: Unified, Real-Time Object Detection arxiv:&nbsp;http://arxiv.org/abs/1506.02640 code:&nbsp;http://pjreddie.com/darknet/yolo/ github:&nbsp;https://github.com/pjreddie/darknet blog:&nbsp;https://pjreddie.com/publications/yolo/ slides:&nbsp;https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p reddit:&nbsp;https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/ github:&nbsp;https://github.com/gliese581gg/YOLO_tensorflow github:&nbsp;https://github.com/xingwangsfu/caffe-yolo github:&nbsp;https://github.com/frankzhangrui/Darknet-Yolo github:&nbsp;https://github.com/BriSkyHekun/py-darknet-yolo github:&nbsp;https://github.com/tommy-qichang/yolo.torch github:&nbsp;https://github.com/frischzenger/yolo-windows github:&nbsp;https://github.com/AlexeyAB/yolo-windows github:&nbsp;https://github.com/nilboy/tensorflow-yolo darkflow - translate darknet to tensorflow. Load trained weights, retrain/fine-tune them using tensorflow, export constant graph def to C++ blog:&nbsp;https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp github:&nbsp;https://github.com/thtrieu/darkflow Start Training YOLO with Our Own Data intro: train with customized data and class numbers/labels. Linux / Windows version for darknet. blog:&nbsp;http://guanghan.info/blog/en/my-works/train-yolo/ github:&nbsp;https://github.com/Guanghan/darknet YOLO: Core ML versus MPSNNGraph intro: Tiny YOLO for iOS implemented using CoreML but also using the new MPS graph API. blog:&nbsp;http://machinethink.net/blog/yolo-coreml-versus-mps-graph/ github:&nbsp;https://github.com/hollance/YOLO-CoreML-MPSNNGraph TensorFlow YOLO object detection on Android intro: Real-time object detection on Android using the YOLO network with TensorFlow github:&nbsp;https://github.com/natanielruiz/android-yolo Computer Vision in iOS – Object Detection blog:&nbsp;https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/ github:https://github.com/r4ghu/iOS-CoreML-Yolo YOLOv2 YOLO9000: Better, Faster, Stronger arxiv:&nbsp;https://arxiv.org/abs/1612.08242 code:&nbsp;http://pjreddie.com/yolo9000/ github(Chainer):&nbsp;https://github.com/leetenki/YOLOv2 github(Keras):&nbsp;https://github.com/allanzelener/YAD2K github(PyTorch):&nbsp;https://github.com/longcw/yolo2-pytorch github(Tensorflow):&nbsp;https://github.com/hizhangp/yolo_tensorflow github(Windows):&nbsp;https://github.com/AlexeyAB/darknet github:&nbsp;https://github.com/choasUp/caffe-yolo9000 github:&nbsp;https://github.com/philipperemy/yolo-9000 Yolo_mark: GUI for marking bounded boxes of objects in images for training Yolo v2 github:&nbsp;https://github.com/AlexeyAB/Yolo_mark R-CNN minus R arxiv:&nbsp;http://arxiv.org/abs/1506.06981 AttentionNet AttentionNet: Aggregating Weak Directions for Accurate Object Detection intro: ICCV 2015 intro: state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 human detection task arxiv:&nbsp;http://arxiv.org/abs/1506.07704 slides:&nbsp;https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf slides:&nbsp;http://image-net.org/challenges/talks/lunit-kaist-slide.pdf DenseBox DenseBox: Unifying Landmark Localization with End to End Object Detection arxiv:&nbsp;http://arxiv.org/abs/1509.04874 demo:&nbsp;http://pan.baidu.com/s/1mgoWWsS KITTI result:&nbsp;http://www.cvlibs.net/datasets/kitti/eval_object.php SSD SSD: Single Shot MultiBox Detector intro: ECCV 2016 Oral arxiv:&nbsp;http://arxiv.org/abs/1512.02325 paper:&nbsp;http://www.cs.unc.edu/~wliu/papers/ssd.pdf slides:&nbsp;http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf github(Official):&nbsp;https://github.com/weiliu89/caffe/tree/ssd video:&nbsp;http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973 github:&nbsp;https://github.com/zhreshold/mxnet-ssd github:&nbsp;https://github.com/zhreshold/mxnet-ssd.cpp github:&nbsp;https://github.com/rykov8/ssd_keras github:&nbsp;https://github.com/balancap/SSD-Tensorflow github:&nbsp;https://github.com/amdegroot/ssd.pytorch github(Caffe):&nbsp;https://github.com/chuanqi305/MobileNet-SSD What’s the diffience in performance between this new code you pushed and the previous code? #327 https://github.com/weiliu89/caffe/issues/327 Enhancement of SSD by concatenating feature maps for object detection intro: rainbow SSD (R-SSD) arxiv:&nbsp;https://arxiv.org/abs/1705.09587 DSSD DSSD : Deconvolutional Single Shot Detector intro: UNC Chapel Hill &amp; Amazon Inc arxiv:&nbsp;https://arxiv.org/abs/1701.06659 demo:&nbsp;http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4 Context-aware Single-Shot Detector keywords: CSSD, DiCSSD, DeCSSD, effective receptive fields (ERFs), theoretical receptive fields (TRFs) arxiv:&nbsp;https://arxiv.org/abs/1707.08682 Feature-Fused SSD: Fast Detection for Small Objects https://arxiv.org/abs/1709.05054 Inside-Outside Net (ION) Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks intro: “0.8s per image on a Titan X GPU (excluding proposal generation) without two-stage bounding-box regression and 1.15s per image with it”. arxiv:&nbsp;http://arxiv.org/abs/1512.04143 slides:&nbsp;http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf coco-leaderboard:&nbsp;http://mscoco.org/dataset/#detections-leaderboard Adaptive Object Detection Using Adjacency and Zoom Prediction intro: CVPR 2016. AZ-Net arxiv:&nbsp;http://arxiv.org/abs/1512.07711 github:&nbsp;https://github.com/luyongxi/az-net youtube:&nbsp;https://www.youtube.com/watch?v=YmFtuNwxaNM G-CNN G-CNN: an Iterative Grid Based Object Detector arxiv:&nbsp;http://arxiv.org/abs/1512.07729 Factors in Finetuning Deep Model for object detection Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution intro: CVPR 2016.rank 3rd for provided data and 2nd for external data on ILSVRC 2015 object detection project page:&nbsp;http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html arxiv:&nbsp;http://arxiv.org/abs/1601.05150 We don’t need no bounding-boxes: Training object class detectors using only human verification arxiv:&nbsp;http://arxiv.org/abs/1602.08405 HyperNet HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection arxiv:&nbsp;http://arxiv.org/abs/1604.00600 MultiPathNet A MultiPath Network for Object Detection intro: BMVC 2016. Facebook AI Research (FAIR) arxiv:&nbsp;http://arxiv.org/abs/1604.02135 github:&nbsp;https://github.com/facebookresearch/multipathnet CRAFT CRAFT Objects from Images intro: CVPR 2016. Cascade Region-proposal-network And FasT-rcnn. an extension of Faster R-CNN project page:&nbsp;http://byangderek.github.io/projects/craft.html arxiv:&nbsp;https://arxiv.org/abs/1604.03239 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf github:&nbsp;https://github.com/byangderek/CRAFT OHEM Training Region-based Object Detectors with Online Hard Example Mining intro: CVPR 2016 Oral. Online hard example mining (OHEM) arxiv:&nbsp;http://arxiv.org/abs/1604.03540 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf github(Official):&nbsp;https://github.com/abhi2610/ohem author page:&nbsp;http://abhinav-shrivastava.info/ Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers intro: CVPR 2016 keywords: scale-dependent pooling (SDP), cascaded rejection classifiers (CRC) paper:&nbsp;http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf R-FCN R-FCN: Object Detection via Region-based Fully Convolutional Networks arxiv:&nbsp;http://arxiv.org/abs/1605.06409 github:&nbsp;https://github.com/daijifeng001/R-FCN github:&nbsp;https://github.com/Orpine/py-R-FCN github:&nbsp;https://github.com/PureDiors/pytorch_RFCN github:&nbsp;https://github.com/bharatsingh430/py-R-FCN-multiGPU github:&nbsp;https://github.com/xdever/RFCN-tensorflow Recycle deep features for better object detection arxiv:&nbsp;http://arxiv.org/abs/1607.05066 MS-CNN A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection intro: ECCV 2016 intro: 640×480: 15 fps, 960×720: 8 fps arxiv:&nbsp;http://arxiv.org/abs/1607.07155 github:&nbsp;https://github.com/zhaoweicai/mscnn poster:&nbsp;http://www.eccv2016.org/files/posters/P-2B-38.pdf Multi-stage Object Detection with Group Recursive Learning intro: VOC2007: 78.6%, VOC2012: 74.9% arxiv:&nbsp;http://arxiv.org/abs/1608.05159 Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection intro: WACV 2017. SubCNN arxiv:&nbsp;http://arxiv.org/abs/1604.04693 github:&nbsp;https://github.com/tanshen/SubCNN PVANET PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection intro: “less channels with more layers”, concatenated ReLU, Inception, and HyperNet, batch normalization, residual connections arxiv:&nbsp;http://arxiv.org/abs/1608.08021 github:&nbsp;https://github.com/sanghoon/pva-faster-rcnn leaderboard(PVANet 9.0):&nbsp;http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4 PVANet: Lightweight Deep Neural Networks for Real-time Object Detection intro: Presented at NIPS 2016 Workshop on Efficient Methods for Deep Neural Networks (EMDNN). Continuation of&nbsp;arXiv:1608.08021 arxiv:&nbsp;https://arxiv.org/abs/1611.08588 GBD-Net Gated Bi-directional CNN for Object Detection intro: The Chinese University of Hong Kong &amp; Sensetime Group Limited paper:&nbsp;http://link.springer.com/chapter/10.1007/978-3-319-46478-7_22 mirror:&nbsp;https://pan.baidu.com/s/1dFohO7v Crafting GBD-Net for Object Detection intro: winner of the ImageNet object detection challenge of 2016. CUImage and CUVideo intro: gated bi-directional CNN (GBD-Net) arxiv:&nbsp;https://arxiv.org/abs/1610.02579 github:&nbsp;https://github.com/craftGBD/craftGBD StuffNet StuffNet: Using ‘Stuff’ to Improve Object Detection arxiv:&nbsp;https://arxiv.org/abs/1610.05861 Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene arxiv:&nbsp;https://arxiv.org/abs/1610.09609 Hierarchical Object Detection with Deep Reinforcement Learning intro: Deep Reinforcement Learning Workshop (NIPS 2016) project page:&nbsp;https://imatge-upc.github.io/detection-2016-nipsws/ arxiv:&nbsp;https://arxiv.org/abs/1611.03718 slides:&nbsp;http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning github:&nbsp;https://github.com/imatge-upc/detection-2016-nipsws blog:&nbsp;http://jorditorres.org/nips/ Learning to detect and localize many objects from few examples arxiv:&nbsp;https://arxiv.org/abs/1611.05664 Speed/accuracy trade-offs for modern convolutional object detectors intro: Google Research arxiv:&nbsp;https://arxiv.org/abs/1611.10012 SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving arxiv:&nbsp;https://arxiv.org/abs/1612.01051 github:&nbsp;https://github.com/BichenWuUCB/squeezeDet github:&nbsp;https://github.com/fregu856/2D_detection Feature Pyramid Network (FPN) Feature Pyramid Networks for Object Detection intro: Facebook AI Research arxiv:&nbsp;https://arxiv.org/abs/1612.03144 Action-Driven Object Detection with Top-Down Visual Attentions arxiv:&nbsp;https://arxiv.org/abs/1612.06704 Beyond Skip Connections: Top-Down Modulation for Object Detection intro: CMU &amp; UC Berkeley &amp; Google Research arxiv:&nbsp;https://arxiv.org/abs/1612.06851 Wide-Residual-Inception Networks for Real-time Object Detection intro: Inha University arxiv:&nbsp;https://arxiv.org/abs/1702.01243 Attentional Network for Visual Object Detection intro: University of Maryland &amp; Mitsubishi Electric Research Laboratories arxiv:&nbsp;https://arxiv.org/abs/1702.01478 CC-Net Learning Chained Deep Features and Classifiers for Cascade in Object Detection intro: chained cascade network (CC-Net). 81.1% mAP on PASCAL VOC 2007 arxiv:&nbsp;https://arxiv.org/abs/1702.07054 DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling https://arxiv.org/abs/1703.10295 Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1704.03944 Spatial Memory for Context Reasoning in Object Detection arxiv:&nbsp;https://arxiv.org/abs/1704.04224 Accurate Single Stage Detector Using Recurrent Rolling Convolution intro: CVPR 2017. SenseTime keywords: Recurrent Rolling Convolution (RRC) arxiv:&nbsp;https://arxiv.org/abs/1704.05776 github:&nbsp;https://github.com/xiaohaoChen/rrc_detection Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection https://arxiv.org/abs/1704.05775 S-OHEM: Stratified Online Hard Example Mining for Object Detection https://arxiv.org/abs/1705.02233 LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems intro: Embedded Vision Workshop in CVPR. UC San Diego &amp; Qualcomm Inc arxiv:&nbsp;https://arxiv.org/abs/1705.05922 Point Linking Network for Object Detection intro: Point Linking Network (PLN) arxiv:&nbsp;https://arxiv.org/abs/1706.03646 Perceptual Generative Adversarial Networks for Small Object Detection https://arxiv.org/abs/1706.05274 Few-shot Object Detection https://arxiv.org/abs/1706.08249 Yes-Net: An effective Detector Based on Global Information https://arxiv.org/abs/1706.09180 SMC Faster R-CNN: Toward a scene-specialized multi-object detector https://arxiv.org/abs/1706.10217 Towards lightweight convolutional neural networks for object detection https://arxiv.org/abs/1707.01395 RON: Reverse Connection with Objectness Prior Networks for Object Detection intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1707.01691 github:&nbsp;https://github.com/taokong/RON Residual Features and Unified Prediction Network for Single Stage Detection https://arxiv.org/abs/1707.05031 Deformable Part-based Fully Convolutional Network for Object Detection intro: BMVC 2017 (oral). Sorbonne Universités &amp; CEDRIC arxiv:&nbsp;https://arxiv.org/abs/1707.06175 Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1707.06399 Recurrent Scale Approximation for Object Detection in CNN intro: ICCV 2017 keywords: Recurrent Scale Approximation (RSA) arxiv:&nbsp;https://arxiv.org/abs/1707.09531 github:&nbsp;https://github.com/sciencefans/RSA-for-object-detection DSOD DSOD: Learning Deeply Supervised Object Detectors from Scratch intro: ICCV 2017. Fudan University &amp; Tsinghua University &amp; Intel Labs China arxiv:&nbsp;https://arxiv.org/abs/1708.01241 github:&nbsp;https://github.com/szq0214/DSOD Focal Loss for Dense Object Detection intro: ICCV 2017 Best student paper award. Facebook AI Research keywords: RetinaNet arxiv:&nbsp;https://arxiv.org/abs/1708.02002 CoupleNet: Coupling Global Structure with Local Parts for Object Detection intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.02863 Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection https://arxiv.org/abs/1709.04347 StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection https://arxiv.org/abs/1709.05788 NMS End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression intro: CVPR 2015 arxiv:&nbsp;http://arxiv.org/abs/1411.5309 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf A convnet for non-maximum suppression arxiv:&nbsp;http://arxiv.org/abs/1511.06437 Improving Object Detection With One Line of Code Soft-NMS – Improving Object Detection With One Line of Code intro: ICCV 2017. University of Maryland keywords: Soft-NMS arxiv:&nbsp;https://arxiv.org/abs/1704.04503 github:&nbsp;https://github.com/bharatsingh430/soft-nms Learning non-maximum suppression https://arxiv.org/abs/1705.02950 Weakly Supervised Object Detection Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection intro: CVPR 2016 arxiv:&nbsp;http://arxiv.org/abs/1604.05766 Weakly supervised object detection using pseudo-strong labels arxiv:&nbsp;http://arxiv.org/abs/1607.04731 Saliency Guided End-to-End Learning for Weakly Supervised Object Detection intro: IJCAI 2017 arxiv:&nbsp;https://arxiv.org/abs/1706.06768 Detection From Video Learning Object Class Detectors from Weakly Annotated Video intro: CVPR 2012 paper:&nbsp;https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf Analysing domain shift factors between videos and images for object detection arxiv:&nbsp;https://arxiv.org/abs/1501.01186 Video Object Recognition slides:&nbsp;http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx Deep Learning for Saliency Prediction in Natural Video intro: Submitted on 12 Jan 2016 keywords: Deep learning, saliency map, optical flow, convolution network, contrast features paper:&nbsp;https://hal.archives-ouvertes.fr/hal-01251614/document T-CNN T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos intro: Winning solution in ILSVRC2015 Object Detection from Video(VID) Task arxiv:&nbsp;http://arxiv.org/abs/1604.02532 github:&nbsp;https://github.com/myfavouritekk/T-CNN Object Detection from Video Tubelets with Convolutional Neural Networks intro: CVPR 2016 Spotlight paper arxiv:&nbsp;https://arxiv.org/abs/1604.04053 paper:&nbsp;http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf gihtub:&nbsp;https://github.com/myfavouritekk/vdetlib Object Detection in Videos with Tubelets and Multi-context Cues intro: SenseTime Group slides:&nbsp;http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf slides:&nbsp;http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf Context Matters: Refining Object Detection in Video with Recurrent Neural Networks intro: BMVC 2016 keywords: pseudo-labeler arxiv:&nbsp;http://arxiv.org/abs/1607.04648 paper:&nbsp;http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf CNN Based Object Detection in Large Video Images intro: WangTao @ 爱奇艺 keywords: object retrieval, object detection, scene classification slides:&nbsp;http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf Object Detection in Videos with Tubelet Proposal Networks arxiv:&nbsp;https://arxiv.org/abs/1702.06355 Flow-Guided Feature Aggregation for Video Object Detection intro: MSRA arxiv:&nbsp;https://arxiv.org/abs/1703.10025 Video Object Detection using Faster R-CNN blog:&nbsp;http://andrewliao11.github.io/object_detection/faster_rcnn/ github:&nbsp;https://github.com/andrewliao11/py-faster-rcnn-imagenet Improving Context Modeling for Video Object Detection and Tracking http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster).pdf Temporal Dynamic Graph LSTM for Action-driven Video Object Detection intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.00666 Object Detection in 3D Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks arxiv:&nbsp;https://arxiv.org/abs/1609.06666 Object Detection on RGB-D Learning Rich Features from RGB-D Images for Object Detection and Segmentation arxiv:&nbsp;http://arxiv.org/abs/1407.5736 Differential Geometry Boosts Convolutional Neural Networks for Object Detection intro: CVPR 2016 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation https://arxiv.org/abs/1703.03347 Salient Object Detection This task involves predicting the salient regions of an image given by human eye fixations. Best Deep Saliency Detection Models (CVPR 2016 &amp; 2015) http://i.cs.hku.hk/~yzyu/vision.html Large-scale optimization of hierarchical features for saliency prediction in natural images paper:&nbsp;http://coxlab.org/pdfs/cvpr2014_vig_saliency.pdf Predicting Eye Fixations using Convolutional Neural Networks paper:&nbsp;http://www.escience.cn/system/file?fileId=72648 Saliency Detection by Multi-Context Deep Learning paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection arxiv:&nbsp;http://arxiv.org/abs/1510.05484 SuperCNN: A Superpixelwise Convolutional Neural Network for Salient Object Detection paper:&nbsp;www.shengfenghe.com/supercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html Shallow and Deep Convolutional Networks for Saliency Prediction intro: CVPR 2016 arxiv:&nbsp;http://arxiv.org/abs/1603.00845 github:&nbsp;https://github.com/imatge-upc/saliency-2016-cvpr Recurrent Attentional Networks for Saliency Detection intro: CVPR 2016. recurrent attentional convolutional-deconvolution network (RACDNN) arxiv:&nbsp;http://arxiv.org/abs/1604.03227 Two-Stream Convolutional Networks for Dynamic Saliency Prediction arxiv:&nbsp;http://arxiv.org/abs/1607.04730 Unconstrained Salient Object Detection Unconstrained Salient Object Detection via Proposal Subset Optimization intro: CVPR 2016 project page:&nbsp;http://cs-people.bu.edu/jmzhang/sod.html paper:&nbsp;http://cs-people.bu.edu/jmzhang/SOD/CVPR16SOD_camera_ready.pdf github:&nbsp;https://github.com/jimmie33/SOD caffe model zoo:&nbsp;https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-object-proposal-models-for-salient-object-detection DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf Salient Object Subitizing intro: CVPR 2015 intro: predicting the existence and the number of salient objects in an image using holistic cues project page:&nbsp;http://cs-people.bu.edu/jmzhang/sos.html arxiv:&nbsp;http://arxiv.org/abs/1607.07525 paper:&nbsp;http://cs-people.bu.edu/jmzhang/SOS/SOS_preprint.pdf caffe model zoo:&nbsp;https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-models-for-salient-object-subitizing Deeply-Supervised Recurrent Convolutional Neural Network for Saliency Detection intro: ACMMM 2016. deeply-supervised recurrent convolutional neural network (DSRCNN) arxiv:&nbsp;http://arxiv.org/abs/1608.05177 Saliency Detection via Combining Region-Level and Pixel-Level Predictions with CNNs intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1608.05186 Edge Preserving and Multi-Scale Contextual Neural Network for Salient Object Detection arxiv:&nbsp;http://arxiv.org/abs/1608.08029 A Deep Multi-Level Network for Saliency Prediction arxiv:&nbsp;http://arxiv.org/abs/1609.01064 Visual Saliency Detection Based on Multiscale Deep CNN Features intro: IEEE Transactions on Image Processing arxiv:&nbsp;http://arxiv.org/abs/1609.02077 A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection intro: DSCLRCN arxiv:&nbsp;https://arxiv.org/abs/1610.01708 Deeply supervised salient object detection with short connections arxiv:&nbsp;https://arxiv.org/abs/1611.04849 Weakly Supervised Top-down Salient Object Detection intro: Nanyang Technological University arxiv:&nbsp;https://arxiv.org/abs/1611.05345 SalGAN: Visual Saliency Prediction with Generative Adversarial Networks project page:&nbsp;https://imatge-upc.github.io/saliency-salgan-2017/ arxiv:&nbsp;https://arxiv.org/abs/1701.01081 Visual Saliency Prediction Using a Mixture of Deep Neural Networks arxiv:&nbsp;https://arxiv.org/abs/1702.00372 A Fast and Compact Salient Score Regression Network Based on Fully Convolutional Network arxiv:&nbsp;https://arxiv.org/abs/1702.00615 Saliency Detection by Forward and Backward Cues in Deep-CNNs https://arxiv.org/abs/1703.00152 Supervised Adversarial Networks for Image Saliency Detection https://arxiv.org/abs/1704.07242 Group-wise Deep Co-saliency Detection https://arxiv.org/abs/1707.07381 Towards the Success Rate of One: Real-time Unconstrained Salient Object Detection intro: University of Maryland College Park &amp; eBay Inc arxiv:&nbsp;https://arxiv.org/abs/1708.00079 Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection intro: ICCV 2017 arixv:&nbsp;https://arxiv.org/abs/1708.02001 Learning Uncertain Convolutional Features for Accurate Saliency Detection intro: Accepted as a poster in ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.02031 Deep Edge-Aware Saliency Detection https://arxiv.org/abs/1708.04366 Self-explanatory Deep Salient Object Detection intro: National University of Defense Technology, China &amp; National University of Singapore arxiv:&nbsp;https://arxiv.org/abs/1708.05595 PiCANet: Learning Pixel-wise Contextual Attention in ConvNets and Its Application in Saliency Detection https://arxiv.org/abs/1708.06433 DeepFeat: A Bottom Up and Top Down Saliency Model Based on Deep Features of Convolutional Neural Nets https://arxiv.org/abs/1709.02495 Saliency Detection in Video Deep Learning For Video Saliency Detection arxiv:&nbsp;https://arxiv.org/abs/1702.00871 Video Salient Object Detection Using Spatiotemporal Deep Features https://arxiv.org/abs/1708.01447 Predicting Video Saliency with Object-to-Motion CNN and Two-layer Convolutional LSTM https://arxiv.org/abs/1709.06316 Visual Relationship Detection Visual Relationship Detection with Language Priors intro: ECCV 2016 oral paper:&nbsp;https://cs.stanford.edu/people/ranjaykrishna/vrd/vrd.pdf github:&nbsp;https://github.com/Prof-Lu-Cewu/Visual-Relationship-Detection ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection intro: Visual Phrase reasoning Convolutional Neural Network (ViP-CNN), Visual Phrase Reasoning Structure (VPRS) arxiv:&nbsp;https://arxiv.org/abs/1702.07191 Visual Translation Embedding Network for Visual Relation Detection arxiv:&nbsp;https://www.arxiv.org/abs/1702.08319 Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection intro: CVPR 2017 spotlight paper arxiv:&nbsp;https://arxiv.org/abs/1703.03054 Detecting Visual Relationships with Deep Relational Networks intro: CVPR 2017 oral. The Chinese University of Hong Kong arxiv:&nbsp;https://arxiv.org/abs/1704.03114 Identifying Spatial Relations in Images using Convolutional Neural Networks https://arxiv.org/abs/1706.04215 PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN intro: ICCV arxiv:&nbsp;https://arxiv.org/abs/1708.01956 Specific Object Deteciton Deep Deformation Network for Object Landmark Localization arxiv:&nbsp;http://arxiv.org/abs/1605.01014 Fashion Landmark Detection in the Wild intro: ECCV 2016 project page:&nbsp;http://personal.ie.cuhk.edu.hk/~lz013/projects/FashionLandmarks.html arxiv:&nbsp;http://arxiv.org/abs/1608.03049 github(Caffe):&nbsp;https://github.com/liuziwei7/fashion-landmarks Deep Learning for Fast and Accurate Fashion Item Detection intro: Kuznech Inc. intro: MultiBox and Fast R-CNN paper:&nbsp;https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf OSMDeepOD - OSM and Deep Learning based Object Detection from Aerial Imagery (formerly known as “OSM-Crosswalk-Detection”) github:&nbsp;https://github.com/geometalab/OSMDeepOD Selfie Detection by Synergy-Constraint Based Convolutional Neural Network intro: IEEE SITIS 2016 arxiv:&nbsp;https://arxiv.org/abs/1611.04357 Associative Embedding:End-to-End Learning for Joint Detection and Grouping arxiv:&nbsp;https://arxiv.org/abs/1611.05424 Deep Cuboid Detection: Beyond 2D Bounding Boxes intro: CMU &amp; Magic Leap arxiv:&nbsp;https://arxiv.org/abs/1611.10010 Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection arxiv:&nbsp;https://arxiv.org/abs/1612.03019 Deep Learning Logo Detection with Data Expansion by Synthesising Context arxiv:&nbsp;https://arxiv.org/abs/1612.09322 Pixel-wise Ear Detection with Convolutional Encoder-Decoder Networks arxiv:&nbsp;https://arxiv.org/abs/1702.00307 Automatic Handgun Detection Alarm in Videos Using Deep Learning arxiv:&nbsp;https://arxiv.org/abs/1702.05147 results:&nbsp;https://github.com/SihamTabik/Pistol-Detection-in-Videos Using Deep Networks for Drone Detection intro: AVSS 2017 arxiv:&nbsp;https://arxiv.org/abs/1706.05726 Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.01642 DeepVoting: An Explainable Framework for Semantic Part Detection under Partial Occlusion https://arxiv.org/abs/1709.04577 Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network https://arxiv.org/abs/1709.09283 Face Deteciton Multi-view Face Detection Using Deep Convolutional Neural Networks intro: Yahoo arxiv:&nbsp;http://arxiv.org/abs/1502.02766 github:&nbsp;https://github.com/guoyilin/FaceDetection_CNN From Facial Parts Responses to Face Detection: A Deep Learning Approach intro: ICCV 2015. CUHK project page:&nbsp;http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/Faceness.html arxiv:&nbsp;https://arxiv.org/abs/1509.06451 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf Compact Convolutional Neural Network Cascade for Face Detection arxiv:&nbsp;http://arxiv.org/abs/1508.01292 github:&nbsp;https://github.com/Bkmz21/FD-Evaluation github:&nbsp;https://github.com/Bkmz21/CompactCNNCascade Face Detection with End-to-End Integration of a ConvNet and a 3D Model intro: ECCV 2016 arxiv:&nbsp;https://arxiv.org/abs/1606.00850 github(MXNet):&nbsp;https://github.com/tfwu/FaceDetection-ConvNet-3D CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection intro: CMU arxiv:&nbsp;https://arxiv.org/abs/1606.05413 Finding Tiny Faces intro: CVPR 2017. CMU project page:&nbsp;http://www.cs.cmu.edu/~peiyunh/tiny/index.html arxiv:&nbsp;https://arxiv.org/abs/1612.04402 github:&nbsp;https://github.com/peiyunh/tiny github(inference-only):&nbsp;https://github.com/chinakook/hr101_mxnet Towards a Deep Learning Framework for Unconstrained Face Detection intro: overlap with CMS-RCNN arxiv:&nbsp;https://arxiv.org/abs/1612.05322 Supervised Transformer Network for Efficient Face Detection arxiv:&nbsp;http://arxiv.org/abs/1607.05477 UnitBox UnitBox: An Advanced Object Detection Network intro: ACM MM 2016 arxiv:&nbsp;http://arxiv.org/abs/1608.01471 Bootstrapping Face Detection with Hard Negative Examples author: 万韶华 @ 小米. intro: Faster R-CNN, hard negative mining. state-of-the-art on the FDDB dataset arxiv:&nbsp;http://arxiv.org/abs/1608.02236 Grid Loss: Detecting Occluded Faces intro: ECCV 2016 arxiv:&nbsp;https://arxiv.org/abs/1609.00129 paper:&nbsp;http://lrs.icg.tugraz.at/pubs/opitz_eccv_16.pdf poster:&nbsp;http://www.eccv2016.org/files/posters/P-2A-34.pdf A Multi-Scale Cascade Fully Convolutional Network Face Detector intro: ICPR 2016 arxiv:&nbsp;http://arxiv.org/abs/1609.03536 MTCNN Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks project page:&nbsp;https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html arxiv:&nbsp;https://arxiv.org/abs/1604.02878 github(Matlab):&nbsp;https://github.com/kpzhang93/MTCNN_face_detection_alignment github:&nbsp;https://github.com/pangyupo/mxnet_mtcnn_face_detection github:&nbsp;https://github.com/DaFuCoding/MTCNN_Caffe github(MXNet):&nbsp;https://github.com/Seanlinx/mtcnn github:&nbsp;https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion github(Caffe):&nbsp;https://github.com/foreverYoungGitHub/MTCNN github:&nbsp;https://github.com/CongWeilin/mtcnn-caffe github:&nbsp;https://github.com/AlphaQi/MTCNN-light Face Detection using Deep Learning: An Improved Faster RCNN Approach intro: DeepIR Inc arxiv:&nbsp;https://arxiv.org/abs/1701.08289 Faceness-Net: Face Detection through Deep Facial Part Responses intro: An extended version of ICCV 2015 paper arxiv:&nbsp;https://arxiv.org/abs/1701.08393 Multi-Path Region-Based Convolutional Neural Network for Accurate Detection of Unconstrained “Hard Faces” intro: CVPR 2017. MP-RCNN, MP-RPN arxiv:&nbsp;https://arxiv.org/abs/1703.09145 End-To-End Face Detection and Recognition https://arxiv.org/abs/1703.10818 Face R-CNN https://arxiv.org/abs/1706.01061 Face Detection through Scale-Friendly Deep Convolutional Networks https://arxiv.org/abs/1706.02863 Scale-Aware Face Detection intro: CVPR 2017. SenseTime &amp; Tsinghua University arxiv:&nbsp;https://arxiv.org/abs/1706.09876 Multi-Branch Fully Convolutional Network for Face Detection https://arxiv.org/abs/1707.06330 SSH: Single Stage Headless Face Detector intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.03979 Dockerface: an easy to install and use Faster R-CNN face detector in a Docker container https://arxiv.org/abs/1708.04370 FaceBoxes: A CPU Real-time Face Detector with High Accuracy intro: IJCB 2017 keywords: Rapidly Digested Convolutional Layers (RDCL), Multiple Scale Convolutional Layers (MSCL) intro: the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images arxiv:&nbsp;https://arxiv.org/abs/1708.05234 S3FD: Single Shot Scale-invariant Face Detector intro: ICCV 2017 intro: can run at 36 FPS on a Nvidia Titan X (Pascal) for VGA-resolution images arxiv:&nbsp;https://arxiv.org/abs/1708.05237 Detecting Faces Using Region-based Fully Convolutional Networks https://arxiv.org/abs/1709.05256 AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection https://arxiv.org/abs/1709.07326 Facial Point / Landmark Detection Deep Convolutional Network Cascade for Facial Point Detection homepage:&nbsp;http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm paper:&nbsp;http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr13.pdf github:&nbsp;https://github.com/luoyetx/deep-landmark Facial Landmark Detection by Deep Multi-task Learning intro: ECCV 2014 project page:&nbsp;http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html paper:&nbsp;http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepfacealign.pdf github(Matlab):&nbsp;https://github.com/zhzhanp/TCDCN-face-alignment A Recurrent Encoder-Decoder Network for Sequential Face Alignment intro: ECCV 2016 arxiv:&nbsp;https://arxiv.org/abs/1608.05477 Detecting facial landmarks in the video based on a hybrid framework arxiv:&nbsp;http://arxiv.org/abs/1609.06441 Deep Constrained Local Models for Facial Landmark Detection arxiv:&nbsp;https://arxiv.org/abs/1611.08657 Effective face landmark localization via single deep network arxiv:&nbsp;https://arxiv.org/abs/1702.02719 A Convolution Tree with Deconvolution Branches: Exploiting Geometric Relationships for Single Shot Keypoint Detection https://arxiv.org/abs/1704.01880 Deep Alignment Network: A convolutional neural network for robust face alignment intro: CVPRW 2017 arxiv:&nbsp;https://arxiv.org/abs/1706.01789 gihtub:&nbsp;https://github.com/MarekKowalski/DeepAlignmentNetwork Joint Multi-view Face Alignment in the Wild https://arxiv.org/abs/1708.06023 FacePoseNet: Making a Case for Landmark-Free Face Alignment https://arxiv.org/abs/1708.07517 People Detection End-to-end people detection in crowded scenes arxiv:&nbsp;http://arxiv.org/abs/1506.04878 github:&nbsp;https://github.com/Russell91/reinspect ipn:&nbsp;http://nbviewer.ipython.org/github/Russell91/ReInspect/blob/master/evaluation_reinspect.ipynb youtube:&nbsp;https://www.youtube.com/watch?v=QeWl0h3kQ24 Detecting People in Artwork with CNNs intro: ECCV 2016 Workshops arxiv:&nbsp;https://arxiv.org/abs/1610.08871 Deep Multi-camera People Detection arxiv:&nbsp;https://arxiv.org/abs/1702.04593 Person Head Detection Context-aware CNNs for person head detection intro: ICCV 2015 project page:&nbsp;http://www.di.ens.fr/willow/research/headdetection/ arxiv:&nbsp;http://arxiv.org/abs/1511.07917 github:&nbsp;https://github.com/aosokin/cnn_head_detection Pedestrian Detection Pedestrian Detection aided by Deep Learning Semantic Tasks intro: CVPR 2015 project page:&nbsp;http://mmlab.ie.cuhk.edu.hk/projects/TA-CNN/ arxiv:&nbsp;http://arxiv.org/abs/1412.0069 Deep Learning Strong Parts for Pedestrian Detection intro: ICCV 2015. CUHK. DeepParts intro: Achieving 11.89% average miss rate on Caltech Pedestrian Dataset paper:&nbsp;http://personal.ie.cuhk.edu.hk/~pluo/pdf/tianLWTiccv15.pdf Taking a Deeper Look at Pedestrians intro: CVPR 2015 arxiv:&nbsp;https://arxiv.org/abs/1501.05790 Convolutional Channel Features intro: ICCV 2015 arxiv:&nbsp;https://arxiv.org/abs/1504.07339 github:&nbsp;https://github.com/byangderek/CCF Learning Complexity-Aware Cascades for Deep Pedestrian Detection intro: ICCV 2015 arxiv:&nbsp;https://arxiv.org/abs/1507.05348 Deep convolutional neural networks for pedestrian detection arxiv:&nbsp;http://arxiv.org/abs/1510.03608 github:&nbsp;https://github.com/DenisTome/DeepPed Scale-aware Fast R-CNN for Pedestrian Detection arxiv:&nbsp;https://arxiv.org/abs/1510.08160 New algorithm improves speed and accuracy of pedestrian detection blog:&nbsp;http://www.eurekalert.org/pub_releases/2016-02/uoc–nai020516.php Pushing the Limits of Deep CNNs for Pedestrian Detection intro: “set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from 11.7% to 8.9%” arxiv:&nbsp;http://arxiv.org/abs/1603.04525 A Real-Time Deep Learning Pedestrian Detector for Robot Navigation arxiv:&nbsp;http://arxiv.org/abs/1607.04436 A Real-Time Pedestrian Detector using Deep Learning for Human-Aware Navigation arxiv:&nbsp;http://arxiv.org/abs/1607.04441 Is Faster R-CNN Doing Well for Pedestrian Detection? intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1607.07032 github:&nbsp;https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian Reduced Memory Region Based Deep Convolutional Neural Network Detection intro: IEEE 2016 ICCE-Berlin arxiv:&nbsp;http://arxiv.org/abs/1609.02500 Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection arxiv:&nbsp;https://arxiv.org/abs/1610.03466 Multispectral Deep Neural Networks for Pedestrian Detection intro: BMVC 2016 oral arxiv:&nbsp;https://arxiv.org/abs/1611.02644 Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters intro: CVPR 2017 project page:&nbsp;http://ml.cs.tsinghua.edu.cn:5000/publications/synunity/ arxiv:&nbsp;https://arxiv.org/abs/1703.06283 github(Tensorflow):&nbsp;https://github.com/huangshiyu13/RPNplus Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation [https://arxiv.org/abs/1706.08564](https://arxiv.org/abs/1706.08564 Rotational Rectification Network for Robust Pedestrian Detection intro: CMU &amp; Volvo Construction arxiv:&nbsp;https://arxiv.org/abs/1706.08917 STD-PD: Generating Synthetic Training Data for Pedestrian Detection in Unannotated Videos intro: The University of North Carolina at Chapel Hill arxiv:&nbsp;https://arxiv.org/abs/1707.09100 Too Far to See? Not Really! — Pedestrian Detection with Scale-aware Localization Policy https://arxiv.org/abs/1709.00235 Vehicle Detection DAVE: A Unified Framework for Fast Vehicle Detection and Annotation intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1607.04564 Evolving Boxes for fast Vehicle Detection arxiv:&nbsp;https://arxiv.org/abs/1702.00254 Fine-Grained Car Detection for Visual Census Estimation intro: AAAI 2016 arxiv:&nbsp;https://arxiv.org/abs/1709.02480 Traffic-Sign Detection Traffic-Sign Detection and Classification in the Wild project page(code+dataset):&nbsp;http://cg.cs.tsinghua.edu.cn/traffic-sign/ paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf code &amp; model:&nbsp;http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/newdata0411.zip Detecting Small Signs from Large Images intro: IEEE Conference on Information Reuse and Integration (IRI) 2017 oral arxiv:&nbsp;https://arxiv.org/abs/1706.08574 Boundary / Edge / Contour Detection Holistically-Nested Edge Detection intro: ICCV 2015, Marr Prize paper:&nbsp;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf arxiv:&nbsp;http://arxiv.org/abs/1504.06375 github:&nbsp;https://github.com/s9xie/hed Unsupervised Learning of Edges intro: CVPR 2016. Facebook AI Research arxiv:&nbsp;http://arxiv.org/abs/1511.04166 zn-blog:&nbsp;http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html Pushing the Boundaries of Boundary Detection using Deep Learning arxiv:&nbsp;http://arxiv.org/abs/1511.07386 Convolutional Oriented Boundaries intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1608.02755 Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks project page:&nbsp;http://www.vision.ee.ethz.ch/~cvlsegmentation/ arxiv:&nbsp;https://arxiv.org/abs/1701.04658 github:&nbsp;https://github.com/kmaninis/COB Richer Convolutional Features for Edge Detection intro: CVPR 2017 keywords: richer convolutional features (RCF) arxiv:&nbsp;https://arxiv.org/abs/1612.02103 github:&nbsp;https://github.com/yun-liu/rcf Contour Detection from Deep Patch-level Boundary Prediction https://arxiv.org/abs/1705.03159 CASENet: Deep Category-Aware Semantic Edge Detection intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1705.09759 Skeleton Detection Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs arxiv:&nbsp;http://arxiv.org/abs/1603.09446 github:&nbsp;https://github.com/zeakey/DeepSkeleton DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images arxiv:&nbsp;http://arxiv.org/abs/1609.03659 SRN: Side-output Residual Network for Object Symmetry Detection in the Wild intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1703.02243 github:&nbsp;https://github.com/KevinKecc/SRN Fruit Detection Deep Fruit Detection in Orchards arxiv:&nbsp;https://arxiv.org/abs/1610.03677 Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards intro: The Journal of Field Robotics in May 2016 project page:&nbsp;http://confluence.acfr.usyd.edu.au/display/AGPub/ arxiv:&nbsp;https://arxiv.org/abs/1610.08120 Part Detection Objects as context for part detection https://arxiv.org/abs/1703.09529 Object Proposal DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers arxiv:&nbsp;http://arxiv.org/abs/1510.04445 github:&nbsp;https://github.com/aghodrati/deepproposal Scale-aware Pixel-wise Object Proposal Networks intro: IEEE Transactions on Image Processing arxiv:&nbsp;http://arxiv.org/abs/1601.04798 Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization intro: BMVC 2016. AttractioNet arxiv:&nbsp;https://arxiv.org/abs/1606.04446 github:&nbsp;https://github.com/gidariss/AttractioNet Learning to Segment Object Proposals via Recursive Neural Networks arxiv:&nbsp;https://arxiv.org/abs/1612.01057 Learning Detection with Diverse Proposals intro: CVPR 2017 keywords: differentiable Determinantal Point Process (DPP) layer, Learning Detection with Diverse Proposals (LDDP) arxiv:&nbsp;https://arxiv.org/abs/1704.03533 ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond keywords: product detection arxiv:&nbsp;https://arxiv.org/abs/1704.06752 Improving Small Object Proposals for Company Logo Detection intro: ICMR 2017 arxiv:&nbsp;https://arxiv.org/abs/1704.08881 Localization Beyond Bounding Boxes: Precise Localization of Objects in Images intro: PhD Thesis homepage:&nbsp;http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.html phd-thesis:&nbsp;http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf github(“SDS using hypercolumns”):&nbsp;https://github.com/bharath272/sds Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning arxiv:&nbsp;http://arxiv.org/abs/1503.00949 Weakly Supervised Object Localization Using Size Estimates arxiv:&nbsp;http://arxiv.org/abs/1608.04314 Active Object Localization with Deep Reinforcement Learning intro: ICCV 2015 keywords: Markov Decision Process arxiv:&nbsp;https://arxiv.org/abs/1511.06015 Localizing objects using referring expressions intro: ECCV 2016 keywords: LSTM, multiple instance learning (MIL) paper:&nbsp;http://www.umiacs.umd.edu/~varun/files/refexp-ECCV16.pdf github:&nbsp;https://github.com/varun-nagaraja/referring-expressions LocNet: Improving Localization Accuracy for Object Detection intro: CVPR 2016 oral arxiv:&nbsp;http://arxiv.org/abs/1511.07763 github:&nbsp;https://github.com/gidariss/LocNet Learning Deep Features for Discriminative Localization homepage:&nbsp;http://cnnlocalization.csail.mit.edu/ arxiv:&nbsp;http://arxiv.org/abs/1512.04150 github(Tensorflow):&nbsp;https://github.com/jazzsaxmafia/Weakly_detector github:&nbsp;https://github.com/metalbubble/CAM github:&nbsp;https://github.com/tdeboissiere/VGG16CAM-keras ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization intro: ECCV 2016 project page:&nbsp;http://www.di.ens.fr/willow/research/contextlocnet/ arxiv:&nbsp;http://arxiv.org/abs/1609.04331 github:&nbsp;https://github.com/vadimkantorov/contextlocnet Ensemble of Part Detectors for Simultaneous Classification and Localization https://arxiv.org/abs/1705.10034 STNet: Selective Tuning of Convolutional Networks for Object Localization https://arxiv.org/abs/1708.06418 Soft Proposal Networks for Weakly Supervised Object Localization intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1709.01829 Fine-grained Discriminative Localization via Saliency-guided Faster R-CNN intro: ACM MM 2017 arxiv:&nbsp;https://arxiv.org/abs/1709.08295 Tutorials / Talks Convolutional Feature Maps: Elements of efficient (and accurate) CNN-based object detection slides:&nbsp;http://research.microsoft.com/en-us/um/people/kahe/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf Towards Good Practices for Recognition &amp; Detection intro: Hikvision Research Institute. Supervised Data Augmentation (SDA) slides:&nbsp;http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf Projects TensorBox: a simple framework for training neural networks to detect objects in images intro: “The basic model implements the simple and robust GoogLeNet-OverFeat algorithm. We additionally provide an implementation of the&nbsp;ReInspect&nbsp;algorithm” github:&nbsp;https://github.com/Russell91/TensorBox Object detection in torch: Implementation of some object detection frameworks in torch github:&nbsp;https://github.com/fmassa/object-detection.torch Using DIGITS to train an Object Detection network github:&nbsp;https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/README.md FCN-MultiBox Detector intro: Full convolution MultiBox Detector (like SSD) implemented in Torch. github:&nbsp;https://github.com/teaonly/FMD.torch KittiBox: A car detection model implemented in Tensorflow. keywords: MultiNet intro: KittiBox is a collection of scripts to train out model FastBox on the Kitti Object Detection Dataset github:&nbsp;https://github.com/MarvinTeichmann/KittiBox Deformable Convolutional Networks + MST + Soft-NMS github:&nbsp;https://github.com/bharatsingh430/Deformable-ConvNets Tools BeaverDam: Video annotation tool for deep learning training labels https://github.com/antingshen/BeaverDam Blogs Convolutional Neural Networks for Object Detection http://rnd.azoft.com/convolutional-neural-networks-object-detection/ Introducing automatic object detection to visual search (Pinterest) keywords: Faster R-CNN blog:&nbsp;https://engineering.pinterest.com/blog/introducing-automatic-object-detection-visual-search demo:&nbsp;https://engineering.pinterest.com/sites/engineering/files/Visual%20Search%20V1%20-%20Video.mp4 review:&nbsp;https://news.developer.nvidia.com/pinterest-introduces-the-future-of-visual-search/?mkt_tok=eyJpIjoiTnpaa01UWXpPRE0xTURFMiIsInQiOiJJRjcybjkwTmtmallORUhLOFFFODBDclFqUlB3SWlRVXJXb1MrQ013TDRIMGxLQWlBczFIeWg0TFRUdnN2UHY2ZWFiXC9QQVwvQzBHM3B0UzBZblpOSmUyU1FcLzNPWXI4cml2VERwTTJsOFwvOEk9In0%3D Deep Learning for Object Detection with DIGITS blog:&nbsp;https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/ Analyzing The Papers Behind Facebook’s Computer Vision Approach keywords: DeepMask, SharpMask, MultiPathNet blog:&nbsp;&lt;a target=&quot;_blank&quot; href=&quot;https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook&#39; rel=&quot; nofollow&quot;s-computer-vision-approach &quot; style=&quot;margin:0px; padding:0px; color:rgb(145,115,107)&quot;&gt;https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook’s-Computer-Vision-Approach/&lt;/a&gt; Easily Create High Quality Object Detectors with Deep Learning intro: dlib v19.2 blog:&nbsp;http://blog.dlib.net/2016/10/easily-create-high-quality-object.html How to Train a Deep-Learned Object Detection Model in the Microsoft Cognitive Toolkit blog:&nbsp;https://blogs.technet.microsoft.com/machinelearning/2016/10/25/how-to-train-a-deep-learned-object-detection-model-in-cntk/ github:&nbsp;https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN Object Detection in Satellite Imagery, a Low Overhead Approach part 1:&nbsp;https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7#.2csh4iwx9 part 2:&nbsp;https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-ii-893f40122f92#.f9b7dgf64 You Only Look Twice — Multi-Scale Object Detection in Satellite Imagery With Convolutional Neural Networks part 1:&nbsp;https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571#.fmmi2o3of part 2:&nbsp;https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588#.nwzarsz1t Faster R-CNN Pedestrian and Car Detection blog:&nbsp;https://bigsnarf.wordpress.com/2016/11/07/faster-r-cnn-pedestrian-and-car-detection/ ipn:&nbsp;https://gist.github.com/bigsnarfdude/2f7b2144065f6056892a98495644d3e0#file-demo_faster_rcnn_notebook-ipynb github:&nbsp;https://github.com/bigsnarfdude/Faster-RCNN_TF Small U-Net for vehicle detection blog:&nbsp;https://medium.com/@vivek.yadav/small-u-net-for-vehicle-detection-9eec216f9fd6#.md4u80kad Region of interest pooling explained blog:&nbsp;https://deepsense.io/region-of-interest-pooling-explained/ github:&nbsp;https://github.com/deepsense-io/roi-pooling Supercharge your Computer Vision models with the TensorFlow Object Detection API blog:&nbsp;https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html github:&nbsp;https://github.com/tensorflow/models/tree/master/object_detection 阅读更多" />
<meta property="og:description" content="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html Jump to... Leaderboard Papers R-CNN Fast R-CNN Faster R-CNN MultiBox SPP-Net DeepID-Net NoC DeepBox MR-CNN YOLO YOLOv2 AttentionNet DenseBox SSD DSSD Inside-Outside Net (ION) G-CNN HyperNet MultiPathNet CRAFT OHEM R-FCN MS-CNN PVANET GBD-Net StuffNet Feature Pyramid Network (FPN) CC-Net DSOD NMS Weakly Supervised Object Detection Detection From Video T-CNN Object Detection in 3D Object Detection on RGB-D Salient Object Detection Saliency Detection in Video Visual Relationship Detection Specific Object Deteciton Face Deteciton UnitBox MTCNN Facial Point / Landmark Detection People Detection Person Head Detection Pedestrian Detection Vehicle Detection Traffic-Sign Detection Boundary / Edge / Contour Detection Skeleton Detection Fruit Detection Part Detection Object Proposal Localization Tutorials / Talks Projects Tools Blogs Method VOC2007 VOC2010 VOC2012 ILSVRC 2013 MSCOCO 2015 Speed OverFeat &nbsp; &nbsp; &nbsp; 24.3% &nbsp; &nbsp; R-CNN (AlexNet) 58.5% 53.7% 53.3% 31.4% &nbsp; &nbsp; R-CNN (VGG16) 66.0% &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; SPP_net(ZF-5) 54.2%(1-model), 60.9%(2-model) &nbsp; &nbsp; 31.84%(1-model), 35.11%(6-model) &nbsp; &nbsp; DeepID-Net 64.1% &nbsp; &nbsp; 50.3% &nbsp; &nbsp; NoC 73.3% &nbsp; 68.8% &nbsp; &nbsp; &nbsp; Fast-RCNN (VGG16) 70.0% 68.8% 68.4% &nbsp; 19.7%(@[0.5-0.95]), 35.9%(@0.5) &nbsp; MR-CNN 78.2% &nbsp; 73.9% &nbsp; &nbsp; &nbsp; Faster-RCNN (VGG16) 78.8% &nbsp; 75.9% &nbsp; 21.9%(@[0.5-0.95]), 42.7%(@0.5) 198ms Faster-RCNN (ResNet-101) 85.6% &nbsp; 83.8% &nbsp; 37.4%(@[0.5-0.95]), 59.0%(@0.5) &nbsp; YOLO 63.4% &nbsp; 57.9% &nbsp; &nbsp; 45 fps YOLO VGG-16 66.4% &nbsp; &nbsp; &nbsp; &nbsp; 21 fps YOLOv2 544 × 544 78.6% &nbsp; 73.4% &nbsp; 21.6%(@[0.5-0.95]), 44.0%(@0.5) 40 fps SSD300 (VGG16) 77.2% &nbsp; 75.8% &nbsp; 25.1%(@[0.5-0.95]), 43.1%(@0.5) 46 fps SSD512 (VGG16) 79.8% &nbsp; 78.5% &nbsp; 28.8%(@[0.5-0.95]), 48.5%(@0.5) 19 fps ION 79.2% &nbsp; 76.4% &nbsp; &nbsp; &nbsp; CRAFT 75.7% &nbsp; 71.3% 48.5% &nbsp; &nbsp; OHEM 78.9% &nbsp; 76.3% &nbsp; 25.5%(@[0.5-0.95]), 45.9%(@0.5) &nbsp; R-FCN (ResNet-50) 77.4% &nbsp; &nbsp; &nbsp; &nbsp; 0.12sec(K40), 0.09sec(TitianX) R-FCN (ResNet-101) 79.5% &nbsp; &nbsp; &nbsp; &nbsp; 0.17sec(K40), 0.12sec(TitianX) R-FCN (ResNet-101),multi sc train 83.6% &nbsp; 82.0% &nbsp; 31.5%(@[0.5-0.95]), 53.2%(@0.5) &nbsp; PVANet 9.0 89.8% &nbsp; 84.2% &nbsp; &nbsp; 750ms(CPU), 46ms(TitianX) Leaderboard Detection Results: VOC2012 intro: Competition “comp4” (train on additional data) homepage:&nbsp;http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4 Papers Deep Neural Networks for Object Detection paper:&nbsp;http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks arxiv:&nbsp;http://arxiv.org/abs/1312.6229 github:&nbsp;https://github.com/sermanet/OverFeat code:&nbsp;http://cilvr.nyu.edu/doku.php?id=software:overfeat:start R-CNN Rich feature hierarchies for accurate object detection and semantic segmentation intro: R-CNN arxiv:&nbsp;http://arxiv.org/abs/1311.2524 supp:&nbsp;http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf slides:&nbsp;http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf slides:&nbsp;http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf github:&nbsp;https://github.com/rbgirshick/rcnn notes:&nbsp;http://zhangliliang.com/2014/07/23/paper-note-rcnn/ caffe-pr(“Make R-CNN the Caffe detection example”):&nbsp;https://github.com/BVLC/caffe/pull/482 Fast R-CNN Fast R-CNN arxiv:&nbsp;http://arxiv.org/abs/1504.08083 slides:&nbsp;http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf github:&nbsp;https://github.com/rbgirshick/fast-rcnn github(COCO-branch):&nbsp;https://github.com/rbgirshick/fast-rcnn/tree/coco webcam demo:&nbsp;https://github.com/rbgirshick/fast-rcnn/pull/29 notes:&nbsp;http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/ notes:&nbsp;http://blog.csdn.net/linj_m/article/details/48930179 github(“Fast R-CNN in MXNet”):&nbsp;https://github.com/precedenceguo/mx-rcnn github:&nbsp;https://github.com/mahyarnajibi/fast-rcnn-torch github:&nbsp;https://github.com/apple2373/chainer-simple-fast-rnn github:&nbsp;https://github.com/zplizzi/tensorflow-fast-rcnn A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1704.03414 paper:&nbsp;http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf github(Caffe):&nbsp;https://github.com/xiaolonw/adversarial-frcnn Faster R-CNN Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks intro: NIPS 2015 arxiv:&nbsp;http://arxiv.org/abs/1506.01497 gitxiv:&nbsp;http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region slides:&nbsp;http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf github(official, Matlab):&nbsp;https://github.com/ShaoqingRen/faster_rcnn github:&nbsp;https://github.com/rbgirshick/py-faster-rcnn github:&nbsp;https://github.com/mitmul/chainer-faster-rcnn github:&nbsp;https://github.com/andreaskoepf/faster-rcnn.torch github:&nbsp;https://github.com/ruotianluo/Faster-RCNN-Densecap-torch github:&nbsp;https://github.com/smallcorgi/Faster-RCNN_TF github:&nbsp;https://github.com/CharlesShang/TFFRCNN github(C++ demo):&nbsp;https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus github:&nbsp;https://github.com/yhenon/keras-frcnn Faster R-CNN in MXNet with distributed implementation and data parallelization github:&nbsp;https://github.com/dmlc/mxnet/tree/master/example/rcnn Contextual Priming and Feedback for Faster R-CNN intro: ECCV 2016. Carnegie Mellon University paper:&nbsp;http://abhinavsh.info/context_priming_feedback.pdf poster:&nbsp;http://www.eccv2016.org/files/posters/P-1A-20.pdf An Implementation of Faster RCNN with Study for Region Sampling intro: Technical Report, 3 pages. CMU arxiv:&nbsp;https://arxiv.org/abs/1702.02138 github:&nbsp;https://github.com/endernewton/tf-faster-rcnn MultiBox Scalable Object Detection using Deep Neural Networks intro: first MultiBox. Train a CNN to predict Region of Interest. arxiv:&nbsp;http://arxiv.org/abs/1312.2249 github:&nbsp;https://github.com/google/multibox blog:&nbsp;https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html Scalable, High-Quality Object Detection intro: second MultiBox arxiv:&nbsp;http://arxiv.org/abs/1412.1441 github:&nbsp;https://github.com/google/multibox SPP-Net Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition intro: ECCV 2014 / TPAMI 2015 arxiv:&nbsp;http://arxiv.org/abs/1406.4729 github:&nbsp;https://github.com/ShaoqingRen/SPP_net notes:&nbsp;http://zhangliliang.com/2014/09/13/paper-note-sppnet/ DeepID-Net DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection intro: PAMI 2016 intro: an extension of R-CNN. box pre-training, cascade on region proposals, deformation layers and context representations project page:&nbsp;http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html arxiv:&nbsp;http://arxiv.org/abs/1412.5661 Object Detectors Emerge in Deep Scene CNNs intro: ICLR 2015 arxiv:&nbsp;http://arxiv.org/abs/1412.6856 paper:&nbsp;https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf paper:&nbsp;https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf slides:&nbsp;http://places.csail.mit.edu/slide_iclr2015.pdf segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection intro: CVPR 2015 project(code+data):&nbsp;https://www.cs.toronto.edu/~yukun/segdeepm.html arxiv:&nbsp;https://arxiv.org/abs/1502.04275 github:&nbsp;https://github.com/YknZhu/segDeepM NoC Object Detection Networks on Convolutional Feature Maps intro: TPAMI 2015 arxiv:&nbsp;http://arxiv.org/abs/1504.06066 Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction arxiv:&nbsp;http://arxiv.org/abs/1504.03293 slides:&nbsp;http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf github:&nbsp;https://github.com/YutingZhang/fgs-obj DeepBox DeepBox: Learning Objectness with Convolutional Networks arxiv:&nbsp;http://arxiv.org/abs/1505.02146 github:&nbsp;https://github.com/weichengkuo/DeepBox MR-CNN Object detection via a multi-region &amp; semantic segmentation-aware CNN model intro: ICCV 2015. MR-CNN arxiv:&nbsp;http://arxiv.org/abs/1505.01749 github:&nbsp;https://github.com/gidariss/mrcnn-object-detection notes:&nbsp;http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/ notes:&nbsp;http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/ YOLO You Only Look Once: Unified, Real-Time Object Detection arxiv:&nbsp;http://arxiv.org/abs/1506.02640 code:&nbsp;http://pjreddie.com/darknet/yolo/ github:&nbsp;https://github.com/pjreddie/darknet blog:&nbsp;https://pjreddie.com/publications/yolo/ slides:&nbsp;https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p reddit:&nbsp;https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/ github:&nbsp;https://github.com/gliese581gg/YOLO_tensorflow github:&nbsp;https://github.com/xingwangsfu/caffe-yolo github:&nbsp;https://github.com/frankzhangrui/Darknet-Yolo github:&nbsp;https://github.com/BriSkyHekun/py-darknet-yolo github:&nbsp;https://github.com/tommy-qichang/yolo.torch github:&nbsp;https://github.com/frischzenger/yolo-windows github:&nbsp;https://github.com/AlexeyAB/yolo-windows github:&nbsp;https://github.com/nilboy/tensorflow-yolo darkflow - translate darknet to tensorflow. Load trained weights, retrain/fine-tune them using tensorflow, export constant graph def to C++ blog:&nbsp;https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp github:&nbsp;https://github.com/thtrieu/darkflow Start Training YOLO with Our Own Data intro: train with customized data and class numbers/labels. Linux / Windows version for darknet. blog:&nbsp;http://guanghan.info/blog/en/my-works/train-yolo/ github:&nbsp;https://github.com/Guanghan/darknet YOLO: Core ML versus MPSNNGraph intro: Tiny YOLO for iOS implemented using CoreML but also using the new MPS graph API. blog:&nbsp;http://machinethink.net/blog/yolo-coreml-versus-mps-graph/ github:&nbsp;https://github.com/hollance/YOLO-CoreML-MPSNNGraph TensorFlow YOLO object detection on Android intro: Real-time object detection on Android using the YOLO network with TensorFlow github:&nbsp;https://github.com/natanielruiz/android-yolo Computer Vision in iOS – Object Detection blog:&nbsp;https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/ github:https://github.com/r4ghu/iOS-CoreML-Yolo YOLOv2 YOLO9000: Better, Faster, Stronger arxiv:&nbsp;https://arxiv.org/abs/1612.08242 code:&nbsp;http://pjreddie.com/yolo9000/ github(Chainer):&nbsp;https://github.com/leetenki/YOLOv2 github(Keras):&nbsp;https://github.com/allanzelener/YAD2K github(PyTorch):&nbsp;https://github.com/longcw/yolo2-pytorch github(Tensorflow):&nbsp;https://github.com/hizhangp/yolo_tensorflow github(Windows):&nbsp;https://github.com/AlexeyAB/darknet github:&nbsp;https://github.com/choasUp/caffe-yolo9000 github:&nbsp;https://github.com/philipperemy/yolo-9000 Yolo_mark: GUI for marking bounded boxes of objects in images for training Yolo v2 github:&nbsp;https://github.com/AlexeyAB/Yolo_mark R-CNN minus R arxiv:&nbsp;http://arxiv.org/abs/1506.06981 AttentionNet AttentionNet: Aggregating Weak Directions for Accurate Object Detection intro: ICCV 2015 intro: state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 human detection task arxiv:&nbsp;http://arxiv.org/abs/1506.07704 slides:&nbsp;https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf slides:&nbsp;http://image-net.org/challenges/talks/lunit-kaist-slide.pdf DenseBox DenseBox: Unifying Landmark Localization with End to End Object Detection arxiv:&nbsp;http://arxiv.org/abs/1509.04874 demo:&nbsp;http://pan.baidu.com/s/1mgoWWsS KITTI result:&nbsp;http://www.cvlibs.net/datasets/kitti/eval_object.php SSD SSD: Single Shot MultiBox Detector intro: ECCV 2016 Oral arxiv:&nbsp;http://arxiv.org/abs/1512.02325 paper:&nbsp;http://www.cs.unc.edu/~wliu/papers/ssd.pdf slides:&nbsp;http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf github(Official):&nbsp;https://github.com/weiliu89/caffe/tree/ssd video:&nbsp;http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973 github:&nbsp;https://github.com/zhreshold/mxnet-ssd github:&nbsp;https://github.com/zhreshold/mxnet-ssd.cpp github:&nbsp;https://github.com/rykov8/ssd_keras github:&nbsp;https://github.com/balancap/SSD-Tensorflow github:&nbsp;https://github.com/amdegroot/ssd.pytorch github(Caffe):&nbsp;https://github.com/chuanqi305/MobileNet-SSD What’s the diffience in performance between this new code you pushed and the previous code? #327 https://github.com/weiliu89/caffe/issues/327 Enhancement of SSD by concatenating feature maps for object detection intro: rainbow SSD (R-SSD) arxiv:&nbsp;https://arxiv.org/abs/1705.09587 DSSD DSSD : Deconvolutional Single Shot Detector intro: UNC Chapel Hill &amp; Amazon Inc arxiv:&nbsp;https://arxiv.org/abs/1701.06659 demo:&nbsp;http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4 Context-aware Single-Shot Detector keywords: CSSD, DiCSSD, DeCSSD, effective receptive fields (ERFs), theoretical receptive fields (TRFs) arxiv:&nbsp;https://arxiv.org/abs/1707.08682 Feature-Fused SSD: Fast Detection for Small Objects https://arxiv.org/abs/1709.05054 Inside-Outside Net (ION) Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks intro: “0.8s per image on a Titan X GPU (excluding proposal generation) without two-stage bounding-box regression and 1.15s per image with it”. arxiv:&nbsp;http://arxiv.org/abs/1512.04143 slides:&nbsp;http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf coco-leaderboard:&nbsp;http://mscoco.org/dataset/#detections-leaderboard Adaptive Object Detection Using Adjacency and Zoom Prediction intro: CVPR 2016. AZ-Net arxiv:&nbsp;http://arxiv.org/abs/1512.07711 github:&nbsp;https://github.com/luyongxi/az-net youtube:&nbsp;https://www.youtube.com/watch?v=YmFtuNwxaNM G-CNN G-CNN: an Iterative Grid Based Object Detector arxiv:&nbsp;http://arxiv.org/abs/1512.07729 Factors in Finetuning Deep Model for object detection Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution intro: CVPR 2016.rank 3rd for provided data and 2nd for external data on ILSVRC 2015 object detection project page:&nbsp;http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html arxiv:&nbsp;http://arxiv.org/abs/1601.05150 We don’t need no bounding-boxes: Training object class detectors using only human verification arxiv:&nbsp;http://arxiv.org/abs/1602.08405 HyperNet HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection arxiv:&nbsp;http://arxiv.org/abs/1604.00600 MultiPathNet A MultiPath Network for Object Detection intro: BMVC 2016. Facebook AI Research (FAIR) arxiv:&nbsp;http://arxiv.org/abs/1604.02135 github:&nbsp;https://github.com/facebookresearch/multipathnet CRAFT CRAFT Objects from Images intro: CVPR 2016. Cascade Region-proposal-network And FasT-rcnn. an extension of Faster R-CNN project page:&nbsp;http://byangderek.github.io/projects/craft.html arxiv:&nbsp;https://arxiv.org/abs/1604.03239 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf github:&nbsp;https://github.com/byangderek/CRAFT OHEM Training Region-based Object Detectors with Online Hard Example Mining intro: CVPR 2016 Oral. Online hard example mining (OHEM) arxiv:&nbsp;http://arxiv.org/abs/1604.03540 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf github(Official):&nbsp;https://github.com/abhi2610/ohem author page:&nbsp;http://abhinav-shrivastava.info/ Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers intro: CVPR 2016 keywords: scale-dependent pooling (SDP), cascaded rejection classifiers (CRC) paper:&nbsp;http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf R-FCN R-FCN: Object Detection via Region-based Fully Convolutional Networks arxiv:&nbsp;http://arxiv.org/abs/1605.06409 github:&nbsp;https://github.com/daijifeng001/R-FCN github:&nbsp;https://github.com/Orpine/py-R-FCN github:&nbsp;https://github.com/PureDiors/pytorch_RFCN github:&nbsp;https://github.com/bharatsingh430/py-R-FCN-multiGPU github:&nbsp;https://github.com/xdever/RFCN-tensorflow Recycle deep features for better object detection arxiv:&nbsp;http://arxiv.org/abs/1607.05066 MS-CNN A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection intro: ECCV 2016 intro: 640×480: 15 fps, 960×720: 8 fps arxiv:&nbsp;http://arxiv.org/abs/1607.07155 github:&nbsp;https://github.com/zhaoweicai/mscnn poster:&nbsp;http://www.eccv2016.org/files/posters/P-2B-38.pdf Multi-stage Object Detection with Group Recursive Learning intro: VOC2007: 78.6%, VOC2012: 74.9% arxiv:&nbsp;http://arxiv.org/abs/1608.05159 Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection intro: WACV 2017. SubCNN arxiv:&nbsp;http://arxiv.org/abs/1604.04693 github:&nbsp;https://github.com/tanshen/SubCNN PVANET PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection intro: “less channels with more layers”, concatenated ReLU, Inception, and HyperNet, batch normalization, residual connections arxiv:&nbsp;http://arxiv.org/abs/1608.08021 github:&nbsp;https://github.com/sanghoon/pva-faster-rcnn leaderboard(PVANet 9.0):&nbsp;http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4 PVANet: Lightweight Deep Neural Networks for Real-time Object Detection intro: Presented at NIPS 2016 Workshop on Efficient Methods for Deep Neural Networks (EMDNN). Continuation of&nbsp;arXiv:1608.08021 arxiv:&nbsp;https://arxiv.org/abs/1611.08588 GBD-Net Gated Bi-directional CNN for Object Detection intro: The Chinese University of Hong Kong &amp; Sensetime Group Limited paper:&nbsp;http://link.springer.com/chapter/10.1007/978-3-319-46478-7_22 mirror:&nbsp;https://pan.baidu.com/s/1dFohO7v Crafting GBD-Net for Object Detection intro: winner of the ImageNet object detection challenge of 2016. CUImage and CUVideo intro: gated bi-directional CNN (GBD-Net) arxiv:&nbsp;https://arxiv.org/abs/1610.02579 github:&nbsp;https://github.com/craftGBD/craftGBD StuffNet StuffNet: Using ‘Stuff’ to Improve Object Detection arxiv:&nbsp;https://arxiv.org/abs/1610.05861 Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene arxiv:&nbsp;https://arxiv.org/abs/1610.09609 Hierarchical Object Detection with Deep Reinforcement Learning intro: Deep Reinforcement Learning Workshop (NIPS 2016) project page:&nbsp;https://imatge-upc.github.io/detection-2016-nipsws/ arxiv:&nbsp;https://arxiv.org/abs/1611.03718 slides:&nbsp;http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning github:&nbsp;https://github.com/imatge-upc/detection-2016-nipsws blog:&nbsp;http://jorditorres.org/nips/ Learning to detect and localize many objects from few examples arxiv:&nbsp;https://arxiv.org/abs/1611.05664 Speed/accuracy trade-offs for modern convolutional object detectors intro: Google Research arxiv:&nbsp;https://arxiv.org/abs/1611.10012 SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving arxiv:&nbsp;https://arxiv.org/abs/1612.01051 github:&nbsp;https://github.com/BichenWuUCB/squeezeDet github:&nbsp;https://github.com/fregu856/2D_detection Feature Pyramid Network (FPN) Feature Pyramid Networks for Object Detection intro: Facebook AI Research arxiv:&nbsp;https://arxiv.org/abs/1612.03144 Action-Driven Object Detection with Top-Down Visual Attentions arxiv:&nbsp;https://arxiv.org/abs/1612.06704 Beyond Skip Connections: Top-Down Modulation for Object Detection intro: CMU &amp; UC Berkeley &amp; Google Research arxiv:&nbsp;https://arxiv.org/abs/1612.06851 Wide-Residual-Inception Networks for Real-time Object Detection intro: Inha University arxiv:&nbsp;https://arxiv.org/abs/1702.01243 Attentional Network for Visual Object Detection intro: University of Maryland &amp; Mitsubishi Electric Research Laboratories arxiv:&nbsp;https://arxiv.org/abs/1702.01478 CC-Net Learning Chained Deep Features and Classifiers for Cascade in Object Detection intro: chained cascade network (CC-Net). 81.1% mAP on PASCAL VOC 2007 arxiv:&nbsp;https://arxiv.org/abs/1702.07054 DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling https://arxiv.org/abs/1703.10295 Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1704.03944 Spatial Memory for Context Reasoning in Object Detection arxiv:&nbsp;https://arxiv.org/abs/1704.04224 Accurate Single Stage Detector Using Recurrent Rolling Convolution intro: CVPR 2017. SenseTime keywords: Recurrent Rolling Convolution (RRC) arxiv:&nbsp;https://arxiv.org/abs/1704.05776 github:&nbsp;https://github.com/xiaohaoChen/rrc_detection Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection https://arxiv.org/abs/1704.05775 S-OHEM: Stratified Online Hard Example Mining for Object Detection https://arxiv.org/abs/1705.02233 LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems intro: Embedded Vision Workshop in CVPR. UC San Diego &amp; Qualcomm Inc arxiv:&nbsp;https://arxiv.org/abs/1705.05922 Point Linking Network for Object Detection intro: Point Linking Network (PLN) arxiv:&nbsp;https://arxiv.org/abs/1706.03646 Perceptual Generative Adversarial Networks for Small Object Detection https://arxiv.org/abs/1706.05274 Few-shot Object Detection https://arxiv.org/abs/1706.08249 Yes-Net: An effective Detector Based on Global Information https://arxiv.org/abs/1706.09180 SMC Faster R-CNN: Toward a scene-specialized multi-object detector https://arxiv.org/abs/1706.10217 Towards lightweight convolutional neural networks for object detection https://arxiv.org/abs/1707.01395 RON: Reverse Connection with Objectness Prior Networks for Object Detection intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1707.01691 github:&nbsp;https://github.com/taokong/RON Residual Features and Unified Prediction Network for Single Stage Detection https://arxiv.org/abs/1707.05031 Deformable Part-based Fully Convolutional Network for Object Detection intro: BMVC 2017 (oral). Sorbonne Universités &amp; CEDRIC arxiv:&nbsp;https://arxiv.org/abs/1707.06175 Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1707.06399 Recurrent Scale Approximation for Object Detection in CNN intro: ICCV 2017 keywords: Recurrent Scale Approximation (RSA) arxiv:&nbsp;https://arxiv.org/abs/1707.09531 github:&nbsp;https://github.com/sciencefans/RSA-for-object-detection DSOD DSOD: Learning Deeply Supervised Object Detectors from Scratch intro: ICCV 2017. Fudan University &amp; Tsinghua University &amp; Intel Labs China arxiv:&nbsp;https://arxiv.org/abs/1708.01241 github:&nbsp;https://github.com/szq0214/DSOD Focal Loss for Dense Object Detection intro: ICCV 2017 Best student paper award. Facebook AI Research keywords: RetinaNet arxiv:&nbsp;https://arxiv.org/abs/1708.02002 CoupleNet: Coupling Global Structure with Local Parts for Object Detection intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.02863 Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection https://arxiv.org/abs/1709.04347 StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection https://arxiv.org/abs/1709.05788 NMS End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression intro: CVPR 2015 arxiv:&nbsp;http://arxiv.org/abs/1411.5309 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf A convnet for non-maximum suppression arxiv:&nbsp;http://arxiv.org/abs/1511.06437 Improving Object Detection With One Line of Code Soft-NMS – Improving Object Detection With One Line of Code intro: ICCV 2017. University of Maryland keywords: Soft-NMS arxiv:&nbsp;https://arxiv.org/abs/1704.04503 github:&nbsp;https://github.com/bharatsingh430/soft-nms Learning non-maximum suppression https://arxiv.org/abs/1705.02950 Weakly Supervised Object Detection Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection intro: CVPR 2016 arxiv:&nbsp;http://arxiv.org/abs/1604.05766 Weakly supervised object detection using pseudo-strong labels arxiv:&nbsp;http://arxiv.org/abs/1607.04731 Saliency Guided End-to-End Learning for Weakly Supervised Object Detection intro: IJCAI 2017 arxiv:&nbsp;https://arxiv.org/abs/1706.06768 Detection From Video Learning Object Class Detectors from Weakly Annotated Video intro: CVPR 2012 paper:&nbsp;https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf Analysing domain shift factors between videos and images for object detection arxiv:&nbsp;https://arxiv.org/abs/1501.01186 Video Object Recognition slides:&nbsp;http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx Deep Learning for Saliency Prediction in Natural Video intro: Submitted on 12 Jan 2016 keywords: Deep learning, saliency map, optical flow, convolution network, contrast features paper:&nbsp;https://hal.archives-ouvertes.fr/hal-01251614/document T-CNN T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos intro: Winning solution in ILSVRC2015 Object Detection from Video(VID) Task arxiv:&nbsp;http://arxiv.org/abs/1604.02532 github:&nbsp;https://github.com/myfavouritekk/T-CNN Object Detection from Video Tubelets with Convolutional Neural Networks intro: CVPR 2016 Spotlight paper arxiv:&nbsp;https://arxiv.org/abs/1604.04053 paper:&nbsp;http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf gihtub:&nbsp;https://github.com/myfavouritekk/vdetlib Object Detection in Videos with Tubelets and Multi-context Cues intro: SenseTime Group slides:&nbsp;http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf slides:&nbsp;http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf Context Matters: Refining Object Detection in Video with Recurrent Neural Networks intro: BMVC 2016 keywords: pseudo-labeler arxiv:&nbsp;http://arxiv.org/abs/1607.04648 paper:&nbsp;http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf CNN Based Object Detection in Large Video Images intro: WangTao @ 爱奇艺 keywords: object retrieval, object detection, scene classification slides:&nbsp;http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf Object Detection in Videos with Tubelet Proposal Networks arxiv:&nbsp;https://arxiv.org/abs/1702.06355 Flow-Guided Feature Aggregation for Video Object Detection intro: MSRA arxiv:&nbsp;https://arxiv.org/abs/1703.10025 Video Object Detection using Faster R-CNN blog:&nbsp;http://andrewliao11.github.io/object_detection/faster_rcnn/ github:&nbsp;https://github.com/andrewliao11/py-faster-rcnn-imagenet Improving Context Modeling for Video Object Detection and Tracking http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster).pdf Temporal Dynamic Graph LSTM for Action-driven Video Object Detection intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.00666 Object Detection in 3D Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks arxiv:&nbsp;https://arxiv.org/abs/1609.06666 Object Detection on RGB-D Learning Rich Features from RGB-D Images for Object Detection and Segmentation arxiv:&nbsp;http://arxiv.org/abs/1407.5736 Differential Geometry Boosts Convolutional Neural Networks for Object Detection intro: CVPR 2016 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation https://arxiv.org/abs/1703.03347 Salient Object Detection This task involves predicting the salient regions of an image given by human eye fixations. Best Deep Saliency Detection Models (CVPR 2016 &amp; 2015) http://i.cs.hku.hk/~yzyu/vision.html Large-scale optimization of hierarchical features for saliency prediction in natural images paper:&nbsp;http://coxlab.org/pdfs/cvpr2014_vig_saliency.pdf Predicting Eye Fixations using Convolutional Neural Networks paper:&nbsp;http://www.escience.cn/system/file?fileId=72648 Saliency Detection by Multi-Context Deep Learning paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection arxiv:&nbsp;http://arxiv.org/abs/1510.05484 SuperCNN: A Superpixelwise Convolutional Neural Network for Salient Object Detection paper:&nbsp;www.shengfenghe.com/supercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html Shallow and Deep Convolutional Networks for Saliency Prediction intro: CVPR 2016 arxiv:&nbsp;http://arxiv.org/abs/1603.00845 github:&nbsp;https://github.com/imatge-upc/saliency-2016-cvpr Recurrent Attentional Networks for Saliency Detection intro: CVPR 2016. recurrent attentional convolutional-deconvolution network (RACDNN) arxiv:&nbsp;http://arxiv.org/abs/1604.03227 Two-Stream Convolutional Networks for Dynamic Saliency Prediction arxiv:&nbsp;http://arxiv.org/abs/1607.04730 Unconstrained Salient Object Detection Unconstrained Salient Object Detection via Proposal Subset Optimization intro: CVPR 2016 project page:&nbsp;http://cs-people.bu.edu/jmzhang/sod.html paper:&nbsp;http://cs-people.bu.edu/jmzhang/SOD/CVPR16SOD_camera_ready.pdf github:&nbsp;https://github.com/jimmie33/SOD caffe model zoo:&nbsp;https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-object-proposal-models-for-salient-object-detection DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf Salient Object Subitizing intro: CVPR 2015 intro: predicting the existence and the number of salient objects in an image using holistic cues project page:&nbsp;http://cs-people.bu.edu/jmzhang/sos.html arxiv:&nbsp;http://arxiv.org/abs/1607.07525 paper:&nbsp;http://cs-people.bu.edu/jmzhang/SOS/SOS_preprint.pdf caffe model zoo:&nbsp;https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-models-for-salient-object-subitizing Deeply-Supervised Recurrent Convolutional Neural Network for Saliency Detection intro: ACMMM 2016. deeply-supervised recurrent convolutional neural network (DSRCNN) arxiv:&nbsp;http://arxiv.org/abs/1608.05177 Saliency Detection via Combining Region-Level and Pixel-Level Predictions with CNNs intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1608.05186 Edge Preserving and Multi-Scale Contextual Neural Network for Salient Object Detection arxiv:&nbsp;http://arxiv.org/abs/1608.08029 A Deep Multi-Level Network for Saliency Prediction arxiv:&nbsp;http://arxiv.org/abs/1609.01064 Visual Saliency Detection Based on Multiscale Deep CNN Features intro: IEEE Transactions on Image Processing arxiv:&nbsp;http://arxiv.org/abs/1609.02077 A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection intro: DSCLRCN arxiv:&nbsp;https://arxiv.org/abs/1610.01708 Deeply supervised salient object detection with short connections arxiv:&nbsp;https://arxiv.org/abs/1611.04849 Weakly Supervised Top-down Salient Object Detection intro: Nanyang Technological University arxiv:&nbsp;https://arxiv.org/abs/1611.05345 SalGAN: Visual Saliency Prediction with Generative Adversarial Networks project page:&nbsp;https://imatge-upc.github.io/saliency-salgan-2017/ arxiv:&nbsp;https://arxiv.org/abs/1701.01081 Visual Saliency Prediction Using a Mixture of Deep Neural Networks arxiv:&nbsp;https://arxiv.org/abs/1702.00372 A Fast and Compact Salient Score Regression Network Based on Fully Convolutional Network arxiv:&nbsp;https://arxiv.org/abs/1702.00615 Saliency Detection by Forward and Backward Cues in Deep-CNNs https://arxiv.org/abs/1703.00152 Supervised Adversarial Networks for Image Saliency Detection https://arxiv.org/abs/1704.07242 Group-wise Deep Co-saliency Detection https://arxiv.org/abs/1707.07381 Towards the Success Rate of One: Real-time Unconstrained Salient Object Detection intro: University of Maryland College Park &amp; eBay Inc arxiv:&nbsp;https://arxiv.org/abs/1708.00079 Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection intro: ICCV 2017 arixv:&nbsp;https://arxiv.org/abs/1708.02001 Learning Uncertain Convolutional Features for Accurate Saliency Detection intro: Accepted as a poster in ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.02031 Deep Edge-Aware Saliency Detection https://arxiv.org/abs/1708.04366 Self-explanatory Deep Salient Object Detection intro: National University of Defense Technology, China &amp; National University of Singapore arxiv:&nbsp;https://arxiv.org/abs/1708.05595 PiCANet: Learning Pixel-wise Contextual Attention in ConvNets and Its Application in Saliency Detection https://arxiv.org/abs/1708.06433 DeepFeat: A Bottom Up and Top Down Saliency Model Based on Deep Features of Convolutional Neural Nets https://arxiv.org/abs/1709.02495 Saliency Detection in Video Deep Learning For Video Saliency Detection arxiv:&nbsp;https://arxiv.org/abs/1702.00871 Video Salient Object Detection Using Spatiotemporal Deep Features https://arxiv.org/abs/1708.01447 Predicting Video Saliency with Object-to-Motion CNN and Two-layer Convolutional LSTM https://arxiv.org/abs/1709.06316 Visual Relationship Detection Visual Relationship Detection with Language Priors intro: ECCV 2016 oral paper:&nbsp;https://cs.stanford.edu/people/ranjaykrishna/vrd/vrd.pdf github:&nbsp;https://github.com/Prof-Lu-Cewu/Visual-Relationship-Detection ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection intro: Visual Phrase reasoning Convolutional Neural Network (ViP-CNN), Visual Phrase Reasoning Structure (VPRS) arxiv:&nbsp;https://arxiv.org/abs/1702.07191 Visual Translation Embedding Network for Visual Relation Detection arxiv:&nbsp;https://www.arxiv.org/abs/1702.08319 Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection intro: CVPR 2017 spotlight paper arxiv:&nbsp;https://arxiv.org/abs/1703.03054 Detecting Visual Relationships with Deep Relational Networks intro: CVPR 2017 oral. The Chinese University of Hong Kong arxiv:&nbsp;https://arxiv.org/abs/1704.03114 Identifying Spatial Relations in Images using Convolutional Neural Networks https://arxiv.org/abs/1706.04215 PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN intro: ICCV arxiv:&nbsp;https://arxiv.org/abs/1708.01956 Specific Object Deteciton Deep Deformation Network for Object Landmark Localization arxiv:&nbsp;http://arxiv.org/abs/1605.01014 Fashion Landmark Detection in the Wild intro: ECCV 2016 project page:&nbsp;http://personal.ie.cuhk.edu.hk/~lz013/projects/FashionLandmarks.html arxiv:&nbsp;http://arxiv.org/abs/1608.03049 github(Caffe):&nbsp;https://github.com/liuziwei7/fashion-landmarks Deep Learning for Fast and Accurate Fashion Item Detection intro: Kuznech Inc. intro: MultiBox and Fast R-CNN paper:&nbsp;https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf OSMDeepOD - OSM and Deep Learning based Object Detection from Aerial Imagery (formerly known as “OSM-Crosswalk-Detection”) github:&nbsp;https://github.com/geometalab/OSMDeepOD Selfie Detection by Synergy-Constraint Based Convolutional Neural Network intro: IEEE SITIS 2016 arxiv:&nbsp;https://arxiv.org/abs/1611.04357 Associative Embedding:End-to-End Learning for Joint Detection and Grouping arxiv:&nbsp;https://arxiv.org/abs/1611.05424 Deep Cuboid Detection: Beyond 2D Bounding Boxes intro: CMU &amp; Magic Leap arxiv:&nbsp;https://arxiv.org/abs/1611.10010 Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection arxiv:&nbsp;https://arxiv.org/abs/1612.03019 Deep Learning Logo Detection with Data Expansion by Synthesising Context arxiv:&nbsp;https://arxiv.org/abs/1612.09322 Pixel-wise Ear Detection with Convolutional Encoder-Decoder Networks arxiv:&nbsp;https://arxiv.org/abs/1702.00307 Automatic Handgun Detection Alarm in Videos Using Deep Learning arxiv:&nbsp;https://arxiv.org/abs/1702.05147 results:&nbsp;https://github.com/SihamTabik/Pistol-Detection-in-Videos Using Deep Networks for Drone Detection intro: AVSS 2017 arxiv:&nbsp;https://arxiv.org/abs/1706.05726 Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.01642 DeepVoting: An Explainable Framework for Semantic Part Detection under Partial Occlusion https://arxiv.org/abs/1709.04577 Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network https://arxiv.org/abs/1709.09283 Face Deteciton Multi-view Face Detection Using Deep Convolutional Neural Networks intro: Yahoo arxiv:&nbsp;http://arxiv.org/abs/1502.02766 github:&nbsp;https://github.com/guoyilin/FaceDetection_CNN From Facial Parts Responses to Face Detection: A Deep Learning Approach intro: ICCV 2015. CUHK project page:&nbsp;http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/Faceness.html arxiv:&nbsp;https://arxiv.org/abs/1509.06451 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf Compact Convolutional Neural Network Cascade for Face Detection arxiv:&nbsp;http://arxiv.org/abs/1508.01292 github:&nbsp;https://github.com/Bkmz21/FD-Evaluation github:&nbsp;https://github.com/Bkmz21/CompactCNNCascade Face Detection with End-to-End Integration of a ConvNet and a 3D Model intro: ECCV 2016 arxiv:&nbsp;https://arxiv.org/abs/1606.00850 github(MXNet):&nbsp;https://github.com/tfwu/FaceDetection-ConvNet-3D CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection intro: CMU arxiv:&nbsp;https://arxiv.org/abs/1606.05413 Finding Tiny Faces intro: CVPR 2017. CMU project page:&nbsp;http://www.cs.cmu.edu/~peiyunh/tiny/index.html arxiv:&nbsp;https://arxiv.org/abs/1612.04402 github:&nbsp;https://github.com/peiyunh/tiny github(inference-only):&nbsp;https://github.com/chinakook/hr101_mxnet Towards a Deep Learning Framework for Unconstrained Face Detection intro: overlap with CMS-RCNN arxiv:&nbsp;https://arxiv.org/abs/1612.05322 Supervised Transformer Network for Efficient Face Detection arxiv:&nbsp;http://arxiv.org/abs/1607.05477 UnitBox UnitBox: An Advanced Object Detection Network intro: ACM MM 2016 arxiv:&nbsp;http://arxiv.org/abs/1608.01471 Bootstrapping Face Detection with Hard Negative Examples author: 万韶华 @ 小米. intro: Faster R-CNN, hard negative mining. state-of-the-art on the FDDB dataset arxiv:&nbsp;http://arxiv.org/abs/1608.02236 Grid Loss: Detecting Occluded Faces intro: ECCV 2016 arxiv:&nbsp;https://arxiv.org/abs/1609.00129 paper:&nbsp;http://lrs.icg.tugraz.at/pubs/opitz_eccv_16.pdf poster:&nbsp;http://www.eccv2016.org/files/posters/P-2A-34.pdf A Multi-Scale Cascade Fully Convolutional Network Face Detector intro: ICPR 2016 arxiv:&nbsp;http://arxiv.org/abs/1609.03536 MTCNN Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks project page:&nbsp;https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html arxiv:&nbsp;https://arxiv.org/abs/1604.02878 github(Matlab):&nbsp;https://github.com/kpzhang93/MTCNN_face_detection_alignment github:&nbsp;https://github.com/pangyupo/mxnet_mtcnn_face_detection github:&nbsp;https://github.com/DaFuCoding/MTCNN_Caffe github(MXNet):&nbsp;https://github.com/Seanlinx/mtcnn github:&nbsp;https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion github(Caffe):&nbsp;https://github.com/foreverYoungGitHub/MTCNN github:&nbsp;https://github.com/CongWeilin/mtcnn-caffe github:&nbsp;https://github.com/AlphaQi/MTCNN-light Face Detection using Deep Learning: An Improved Faster RCNN Approach intro: DeepIR Inc arxiv:&nbsp;https://arxiv.org/abs/1701.08289 Faceness-Net: Face Detection through Deep Facial Part Responses intro: An extended version of ICCV 2015 paper arxiv:&nbsp;https://arxiv.org/abs/1701.08393 Multi-Path Region-Based Convolutional Neural Network for Accurate Detection of Unconstrained “Hard Faces” intro: CVPR 2017. MP-RCNN, MP-RPN arxiv:&nbsp;https://arxiv.org/abs/1703.09145 End-To-End Face Detection and Recognition https://arxiv.org/abs/1703.10818 Face R-CNN https://arxiv.org/abs/1706.01061 Face Detection through Scale-Friendly Deep Convolutional Networks https://arxiv.org/abs/1706.02863 Scale-Aware Face Detection intro: CVPR 2017. SenseTime &amp; Tsinghua University arxiv:&nbsp;https://arxiv.org/abs/1706.09876 Multi-Branch Fully Convolutional Network for Face Detection https://arxiv.org/abs/1707.06330 SSH: Single Stage Headless Face Detector intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.03979 Dockerface: an easy to install and use Faster R-CNN face detector in a Docker container https://arxiv.org/abs/1708.04370 FaceBoxes: A CPU Real-time Face Detector with High Accuracy intro: IJCB 2017 keywords: Rapidly Digested Convolutional Layers (RDCL), Multiple Scale Convolutional Layers (MSCL) intro: the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images arxiv:&nbsp;https://arxiv.org/abs/1708.05234 S3FD: Single Shot Scale-invariant Face Detector intro: ICCV 2017 intro: can run at 36 FPS on a Nvidia Titan X (Pascal) for VGA-resolution images arxiv:&nbsp;https://arxiv.org/abs/1708.05237 Detecting Faces Using Region-based Fully Convolutional Networks https://arxiv.org/abs/1709.05256 AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection https://arxiv.org/abs/1709.07326 Facial Point / Landmark Detection Deep Convolutional Network Cascade for Facial Point Detection homepage:&nbsp;http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm paper:&nbsp;http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr13.pdf github:&nbsp;https://github.com/luoyetx/deep-landmark Facial Landmark Detection by Deep Multi-task Learning intro: ECCV 2014 project page:&nbsp;http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html paper:&nbsp;http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepfacealign.pdf github(Matlab):&nbsp;https://github.com/zhzhanp/TCDCN-face-alignment A Recurrent Encoder-Decoder Network for Sequential Face Alignment intro: ECCV 2016 arxiv:&nbsp;https://arxiv.org/abs/1608.05477 Detecting facial landmarks in the video based on a hybrid framework arxiv:&nbsp;http://arxiv.org/abs/1609.06441 Deep Constrained Local Models for Facial Landmark Detection arxiv:&nbsp;https://arxiv.org/abs/1611.08657 Effective face landmark localization via single deep network arxiv:&nbsp;https://arxiv.org/abs/1702.02719 A Convolution Tree with Deconvolution Branches: Exploiting Geometric Relationships for Single Shot Keypoint Detection https://arxiv.org/abs/1704.01880 Deep Alignment Network: A convolutional neural network for robust face alignment intro: CVPRW 2017 arxiv:&nbsp;https://arxiv.org/abs/1706.01789 gihtub:&nbsp;https://github.com/MarekKowalski/DeepAlignmentNetwork Joint Multi-view Face Alignment in the Wild https://arxiv.org/abs/1708.06023 FacePoseNet: Making a Case for Landmark-Free Face Alignment https://arxiv.org/abs/1708.07517 People Detection End-to-end people detection in crowded scenes arxiv:&nbsp;http://arxiv.org/abs/1506.04878 github:&nbsp;https://github.com/Russell91/reinspect ipn:&nbsp;http://nbviewer.ipython.org/github/Russell91/ReInspect/blob/master/evaluation_reinspect.ipynb youtube:&nbsp;https://www.youtube.com/watch?v=QeWl0h3kQ24 Detecting People in Artwork with CNNs intro: ECCV 2016 Workshops arxiv:&nbsp;https://arxiv.org/abs/1610.08871 Deep Multi-camera People Detection arxiv:&nbsp;https://arxiv.org/abs/1702.04593 Person Head Detection Context-aware CNNs for person head detection intro: ICCV 2015 project page:&nbsp;http://www.di.ens.fr/willow/research/headdetection/ arxiv:&nbsp;http://arxiv.org/abs/1511.07917 github:&nbsp;https://github.com/aosokin/cnn_head_detection Pedestrian Detection Pedestrian Detection aided by Deep Learning Semantic Tasks intro: CVPR 2015 project page:&nbsp;http://mmlab.ie.cuhk.edu.hk/projects/TA-CNN/ arxiv:&nbsp;http://arxiv.org/abs/1412.0069 Deep Learning Strong Parts for Pedestrian Detection intro: ICCV 2015. CUHK. DeepParts intro: Achieving 11.89% average miss rate on Caltech Pedestrian Dataset paper:&nbsp;http://personal.ie.cuhk.edu.hk/~pluo/pdf/tianLWTiccv15.pdf Taking a Deeper Look at Pedestrians intro: CVPR 2015 arxiv:&nbsp;https://arxiv.org/abs/1501.05790 Convolutional Channel Features intro: ICCV 2015 arxiv:&nbsp;https://arxiv.org/abs/1504.07339 github:&nbsp;https://github.com/byangderek/CCF Learning Complexity-Aware Cascades for Deep Pedestrian Detection intro: ICCV 2015 arxiv:&nbsp;https://arxiv.org/abs/1507.05348 Deep convolutional neural networks for pedestrian detection arxiv:&nbsp;http://arxiv.org/abs/1510.03608 github:&nbsp;https://github.com/DenisTome/DeepPed Scale-aware Fast R-CNN for Pedestrian Detection arxiv:&nbsp;https://arxiv.org/abs/1510.08160 New algorithm improves speed and accuracy of pedestrian detection blog:&nbsp;http://www.eurekalert.org/pub_releases/2016-02/uoc–nai020516.php Pushing the Limits of Deep CNNs for Pedestrian Detection intro: “set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from 11.7% to 8.9%” arxiv:&nbsp;http://arxiv.org/abs/1603.04525 A Real-Time Deep Learning Pedestrian Detector for Robot Navigation arxiv:&nbsp;http://arxiv.org/abs/1607.04436 A Real-Time Pedestrian Detector using Deep Learning for Human-Aware Navigation arxiv:&nbsp;http://arxiv.org/abs/1607.04441 Is Faster R-CNN Doing Well for Pedestrian Detection? intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1607.07032 github:&nbsp;https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian Reduced Memory Region Based Deep Convolutional Neural Network Detection intro: IEEE 2016 ICCE-Berlin arxiv:&nbsp;http://arxiv.org/abs/1609.02500 Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection arxiv:&nbsp;https://arxiv.org/abs/1610.03466 Multispectral Deep Neural Networks for Pedestrian Detection intro: BMVC 2016 oral arxiv:&nbsp;https://arxiv.org/abs/1611.02644 Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters intro: CVPR 2017 project page:&nbsp;http://ml.cs.tsinghua.edu.cn:5000/publications/synunity/ arxiv:&nbsp;https://arxiv.org/abs/1703.06283 github(Tensorflow):&nbsp;https://github.com/huangshiyu13/RPNplus Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation [https://arxiv.org/abs/1706.08564](https://arxiv.org/abs/1706.08564 Rotational Rectification Network for Robust Pedestrian Detection intro: CMU &amp; Volvo Construction arxiv:&nbsp;https://arxiv.org/abs/1706.08917 STD-PD: Generating Synthetic Training Data for Pedestrian Detection in Unannotated Videos intro: The University of North Carolina at Chapel Hill arxiv:&nbsp;https://arxiv.org/abs/1707.09100 Too Far to See? Not Really! — Pedestrian Detection with Scale-aware Localization Policy https://arxiv.org/abs/1709.00235 Vehicle Detection DAVE: A Unified Framework for Fast Vehicle Detection and Annotation intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1607.04564 Evolving Boxes for fast Vehicle Detection arxiv:&nbsp;https://arxiv.org/abs/1702.00254 Fine-Grained Car Detection for Visual Census Estimation intro: AAAI 2016 arxiv:&nbsp;https://arxiv.org/abs/1709.02480 Traffic-Sign Detection Traffic-Sign Detection and Classification in the Wild project page(code+dataset):&nbsp;http://cg.cs.tsinghua.edu.cn/traffic-sign/ paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf code &amp; model:&nbsp;http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/newdata0411.zip Detecting Small Signs from Large Images intro: IEEE Conference on Information Reuse and Integration (IRI) 2017 oral arxiv:&nbsp;https://arxiv.org/abs/1706.08574 Boundary / Edge / Contour Detection Holistically-Nested Edge Detection intro: ICCV 2015, Marr Prize paper:&nbsp;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf arxiv:&nbsp;http://arxiv.org/abs/1504.06375 github:&nbsp;https://github.com/s9xie/hed Unsupervised Learning of Edges intro: CVPR 2016. Facebook AI Research arxiv:&nbsp;http://arxiv.org/abs/1511.04166 zn-blog:&nbsp;http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html Pushing the Boundaries of Boundary Detection using Deep Learning arxiv:&nbsp;http://arxiv.org/abs/1511.07386 Convolutional Oriented Boundaries intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1608.02755 Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks project page:&nbsp;http://www.vision.ee.ethz.ch/~cvlsegmentation/ arxiv:&nbsp;https://arxiv.org/abs/1701.04658 github:&nbsp;https://github.com/kmaninis/COB Richer Convolutional Features for Edge Detection intro: CVPR 2017 keywords: richer convolutional features (RCF) arxiv:&nbsp;https://arxiv.org/abs/1612.02103 github:&nbsp;https://github.com/yun-liu/rcf Contour Detection from Deep Patch-level Boundary Prediction https://arxiv.org/abs/1705.03159 CASENet: Deep Category-Aware Semantic Edge Detection intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1705.09759 Skeleton Detection Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs arxiv:&nbsp;http://arxiv.org/abs/1603.09446 github:&nbsp;https://github.com/zeakey/DeepSkeleton DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images arxiv:&nbsp;http://arxiv.org/abs/1609.03659 SRN: Side-output Residual Network for Object Symmetry Detection in the Wild intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1703.02243 github:&nbsp;https://github.com/KevinKecc/SRN Fruit Detection Deep Fruit Detection in Orchards arxiv:&nbsp;https://arxiv.org/abs/1610.03677 Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards intro: The Journal of Field Robotics in May 2016 project page:&nbsp;http://confluence.acfr.usyd.edu.au/display/AGPub/ arxiv:&nbsp;https://arxiv.org/abs/1610.08120 Part Detection Objects as context for part detection https://arxiv.org/abs/1703.09529 Object Proposal DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers arxiv:&nbsp;http://arxiv.org/abs/1510.04445 github:&nbsp;https://github.com/aghodrati/deepproposal Scale-aware Pixel-wise Object Proposal Networks intro: IEEE Transactions on Image Processing arxiv:&nbsp;http://arxiv.org/abs/1601.04798 Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization intro: BMVC 2016. AttractioNet arxiv:&nbsp;https://arxiv.org/abs/1606.04446 github:&nbsp;https://github.com/gidariss/AttractioNet Learning to Segment Object Proposals via Recursive Neural Networks arxiv:&nbsp;https://arxiv.org/abs/1612.01057 Learning Detection with Diverse Proposals intro: CVPR 2017 keywords: differentiable Determinantal Point Process (DPP) layer, Learning Detection with Diverse Proposals (LDDP) arxiv:&nbsp;https://arxiv.org/abs/1704.03533 ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond keywords: product detection arxiv:&nbsp;https://arxiv.org/abs/1704.06752 Improving Small Object Proposals for Company Logo Detection intro: ICMR 2017 arxiv:&nbsp;https://arxiv.org/abs/1704.08881 Localization Beyond Bounding Boxes: Precise Localization of Objects in Images intro: PhD Thesis homepage:&nbsp;http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.html phd-thesis:&nbsp;http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf github(“SDS using hypercolumns”):&nbsp;https://github.com/bharath272/sds Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning arxiv:&nbsp;http://arxiv.org/abs/1503.00949 Weakly Supervised Object Localization Using Size Estimates arxiv:&nbsp;http://arxiv.org/abs/1608.04314 Active Object Localization with Deep Reinforcement Learning intro: ICCV 2015 keywords: Markov Decision Process arxiv:&nbsp;https://arxiv.org/abs/1511.06015 Localizing objects using referring expressions intro: ECCV 2016 keywords: LSTM, multiple instance learning (MIL) paper:&nbsp;http://www.umiacs.umd.edu/~varun/files/refexp-ECCV16.pdf github:&nbsp;https://github.com/varun-nagaraja/referring-expressions LocNet: Improving Localization Accuracy for Object Detection intro: CVPR 2016 oral arxiv:&nbsp;http://arxiv.org/abs/1511.07763 github:&nbsp;https://github.com/gidariss/LocNet Learning Deep Features for Discriminative Localization homepage:&nbsp;http://cnnlocalization.csail.mit.edu/ arxiv:&nbsp;http://arxiv.org/abs/1512.04150 github(Tensorflow):&nbsp;https://github.com/jazzsaxmafia/Weakly_detector github:&nbsp;https://github.com/metalbubble/CAM github:&nbsp;https://github.com/tdeboissiere/VGG16CAM-keras ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization intro: ECCV 2016 project page:&nbsp;http://www.di.ens.fr/willow/research/contextlocnet/ arxiv:&nbsp;http://arxiv.org/abs/1609.04331 github:&nbsp;https://github.com/vadimkantorov/contextlocnet Ensemble of Part Detectors for Simultaneous Classification and Localization https://arxiv.org/abs/1705.10034 STNet: Selective Tuning of Convolutional Networks for Object Localization https://arxiv.org/abs/1708.06418 Soft Proposal Networks for Weakly Supervised Object Localization intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1709.01829 Fine-grained Discriminative Localization via Saliency-guided Faster R-CNN intro: ACM MM 2017 arxiv:&nbsp;https://arxiv.org/abs/1709.08295 Tutorials / Talks Convolutional Feature Maps: Elements of efficient (and accurate) CNN-based object detection slides:&nbsp;http://research.microsoft.com/en-us/um/people/kahe/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf Towards Good Practices for Recognition &amp; Detection intro: Hikvision Research Institute. Supervised Data Augmentation (SDA) slides:&nbsp;http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf Projects TensorBox: a simple framework for training neural networks to detect objects in images intro: “The basic model implements the simple and robust GoogLeNet-OverFeat algorithm. We additionally provide an implementation of the&nbsp;ReInspect&nbsp;algorithm” github:&nbsp;https://github.com/Russell91/TensorBox Object detection in torch: Implementation of some object detection frameworks in torch github:&nbsp;https://github.com/fmassa/object-detection.torch Using DIGITS to train an Object Detection network github:&nbsp;https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/README.md FCN-MultiBox Detector intro: Full convolution MultiBox Detector (like SSD) implemented in Torch. github:&nbsp;https://github.com/teaonly/FMD.torch KittiBox: A car detection model implemented in Tensorflow. keywords: MultiNet intro: KittiBox is a collection of scripts to train out model FastBox on the Kitti Object Detection Dataset github:&nbsp;https://github.com/MarvinTeichmann/KittiBox Deformable Convolutional Networks + MST + Soft-NMS github:&nbsp;https://github.com/bharatsingh430/Deformable-ConvNets Tools BeaverDam: Video annotation tool for deep learning training labels https://github.com/antingshen/BeaverDam Blogs Convolutional Neural Networks for Object Detection http://rnd.azoft.com/convolutional-neural-networks-object-detection/ Introducing automatic object detection to visual search (Pinterest) keywords: Faster R-CNN blog:&nbsp;https://engineering.pinterest.com/blog/introducing-automatic-object-detection-visual-search demo:&nbsp;https://engineering.pinterest.com/sites/engineering/files/Visual%20Search%20V1%20-%20Video.mp4 review:&nbsp;https://news.developer.nvidia.com/pinterest-introduces-the-future-of-visual-search/?mkt_tok=eyJpIjoiTnpaa01UWXpPRE0xTURFMiIsInQiOiJJRjcybjkwTmtmallORUhLOFFFODBDclFqUlB3SWlRVXJXb1MrQ013TDRIMGxLQWlBczFIeWg0TFRUdnN2UHY2ZWFiXC9QQVwvQzBHM3B0UzBZblpOSmUyU1FcLzNPWXI4cml2VERwTTJsOFwvOEk9In0%3D Deep Learning for Object Detection with DIGITS blog:&nbsp;https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/ Analyzing The Papers Behind Facebook’s Computer Vision Approach keywords: DeepMask, SharpMask, MultiPathNet blog:&nbsp;&lt;a target=&quot;_blank&quot; href=&quot;https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook&#39; rel=&quot; nofollow&quot;s-computer-vision-approach &quot; style=&quot;margin:0px; padding:0px; color:rgb(145,115,107)&quot;&gt;https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook’s-Computer-Vision-Approach/&lt;/a&gt; Easily Create High Quality Object Detectors with Deep Learning intro: dlib v19.2 blog:&nbsp;http://blog.dlib.net/2016/10/easily-create-high-quality-object.html How to Train a Deep-Learned Object Detection Model in the Microsoft Cognitive Toolkit blog:&nbsp;https://blogs.technet.microsoft.com/machinelearning/2016/10/25/how-to-train-a-deep-learned-object-detection-model-in-cntk/ github:&nbsp;https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN Object Detection in Satellite Imagery, a Low Overhead Approach part 1:&nbsp;https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7#.2csh4iwx9 part 2:&nbsp;https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-ii-893f40122f92#.f9b7dgf64 You Only Look Twice — Multi-Scale Object Detection in Satellite Imagery With Convolutional Neural Networks part 1:&nbsp;https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571#.fmmi2o3of part 2:&nbsp;https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588#.nwzarsz1t Faster R-CNN Pedestrian and Car Detection blog:&nbsp;https://bigsnarf.wordpress.com/2016/11/07/faster-r-cnn-pedestrian-and-car-detection/ ipn:&nbsp;https://gist.github.com/bigsnarfdude/2f7b2144065f6056892a98495644d3e0#file-demo_faster_rcnn_notebook-ipynb github:&nbsp;https://github.com/bigsnarfdude/Faster-RCNN_TF Small U-Net for vehicle detection blog:&nbsp;https://medium.com/@vivek.yadav/small-u-net-for-vehicle-detection-9eec216f9fd6#.md4u80kad Region of interest pooling explained blog:&nbsp;https://deepsense.io/region-of-interest-pooling-explained/ github:&nbsp;https://github.com/deepsense-io/roi-pooling Supercharge your Computer Vision models with the TensorFlow Object Detection API blog:&nbsp;https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html github:&nbsp;https://github.com/tensorflow/models/tree/master/object_detection 阅读更多" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-11-14T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html Jump to... Leaderboard Papers R-CNN Fast R-CNN Faster R-CNN MultiBox SPP-Net DeepID-Net NoC DeepBox MR-CNN YOLO YOLOv2 AttentionNet DenseBox SSD DSSD Inside-Outside Net (ION) G-CNN HyperNet MultiPathNet CRAFT OHEM R-FCN MS-CNN PVANET GBD-Net StuffNet Feature Pyramid Network (FPN) CC-Net DSOD NMS Weakly Supervised Object Detection Detection From Video T-CNN Object Detection in 3D Object Detection on RGB-D Salient Object Detection Saliency Detection in Video Visual Relationship Detection Specific Object Deteciton Face Deteciton UnitBox MTCNN Facial Point / Landmark Detection People Detection Person Head Detection Pedestrian Detection Vehicle Detection Traffic-Sign Detection Boundary / Edge / Contour Detection Skeleton Detection Fruit Detection Part Detection Object Proposal Localization Tutorials / Talks Projects Tools Blogs Method VOC2007 VOC2010 VOC2012 ILSVRC 2013 MSCOCO 2015 Speed OverFeat &nbsp; &nbsp; &nbsp; 24.3% &nbsp; &nbsp; R-CNN (AlexNet) 58.5% 53.7% 53.3% 31.4% &nbsp; &nbsp; R-CNN (VGG16) 66.0% &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; SPP_net(ZF-5) 54.2%(1-model), 60.9%(2-model) &nbsp; &nbsp; 31.84%(1-model), 35.11%(6-model) &nbsp; &nbsp; DeepID-Net 64.1% &nbsp; &nbsp; 50.3% &nbsp; &nbsp; NoC 73.3% &nbsp; 68.8% &nbsp; &nbsp; &nbsp; Fast-RCNN (VGG16) 70.0% 68.8% 68.4% &nbsp; 19.7%(@[0.5-0.95]), 35.9%(@0.5) &nbsp; MR-CNN 78.2% &nbsp; 73.9% &nbsp; &nbsp; &nbsp; Faster-RCNN (VGG16) 78.8% &nbsp; 75.9% &nbsp; 21.9%(@[0.5-0.95]), 42.7%(@0.5) 198ms Faster-RCNN (ResNet-101) 85.6% &nbsp; 83.8% &nbsp; 37.4%(@[0.5-0.95]), 59.0%(@0.5) &nbsp; YOLO 63.4% &nbsp; 57.9% &nbsp; &nbsp; 45 fps YOLO VGG-16 66.4% &nbsp; &nbsp; &nbsp; &nbsp; 21 fps YOLOv2 544 × 544 78.6% &nbsp; 73.4% &nbsp; 21.6%(@[0.5-0.95]), 44.0%(@0.5) 40 fps SSD300 (VGG16) 77.2% &nbsp; 75.8% &nbsp; 25.1%(@[0.5-0.95]), 43.1%(@0.5) 46 fps SSD512 (VGG16) 79.8% &nbsp; 78.5% &nbsp; 28.8%(@[0.5-0.95]), 48.5%(@0.5) 19 fps ION 79.2% &nbsp; 76.4% &nbsp; &nbsp; &nbsp; CRAFT 75.7% &nbsp; 71.3% 48.5% &nbsp; &nbsp; OHEM 78.9% &nbsp; 76.3% &nbsp; 25.5%(@[0.5-0.95]), 45.9%(@0.5) &nbsp; R-FCN (ResNet-50) 77.4% &nbsp; &nbsp; &nbsp; &nbsp; 0.12sec(K40), 0.09sec(TitianX) R-FCN (ResNet-101) 79.5% &nbsp; &nbsp; &nbsp; &nbsp; 0.17sec(K40), 0.12sec(TitianX) R-FCN (ResNet-101),multi sc train 83.6% &nbsp; 82.0% &nbsp; 31.5%(@[0.5-0.95]), 53.2%(@0.5) &nbsp; PVANet 9.0 89.8% &nbsp; 84.2% &nbsp; &nbsp; 750ms(CPU), 46ms(TitianX) Leaderboard Detection Results: VOC2012 intro: Competition “comp4” (train on additional data) homepage:&nbsp;http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4 Papers Deep Neural Networks for Object Detection paper:&nbsp;http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks arxiv:&nbsp;http://arxiv.org/abs/1312.6229 github:&nbsp;https://github.com/sermanet/OverFeat code:&nbsp;http://cilvr.nyu.edu/doku.php?id=software:overfeat:start R-CNN Rich feature hierarchies for accurate object detection and semantic segmentation intro: R-CNN arxiv:&nbsp;http://arxiv.org/abs/1311.2524 supp:&nbsp;http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf slides:&nbsp;http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf slides:&nbsp;http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf github:&nbsp;https://github.com/rbgirshick/rcnn notes:&nbsp;http://zhangliliang.com/2014/07/23/paper-note-rcnn/ caffe-pr(“Make R-CNN the Caffe detection example”):&nbsp;https://github.com/BVLC/caffe/pull/482 Fast R-CNN Fast R-CNN arxiv:&nbsp;http://arxiv.org/abs/1504.08083 slides:&nbsp;http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf github:&nbsp;https://github.com/rbgirshick/fast-rcnn github(COCO-branch):&nbsp;https://github.com/rbgirshick/fast-rcnn/tree/coco webcam demo:&nbsp;https://github.com/rbgirshick/fast-rcnn/pull/29 notes:&nbsp;http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/ notes:&nbsp;http://blog.csdn.net/linj_m/article/details/48930179 github(“Fast R-CNN in MXNet”):&nbsp;https://github.com/precedenceguo/mx-rcnn github:&nbsp;https://github.com/mahyarnajibi/fast-rcnn-torch github:&nbsp;https://github.com/apple2373/chainer-simple-fast-rnn github:&nbsp;https://github.com/zplizzi/tensorflow-fast-rcnn A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1704.03414 paper:&nbsp;http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf github(Caffe):&nbsp;https://github.com/xiaolonw/adversarial-frcnn Faster R-CNN Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks intro: NIPS 2015 arxiv:&nbsp;http://arxiv.org/abs/1506.01497 gitxiv:&nbsp;http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region slides:&nbsp;http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf github(official, Matlab):&nbsp;https://github.com/ShaoqingRen/faster_rcnn github:&nbsp;https://github.com/rbgirshick/py-faster-rcnn github:&nbsp;https://github.com/mitmul/chainer-faster-rcnn github:&nbsp;https://github.com/andreaskoepf/faster-rcnn.torch github:&nbsp;https://github.com/ruotianluo/Faster-RCNN-Densecap-torch github:&nbsp;https://github.com/smallcorgi/Faster-RCNN_TF github:&nbsp;https://github.com/CharlesShang/TFFRCNN github(C++ demo):&nbsp;https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus github:&nbsp;https://github.com/yhenon/keras-frcnn Faster R-CNN in MXNet with distributed implementation and data parallelization github:&nbsp;https://github.com/dmlc/mxnet/tree/master/example/rcnn Contextual Priming and Feedback for Faster R-CNN intro: ECCV 2016. Carnegie Mellon University paper:&nbsp;http://abhinavsh.info/context_priming_feedback.pdf poster:&nbsp;http://www.eccv2016.org/files/posters/P-1A-20.pdf An Implementation of Faster RCNN with Study for Region Sampling intro: Technical Report, 3 pages. CMU arxiv:&nbsp;https://arxiv.org/abs/1702.02138 github:&nbsp;https://github.com/endernewton/tf-faster-rcnn MultiBox Scalable Object Detection using Deep Neural Networks intro: first MultiBox. Train a CNN to predict Region of Interest. arxiv:&nbsp;http://arxiv.org/abs/1312.2249 github:&nbsp;https://github.com/google/multibox blog:&nbsp;https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html Scalable, High-Quality Object Detection intro: second MultiBox arxiv:&nbsp;http://arxiv.org/abs/1412.1441 github:&nbsp;https://github.com/google/multibox SPP-Net Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition intro: ECCV 2014 / TPAMI 2015 arxiv:&nbsp;http://arxiv.org/abs/1406.4729 github:&nbsp;https://github.com/ShaoqingRen/SPP_net notes:&nbsp;http://zhangliliang.com/2014/09/13/paper-note-sppnet/ DeepID-Net DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection intro: PAMI 2016 intro: an extension of R-CNN. box pre-training, cascade on region proposals, deformation layers and context representations project page:&nbsp;http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html arxiv:&nbsp;http://arxiv.org/abs/1412.5661 Object Detectors Emerge in Deep Scene CNNs intro: ICLR 2015 arxiv:&nbsp;http://arxiv.org/abs/1412.6856 paper:&nbsp;https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf paper:&nbsp;https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf slides:&nbsp;http://places.csail.mit.edu/slide_iclr2015.pdf segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection intro: CVPR 2015 project(code+data):&nbsp;https://www.cs.toronto.edu/~yukun/segdeepm.html arxiv:&nbsp;https://arxiv.org/abs/1502.04275 github:&nbsp;https://github.com/YknZhu/segDeepM NoC Object Detection Networks on Convolutional Feature Maps intro: TPAMI 2015 arxiv:&nbsp;http://arxiv.org/abs/1504.06066 Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction arxiv:&nbsp;http://arxiv.org/abs/1504.03293 slides:&nbsp;http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf github:&nbsp;https://github.com/YutingZhang/fgs-obj DeepBox DeepBox: Learning Objectness with Convolutional Networks arxiv:&nbsp;http://arxiv.org/abs/1505.02146 github:&nbsp;https://github.com/weichengkuo/DeepBox MR-CNN Object detection via a multi-region &amp; semantic segmentation-aware CNN model intro: ICCV 2015. MR-CNN arxiv:&nbsp;http://arxiv.org/abs/1505.01749 github:&nbsp;https://github.com/gidariss/mrcnn-object-detection notes:&nbsp;http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/ notes:&nbsp;http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/ YOLO You Only Look Once: Unified, Real-Time Object Detection arxiv:&nbsp;http://arxiv.org/abs/1506.02640 code:&nbsp;http://pjreddie.com/darknet/yolo/ github:&nbsp;https://github.com/pjreddie/darknet blog:&nbsp;https://pjreddie.com/publications/yolo/ slides:&nbsp;https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p reddit:&nbsp;https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/ github:&nbsp;https://github.com/gliese581gg/YOLO_tensorflow github:&nbsp;https://github.com/xingwangsfu/caffe-yolo github:&nbsp;https://github.com/frankzhangrui/Darknet-Yolo github:&nbsp;https://github.com/BriSkyHekun/py-darknet-yolo github:&nbsp;https://github.com/tommy-qichang/yolo.torch github:&nbsp;https://github.com/frischzenger/yolo-windows github:&nbsp;https://github.com/AlexeyAB/yolo-windows github:&nbsp;https://github.com/nilboy/tensorflow-yolo darkflow - translate darknet to tensorflow. Load trained weights, retrain/fine-tune them using tensorflow, export constant graph def to C++ blog:&nbsp;https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp github:&nbsp;https://github.com/thtrieu/darkflow Start Training YOLO with Our Own Data intro: train with customized data and class numbers/labels. Linux / Windows version for darknet. blog:&nbsp;http://guanghan.info/blog/en/my-works/train-yolo/ github:&nbsp;https://github.com/Guanghan/darknet YOLO: Core ML versus MPSNNGraph intro: Tiny YOLO for iOS implemented using CoreML but also using the new MPS graph API. blog:&nbsp;http://machinethink.net/blog/yolo-coreml-versus-mps-graph/ github:&nbsp;https://github.com/hollance/YOLO-CoreML-MPSNNGraph TensorFlow YOLO object detection on Android intro: Real-time object detection on Android using the YOLO network with TensorFlow github:&nbsp;https://github.com/natanielruiz/android-yolo Computer Vision in iOS – Object Detection blog:&nbsp;https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/ github:https://github.com/r4ghu/iOS-CoreML-Yolo YOLOv2 YOLO9000: Better, Faster, Stronger arxiv:&nbsp;https://arxiv.org/abs/1612.08242 code:&nbsp;http://pjreddie.com/yolo9000/ github(Chainer):&nbsp;https://github.com/leetenki/YOLOv2 github(Keras):&nbsp;https://github.com/allanzelener/YAD2K github(PyTorch):&nbsp;https://github.com/longcw/yolo2-pytorch github(Tensorflow):&nbsp;https://github.com/hizhangp/yolo_tensorflow github(Windows):&nbsp;https://github.com/AlexeyAB/darknet github:&nbsp;https://github.com/choasUp/caffe-yolo9000 github:&nbsp;https://github.com/philipperemy/yolo-9000 Yolo_mark: GUI for marking bounded boxes of objects in images for training Yolo v2 github:&nbsp;https://github.com/AlexeyAB/Yolo_mark R-CNN minus R arxiv:&nbsp;http://arxiv.org/abs/1506.06981 AttentionNet AttentionNet: Aggregating Weak Directions for Accurate Object Detection intro: ICCV 2015 intro: state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 human detection task arxiv:&nbsp;http://arxiv.org/abs/1506.07704 slides:&nbsp;https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf slides:&nbsp;http://image-net.org/challenges/talks/lunit-kaist-slide.pdf DenseBox DenseBox: Unifying Landmark Localization with End to End Object Detection arxiv:&nbsp;http://arxiv.org/abs/1509.04874 demo:&nbsp;http://pan.baidu.com/s/1mgoWWsS KITTI result:&nbsp;http://www.cvlibs.net/datasets/kitti/eval_object.php SSD SSD: Single Shot MultiBox Detector intro: ECCV 2016 Oral arxiv:&nbsp;http://arxiv.org/abs/1512.02325 paper:&nbsp;http://www.cs.unc.edu/~wliu/papers/ssd.pdf slides:&nbsp;http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf github(Official):&nbsp;https://github.com/weiliu89/caffe/tree/ssd video:&nbsp;http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973 github:&nbsp;https://github.com/zhreshold/mxnet-ssd github:&nbsp;https://github.com/zhreshold/mxnet-ssd.cpp github:&nbsp;https://github.com/rykov8/ssd_keras github:&nbsp;https://github.com/balancap/SSD-Tensorflow github:&nbsp;https://github.com/amdegroot/ssd.pytorch github(Caffe):&nbsp;https://github.com/chuanqi305/MobileNet-SSD What’s the diffience in performance between this new code you pushed and the previous code? #327 https://github.com/weiliu89/caffe/issues/327 Enhancement of SSD by concatenating feature maps for object detection intro: rainbow SSD (R-SSD) arxiv:&nbsp;https://arxiv.org/abs/1705.09587 DSSD DSSD : Deconvolutional Single Shot Detector intro: UNC Chapel Hill &amp; Amazon Inc arxiv:&nbsp;https://arxiv.org/abs/1701.06659 demo:&nbsp;http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4 Context-aware Single-Shot Detector keywords: CSSD, DiCSSD, DeCSSD, effective receptive fields (ERFs), theoretical receptive fields (TRFs) arxiv:&nbsp;https://arxiv.org/abs/1707.08682 Feature-Fused SSD: Fast Detection for Small Objects https://arxiv.org/abs/1709.05054 Inside-Outside Net (ION) Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks intro: “0.8s per image on a Titan X GPU (excluding proposal generation) without two-stage bounding-box regression and 1.15s per image with it”. arxiv:&nbsp;http://arxiv.org/abs/1512.04143 slides:&nbsp;http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf coco-leaderboard:&nbsp;http://mscoco.org/dataset/#detections-leaderboard Adaptive Object Detection Using Adjacency and Zoom Prediction intro: CVPR 2016. AZ-Net arxiv:&nbsp;http://arxiv.org/abs/1512.07711 github:&nbsp;https://github.com/luyongxi/az-net youtube:&nbsp;https://www.youtube.com/watch?v=YmFtuNwxaNM G-CNN G-CNN: an Iterative Grid Based Object Detector arxiv:&nbsp;http://arxiv.org/abs/1512.07729 Factors in Finetuning Deep Model for object detection Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution intro: CVPR 2016.rank 3rd for provided data and 2nd for external data on ILSVRC 2015 object detection project page:&nbsp;http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html arxiv:&nbsp;http://arxiv.org/abs/1601.05150 We don’t need no bounding-boxes: Training object class detectors using only human verification arxiv:&nbsp;http://arxiv.org/abs/1602.08405 HyperNet HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection arxiv:&nbsp;http://arxiv.org/abs/1604.00600 MultiPathNet A MultiPath Network for Object Detection intro: BMVC 2016. Facebook AI Research (FAIR) arxiv:&nbsp;http://arxiv.org/abs/1604.02135 github:&nbsp;https://github.com/facebookresearch/multipathnet CRAFT CRAFT Objects from Images intro: CVPR 2016. Cascade Region-proposal-network And FasT-rcnn. an extension of Faster R-CNN project page:&nbsp;http://byangderek.github.io/projects/craft.html arxiv:&nbsp;https://arxiv.org/abs/1604.03239 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf github:&nbsp;https://github.com/byangderek/CRAFT OHEM Training Region-based Object Detectors with Online Hard Example Mining intro: CVPR 2016 Oral. Online hard example mining (OHEM) arxiv:&nbsp;http://arxiv.org/abs/1604.03540 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf github(Official):&nbsp;https://github.com/abhi2610/ohem author page:&nbsp;http://abhinav-shrivastava.info/ Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers intro: CVPR 2016 keywords: scale-dependent pooling (SDP), cascaded rejection classifiers (CRC) paper:&nbsp;http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf R-FCN R-FCN: Object Detection via Region-based Fully Convolutional Networks arxiv:&nbsp;http://arxiv.org/abs/1605.06409 github:&nbsp;https://github.com/daijifeng001/R-FCN github:&nbsp;https://github.com/Orpine/py-R-FCN github:&nbsp;https://github.com/PureDiors/pytorch_RFCN github:&nbsp;https://github.com/bharatsingh430/py-R-FCN-multiGPU github:&nbsp;https://github.com/xdever/RFCN-tensorflow Recycle deep features for better object detection arxiv:&nbsp;http://arxiv.org/abs/1607.05066 MS-CNN A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection intro: ECCV 2016 intro: 640×480: 15 fps, 960×720: 8 fps arxiv:&nbsp;http://arxiv.org/abs/1607.07155 github:&nbsp;https://github.com/zhaoweicai/mscnn poster:&nbsp;http://www.eccv2016.org/files/posters/P-2B-38.pdf Multi-stage Object Detection with Group Recursive Learning intro: VOC2007: 78.6%, VOC2012: 74.9% arxiv:&nbsp;http://arxiv.org/abs/1608.05159 Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection intro: WACV 2017. SubCNN arxiv:&nbsp;http://arxiv.org/abs/1604.04693 github:&nbsp;https://github.com/tanshen/SubCNN PVANET PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection intro: “less channels with more layers”, concatenated ReLU, Inception, and HyperNet, batch normalization, residual connections arxiv:&nbsp;http://arxiv.org/abs/1608.08021 github:&nbsp;https://github.com/sanghoon/pva-faster-rcnn leaderboard(PVANet 9.0):&nbsp;http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4 PVANet: Lightweight Deep Neural Networks for Real-time Object Detection intro: Presented at NIPS 2016 Workshop on Efficient Methods for Deep Neural Networks (EMDNN). Continuation of&nbsp;arXiv:1608.08021 arxiv:&nbsp;https://arxiv.org/abs/1611.08588 GBD-Net Gated Bi-directional CNN for Object Detection intro: The Chinese University of Hong Kong &amp; Sensetime Group Limited paper:&nbsp;http://link.springer.com/chapter/10.1007/978-3-319-46478-7_22 mirror:&nbsp;https://pan.baidu.com/s/1dFohO7v Crafting GBD-Net for Object Detection intro: winner of the ImageNet object detection challenge of 2016. CUImage and CUVideo intro: gated bi-directional CNN (GBD-Net) arxiv:&nbsp;https://arxiv.org/abs/1610.02579 github:&nbsp;https://github.com/craftGBD/craftGBD StuffNet StuffNet: Using ‘Stuff’ to Improve Object Detection arxiv:&nbsp;https://arxiv.org/abs/1610.05861 Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene arxiv:&nbsp;https://arxiv.org/abs/1610.09609 Hierarchical Object Detection with Deep Reinforcement Learning intro: Deep Reinforcement Learning Workshop (NIPS 2016) project page:&nbsp;https://imatge-upc.github.io/detection-2016-nipsws/ arxiv:&nbsp;https://arxiv.org/abs/1611.03718 slides:&nbsp;http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning github:&nbsp;https://github.com/imatge-upc/detection-2016-nipsws blog:&nbsp;http://jorditorres.org/nips/ Learning to detect and localize many objects from few examples arxiv:&nbsp;https://arxiv.org/abs/1611.05664 Speed/accuracy trade-offs for modern convolutional object detectors intro: Google Research arxiv:&nbsp;https://arxiv.org/abs/1611.10012 SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving arxiv:&nbsp;https://arxiv.org/abs/1612.01051 github:&nbsp;https://github.com/BichenWuUCB/squeezeDet github:&nbsp;https://github.com/fregu856/2D_detection Feature Pyramid Network (FPN) Feature Pyramid Networks for Object Detection intro: Facebook AI Research arxiv:&nbsp;https://arxiv.org/abs/1612.03144 Action-Driven Object Detection with Top-Down Visual Attentions arxiv:&nbsp;https://arxiv.org/abs/1612.06704 Beyond Skip Connections: Top-Down Modulation for Object Detection intro: CMU &amp; UC Berkeley &amp; Google Research arxiv:&nbsp;https://arxiv.org/abs/1612.06851 Wide-Residual-Inception Networks for Real-time Object Detection intro: Inha University arxiv:&nbsp;https://arxiv.org/abs/1702.01243 Attentional Network for Visual Object Detection intro: University of Maryland &amp; Mitsubishi Electric Research Laboratories arxiv:&nbsp;https://arxiv.org/abs/1702.01478 CC-Net Learning Chained Deep Features and Classifiers for Cascade in Object Detection intro: chained cascade network (CC-Net). 81.1% mAP on PASCAL VOC 2007 arxiv:&nbsp;https://arxiv.org/abs/1702.07054 DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling https://arxiv.org/abs/1703.10295 Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1704.03944 Spatial Memory for Context Reasoning in Object Detection arxiv:&nbsp;https://arxiv.org/abs/1704.04224 Accurate Single Stage Detector Using Recurrent Rolling Convolution intro: CVPR 2017. SenseTime keywords: Recurrent Rolling Convolution (RRC) arxiv:&nbsp;https://arxiv.org/abs/1704.05776 github:&nbsp;https://github.com/xiaohaoChen/rrc_detection Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection https://arxiv.org/abs/1704.05775 S-OHEM: Stratified Online Hard Example Mining for Object Detection https://arxiv.org/abs/1705.02233 LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems intro: Embedded Vision Workshop in CVPR. UC San Diego &amp; Qualcomm Inc arxiv:&nbsp;https://arxiv.org/abs/1705.05922 Point Linking Network for Object Detection intro: Point Linking Network (PLN) arxiv:&nbsp;https://arxiv.org/abs/1706.03646 Perceptual Generative Adversarial Networks for Small Object Detection https://arxiv.org/abs/1706.05274 Few-shot Object Detection https://arxiv.org/abs/1706.08249 Yes-Net: An effective Detector Based on Global Information https://arxiv.org/abs/1706.09180 SMC Faster R-CNN: Toward a scene-specialized multi-object detector https://arxiv.org/abs/1706.10217 Towards lightweight convolutional neural networks for object detection https://arxiv.org/abs/1707.01395 RON: Reverse Connection with Objectness Prior Networks for Object Detection intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1707.01691 github:&nbsp;https://github.com/taokong/RON Residual Features and Unified Prediction Network for Single Stage Detection https://arxiv.org/abs/1707.05031 Deformable Part-based Fully Convolutional Network for Object Detection intro: BMVC 2017 (oral). Sorbonne Universités &amp; CEDRIC arxiv:&nbsp;https://arxiv.org/abs/1707.06175 Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1707.06399 Recurrent Scale Approximation for Object Detection in CNN intro: ICCV 2017 keywords: Recurrent Scale Approximation (RSA) arxiv:&nbsp;https://arxiv.org/abs/1707.09531 github:&nbsp;https://github.com/sciencefans/RSA-for-object-detection DSOD DSOD: Learning Deeply Supervised Object Detectors from Scratch intro: ICCV 2017. Fudan University &amp; Tsinghua University &amp; Intel Labs China arxiv:&nbsp;https://arxiv.org/abs/1708.01241 github:&nbsp;https://github.com/szq0214/DSOD Focal Loss for Dense Object Detection intro: ICCV 2017 Best student paper award. Facebook AI Research keywords: RetinaNet arxiv:&nbsp;https://arxiv.org/abs/1708.02002 CoupleNet: Coupling Global Structure with Local Parts for Object Detection intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.02863 Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection https://arxiv.org/abs/1709.04347 StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection https://arxiv.org/abs/1709.05788 NMS End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression intro: CVPR 2015 arxiv:&nbsp;http://arxiv.org/abs/1411.5309 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf A convnet for non-maximum suppression arxiv:&nbsp;http://arxiv.org/abs/1511.06437 Improving Object Detection With One Line of Code Soft-NMS – Improving Object Detection With One Line of Code intro: ICCV 2017. University of Maryland keywords: Soft-NMS arxiv:&nbsp;https://arxiv.org/abs/1704.04503 github:&nbsp;https://github.com/bharatsingh430/soft-nms Learning non-maximum suppression https://arxiv.org/abs/1705.02950 Weakly Supervised Object Detection Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection intro: CVPR 2016 arxiv:&nbsp;http://arxiv.org/abs/1604.05766 Weakly supervised object detection using pseudo-strong labels arxiv:&nbsp;http://arxiv.org/abs/1607.04731 Saliency Guided End-to-End Learning for Weakly Supervised Object Detection intro: IJCAI 2017 arxiv:&nbsp;https://arxiv.org/abs/1706.06768 Detection From Video Learning Object Class Detectors from Weakly Annotated Video intro: CVPR 2012 paper:&nbsp;https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf Analysing domain shift factors between videos and images for object detection arxiv:&nbsp;https://arxiv.org/abs/1501.01186 Video Object Recognition slides:&nbsp;http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx Deep Learning for Saliency Prediction in Natural Video intro: Submitted on 12 Jan 2016 keywords: Deep learning, saliency map, optical flow, convolution network, contrast features paper:&nbsp;https://hal.archives-ouvertes.fr/hal-01251614/document T-CNN T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos intro: Winning solution in ILSVRC2015 Object Detection from Video(VID) Task arxiv:&nbsp;http://arxiv.org/abs/1604.02532 github:&nbsp;https://github.com/myfavouritekk/T-CNN Object Detection from Video Tubelets with Convolutional Neural Networks intro: CVPR 2016 Spotlight paper arxiv:&nbsp;https://arxiv.org/abs/1604.04053 paper:&nbsp;http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf gihtub:&nbsp;https://github.com/myfavouritekk/vdetlib Object Detection in Videos with Tubelets and Multi-context Cues intro: SenseTime Group slides:&nbsp;http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf slides:&nbsp;http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf Context Matters: Refining Object Detection in Video with Recurrent Neural Networks intro: BMVC 2016 keywords: pseudo-labeler arxiv:&nbsp;http://arxiv.org/abs/1607.04648 paper:&nbsp;http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf CNN Based Object Detection in Large Video Images intro: WangTao @ 爱奇艺 keywords: object retrieval, object detection, scene classification slides:&nbsp;http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf Object Detection in Videos with Tubelet Proposal Networks arxiv:&nbsp;https://arxiv.org/abs/1702.06355 Flow-Guided Feature Aggregation for Video Object Detection intro: MSRA arxiv:&nbsp;https://arxiv.org/abs/1703.10025 Video Object Detection using Faster R-CNN blog:&nbsp;http://andrewliao11.github.io/object_detection/faster_rcnn/ github:&nbsp;https://github.com/andrewliao11/py-faster-rcnn-imagenet Improving Context Modeling for Video Object Detection and Tracking http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster).pdf Temporal Dynamic Graph LSTM for Action-driven Video Object Detection intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.00666 Object Detection in 3D Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks arxiv:&nbsp;https://arxiv.org/abs/1609.06666 Object Detection on RGB-D Learning Rich Features from RGB-D Images for Object Detection and Segmentation arxiv:&nbsp;http://arxiv.org/abs/1407.5736 Differential Geometry Boosts Convolutional Neural Networks for Object Detection intro: CVPR 2016 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation https://arxiv.org/abs/1703.03347 Salient Object Detection This task involves predicting the salient regions of an image given by human eye fixations. Best Deep Saliency Detection Models (CVPR 2016 &amp; 2015) http://i.cs.hku.hk/~yzyu/vision.html Large-scale optimization of hierarchical features for saliency prediction in natural images paper:&nbsp;http://coxlab.org/pdfs/cvpr2014_vig_saliency.pdf Predicting Eye Fixations using Convolutional Neural Networks paper:&nbsp;http://www.escience.cn/system/file?fileId=72648 Saliency Detection by Multi-Context Deep Learning paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection arxiv:&nbsp;http://arxiv.org/abs/1510.05484 SuperCNN: A Superpixelwise Convolutional Neural Network for Salient Object Detection paper:&nbsp;www.shengfenghe.com/supercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html Shallow and Deep Convolutional Networks for Saliency Prediction intro: CVPR 2016 arxiv:&nbsp;http://arxiv.org/abs/1603.00845 github:&nbsp;https://github.com/imatge-upc/saliency-2016-cvpr Recurrent Attentional Networks for Saliency Detection intro: CVPR 2016. recurrent attentional convolutional-deconvolution network (RACDNN) arxiv:&nbsp;http://arxiv.org/abs/1604.03227 Two-Stream Convolutional Networks for Dynamic Saliency Prediction arxiv:&nbsp;http://arxiv.org/abs/1607.04730 Unconstrained Salient Object Detection Unconstrained Salient Object Detection via Proposal Subset Optimization intro: CVPR 2016 project page:&nbsp;http://cs-people.bu.edu/jmzhang/sod.html paper:&nbsp;http://cs-people.bu.edu/jmzhang/SOD/CVPR16SOD_camera_ready.pdf github:&nbsp;https://github.com/jimmie33/SOD caffe model zoo:&nbsp;https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-object-proposal-models-for-salient-object-detection DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf Salient Object Subitizing intro: CVPR 2015 intro: predicting the existence and the number of salient objects in an image using holistic cues project page:&nbsp;http://cs-people.bu.edu/jmzhang/sos.html arxiv:&nbsp;http://arxiv.org/abs/1607.07525 paper:&nbsp;http://cs-people.bu.edu/jmzhang/SOS/SOS_preprint.pdf caffe model zoo:&nbsp;https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-models-for-salient-object-subitizing Deeply-Supervised Recurrent Convolutional Neural Network for Saliency Detection intro: ACMMM 2016. deeply-supervised recurrent convolutional neural network (DSRCNN) arxiv:&nbsp;http://arxiv.org/abs/1608.05177 Saliency Detection via Combining Region-Level and Pixel-Level Predictions with CNNs intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1608.05186 Edge Preserving and Multi-Scale Contextual Neural Network for Salient Object Detection arxiv:&nbsp;http://arxiv.org/abs/1608.08029 A Deep Multi-Level Network for Saliency Prediction arxiv:&nbsp;http://arxiv.org/abs/1609.01064 Visual Saliency Detection Based on Multiscale Deep CNN Features intro: IEEE Transactions on Image Processing arxiv:&nbsp;http://arxiv.org/abs/1609.02077 A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection intro: DSCLRCN arxiv:&nbsp;https://arxiv.org/abs/1610.01708 Deeply supervised salient object detection with short connections arxiv:&nbsp;https://arxiv.org/abs/1611.04849 Weakly Supervised Top-down Salient Object Detection intro: Nanyang Technological University arxiv:&nbsp;https://arxiv.org/abs/1611.05345 SalGAN: Visual Saliency Prediction with Generative Adversarial Networks project page:&nbsp;https://imatge-upc.github.io/saliency-salgan-2017/ arxiv:&nbsp;https://arxiv.org/abs/1701.01081 Visual Saliency Prediction Using a Mixture of Deep Neural Networks arxiv:&nbsp;https://arxiv.org/abs/1702.00372 A Fast and Compact Salient Score Regression Network Based on Fully Convolutional Network arxiv:&nbsp;https://arxiv.org/abs/1702.00615 Saliency Detection by Forward and Backward Cues in Deep-CNNs https://arxiv.org/abs/1703.00152 Supervised Adversarial Networks for Image Saliency Detection https://arxiv.org/abs/1704.07242 Group-wise Deep Co-saliency Detection https://arxiv.org/abs/1707.07381 Towards the Success Rate of One: Real-time Unconstrained Salient Object Detection intro: University of Maryland College Park &amp; eBay Inc arxiv:&nbsp;https://arxiv.org/abs/1708.00079 Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection intro: ICCV 2017 arixv:&nbsp;https://arxiv.org/abs/1708.02001 Learning Uncertain Convolutional Features for Accurate Saliency Detection intro: Accepted as a poster in ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.02031 Deep Edge-Aware Saliency Detection https://arxiv.org/abs/1708.04366 Self-explanatory Deep Salient Object Detection intro: National University of Defense Technology, China &amp; National University of Singapore arxiv:&nbsp;https://arxiv.org/abs/1708.05595 PiCANet: Learning Pixel-wise Contextual Attention in ConvNets and Its Application in Saliency Detection https://arxiv.org/abs/1708.06433 DeepFeat: A Bottom Up and Top Down Saliency Model Based on Deep Features of Convolutional Neural Nets https://arxiv.org/abs/1709.02495 Saliency Detection in Video Deep Learning For Video Saliency Detection arxiv:&nbsp;https://arxiv.org/abs/1702.00871 Video Salient Object Detection Using Spatiotemporal Deep Features https://arxiv.org/abs/1708.01447 Predicting Video Saliency with Object-to-Motion CNN and Two-layer Convolutional LSTM https://arxiv.org/abs/1709.06316 Visual Relationship Detection Visual Relationship Detection with Language Priors intro: ECCV 2016 oral paper:&nbsp;https://cs.stanford.edu/people/ranjaykrishna/vrd/vrd.pdf github:&nbsp;https://github.com/Prof-Lu-Cewu/Visual-Relationship-Detection ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection intro: Visual Phrase reasoning Convolutional Neural Network (ViP-CNN), Visual Phrase Reasoning Structure (VPRS) arxiv:&nbsp;https://arxiv.org/abs/1702.07191 Visual Translation Embedding Network for Visual Relation Detection arxiv:&nbsp;https://www.arxiv.org/abs/1702.08319 Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection intro: CVPR 2017 spotlight paper arxiv:&nbsp;https://arxiv.org/abs/1703.03054 Detecting Visual Relationships with Deep Relational Networks intro: CVPR 2017 oral. The Chinese University of Hong Kong arxiv:&nbsp;https://arxiv.org/abs/1704.03114 Identifying Spatial Relations in Images using Convolutional Neural Networks https://arxiv.org/abs/1706.04215 PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN intro: ICCV arxiv:&nbsp;https://arxiv.org/abs/1708.01956 Specific Object Deteciton Deep Deformation Network for Object Landmark Localization arxiv:&nbsp;http://arxiv.org/abs/1605.01014 Fashion Landmark Detection in the Wild intro: ECCV 2016 project page:&nbsp;http://personal.ie.cuhk.edu.hk/~lz013/projects/FashionLandmarks.html arxiv:&nbsp;http://arxiv.org/abs/1608.03049 github(Caffe):&nbsp;https://github.com/liuziwei7/fashion-landmarks Deep Learning for Fast and Accurate Fashion Item Detection intro: Kuznech Inc. intro: MultiBox and Fast R-CNN paper:&nbsp;https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf OSMDeepOD - OSM and Deep Learning based Object Detection from Aerial Imagery (formerly known as “OSM-Crosswalk-Detection”) github:&nbsp;https://github.com/geometalab/OSMDeepOD Selfie Detection by Synergy-Constraint Based Convolutional Neural Network intro: IEEE SITIS 2016 arxiv:&nbsp;https://arxiv.org/abs/1611.04357 Associative Embedding:End-to-End Learning for Joint Detection and Grouping arxiv:&nbsp;https://arxiv.org/abs/1611.05424 Deep Cuboid Detection: Beyond 2D Bounding Boxes intro: CMU &amp; Magic Leap arxiv:&nbsp;https://arxiv.org/abs/1611.10010 Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection arxiv:&nbsp;https://arxiv.org/abs/1612.03019 Deep Learning Logo Detection with Data Expansion by Synthesising Context arxiv:&nbsp;https://arxiv.org/abs/1612.09322 Pixel-wise Ear Detection with Convolutional Encoder-Decoder Networks arxiv:&nbsp;https://arxiv.org/abs/1702.00307 Automatic Handgun Detection Alarm in Videos Using Deep Learning arxiv:&nbsp;https://arxiv.org/abs/1702.05147 results:&nbsp;https://github.com/SihamTabik/Pistol-Detection-in-Videos Using Deep Networks for Drone Detection intro: AVSS 2017 arxiv:&nbsp;https://arxiv.org/abs/1706.05726 Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.01642 DeepVoting: An Explainable Framework for Semantic Part Detection under Partial Occlusion https://arxiv.org/abs/1709.04577 Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network https://arxiv.org/abs/1709.09283 Face Deteciton Multi-view Face Detection Using Deep Convolutional Neural Networks intro: Yahoo arxiv:&nbsp;http://arxiv.org/abs/1502.02766 github:&nbsp;https://github.com/guoyilin/FaceDetection_CNN From Facial Parts Responses to Face Detection: A Deep Learning Approach intro: ICCV 2015. CUHK project page:&nbsp;http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/Faceness.html arxiv:&nbsp;https://arxiv.org/abs/1509.06451 paper:&nbsp;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf Compact Convolutional Neural Network Cascade for Face Detection arxiv:&nbsp;http://arxiv.org/abs/1508.01292 github:&nbsp;https://github.com/Bkmz21/FD-Evaluation github:&nbsp;https://github.com/Bkmz21/CompactCNNCascade Face Detection with End-to-End Integration of a ConvNet and a 3D Model intro: ECCV 2016 arxiv:&nbsp;https://arxiv.org/abs/1606.00850 github(MXNet):&nbsp;https://github.com/tfwu/FaceDetection-ConvNet-3D CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection intro: CMU arxiv:&nbsp;https://arxiv.org/abs/1606.05413 Finding Tiny Faces intro: CVPR 2017. CMU project page:&nbsp;http://www.cs.cmu.edu/~peiyunh/tiny/index.html arxiv:&nbsp;https://arxiv.org/abs/1612.04402 github:&nbsp;https://github.com/peiyunh/tiny github(inference-only):&nbsp;https://github.com/chinakook/hr101_mxnet Towards a Deep Learning Framework for Unconstrained Face Detection intro: overlap with CMS-RCNN arxiv:&nbsp;https://arxiv.org/abs/1612.05322 Supervised Transformer Network for Efficient Face Detection arxiv:&nbsp;http://arxiv.org/abs/1607.05477 UnitBox UnitBox: An Advanced Object Detection Network intro: ACM MM 2016 arxiv:&nbsp;http://arxiv.org/abs/1608.01471 Bootstrapping Face Detection with Hard Negative Examples author: 万韶华 @ 小米. intro: Faster R-CNN, hard negative mining. state-of-the-art on the FDDB dataset arxiv:&nbsp;http://arxiv.org/abs/1608.02236 Grid Loss: Detecting Occluded Faces intro: ECCV 2016 arxiv:&nbsp;https://arxiv.org/abs/1609.00129 paper:&nbsp;http://lrs.icg.tugraz.at/pubs/opitz_eccv_16.pdf poster:&nbsp;http://www.eccv2016.org/files/posters/P-2A-34.pdf A Multi-Scale Cascade Fully Convolutional Network Face Detector intro: ICPR 2016 arxiv:&nbsp;http://arxiv.org/abs/1609.03536 MTCNN Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks project page:&nbsp;https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html arxiv:&nbsp;https://arxiv.org/abs/1604.02878 github(Matlab):&nbsp;https://github.com/kpzhang93/MTCNN_face_detection_alignment github:&nbsp;https://github.com/pangyupo/mxnet_mtcnn_face_detection github:&nbsp;https://github.com/DaFuCoding/MTCNN_Caffe github(MXNet):&nbsp;https://github.com/Seanlinx/mtcnn github:&nbsp;https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion github(Caffe):&nbsp;https://github.com/foreverYoungGitHub/MTCNN github:&nbsp;https://github.com/CongWeilin/mtcnn-caffe github:&nbsp;https://github.com/AlphaQi/MTCNN-light Face Detection using Deep Learning: An Improved Faster RCNN Approach intro: DeepIR Inc arxiv:&nbsp;https://arxiv.org/abs/1701.08289 Faceness-Net: Face Detection through Deep Facial Part Responses intro: An extended version of ICCV 2015 paper arxiv:&nbsp;https://arxiv.org/abs/1701.08393 Multi-Path Region-Based Convolutional Neural Network for Accurate Detection of Unconstrained “Hard Faces” intro: CVPR 2017. MP-RCNN, MP-RPN arxiv:&nbsp;https://arxiv.org/abs/1703.09145 End-To-End Face Detection and Recognition https://arxiv.org/abs/1703.10818 Face R-CNN https://arxiv.org/abs/1706.01061 Face Detection through Scale-Friendly Deep Convolutional Networks https://arxiv.org/abs/1706.02863 Scale-Aware Face Detection intro: CVPR 2017. SenseTime &amp; Tsinghua University arxiv:&nbsp;https://arxiv.org/abs/1706.09876 Multi-Branch Fully Convolutional Network for Face Detection https://arxiv.org/abs/1707.06330 SSH: Single Stage Headless Face Detector intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1708.03979 Dockerface: an easy to install and use Faster R-CNN face detector in a Docker container https://arxiv.org/abs/1708.04370 FaceBoxes: A CPU Real-time Face Detector with High Accuracy intro: IJCB 2017 keywords: Rapidly Digested Convolutional Layers (RDCL), Multiple Scale Convolutional Layers (MSCL) intro: the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images arxiv:&nbsp;https://arxiv.org/abs/1708.05234 S3FD: Single Shot Scale-invariant Face Detector intro: ICCV 2017 intro: can run at 36 FPS on a Nvidia Titan X (Pascal) for VGA-resolution images arxiv:&nbsp;https://arxiv.org/abs/1708.05237 Detecting Faces Using Region-based Fully Convolutional Networks https://arxiv.org/abs/1709.05256 AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection https://arxiv.org/abs/1709.07326 Facial Point / Landmark Detection Deep Convolutional Network Cascade for Facial Point Detection homepage:&nbsp;http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm paper:&nbsp;http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr13.pdf github:&nbsp;https://github.com/luoyetx/deep-landmark Facial Landmark Detection by Deep Multi-task Learning intro: ECCV 2014 project page:&nbsp;http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html paper:&nbsp;http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepfacealign.pdf github(Matlab):&nbsp;https://github.com/zhzhanp/TCDCN-face-alignment A Recurrent Encoder-Decoder Network for Sequential Face Alignment intro: ECCV 2016 arxiv:&nbsp;https://arxiv.org/abs/1608.05477 Detecting facial landmarks in the video based on a hybrid framework arxiv:&nbsp;http://arxiv.org/abs/1609.06441 Deep Constrained Local Models for Facial Landmark Detection arxiv:&nbsp;https://arxiv.org/abs/1611.08657 Effective face landmark localization via single deep network arxiv:&nbsp;https://arxiv.org/abs/1702.02719 A Convolution Tree with Deconvolution Branches: Exploiting Geometric Relationships for Single Shot Keypoint Detection https://arxiv.org/abs/1704.01880 Deep Alignment Network: A convolutional neural network for robust face alignment intro: CVPRW 2017 arxiv:&nbsp;https://arxiv.org/abs/1706.01789 gihtub:&nbsp;https://github.com/MarekKowalski/DeepAlignmentNetwork Joint Multi-view Face Alignment in the Wild https://arxiv.org/abs/1708.06023 FacePoseNet: Making a Case for Landmark-Free Face Alignment https://arxiv.org/abs/1708.07517 People Detection End-to-end people detection in crowded scenes arxiv:&nbsp;http://arxiv.org/abs/1506.04878 github:&nbsp;https://github.com/Russell91/reinspect ipn:&nbsp;http://nbviewer.ipython.org/github/Russell91/ReInspect/blob/master/evaluation_reinspect.ipynb youtube:&nbsp;https://www.youtube.com/watch?v=QeWl0h3kQ24 Detecting People in Artwork with CNNs intro: ECCV 2016 Workshops arxiv:&nbsp;https://arxiv.org/abs/1610.08871 Deep Multi-camera People Detection arxiv:&nbsp;https://arxiv.org/abs/1702.04593 Person Head Detection Context-aware CNNs for person head detection intro: ICCV 2015 project page:&nbsp;http://www.di.ens.fr/willow/research/headdetection/ arxiv:&nbsp;http://arxiv.org/abs/1511.07917 github:&nbsp;https://github.com/aosokin/cnn_head_detection Pedestrian Detection Pedestrian Detection aided by Deep Learning Semantic Tasks intro: CVPR 2015 project page:&nbsp;http://mmlab.ie.cuhk.edu.hk/projects/TA-CNN/ arxiv:&nbsp;http://arxiv.org/abs/1412.0069 Deep Learning Strong Parts for Pedestrian Detection intro: ICCV 2015. CUHK. DeepParts intro: Achieving 11.89% average miss rate on Caltech Pedestrian Dataset paper:&nbsp;http://personal.ie.cuhk.edu.hk/~pluo/pdf/tianLWTiccv15.pdf Taking a Deeper Look at Pedestrians intro: CVPR 2015 arxiv:&nbsp;https://arxiv.org/abs/1501.05790 Convolutional Channel Features intro: ICCV 2015 arxiv:&nbsp;https://arxiv.org/abs/1504.07339 github:&nbsp;https://github.com/byangderek/CCF Learning Complexity-Aware Cascades for Deep Pedestrian Detection intro: ICCV 2015 arxiv:&nbsp;https://arxiv.org/abs/1507.05348 Deep convolutional neural networks for pedestrian detection arxiv:&nbsp;http://arxiv.org/abs/1510.03608 github:&nbsp;https://github.com/DenisTome/DeepPed Scale-aware Fast R-CNN for Pedestrian Detection arxiv:&nbsp;https://arxiv.org/abs/1510.08160 New algorithm improves speed and accuracy of pedestrian detection blog:&nbsp;http://www.eurekalert.org/pub_releases/2016-02/uoc–nai020516.php Pushing the Limits of Deep CNNs for Pedestrian Detection intro: “set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from 11.7% to 8.9%” arxiv:&nbsp;http://arxiv.org/abs/1603.04525 A Real-Time Deep Learning Pedestrian Detector for Robot Navigation arxiv:&nbsp;http://arxiv.org/abs/1607.04436 A Real-Time Pedestrian Detector using Deep Learning for Human-Aware Navigation arxiv:&nbsp;http://arxiv.org/abs/1607.04441 Is Faster R-CNN Doing Well for Pedestrian Detection? intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1607.07032 github:&nbsp;https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian Reduced Memory Region Based Deep Convolutional Neural Network Detection intro: IEEE 2016 ICCE-Berlin arxiv:&nbsp;http://arxiv.org/abs/1609.02500 Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection arxiv:&nbsp;https://arxiv.org/abs/1610.03466 Multispectral Deep Neural Networks for Pedestrian Detection intro: BMVC 2016 oral arxiv:&nbsp;https://arxiv.org/abs/1611.02644 Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters intro: CVPR 2017 project page:&nbsp;http://ml.cs.tsinghua.edu.cn:5000/publications/synunity/ arxiv:&nbsp;https://arxiv.org/abs/1703.06283 github(Tensorflow):&nbsp;https://github.com/huangshiyu13/RPNplus Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation [https://arxiv.org/abs/1706.08564](https://arxiv.org/abs/1706.08564 Rotational Rectification Network for Robust Pedestrian Detection intro: CMU &amp; Volvo Construction arxiv:&nbsp;https://arxiv.org/abs/1706.08917 STD-PD: Generating Synthetic Training Data for Pedestrian Detection in Unannotated Videos intro: The University of North Carolina at Chapel Hill arxiv:&nbsp;https://arxiv.org/abs/1707.09100 Too Far to See? Not Really! — Pedestrian Detection with Scale-aware Localization Policy https://arxiv.org/abs/1709.00235 Vehicle Detection DAVE: A Unified Framework for Fast Vehicle Detection and Annotation intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1607.04564 Evolving Boxes for fast Vehicle Detection arxiv:&nbsp;https://arxiv.org/abs/1702.00254 Fine-Grained Car Detection for Visual Census Estimation intro: AAAI 2016 arxiv:&nbsp;https://arxiv.org/abs/1709.02480 Traffic-Sign Detection Traffic-Sign Detection and Classification in the Wild project page(code+dataset):&nbsp;http://cg.cs.tsinghua.edu.cn/traffic-sign/ paper:&nbsp;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf code &amp; model:&nbsp;http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/newdata0411.zip Detecting Small Signs from Large Images intro: IEEE Conference on Information Reuse and Integration (IRI) 2017 oral arxiv:&nbsp;https://arxiv.org/abs/1706.08574 Boundary / Edge / Contour Detection Holistically-Nested Edge Detection intro: ICCV 2015, Marr Prize paper:&nbsp;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf arxiv:&nbsp;http://arxiv.org/abs/1504.06375 github:&nbsp;https://github.com/s9xie/hed Unsupervised Learning of Edges intro: CVPR 2016. Facebook AI Research arxiv:&nbsp;http://arxiv.org/abs/1511.04166 zn-blog:&nbsp;http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html Pushing the Boundaries of Boundary Detection using Deep Learning arxiv:&nbsp;http://arxiv.org/abs/1511.07386 Convolutional Oriented Boundaries intro: ECCV 2016 arxiv:&nbsp;http://arxiv.org/abs/1608.02755 Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks project page:&nbsp;http://www.vision.ee.ethz.ch/~cvlsegmentation/ arxiv:&nbsp;https://arxiv.org/abs/1701.04658 github:&nbsp;https://github.com/kmaninis/COB Richer Convolutional Features for Edge Detection intro: CVPR 2017 keywords: richer convolutional features (RCF) arxiv:&nbsp;https://arxiv.org/abs/1612.02103 github:&nbsp;https://github.com/yun-liu/rcf Contour Detection from Deep Patch-level Boundary Prediction https://arxiv.org/abs/1705.03159 CASENet: Deep Category-Aware Semantic Edge Detection intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1705.09759 Skeleton Detection Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs arxiv:&nbsp;http://arxiv.org/abs/1603.09446 github:&nbsp;https://github.com/zeakey/DeepSkeleton DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images arxiv:&nbsp;http://arxiv.org/abs/1609.03659 SRN: Side-output Residual Network for Object Symmetry Detection in the Wild intro: CVPR 2017 arxiv:&nbsp;https://arxiv.org/abs/1703.02243 github:&nbsp;https://github.com/KevinKecc/SRN Fruit Detection Deep Fruit Detection in Orchards arxiv:&nbsp;https://arxiv.org/abs/1610.03677 Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards intro: The Journal of Field Robotics in May 2016 project page:&nbsp;http://confluence.acfr.usyd.edu.au/display/AGPub/ arxiv:&nbsp;https://arxiv.org/abs/1610.08120 Part Detection Objects as context for part detection https://arxiv.org/abs/1703.09529 Object Proposal DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers arxiv:&nbsp;http://arxiv.org/abs/1510.04445 github:&nbsp;https://github.com/aghodrati/deepproposal Scale-aware Pixel-wise Object Proposal Networks intro: IEEE Transactions on Image Processing arxiv:&nbsp;http://arxiv.org/abs/1601.04798 Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization intro: BMVC 2016. AttractioNet arxiv:&nbsp;https://arxiv.org/abs/1606.04446 github:&nbsp;https://github.com/gidariss/AttractioNet Learning to Segment Object Proposals via Recursive Neural Networks arxiv:&nbsp;https://arxiv.org/abs/1612.01057 Learning Detection with Diverse Proposals intro: CVPR 2017 keywords: differentiable Determinantal Point Process (DPP) layer, Learning Detection with Diverse Proposals (LDDP) arxiv:&nbsp;https://arxiv.org/abs/1704.03533 ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond keywords: product detection arxiv:&nbsp;https://arxiv.org/abs/1704.06752 Improving Small Object Proposals for Company Logo Detection intro: ICMR 2017 arxiv:&nbsp;https://arxiv.org/abs/1704.08881 Localization Beyond Bounding Boxes: Precise Localization of Objects in Images intro: PhD Thesis homepage:&nbsp;http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.html phd-thesis:&nbsp;http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf github(“SDS using hypercolumns”):&nbsp;https://github.com/bharath272/sds Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning arxiv:&nbsp;http://arxiv.org/abs/1503.00949 Weakly Supervised Object Localization Using Size Estimates arxiv:&nbsp;http://arxiv.org/abs/1608.04314 Active Object Localization with Deep Reinforcement Learning intro: ICCV 2015 keywords: Markov Decision Process arxiv:&nbsp;https://arxiv.org/abs/1511.06015 Localizing objects using referring expressions intro: ECCV 2016 keywords: LSTM, multiple instance learning (MIL) paper:&nbsp;http://www.umiacs.umd.edu/~varun/files/refexp-ECCV16.pdf github:&nbsp;https://github.com/varun-nagaraja/referring-expressions LocNet: Improving Localization Accuracy for Object Detection intro: CVPR 2016 oral arxiv:&nbsp;http://arxiv.org/abs/1511.07763 github:&nbsp;https://github.com/gidariss/LocNet Learning Deep Features for Discriminative Localization homepage:&nbsp;http://cnnlocalization.csail.mit.edu/ arxiv:&nbsp;http://arxiv.org/abs/1512.04150 github(Tensorflow):&nbsp;https://github.com/jazzsaxmafia/Weakly_detector github:&nbsp;https://github.com/metalbubble/CAM github:&nbsp;https://github.com/tdeboissiere/VGG16CAM-keras ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization intro: ECCV 2016 project page:&nbsp;http://www.di.ens.fr/willow/research/contextlocnet/ arxiv:&nbsp;http://arxiv.org/abs/1609.04331 github:&nbsp;https://github.com/vadimkantorov/contextlocnet Ensemble of Part Detectors for Simultaneous Classification and Localization https://arxiv.org/abs/1705.10034 STNet: Selective Tuning of Convolutional Networks for Object Localization https://arxiv.org/abs/1708.06418 Soft Proposal Networks for Weakly Supervised Object Localization intro: ICCV 2017 arxiv:&nbsp;https://arxiv.org/abs/1709.01829 Fine-grained Discriminative Localization via Saliency-guided Faster R-CNN intro: ACM MM 2017 arxiv:&nbsp;https://arxiv.org/abs/1709.08295 Tutorials / Talks Convolutional Feature Maps: Elements of efficient (and accurate) CNN-based object detection slides:&nbsp;http://research.microsoft.com/en-us/um/people/kahe/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf Towards Good Practices for Recognition &amp; Detection intro: Hikvision Research Institute. Supervised Data Augmentation (SDA) slides:&nbsp;http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf Projects TensorBox: a simple framework for training neural networks to detect objects in images intro: “The basic model implements the simple and robust GoogLeNet-OverFeat algorithm. We additionally provide an implementation of the&nbsp;ReInspect&nbsp;algorithm” github:&nbsp;https://github.com/Russell91/TensorBox Object detection in torch: Implementation of some object detection frameworks in torch github:&nbsp;https://github.com/fmassa/object-detection.torch Using DIGITS to train an Object Detection network github:&nbsp;https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/README.md FCN-MultiBox Detector intro: Full convolution MultiBox Detector (like SSD) implemented in Torch. github:&nbsp;https://github.com/teaonly/FMD.torch KittiBox: A car detection model implemented in Tensorflow. keywords: MultiNet intro: KittiBox is a collection of scripts to train out model FastBox on the Kitti Object Detection Dataset github:&nbsp;https://github.com/MarvinTeichmann/KittiBox Deformable Convolutional Networks + MST + Soft-NMS github:&nbsp;https://github.com/bharatsingh430/Deformable-ConvNets Tools BeaverDam: Video annotation tool for deep learning training labels https://github.com/antingshen/BeaverDam Blogs Convolutional Neural Networks for Object Detection http://rnd.azoft.com/convolutional-neural-networks-object-detection/ Introducing automatic object detection to visual search (Pinterest) keywords: Faster R-CNN blog:&nbsp;https://engineering.pinterest.com/blog/introducing-automatic-object-detection-visual-search demo:&nbsp;https://engineering.pinterest.com/sites/engineering/files/Visual%20Search%20V1%20-%20Video.mp4 review:&nbsp;https://news.developer.nvidia.com/pinterest-introduces-the-future-of-visual-search/?mkt_tok=eyJpIjoiTnpaa01UWXpPRE0xTURFMiIsInQiOiJJRjcybjkwTmtmallORUhLOFFFODBDclFqUlB3SWlRVXJXb1MrQ013TDRIMGxLQWlBczFIeWg0TFRUdnN2UHY2ZWFiXC9QQVwvQzBHM3B0UzBZblpOSmUyU1FcLzNPWXI4cml2VERwTTJsOFwvOEk9In0%3D Deep Learning for Object Detection with DIGITS blog:&nbsp;https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/ Analyzing The Papers Behind Facebook’s Computer Vision Approach keywords: DeepMask, SharpMask, MultiPathNet blog:&nbsp;&lt;a target=&quot;_blank&quot; href=&quot;https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook&#39; rel=&quot; nofollow&quot;s-computer-vision-approach &quot; style=&quot;margin:0px; padding:0px; color:rgb(145,115,107)&quot;&gt;https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook’s-Computer-Vision-Approach/&lt;/a&gt; Easily Create High Quality Object Detectors with Deep Learning intro: dlib v19.2 blog:&nbsp;http://blog.dlib.net/2016/10/easily-create-high-quality-object.html How to Train a Deep-Learned Object Detection Model in the Microsoft Cognitive Toolkit blog:&nbsp;https://blogs.technet.microsoft.com/machinelearning/2016/10/25/how-to-train-a-deep-learned-object-detection-model-in-cntk/ github:&nbsp;https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN Object Detection in Satellite Imagery, a Low Overhead Approach part 1:&nbsp;https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7#.2csh4iwx9 part 2:&nbsp;https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-ii-893f40122f92#.f9b7dgf64 You Only Look Twice — Multi-Scale Object Detection in Satellite Imagery With Convolutional Neural Networks part 1:&nbsp;https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571#.fmmi2o3of part 2:&nbsp;https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588#.nwzarsz1t Faster R-CNN Pedestrian and Car Detection blog:&nbsp;https://bigsnarf.wordpress.com/2016/11/07/faster-r-cnn-pedestrian-and-car-detection/ ipn:&nbsp;https://gist.github.com/bigsnarfdude/2f7b2144065f6056892a98495644d3e0#file-demo_faster_rcnn_notebook-ipynb github:&nbsp;https://github.com/bigsnarfdude/Faster-RCNN_TF Small U-Net for vehicle detection blog:&nbsp;https://medium.com/@vivek.yadav/small-u-net-for-vehicle-detection-9eec216f9fd6#.md4u80kad Region of interest pooling explained blog:&nbsp;https://deepsense.io/region-of-interest-pooling-explained/ github:&nbsp;https://github.com/deepsense-io/roi-pooling Supercharge your Computer Vision models with the TensorFlow Object Detection API blog:&nbsp;https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html github:&nbsp;https://github.com/tensorflow/models/tree/master/object_detection 阅读更多","@type":"BlogPosting","url":"/2017/11/14/32ce275e226436eded3731775ecd9bf2.html","headline":"Object Detection–的一个好的对比总结文章","dateModified":"2017-11-14T00:00:00+08:00","datePublished":"2017-11-14T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"/2017/11/14/32ce275e226436eded3731775ecd9bf2.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-1');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>Object Detection--的一个好的对比总结文章</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-e2445db1a8.css"> 
 <div class="htmledit_views"> 
  <p><br> https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html<br> </p> 
  <p><br> </p> 
  <p><br> </p> 
  <p></p> 
  <div id="toc" style="margin:0px; padding:10px; border:1px solid rgb(170,170,170); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <span style="margin:0px; padding:0px">Jump to...</span> 
   <ol style="margin:0px 0px 1em; padding:0px 0px 0px 20px; list-style-type:disc"> 
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#leaderboard" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Leaderboard</a></li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#papers" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Papers</a> 
     <ol style="margin:0px 0px 1em; padding:0px 0px 0px 20px; list-style-type:disc"> 
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#r-cnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">R-CNN</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#fast-r-cnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Fast R-CNN</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#faster-r-cnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Faster R-CNN</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#multibox" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">MultiBox</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#spp-net" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">SPP-Net</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#deepid-net" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">DeepID-Net</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#noc" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">NoC</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#deepbox" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">DeepBox</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#mr-cnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">MR-CNN</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#yolo" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">YOLO</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#yolov2" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">YOLOv2</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#attentionnet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">AttentionNet</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#densebox" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">DenseBox</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#ssd" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">SSD</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#dssd" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">DSSD</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#inside-outside-net-ion" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Inside-Outside Net (ION)</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#g-cnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">G-CNN</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#hypernet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">HyperNet</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#multipathnet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">MultiPathNet</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#craft" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">CRAFT</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#ohem" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">OHEM</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#r-fcn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">R-FCN</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#ms-cnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">MS-CNN</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#pvanet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">PVANET</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#gbd-net" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">GBD-Net</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#stuffnet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">StuffNet</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#feature-pyramid-network-fpn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Feature Pyramid Network (FPN)</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#cc-net" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">CC-Net</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#dsod" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">DSOD</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#nms" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">NMS</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#weakly-supervised-object-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Weakly Supervised Object Detection</a></li>
     </ol> </li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#detection-from-video" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Detection From Video</a> 
     <ol style="margin:0px 0px 1em; padding:0px 0px 0px 20px; list-style-type:disc"> 
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#t-cnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">T-CNN</a></li>
     </ol> </li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#object-detection-in-3d" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Object Detection in 3D</a></li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#object-detection-on-rgb-d" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Object Detection on RGB-D</a></li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#salient-object-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Salient Object Detection</a> 
     <ol style="margin:0px 0px 1em; padding:0px 0px 0px 20px; list-style-type:disc"> 
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#saliency-detection-in-video" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Saliency Detection in Video</a></li>
     </ol> </li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#visual-relationship-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Visual Relationship Detection</a></li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#specific-object-deteciton" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Specific Object Deteciton</a> 
     <ol style="margin:0px 0px 1em; padding:0px 0px 0px 20px; list-style-type:disc"> 
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#face-deteciton" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Face Deteciton</a> 
       <ol style="margin:0px 0px 1em; padding:0px 0px 0px 20px; list-style-type:disc"> 
        <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#unitbox" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">UnitBox</a></li>
        <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#mtcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">MTCNN</a></li>
       </ol> </li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#facial-point--landmark-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Facial Point / Landmark Detection</a></li>
     </ol> </li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#people-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">People Detection</a> 
     <ol style="margin:0px 0px 1em; padding:0px 0px 0px 20px; list-style-type:disc"> 
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#person-head-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Person Head Detection</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#pedestrian-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Pedestrian Detection</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#vehicle-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Vehicle Detection</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#traffic-sign-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Traffic-Sign Detection</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#boundary--edge--contour-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Boundary / Edge / Contour Detection</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#skeleton-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Skeleton Detection</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#fruit-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Fruit Detection</a></li>
      <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#part-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Part Detection</a></li>
     </ol> </li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#object-proposal" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Object Proposal</a></li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#localization" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Localization</a></li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#tutorials--talks" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Tutorials / Talks</a></li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#projects" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Projects</a></li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#tools" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Tools</a></li>
    <li style="margin:0px; padding:0px; line-height:24px"><a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#blogs" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">Blogs</a></li>
   </ol> 
  </div> 
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <table style="margin:0px 0px 1em; padding:0px; position:relative; width:600px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <thead style="margin:0px; padding:0px"> 
    <tr style="margin:0px; padding:0px"> 
     <th style="margin:0px; padding:4px 0px; text-align:center">Method</th> 
     <th style="margin:0px; padding:4px 0px; text-align:center">VOC2007</th> 
     <th style="margin:0px; padding:4px 0px; text-align:center">VOC2010</th> 
     <th style="margin:0px; padding:4px 0px; text-align:center">VOC2012</th> 
     <th style="margin:0px; padding:4px 0px; text-align:center">ILSVRC 2013</th> 
     <th style="margin:0px; padding:4px 0px; text-align:center">MSCOCO 2015</th> 
     <th style="margin:0px; padding:4px 0px; text-align:center">Speed</th> 
    </tr> 
   </thead> 
   <tbody style="margin:0px; padding:0px"> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">OverFeat</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">24.3%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">R-CNN (AlexNet)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">58.5%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">53.7%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">53.3%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">31.4%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">R-CNN (VGG16)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">66.0%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">SPP_net(ZF-5)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">54.2%(1-model), 60.9%(2-model)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">31.84%(1-model), 35.11%(6-model)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">DeepID-Net</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">64.1%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">50.3%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">NoC</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">73.3%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">68.8%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">Fast-RCNN (VGG16)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">70.0%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">68.8%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">68.4%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">19.7%(@[0.5-0.95]), 35.9%(@0.5)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">MR-CNN</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">78.2%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">73.9%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">Faster-RCNN (VGG16)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">78.8%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">75.9%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">21.9%(@[0.5-0.95]), 42.7%(@0.5)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">198ms</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">Faster-RCNN (ResNet-101)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">85.6%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">83.8%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">37.4%(@[0.5-0.95]), 59.0%(@0.5)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">YOLO</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">63.4%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">57.9%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">45 fps</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">YOLO VGG-16</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">66.4%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">21 fps</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">YOLOv2 544 × 544</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">78.6%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">73.4%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">21.6%(@[0.5-0.95]), 44.0%(@0.5)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">40 fps</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">SSD300 (VGG16)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">77.2%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">75.8%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">25.1%(@[0.5-0.95]), 43.1%(@0.5)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">46 fps</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">SSD512 (VGG16)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">79.8%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">78.5%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">28.8%(@[0.5-0.95]), 48.5%(@0.5)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">19 fps</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">ION</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">79.2%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">76.4%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">CRAFT</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">75.7%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">71.3%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">48.5%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">OHEM</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">78.9%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">76.3%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">25.5%(@[0.5-0.95]), 45.9%(@0.5)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">R-FCN (ResNet-50)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">77.4%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">0.12sec(K40), 0.09sec(TitianX)</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">R-FCN (ResNet-101)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">79.5%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">0.17sec(K40), 0.12sec(TitianX)</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">R-FCN (ResNet-101),multi sc train</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">83.6%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">82.0%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">31.5%(@[0.5-0.95]), 53.2%(@0.5)</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
    </tr> 
    <tr style="margin:0px; padding:0px"> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">PVANet 9.0</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">89.8%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">84.2%</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">&nbsp;</td> 
     <td style="margin:0px; padding:4px 0px; text-indent:6px; text-align:center">750ms(CPU), 46ms(TitianX)</td> 
    </tr> 
   </tbody> 
  </table> 
  <h1 id="leaderboard" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Leaderboard</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Detection Results: VOC2012</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Competition “comp4” (train on additional data)</li>
   <li style="margin:0px; padding:0px; line-height:24px">homepage:&nbsp;<a target="_blank" href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4</a></li>
  </ul> 
  <h1 id="papers" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Papers</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Neural Networks for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1312.6229" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1312.6229</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/sermanet/OverFeat" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/sermanet/OverFeat</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">code:&nbsp;<a target="_blank" href="http://cilvr.nyu.edu/doku.php?id=software:overfeat:start" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://cilvr.nyu.edu/doku.php?id=software:overfeat:start</a></li>
  </ul> 
  <h2 id="r-cnn" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> R-CNN</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Rich feature hierarchies for accurate object detection and semantic segmentation</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: R-CNN</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1311.2524" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1311.2524</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">supp:&nbsp;<a target="_blank" href="http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/rbgirshick/rcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/rbgirshick/rcnn</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">notes:&nbsp;<a target="_blank" href="http://zhangliliang.com/2014/07/23/paper-note-rcnn/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://zhangliliang.com/2014/07/23/paper-note-rcnn/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">caffe-pr(“Make R-CNN the Caffe detection example”):&nbsp;<a target="_blank" href="https://github.com/BVLC/caffe/pull/482" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/BVLC/caffe/pull/482</a></li>
  </ul> 
  <h2 id="fast-r-cnn" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Fast R-CNN</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Fast R-CNN</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1504.08083" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1504.08083</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/rbgirshick/fast-rcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/rbgirshick/fast-rcnn</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(COCO-branch):&nbsp;<a target="_blank" href="https://github.com/rbgirshick/fast-rcnn/tree/coco" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/rbgirshick/fast-rcnn/tree/coco</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">webcam demo:&nbsp;<a target="_blank" href="https://github.com/rbgirshick/fast-rcnn/pull/29" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/rbgirshick/fast-rcnn/pull/29</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">notes:&nbsp;<a target="_blank" href="http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">notes:&nbsp;<a target="_blank" href="http://blog.csdn.net/linj_m/article/details/48930179" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://blog.csdn.net/linj_m/article/details/48930179</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(“Fast R-CNN in MXNet”):&nbsp;<a target="_blank" href="https://github.com/precedenceguo/mx-rcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/precedenceguo/mx-rcnn</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/mahyarnajibi/fast-rcnn-torch" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/mahyarnajibi/fast-rcnn-torch</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/apple2373/chainer-simple-fast-rnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/apple2373/chainer-simple-fast-rnn</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/zplizzi/tensorflow-fast-rcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/zplizzi/tensorflow-fast-rcnn</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.03414" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.03414</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Caffe):&nbsp;<a target="_blank" href="https://github.com/xiaolonw/adversarial-frcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/xiaolonw/adversarial-frcnn</a></li>
  </ul> 
  <h2 id="faster-r-cnn" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Faster R-CNN</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: NIPS 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1506.01497" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1506.01497</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">gitxiv:&nbsp;<a target="_blank" href="http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(official, Matlab):&nbsp;<a target="_blank" href="https://github.com/ShaoqingRen/faster_rcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/ShaoqingRen/faster_rcnn</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/rbgirshick/py-faster-rcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/rbgirshick/py-faster-rcnn</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/mitmul/chainer-faster-rcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/mitmul/chainer-faster-rcnn</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/andreaskoepf/faster-rcnn.torch" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/andreaskoepf/faster-rcnn.torch</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/ruotianluo/Faster-RCNN-Densecap-torch" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/ruotianluo/Faster-RCNN-Densecap-torch</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/smallcorgi/Faster-RCNN_TF" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/smallcorgi/Faster-RCNN_TF</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/CharlesShang/TFFRCNN" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/CharlesShang/TFFRCNN</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(C++ demo):&nbsp;<a target="_blank" href="https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/yhenon/keras-frcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/yhenon/keras-frcnn</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Faster R-CNN in MXNet with distributed implementation and data parallelization</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/dmlc/mxnet/tree/master/example/rcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/dmlc/mxnet/tree/master/example/rcnn</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Contextual Priming and Feedback for Faster R-CNN</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016. Carnegie Mellon University</li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://abhinavsh.info/context_priming_feedback.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://abhinavsh.info/context_priming_feedback.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">poster:&nbsp;<a target="_blank" href="http://www.eccv2016.org/files/posters/P-1A-20.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.eccv2016.org/files/posters/P-1A-20.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">An Implementation of Faster RCNN with Study for Region Sampling</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Technical Report, 3 pages. CMU</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.02138" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.02138</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/endernewton/tf-faster-rcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/endernewton/tf-faster-rcnn</a></li>
  </ul> 
  <hr style="margin:0px; padding:0px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
  <h2 id="multibox" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> MultiBox</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Scalable Object Detection using Deep Neural Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: first MultiBox. Train a CNN to predict Region of Interest.</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1312.2249" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1312.2249</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/google/multibox" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/google/multibox</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Scalable, High-Quality Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: second MultiBox</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1412.1441" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1412.1441</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/google/multibox" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/google/multibox</a></li>
  </ul> 
  <h2 id="spp-net" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> SPP-Net</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2014 / TPAMI 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1406.4729" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1406.4729</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/ShaoqingRen/SPP_net" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/ShaoqingRen/SPP_net</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">notes:&nbsp;<a target="_blank" href="http://zhangliliang.com/2014/09/13/paper-note-sppnet/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://zhangliliang.com/2014/09/13/paper-note-sppnet/</a></li>
  </ul> 
  <h2 id="deepid-net" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> DeepID-Net</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: PAMI 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">intro: an extension of R-CNN. box pre-training, cascade on region proposals, deformation layers and context representations</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1412.5661" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1412.5661</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Object Detectors Emerge in Deep Scene CNNs</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICLR 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1412.6856" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1412.6856</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://places.csail.mit.edu/slide_iclr2015.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://places.csail.mit.edu/slide_iclr2015.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">project(code+data):&nbsp;<a target="_blank" href="https://www.cs.toronto.edu/~yukun/segdeepm.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://www.cs.toronto.edu/~yukun/segdeepm.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1502.04275" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1502.04275</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/YknZhu/segDeepM" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/YknZhu/segDeepM</a></li>
  </ul> 
  <h2 id="noc" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> NoC</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Object Detection Networks on Convolutional Feature Maps</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: TPAMI 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1504.06066" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1504.06066</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1504.03293" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1504.03293</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/YutingZhang/fgs-obj" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/YutingZhang/fgs-obj</a></li>
  </ul> 
  <h2 id="deepbox" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> DeepBox</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DeepBox: Learning Objectness with Convolutional Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1505.02146" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1505.02146</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/weichengkuo/DeepBox" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/weichengkuo/DeepBox</a></li>
  </ul> 
  <h2 id="mr-cnn" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> MR-CNN</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Object detection via a multi-region &amp; semantic segmentation-aware CNN model</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2015. MR-CNN</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1505.01749" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1505.01749</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/gidariss/mrcnn-object-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/gidariss/mrcnn-object-detection</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">notes:&nbsp;<a target="_blank" href="http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">notes:&nbsp;<a target="_blank" href="http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/</a></li>
  </ul> 
  <h2 id="yolo" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> YOLO</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">You Only Look Once: Unified, Real-Time Object Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="https://camo.githubusercontent.com/e69d4118b20a42de4e23b9549f9a6ec6dbbb0814/687474703a2f2f706a7265646469652e636f6d2f6d656469612f66696c65732f6461726b6e65742d626c61636b2d736d616c6c2e706e67" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1506.02640" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1506.02640</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">code:&nbsp;<a target="_blank" href="http://pjreddie.com/darknet/yolo/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://pjreddie.com/darknet/yolo/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/pjreddie/darknet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/pjreddie/darknet</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://pjreddie.com/publications/yolo/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://pjreddie.com/publications/yolo/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">reddit:&nbsp;<a target="_blank" href="https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/gliese581gg/YOLO_tensorflow" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/gliese581gg/YOLO_tensorflow</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/xingwangsfu/caffe-yolo" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/xingwangsfu/caffe-yolo</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/frankzhangrui/Darknet-Yolo" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/frankzhangrui/Darknet-Yolo</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/BriSkyHekun/py-darknet-yolo" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/BriSkyHekun/py-darknet-yolo</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/tommy-qichang/yolo.torch" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/tommy-qichang/yolo.torch</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/frischzenger/yolo-windows" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/frischzenger/yolo-windows</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/AlexeyAB/yolo-windows" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/AlexeyAB/yolo-windows</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/nilboy/tensorflow-yolo" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/nilboy/tensorflow-yolo</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">darkflow - translate darknet to tensorflow. Load trained weights, retrain/fine-tune them using tensorflow, export constant graph def to C++</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/thtrieu/darkflow" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/thtrieu/darkflow</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Start Training YOLO with Our Own Data</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="http://guanghan.info/blog/en/wp-content/uploads/2015/12/images-40.jpg" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: train with customized data and class numbers/labels. Linux / Windows version for darknet.</li>
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="http://guanghan.info/blog/en/my-works/train-yolo/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://guanghan.info/blog/en/my-works/train-yolo/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/Guanghan/darknet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/Guanghan/darknet</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">YOLO: Core ML versus MPSNNGraph</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Tiny YOLO for iOS implemented using CoreML but also using the new MPS graph API.</li>
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="http://machinethink.net/blog/yolo-coreml-versus-mps-graph/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://machinethink.net/blog/yolo-coreml-versus-mps-graph/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/hollance/YOLO-CoreML-MPSNNGraph" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/hollance/YOLO-CoreML-MPSNNGraph</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">TensorFlow YOLO object detection on Android</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Real-time object detection on Android using the YOLO network with TensorFlow</li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/natanielruiz/android-yolo" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/natanielruiz/android-yolo</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Computer Vision in iOS – Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:<a target="_blank" href="https://github.com/r4ghu/iOS-CoreML-Yolo" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/r4ghu/iOS-CoreML-Yolo</a></li>
  </ul> 
  <h2 id="yolov2" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> YOLOv2</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">YOLO9000: Better, Faster, Stronger</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1612.08242" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1612.08242</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">code:&nbsp;<a target="_blank" href="http://pjreddie.com/yolo9000/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://pjreddie.com/yolo9000/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Chainer):&nbsp;<a target="_blank" href="https://github.com/leetenki/YOLOv2" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/leetenki/YOLOv2</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Keras):&nbsp;<a target="_blank" href="https://github.com/allanzelener/YAD2K" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/allanzelener/YAD2K</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(PyTorch):&nbsp;<a target="_blank" href="https://github.com/longcw/yolo2-pytorch" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/longcw/yolo2-pytorch</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Tensorflow):&nbsp;<a target="_blank" href="https://github.com/hizhangp/yolo_tensorflow" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/hizhangp/yolo_tensorflow</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Windows):&nbsp;<a target="_blank" href="https://github.com/AlexeyAB/darknet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/AlexeyAB/darknet</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/choasUp/caffe-yolo9000" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/choasUp/caffe-yolo9000</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/philipperemy/yolo-9000" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/philipperemy/yolo-9000</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Yolo_mark: GUI for marking bounded boxes of objects in images for training Yolo v2</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/AlexeyAB/Yolo_mark" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/AlexeyAB/Yolo_mark</a></li>
  </ul> 
  <hr style="margin:0px; padding:0px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">R-CNN minus R</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1506.06981" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1506.06981</a></li>
  </ul> 
  <h2 id="attentionnet" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> AttentionNet</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">AttentionNet: Aggregating Weak Directions for Accurate Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">intro: state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 human detection task</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1506.07704" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1506.07704</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://image-net.org/challenges/talks/lunit-kaist-slide.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://image-net.org/challenges/talks/lunit-kaist-slide.pdf</a></li>
  </ul> 
  <h2 id="densebox" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> DenseBox</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DenseBox: Unifying Landmark Localization with End to End Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1509.04874" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1509.04874</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">demo:&nbsp;<a target="_blank" href="http://pan.baidu.com/s/1mgoWWsS" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://pan.baidu.com/s/1mgoWWsS</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">KITTI result:&nbsp;<a target="_blank" href="http://www.cvlibs.net/datasets/kitti/eval_object.php" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cvlibs.net/datasets/kitti/eval_object.php</a></li>
  </ul> 
  <h2 id="ssd" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> SSD</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">SSD: Single Shot MultiBox Detector</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="https://camo.githubusercontent.com/ad9b147ed3a5f48ffb7c3540711c15aa04ce49c6/687474703a2f2f7777772e63732e756e632e6564752f7e776c69752f7061706572732f7373642e706e67" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016 Oral</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1512.02325" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1512.02325</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.cs.unc.edu/~wliu/papers/ssd.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cs.unc.edu/~wliu/papers/ssd.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://www.cs.unc.edu/~wliu/papers/ssd_eccv2016_slide.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Official):&nbsp;<a target="_blank" href="https://github.com/weiliu89/caffe/tree/ssd" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/weiliu89/caffe/tree/ssd</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">video:&nbsp;<a target="_blank" href="http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/zhreshold/mxnet-ssd" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/zhreshold/mxnet-ssd</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/zhreshold/mxnet-ssd.cpp" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/zhreshold/mxnet-ssd.cpp</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/rykov8/ssd_keras" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/rykov8/ssd_keras</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/balancap/SSD-Tensorflow" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/balancap/SSD-Tensorflow</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/amdegroot/ssd.pytorch" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/amdegroot/ssd.pytorch</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Caffe):&nbsp;<a target="_blank" href="https://github.com/chuanqi305/MobileNet-SSD" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/chuanqi305/MobileNet-SSD</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">What’s the diffience in performance between this new code you pushed and the previous code? #327</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://github.com/weiliu89/caffe/issues/327" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/weiliu89/caffe/issues/327</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Enhancement of SSD by concatenating feature maps for object detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: rainbow SSD (R-SSD)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1705.09587" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1705.09587</a></li>
  </ul> 
  <h2 id="dssd" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> DSSD</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DSSD : Deconvolutional Single Shot Detector</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: UNC Chapel Hill &amp; Amazon Inc</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1701.06659" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1701.06659</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">demo:&nbsp;<a target="_blank" href="http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Context-aware Single-Shot Detector</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">keywords: CSSD, DiCSSD, DeCSSD, effective receptive fields (ERFs), theoretical receptive fields (TRFs)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1707.08682" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1707.08682</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Feature-Fused SSD: Fast Detection for Small Objects</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1709.05054" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.05054</a></p> 
  <hr style="margin:0px; padding:0px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
  <h2 id="inside-outside-net-ion" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Inside-Outside Net (ION)</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: “0.8s per image on a Titan X GPU (excluding proposal generation) without two-stage bounding-box regression and 1.15s per image with it”.</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1512.04143" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1512.04143</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">coco-leaderboard:&nbsp;<a target="_blank" href="http://mscoco.org/dataset/#detections-leaderboard" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://mscoco.org/dataset/#detections-leaderboard</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Adaptive Object Detection Using Adjacency and Zoom Prediction</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016. AZ-Net</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1512.07711" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1512.07711</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/luyongxi/az-net" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/luyongxi/az-net</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">youtube:&nbsp;<a target="_blank" href="https://www.youtube.com/watch?v=YmFtuNwxaNM" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://www.youtube.com/watch?v=YmFtuNwxaNM</a></li>
  </ul> 
  <h2 id="g-cnn" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> G-CNN</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">G-CNN: an Iterative Grid Based Object Detector</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1512.07729" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1512.07729</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Factors in Finetuning Deep Model for object detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016.rank 3rd for provided data and 2nd for external data on ILSVRC 2015 object detection</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1601.05150" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1601.05150</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">We don’t need no bounding-boxes: Training object class detectors using only human verification</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1602.08405" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1602.08405</a></li>
  </ul> 
  <h2 id="hypernet" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> HyperNet</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1604.00600" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1604.00600</a></li>
  </ul> 
  <h2 id="multipathnet" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> MultiPathNet</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A MultiPath Network for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: BMVC 2016. Facebook AI Research (FAIR)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1604.02135" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1604.02135</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/facebookresearch/multipathnet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/facebookresearch/multipathnet</a></li>
  </ul> 
  <h2 id="craft" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> CRAFT</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">CRAFT Objects from Images</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016. Cascade Region-proposal-network And FasT-rcnn. an extension of Faster R-CNN</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://byangderek.github.io/projects/craft.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://byangderek.github.io/projects/craft.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1604.03239" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1604.03239</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/byangderek/CRAFT" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/byangderek/CRAFT</a></li>
  </ul> 
  <h2 id="ohem" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> OHEM</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Training Region-based Object Detectors with Online Hard Example Mining</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016 Oral. Online hard example mining (OHEM)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1604.03540" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1604.03540</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Official):&nbsp;<a target="_blank" href="https://github.com/abhi2610/ohem" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/abhi2610/ohem</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">author page:&nbsp;<a target="_blank" href="http://abhinav-shrivastava.info/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://abhinav-shrivastava.info/</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: scale-dependent pooling (SDP), cascaded rejection classifiers (CRC)</li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf</a></li>
  </ul> 
  <h2 id="r-fcn" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> R-FCN</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">R-FCN: Object Detection via Region-based Fully Convolutional Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1605.06409" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1605.06409</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/daijifeng001/R-FCN" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/daijifeng001/R-FCN</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/Orpine/py-R-FCN" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/Orpine/py-R-FCN</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/PureDiors/pytorch_RFCN" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/PureDiors/pytorch_RFCN</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/bharatsingh430/py-R-FCN-multiGPU" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/bharatsingh430/py-R-FCN-multiGPU</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/xdever/RFCN-tensorflow" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/xdever/RFCN-tensorflow</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Recycle deep features for better object detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1607.05066" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1607.05066</a></li>
  </ul> 
  <h2 id="ms-cnn" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> MS-CNN</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">intro: 640×480: 15 fps, 960×720: 8 fps</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1607.07155" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1607.07155</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/zhaoweicai/mscnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/zhaoweicai/mscnn</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">poster:&nbsp;<a target="_blank" href="http://www.eccv2016.org/files/posters/P-2B-38.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.eccv2016.org/files/posters/P-2B-38.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Multi-stage Object Detection with Group Recursive Learning</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: VOC2007: 78.6%, VOC2012: 74.9%</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1608.05159" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1608.05159</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: WACV 2017. SubCNN</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1604.04693" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1604.04693</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/tanshen/SubCNN" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/tanshen/SubCNN</a></li>
  </ul> 
  <h2 id="pvanet" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> PVANET</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: “less channels with more layers”, concatenated ReLU, Inception, and HyperNet, batch normalization, residual connections</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1608.08021" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1608.08021</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/sanghoon/pva-faster-rcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/sanghoon/pva-faster-rcnn</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">leaderboard(PVANet 9.0):&nbsp;<a target="_blank" href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">PVANet: Lightweight Deep Neural Networks for Real-time Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Presented at NIPS 2016 Workshop on Efficient Methods for Deep Neural Networks (EMDNN). Continuation of&nbsp;<a target="_blank" href="https://arxiv.org/abs/1608.08021" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">arXiv:1608.08021</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1611.08588" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1611.08588</a></li>
  </ul> 
  <h2 id="gbd-net" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> GBD-Net</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Gated Bi-directional CNN for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: The Chinese University of Hong Kong &amp; Sensetime Group Limited</li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://link.springer.com/chapter/10.1007/978-3-319-46478-7_22" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://link.springer.com/chapter/10.1007/978-3-319-46478-7_22</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">mirror:&nbsp;<a target="_blank" href="https://pan.baidu.com/s/1dFohO7v" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://pan.baidu.com/s/1dFohO7v</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Crafting GBD-Net for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: winner of the ImageNet object detection challenge of 2016. CUImage and CUVideo</li>
   <li style="margin:0px; padding:0px; line-height:24px">intro: gated bi-directional CNN (GBD-Net)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1610.02579" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1610.02579</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/craftGBD/craftGBD" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/craftGBD/craftGBD</a></li>
  </ul> 
  <h2 id="stuffnet" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> StuffNet</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">StuffNet: Using ‘Stuff’ to Improve Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1610.05861" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1610.05861</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1610.09609" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1610.09609</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Hierarchical Object Detection with Deep Reinforcement Learning</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Deep Reinforcement Learning Workshop (NIPS 2016)</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="https://imatge-upc.github.io/detection-2016-nipsws/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://imatge-upc.github.io/detection-2016-nipsws/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1611.03718" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1611.03718</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/imatge-upc/detection-2016-nipsws" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/imatge-upc/detection-2016-nipsws</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="http://jorditorres.org/nips/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://jorditorres.org/nips/</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Learning to detect and localize many objects from few examples</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1611.05664" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1611.05664</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Speed/accuracy trade-offs for modern convolutional object detectors</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Google Research</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1611.10012" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1611.10012</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1612.01051" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1612.01051</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/BichenWuUCB/squeezeDet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/BichenWuUCB/squeezeDet</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/fregu856/2D_detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/fregu856/2D_detection</a></li>
  </ul> 
  <h2 id="feature-pyramid-network-fpn" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Feature Pyramid Network (FPN)</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Feature Pyramid Networks for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Facebook AI Research</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1612.03144" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1612.03144</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Action-Driven Object Detection with Top-Down Visual Attentions</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1612.06704" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1612.06704</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Beyond Skip Connections: Top-Down Modulation for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CMU &amp; UC Berkeley &amp; Google Research</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1612.06851" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1612.06851</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Wide-Residual-Inception Networks for Real-time Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Inha University</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.01243" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.01243</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Attentional Network for Visual Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: University of Maryland &amp; Mitsubishi Electric Research Laboratories</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.01478" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.01478</a></li>
  </ul> 
  <h2 id="cc-net" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> CC-Net</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Learning Chained Deep Features and Classifiers for Cascade in Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: chained cascade network (CC-Net). 81.1% mAP on PASCAL VOC 2007</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.07054" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.07054</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1703.10295" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1703.10295</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.03944" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.03944</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Spatial Memory for Context Reasoning in Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.04224" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.04224</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Accurate Single Stage Detector Using Recurrent Rolling Convolution</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017. SenseTime</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: Recurrent Rolling Convolution (RRC)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.05776" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.05776</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/xiaohaoChen/rrc_detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/xiaohaoChen/rrc_detection</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1704.05775" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.05775</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">S-OHEM: Stratified Online Hard Example Mining for Object Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1705.02233" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1705.02233</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Embedded Vision Workshop in CVPR. UC San Diego &amp; Qualcomm Inc</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1705.05922" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1705.05922</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Point Linking Network for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Point Linking Network (PLN)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1706.03646" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.03646</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Perceptual Generative Adversarial Networks for Small Object Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1706.05274" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.05274</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Few-shot Object Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1706.08249" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.08249</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Yes-Net: An effective Detector Based on Global Information</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1706.09180" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.09180</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">SMC Faster R-CNN: Toward a scene-specialized multi-object detector</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1706.10217" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.10217</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Towards lightweight convolutional neural networks for object detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1707.01395" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1707.01395</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">RON: Reverse Connection with Objectness Prior Networks for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1707.01691" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1707.01691</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/taokong/RON" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/taokong/RON</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Residual Features and Unified Prediction Network for Single Stage Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1707.05031" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1707.05031</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deformable Part-based Fully Convolutional Network for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: BMVC 2017 (oral). Sorbonne Universités &amp; CEDRIC</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1707.06175" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1707.06175</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1707.06399" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1707.06399</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Recurrent Scale Approximation for Object Detection in CNN</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: Recurrent Scale Approximation (RSA)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1707.09531" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1707.09531</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/sciencefans/RSA-for-object-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/sciencefans/RSA-for-object-detection</a></li>
  </ul> 
  <h2 id="dsod" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> DSOD</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DSOD: Learning Deeply Supervised Object Detectors from Scratch</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="https://user-images.githubusercontent.com/3794909/28934967-718c9302-78b5-11e7-89ee-8b514e53e23c.png" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017. Fudan University &amp; Tsinghua University &amp; Intel Labs China</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.01241" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.01241</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/szq0214/DSOD" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/szq0214/DSOD</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Focal Loss for Dense Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017 Best student paper award. Facebook AI Research</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: RetinaNet</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.02002" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.02002</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">CoupleNet: Coupling Global Structure with Local Parts for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.02863" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.02863</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1709.04347" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.04347</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1709.05788" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.05788</a></p> 
  <h2 id="nms" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> NMS</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1411.5309" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1411.5309</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A convnet for non-maximum suppression</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1511.06437" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1511.06437</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Improving Object Detection With One Line of Code</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Soft-NMS – Improving Object Detection With One Line of Code</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017. University of Maryland</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: Soft-NMS</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.04503" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.04503</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/bharatsingh430/soft-nms" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/bharatsingh430/soft-nms</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Learning non-maximum suppression</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1705.02950" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1705.02950</a></p> 
  <h2 id="weakly-supervised-object-detection" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Weakly Supervised Object Detection</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1604.05766" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1604.05766</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Weakly supervised object detection using pseudo-strong labels</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1607.04731" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1607.04731</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Saliency Guided End-to-End Learning for Weakly Supervised Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: IJCAI 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1706.06768" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.06768</a></li>
  </ul> 
  <h1 id="detection-from-video" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Detection From Video</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Learning Object Class Detectors from Weakly Annotated Video</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2012</li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Analysing domain shift factors between videos and images for object detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1501.01186" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1501.01186</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Video Object Recognition</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Learning for Saliency Prediction in Natural Video</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Submitted on 12 Jan 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: Deep learning, saliency map, optical flow, convolution network, contrast features</li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="https://hal.archives-ouvertes.fr/hal-01251614/document" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://hal.archives-ouvertes.fr/hal-01251614/document</a></li>
  </ul> 
  <h2 id="t-cnn" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> T-CNN</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Winning solution in ILSVRC2015 Object Detection from Video(VID) Task</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1604.02532" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1604.02532</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/myfavouritekk/T-CNN" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/myfavouritekk/T-CNN</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Object Detection from Video Tubelets with Convolutional Neural Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016 Spotlight paper</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1604.04053" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1604.04053</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">gihtub:&nbsp;<a target="_blank" href="https://github.com/myfavouritekk/vdetlib" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/myfavouritekk/vdetlib</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Object Detection in Videos with Tubelets and Multi-context Cues</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: SenseTime Group</li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Context Matters: Refining Object Detection in Video with Recurrent Neural Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: BMVC 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: pseudo-labeler</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1607.04648" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1607.04648</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">CNN Based Object Detection in Large Video Images</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: WangTao @ 爱奇艺</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: object retrieval, object detection, scene classification</li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Object Detection in Videos with Tubelet Proposal Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.06355" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.06355</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Flow-Guided Feature Aggregation for Video Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: MSRA</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1703.10025" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1703.10025</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Video Object Detection using Faster R-CNN</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="http://andrewliao11.github.io/object_detection/faster_rcnn/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://andrewliao11.github.io/object_detection/faster_rcnn/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/andrewliao11/py-faster-rcnn-imagenet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/andrewliao11/py-faster-rcnn-imagenet</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Improving Context Modeling for Video Object Detection and Tracking</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster).pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster).pdf</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Temporal Dynamic Graph LSTM for Action-driven Video Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.00666" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.00666</a></li>
  </ul> 
  <h1 id="object-detection-in-3d" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Object Detection in 3D</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1609.06666" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1609.06666</a></li>
  </ul> 
  <h1 id="object-detection-on-rgb-d" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Object Detection on RGB-D</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Learning Rich Features from RGB-D Images for Object Detection and Segmentation</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1407.5736" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1407.5736</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Differential Geometry Boosts Convolutional Neural Networks for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1703.03347" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1703.03347</a></p> 
  <h1 id="salient-object-detection" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Salient Object Detection</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> This task involves predicting the salient regions of an image given by human eye fixations.</p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Best Deep Saliency Detection Models (CVPR 2016 &amp; 2015)</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="http://i.cs.hku.hk/~yzyu/vision.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://i.cs.hku.hk/~yzyu/vision.html</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Large-scale optimization of hierarchical features for saliency prediction in natural images</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://coxlab.org/pdfs/cvpr2014_vig_saliency.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://coxlab.org/pdfs/cvpr2014_vig_saliency.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Predicting Eye Fixations using Convolutional Neural Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.escience.cn/system/file?fileId=72648" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.escience.cn/system/file?fileId=72648</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Saliency Detection by Multi-Context Deep Learning</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1510.05484" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1510.05484</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">SuperCNN: A Superpixelwise Convolutional Neural Network for Salient Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="https://handong1587.github.io/deep_learning/2015/10/09/www.shengfenghe.com/supercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">www.shengfenghe.com/supercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Shallow and Deep Convolutional Networks for Saliency Prediction</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1603.00845" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1603.00845</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/imatge-upc/saliency-2016-cvpr" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/imatge-upc/saliency-2016-cvpr</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Recurrent Attentional Networks for Saliency Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016. recurrent attentional convolutional-deconvolution network (RACDNN)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1604.03227" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1604.03227</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Two-Stream Convolutional Networks for Dynamic Saliency Prediction</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1607.04730" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1607.04730</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Unconstrained Salient Object Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Unconstrained Salient Object Detection via Proposal Subset Optimization</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="http://cs-people.bu.edu/jmzhang/images/pasted%20image%201465x373.jpg" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://cs-people.bu.edu/jmzhang/sod.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://cs-people.bu.edu/jmzhang/sod.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://cs-people.bu.edu/jmzhang/SOD/CVPR16SOD_camera_ready.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://cs-people.bu.edu/jmzhang/SOD/CVPR16SOD_camera_ready.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/jimmie33/SOD" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/jimmie33/SOD</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">caffe model zoo:&nbsp;<a target="_blank" href="https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-object-proposal-models-for-salient-object-detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-object-proposal-models-for-salient-object-detection</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Salient Object Subitizing</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="http://cs-people.bu.edu/jmzhang/images/frontpage.png?crc=123070793" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">intro: predicting the existence and the number of salient objects in an image using holistic cues</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://cs-people.bu.edu/jmzhang/sos.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://cs-people.bu.edu/jmzhang/sos.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1607.07525" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1607.07525</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://cs-people.bu.edu/jmzhang/SOS/SOS_preprint.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://cs-people.bu.edu/jmzhang/SOS/SOS_preprint.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">caffe model zoo:&nbsp;<a target="_blank" href="https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-models-for-salient-object-subitizing" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-models-for-salient-object-subitizing</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deeply-Supervised Recurrent Convolutional Neural Network for Saliency Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ACMMM 2016. deeply-supervised recurrent convolutional neural network (DSRCNN)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1608.05177" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1608.05177</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Saliency Detection via Combining Region-Level and Pixel-Level Predictions with CNNs</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1608.05186" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1608.05186</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Edge Preserving and Multi-Scale Contextual Neural Network for Salient Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1608.08029" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1608.08029</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A Deep Multi-Level Network for Saliency Prediction</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1609.01064" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1609.01064</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Visual Saliency Detection Based on Multiscale Deep CNN Features</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: IEEE Transactions on Image Processing</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1609.02077" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1609.02077</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: DSCLRCN</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1610.01708" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1610.01708</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deeply supervised salient object detection with short connections</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1611.04849" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1611.04849</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Weakly Supervised Top-down Salient Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Nanyang Technological University</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1611.05345" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1611.05345</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">SalGAN: Visual Saliency Prediction with Generative Adversarial Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="https://imatge-upc.github.io/saliency-salgan-2017/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://imatge-upc.github.io/saliency-salgan-2017/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1701.01081" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1701.01081</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Visual Saliency Prediction Using a Mixture of Deep Neural Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.00372" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.00372</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A Fast and Compact Salient Score Regression Network Based on Fully Convolutional Network</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.00615" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.00615</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Saliency Detection by Forward and Backward Cues in Deep-CNNs</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1703.00152" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1703.00152</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Supervised Adversarial Networks for Image Saliency Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1704.07242" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.07242</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Group-wise Deep Co-saliency Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1707.07381" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1707.07381</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Towards the Success Rate of One: Real-time Unconstrained Salient Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: University of Maryland College Park &amp; eBay Inc</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.00079" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.00079</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arixv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.02001" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.02001</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Learning Uncertain Convolutional Features for Accurate Saliency Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Accepted as a poster in ICCV 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.02031" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.02031</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Edge-Aware Saliency Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1708.04366" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.04366</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Self-explanatory Deep Salient Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: National University of Defense Technology, China &amp; National University of Singapore</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.05595" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.05595</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">PiCANet: Learning Pixel-wise Contextual Attention in ConvNets and Its Application in Saliency Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1708.06433" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.06433</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DeepFeat: A Bottom Up and Top Down Saliency Model Based on Deep Features of Convolutional Neural Nets</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1709.02495" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.02495</a></p> 
  <h2 id="saliency-detection-in-video" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Saliency Detection in Video</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Learning For Video Saliency Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.00871" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.00871</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Video Salient Object Detection Using Spatiotemporal Deep Features</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1708.01447" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.01447</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Predicting Video Saliency with Object-to-Motion CNN and Two-layer Convolutional LSTM</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1709.06316" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.06316</a></p> 
  <h1 id="visual-relationship-detection" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Visual Relationship Detection</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Visual Relationship Detection with Language Priors</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016 oral</li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="https://cs.stanford.edu/people/ranjaykrishna/vrd/vrd.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://cs.stanford.edu/people/ranjaykrishna/vrd/vrd.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/Prof-Lu-Cewu/Visual-Relationship-Detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/Prof-Lu-Cewu/Visual-Relationship-Detection</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Visual Phrase reasoning Convolutional Neural Network (ViP-CNN), Visual Phrase Reasoning Structure (VPRS)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.07191" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.07191</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Visual Translation Embedding Network for Visual Relation Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://www.arxiv.org/abs/1702.08319" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://www.arxiv.org/abs/1702.08319</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017 spotlight paper</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1703.03054" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1703.03054</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Detecting Visual Relationships with Deep Relational Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017 oral. The Chinese University of Hong Kong</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.03114" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.03114</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Identifying Spatial Relations in Images using Convolutional Neural Networks</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1706.04215" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.04215</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.01956" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.01956</a></li>
  </ul> 
  <h1 id="specific-object-deteciton" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Specific Object Deteciton</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Deformation Network for Object Landmark Localization</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1605.01014" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1605.01014</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Fashion Landmark Detection in the Wild</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://personal.ie.cuhk.edu.hk/~lz013/projects/FashionLandmarks.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://personal.ie.cuhk.edu.hk/~lz013/projects/FashionLandmarks.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1608.03049" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1608.03049</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Caffe):&nbsp;<a target="_blank" href="https://github.com/liuziwei7/fashion-landmarks" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/liuziwei7/fashion-landmarks</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Learning for Fast and Accurate Fashion Item Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Kuznech Inc.</li>
   <li style="margin:0px; padding:0px; line-height:24px">intro: MultiBox and Fast R-CNN</li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">OSMDeepOD - OSM and Deep Learning based Object Detection from Aerial Imagery (formerly known as “OSM-Crosswalk-Detection”)</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="https://raw.githubusercontent.com/geometalab/OSMDeepOD/master/imgs/process.png" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/geometalab/OSMDeepOD" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/geometalab/OSMDeepOD</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Selfie Detection by Synergy-Constraint Based Convolutional Neural Network</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: IEEE SITIS 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1611.04357" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1611.04357</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Associative Embedding:End-to-End Learning for Joint Detection and Grouping</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1611.05424" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1611.05424</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Cuboid Detection: Beyond 2D Bounding Boxes</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CMU &amp; Magic Leap</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1611.10010" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1611.10010</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1612.03019" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1612.03019</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Learning Logo Detection with Data Expansion by Synthesising Context</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1612.09322" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1612.09322</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Pixel-wise Ear Detection with Convolutional Encoder-Decoder Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.00307" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.00307</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Automatic Handgun Detection Alarm in Videos Using Deep Learning</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.05147" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.05147</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">results:&nbsp;<a target="_blank" href="https://github.com/SihamTabik/Pistol-Detection-in-Videos" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/SihamTabik/Pistol-Detection-in-Videos</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Using Deep Networks for Drone Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: AVSS 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1706.05726" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.05726</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.01642" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.01642</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DeepVoting: An Explainable Framework for Semantic Part Detection under Partial Occlusion</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1709.04577" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.04577</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1709.09283" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.09283</a></p> 
  <h2 id="face-deteciton" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Face Deteciton</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Multi-view Face Detection Using Deep Convolutional Neural Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Yahoo</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1502.02766" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1502.02766</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/guoyilin/FaceDetection_CNN" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/guoyilin/FaceDetection_CNN</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">From Facial Parts Responses to Face Detection: A Deep Learning Approach</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/support/index.png" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2015. CUHK</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/Faceness.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/Faceness.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1509.06451" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1509.06451</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Compact Convolutional Neural Network Cascade for Face Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1508.01292" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1508.01292</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/Bkmz21/FD-Evaluation" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/Bkmz21/FD-Evaluation</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/Bkmz21/CompactCNNCascade" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/Bkmz21/CompactCNNCascade</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Face Detection with End-to-End Integration of a ConvNet and a 3D Model</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1606.00850" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1606.00850</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(MXNet):&nbsp;<a target="_blank" href="https://github.com/tfwu/FaceDetection-ConvNet-3D" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/tfwu/FaceDetection-ConvNet-3D</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CMU</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1606.05413" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1606.05413</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Finding Tiny Faces</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017. CMU</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://www.cs.cmu.edu/~peiyunh/tiny/index.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cs.cmu.edu/~peiyunh/tiny/index.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1612.04402" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1612.04402</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/peiyunh/tiny" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/peiyunh/tiny</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(inference-only):&nbsp;<a target="_blank" href="https://github.com/chinakook/hr101_mxnet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/chinakook/hr101_mxnet</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Towards a Deep Learning Framework for Unconstrained Face Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: overlap with CMS-RCNN</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1612.05322" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1612.05322</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Supervised Transformer Network for Efficient Face Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1607.05477" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1607.05477</a></li>
  </ul> 
  <h3 id="unitbox" class="clickable-header" style="margin:0px; padding:0px; font-size:18px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> UnitBox</h3> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">UnitBox: An Advanced Object Detection Network</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ACM MM 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1608.01471" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1608.01471</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Bootstrapping Face Detection with Hard Negative Examples</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">author: 万韶华 @ 小米.</li>
   <li style="margin:0px; padding:0px; line-height:24px">intro: Faster R-CNN, hard negative mining. state-of-the-art on the FDDB dataset</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1608.02236" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1608.02236</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Grid Loss: Detecting Occluded Faces</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1609.00129" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1609.00129</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://lrs.icg.tugraz.at/pubs/opitz_eccv_16.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://lrs.icg.tugraz.at/pubs/opitz_eccv_16.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">poster:&nbsp;<a target="_blank" href="http://www.eccv2016.org/files/posters/P-2A-34.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.eccv2016.org/files/posters/P-2A-34.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A Multi-Scale Cascade Fully Convolutional Network Face Detector</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICPR 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1609.03536" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1609.03536</a></li>
  </ul> 
  <h3 id="mtcnn" class="clickable-header" style="margin:0px; padding:0px; font-size:18px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> MTCNN</h3> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="https://kpzhang93.github.io/MTCNN_face_detection_alignment/support/index.png" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1604.02878" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1604.02878</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Matlab):&nbsp;<a target="_blank" href="https://github.com/kpzhang93/MTCNN_face_detection_alignment" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/kpzhang93/MTCNN_face_detection_alignment</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/pangyupo/mxnet_mtcnn_face_detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/pangyupo/mxnet_mtcnn_face_detection</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/DaFuCoding/MTCNN_Caffe" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/DaFuCoding/MTCNN_Caffe</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(MXNet):&nbsp;<a target="_blank" href="https://github.com/Seanlinx/mtcnn" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/Seanlinx/mtcnn</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Caffe):&nbsp;<a target="_blank" href="https://github.com/foreverYoungGitHub/MTCNN" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/foreverYoungGitHub/MTCNN</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/CongWeilin/mtcnn-caffe" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/CongWeilin/mtcnn-caffe</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/AlphaQi/MTCNN-light" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/AlphaQi/MTCNN-light</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Face Detection using Deep Learning: An Improved Faster RCNN Approach</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: DeepIR Inc</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1701.08289" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1701.08289</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Faceness-Net: Face Detection through Deep Facial Part Responses</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: An extended version of ICCV 2015 paper</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1701.08393" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1701.08393</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Multi-Path Region-Based Convolutional Neural Network for Accurate Detection of Unconstrained “Hard Faces”</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017. MP-RCNN, MP-RPN</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1703.09145" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1703.09145</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">End-To-End Face Detection and Recognition</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1703.10818" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1703.10818</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Face R-CNN</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1706.01061" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.01061</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Face Detection through Scale-Friendly Deep Convolutional Networks</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1706.02863" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.02863</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Scale-Aware Face Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017. SenseTime &amp; Tsinghua University</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1706.09876" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.09876</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Multi-Branch Fully Convolutional Network for Face Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1707.06330" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1707.06330</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">SSH: Single Stage Headless Face Detector</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.03979" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.03979</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Dockerface: an easy to install and use Faster R-CNN face detector in a Docker container</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1708.04370" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.04370</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">FaceBoxes: A CPU Real-time Face Detector with High Accuracy</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: IJCB 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: Rapidly Digested Convolutional Layers (RDCL), Multiple Scale Convolutional Layers (MSCL)</li>
   <li style="margin:0px; padding:0px; line-height:24px">intro: the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.05234" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.05234</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">S3FD: Single Shot Scale-invariant Face Detector</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">intro: can run at 36 FPS on a Nvidia Titan X (Pascal) for VGA-resolution images</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1708.05237" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.05237</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Detecting Faces Using Region-based Fully Convolutional Networks</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1709.05256" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.05256</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1709.07326" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.07326</a></p> 
  <h2 id="facial-point--landmark-detection" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Facial Point / Landmark Detection</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Convolutional Network Cascade for Facial Point Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="http://mmlab.ie.cuhk.edu.hk/archive/CNN/data/Picture1.png" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">homepage:&nbsp;<a target="_blank" href="http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr13.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.ee.cuhk.edu.hk/~xgwang/papers/sunWTcvpr13.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/luoyetx/deep-landmark" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/luoyetx/deep-landmark</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Facial Landmark Detection by Deep Multi-task Learning</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2014</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://mmlab.ie.cuhk.edu.hk/projects/TCDCN.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepfacealign.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://personal.ie.cuhk.edu.hk/~ccloy/files/eccv_2014_deepfacealign.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Matlab):&nbsp;<a target="_blank" href="https://github.com/zhzhanp/TCDCN-face-alignment" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/zhzhanp/TCDCN-face-alignment</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A Recurrent Encoder-Decoder Network for Sequential Face Alignment</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1608.05477" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1608.05477</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Detecting facial landmarks in the video based on a hybrid framework</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1609.06441" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1609.06441</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Constrained Local Models for Facial Landmark Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1611.08657" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1611.08657</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Effective face landmark localization via single deep network</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.02719" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.02719</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A Convolution Tree with Deconvolution Branches: Exploiting Geometric Relationships for Single Shot Keypoint Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1704.01880" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.01880</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Alignment Network: A convolutional neural network for robust face alignment</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPRW 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1706.01789" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.01789</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">gihtub:&nbsp;<a target="_blank" href="https://github.com/MarekKowalski/DeepAlignmentNetwork" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/MarekKowalski/DeepAlignmentNetwork</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Joint Multi-view Face Alignment in the Wild</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1708.06023" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.06023</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">FacePoseNet: Making a Case for Landmark-Free Face Alignment</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1708.07517" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.07517</a></p> 
  <h1 id="people-detection" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> People Detection</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">End-to-end people detection in crowded scenes</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="https://handong1587.github.io/assets/object-detection-materials/end_to_end_people_detection_in_crowded_scenes.jpg" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1506.04878" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1506.04878</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/Russell91/reinspect" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/Russell91/reinspect</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">ipn:&nbsp;<a target="_blank" href="http://nbviewer.ipython.org/github/Russell91/ReInspect/blob/master/evaluation_reinspect.ipynb" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://nbviewer.ipython.org/github/Russell91/ReInspect/blob/master/evaluation_reinspect.ipynb</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">youtube:&nbsp;<a target="_blank" href="https://www.youtube.com/watch?v=QeWl0h3kQ24" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://www.youtube.com/watch?v=QeWl0h3kQ24</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Detecting People in Artwork with CNNs</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016 Workshops</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1610.08871" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1610.08871</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Multi-camera People Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.04593" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.04593</a></li>
  </ul> 
  <h2 id="person-head-detection" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Person Head Detection</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Context-aware CNNs for person head detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://www.di.ens.fr/willow/research/headdetection/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.di.ens.fr/willow/research/headdetection/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1511.07917" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1511.07917</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/aosokin/cnn_head_detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/aosokin/cnn_head_detection</a></li>
  </ul> 
  <h2 id="pedestrian-detection" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Pedestrian Detection</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Pedestrian Detection aided by Deep Learning Semantic Tasks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://mmlab.ie.cuhk.edu.hk/projects/TA-CNN/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://mmlab.ie.cuhk.edu.hk/projects/TA-CNN/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1412.0069" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1412.0069</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Learning Strong Parts for Pedestrian Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2015. CUHK. DeepParts</li>
   <li style="margin:0px; padding:0px; line-height:24px">intro: Achieving 11.89% average miss rate on Caltech Pedestrian Dataset</li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://personal.ie.cuhk.edu.hk/~pluo/pdf/tianLWTiccv15.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://personal.ie.cuhk.edu.hk/~pluo/pdf/tianLWTiccv15.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Taking a Deeper Look at Pedestrians</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1501.05790" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1501.05790</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Convolutional Channel Features</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1504.07339" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1504.07339</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/byangderek/CCF" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/byangderek/CCF</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Learning Complexity-Aware Cascades for Deep Pedestrian Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1507.05348" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1507.05348</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep convolutional neural networks for pedestrian detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1510.03608" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1510.03608</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/DenisTome/DeepPed" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/DenisTome/DeepPed</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Scale-aware Fast R-CNN for Pedestrian Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1510.08160" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1510.08160</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">New algorithm improves speed and accuracy of pedestrian detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="http://www.eurekalert.org/pub_releases/2016-02/uoc--nai020516.php" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.eurekalert.org/pub_releases/2016-02/uoc–nai020516.php</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Pushing the Limits of Deep CNNs for Pedestrian Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: “set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from 11.7% to 8.9%”</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1603.04525" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1603.04525</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A Real-Time Deep Learning Pedestrian Detector for Robot Navigation</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1607.04436" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1607.04436</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">A Real-Time Pedestrian Detector using Deep Learning for Human-Aware Navigation</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1607.04441" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1607.04441</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Is Faster R-CNN Doing Well for Pedestrian Detection?</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1607.07032" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1607.07032</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Reduced Memory Region Based Deep Convolutional Neural Network Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: IEEE 2016 ICCE-Berlin</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1609.02500" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1609.02500</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1610.03466" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1610.03466</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Multispectral Deep Neural Networks for Pedestrian Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: BMVC 2016 oral</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1611.02644" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1611.02644</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://ml.cs.tsinghua.edu.cn:5000/publications/synunity/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://ml.cs.tsinghua.edu.cn:5000/publications/synunity/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1703.06283" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1703.06283</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Tensorflow):&nbsp;<a target="_blank" href="https://github.com/huangshiyu13/RPNplus" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/huangshiyu13/RPNplus</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> [https://arxiv.org/abs/1706.08564](https://arxiv.org/abs/1706.08564</p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Rotational Rectification Network for Robust Pedestrian Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CMU &amp; Volvo Construction</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1706.08917" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.08917</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">STD-PD: Generating Synthetic Training Data for Pedestrian Detection in Unannotated Videos</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: The University of North Carolina at Chapel Hill</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1707.09100" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1707.09100</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Too Far to See? Not Really! — Pedestrian Detection with Scale-aware Localization Policy</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1709.00235" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.00235</a></p> 
  <h2 id="vehicle-detection" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Vehicle Detection</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DAVE: A Unified Framework for Fast Vehicle Detection and Annotation</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1607.04564" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1607.04564</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Evolving Boxes for fast Vehicle Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1702.00254" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1702.00254</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Fine-Grained Car Detection for Visual Census Estimation</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: AAAI 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1709.02480" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.02480</a></li>
  </ul> 
  <h2 id="traffic-sign-detection" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Traffic-Sign Detection</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Traffic-Sign Detection and Classification in the Wild</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">project page(code+dataset):&nbsp;<a target="_blank" href="http://cg.cs.tsinghua.edu.cn/traffic-sign/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://cg.cs.tsinghua.edu.cn/traffic-sign/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">code &amp; model:&nbsp;<a target="_blank" href="http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/newdata0411.zip" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/newdata0411.zip</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Detecting Small Signs from Large Images</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: IEEE Conference on Information Reuse and Integration (IRI) 2017 oral</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1706.08574" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1706.08574</a></li>
  </ul> 
  <h2 id="boundary--edge--contour-detection" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Boundary / Edge / Contour Detection</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Holistically-Nested Edge Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="https://camo.githubusercontent.com/da32e7e3275c2a9693dd2a6925b03a1151e2b098/687474703a2f2f70616765732e756373642e6564752f7e7a74752f6865642e6a7067" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2015, Marr Prize</li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1504.06375" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1504.06375</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/s9xie/hed" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/s9xie/hed</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Unsupervised Learning of Edges</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016. Facebook AI Research</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1511.04166" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1511.04166</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">zn-blog:&nbsp;<a target="_blank" href="http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.leiphone.com/news/201607/b1trsg9j6GSMnjOP.html</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Pushing the Boundaries of Boundary Detection using Deep Learning</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1511.07386" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1511.07386</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Convolutional Oriented Boundaries</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1608.02755" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1608.02755</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://www.vision.ee.ethz.ch/~cvlsegmentation/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.vision.ee.ethz.ch/~cvlsegmentation/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1701.04658" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1701.04658</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/kmaninis/COB" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/kmaninis/COB</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Richer Convolutional Features for Edge Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: richer convolutional features (RCF)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1612.02103" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1612.02103</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/yun-liu/rcf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/yun-liu/rcf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Contour Detection from Deep Patch-level Boundary Prediction</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1705.03159" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1705.03159</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">CASENet: Deep Category-Aware Semantic Edge Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1705.09759" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1705.09759</a></li>
  </ul> 
  <h2 id="skeleton-detection" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Skeleton Detection</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="https://camo.githubusercontent.com/88a65f132aa4ae4b0477e3ad02c13cdc498377d9/687474703a2f2f37786e37777a2e636f6d312e7a302e676c622e636c6f7564646e2e636f6d2f44656570536b656c65746f6e2e706e673f696d61676556696577322f322f772f353030" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1603.09446" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1603.09446</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/zeakey/DeepSkeleton" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/zeakey/DeepSkeleton</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1609.03659" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1609.03659</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">SRN: Side-output Residual Network for Object Symmetry Detection in the Wild</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1703.02243" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1703.02243</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/KevinKecc/SRN" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/KevinKecc/SRN</a></li>
  </ul> 
  <h2 id="fruit-detection" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Fruit Detection</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Fruit Detection in Orchards</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1610.03677" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1610.03677</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: The Journal of Field Robotics in May 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://confluence.acfr.usyd.edu.au/display/AGPub/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://confluence.acfr.usyd.edu.au/display/AGPub/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1610.08120" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1610.08120</a></li>
  </ul> 
  <h2 id="part-detection" class="clickable-header" style="margin:0px; padding:0px; font-size:20px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Part Detection</h2> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Objects as context for part detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1703.09529" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1703.09529</a></p> 
  <h1 id="object-proposal" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Object Proposal</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1510.04445" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1510.04445</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/aghodrati/deepproposal" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/aghodrati/deepproposal</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Scale-aware Pixel-wise Object Proposal Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: IEEE Transactions on Image Processing</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1601.04798" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1601.04798</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: BMVC 2016. AttractioNet</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1606.04446" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1606.04446</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/gidariss/AttractioNet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/gidariss/AttractioNet</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Learning to Segment Object Proposals via Recursive Neural Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1612.01057" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1612.01057</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Learning Detection with Diverse Proposals</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: differentiable Determinantal Point Process (DPP) layer, Learning Detection with Diverse Proposals (LDDP)</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.03533" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.03533</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">keywords: product detection</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.06752" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.06752</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Improving Small Object Proposals for Company Logo Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICMR 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1704.08881" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1704.08881</a></li>
  </ul> 
  <h1 id="localization" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Localization</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Beyond Bounding Boxes: Precise Localization of Objects in Images</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: PhD Thesis</li>
   <li style="margin:0px; padding:0px; line-height:24px">homepage:&nbsp;<a target="_blank" href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">phd-thesis:&nbsp;<a target="_blank" href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(“SDS using hypercolumns”):&nbsp;<a target="_blank" href="https://github.com/bharath272/sds" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/bharath272/sds</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1503.00949" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1503.00949</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Weakly Supervised Object Localization Using Size Estimates</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1608.04314" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1608.04314</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Active Object Localization with Deep Reinforcement Learning</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2015</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: Markov Decision Process</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1511.06015" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1511.06015</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Localizing objects using referring expressions</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">keywords: LSTM, multiple instance learning (MIL)</li>
   <li style="margin:0px; padding:0px; line-height:24px">paper:&nbsp;<a target="_blank" href="http://www.umiacs.umd.edu/~varun/files/refexp-ECCV16.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.umiacs.umd.edu/~varun/files/refexp-ECCV16.pdf</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/varun-nagaraja/referring-expressions" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/varun-nagaraja/referring-expressions</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">LocNet: Improving Localization Accuracy for Object Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: CVPR 2016 oral</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1511.07763" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1511.07763</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/gidariss/LocNet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/gidariss/LocNet</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Learning Deep Features for Discriminative Localization</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="http://cnnlocalization.csail.mit.edu/framework.jpg" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">homepage:&nbsp;<a target="_blank" href="http://cnnlocalization.csail.mit.edu/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://cnnlocalization.csail.mit.edu/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1512.04150" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1512.04150</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github(Tensorflow):&nbsp;<a target="_blank" href="https://github.com/jazzsaxmafia/Weakly_detector" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/jazzsaxmafia/Weakly_detector</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/metalbubble/CAM" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/metalbubble/CAM</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/tdeboissiere/VGG16CAM-keras" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/tdeboissiere/VGG16CAM-keras</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <img src="http://www.di.ens.fr/willow/research/contextlocnet/model.png" alt="" style="margin:0px; padding:0px; position:relative; max-width:100%"></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ECCV 2016</li>
   <li style="margin:0px; padding:0px; line-height:24px">project page:&nbsp;<a target="_blank" href="http://www.di.ens.fr/willow/research/contextlocnet/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://www.di.ens.fr/willow/research/contextlocnet/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="http://arxiv.org/abs/1609.04331" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://arxiv.org/abs/1609.04331</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/vadimkantorov/contextlocnet" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/vadimkantorov/contextlocnet</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Ensemble of Part Detectors for Simultaneous Classification and Localization</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1705.10034" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1705.10034</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">STNet: Selective Tuning of Convolutional Networks for Object Localization</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://arxiv.org/abs/1708.06418" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1708.06418</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Soft Proposal Networks for Weakly Supervised Object Localization</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ICCV 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1709.01829" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.01829</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Fine-grained Discriminative Localization via Saliency-guided Faster R-CNN</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: ACM MM 2017</li>
   <li style="margin:0px; padding:0px; line-height:24px">arxiv:&nbsp;<a target="_blank" href="https://arxiv.org/abs/1709.08295" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://arxiv.org/abs/1709.08295</a></li>
  </ul> 
  <h1 id="tutorials--talks" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Tutorials / Talks</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Convolutional Feature Maps: Elements of efficient (and accurate) CNN-based object detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://research.microsoft.com/en-us/um/people/kahe/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://research.microsoft.com/en-us/um/people/kahe/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Towards Good Practices for Recognition &amp; Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Hikvision Research Institute. Supervised Data Augmentation (SDA)</li>
   <li style="margin:0px; padding:0px; line-height:24px">slides:&nbsp;<a target="_blank" href="http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf</a></li>
  </ul> 
  <h1 id="projects" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Projects</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">TensorBox: a simple framework for training neural networks to detect objects in images</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: “The basic model implements the simple and robust GoogLeNet-OverFeat algorithm. We additionally provide an implementation of the&nbsp;<a target="_blank" href="https://github.com/Russell91/ReInspect/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">ReInspect</a>&nbsp;algorithm”</li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/Russell91/TensorBox" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/Russell91/TensorBox</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Object detection in torch: Implementation of some object detection frameworks in torch</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/fmassa/object-detection.torch" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/fmassa/object-detection.torch</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Using DIGITS to train an Object Detection network</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/README.md" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/README.md</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">FCN-MultiBox Detector</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: Full convolution MultiBox Detector (like SSD) implemented in Torch.</li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/teaonly/FMD.torch" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/teaonly/FMD.torch</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">KittiBox: A car detection model implemented in Tensorflow.</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">keywords: MultiNet</li>
   <li style="margin:0px; padding:0px; line-height:24px">intro: KittiBox is a collection of scripts to train out model FastBox on the Kitti Object Detection Dataset</li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/MarvinTeichmann/KittiBox" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/MarvinTeichmann/KittiBox</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deformable Convolutional Networks + MST + Soft-NMS</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/bharatsingh430/Deformable-ConvNets" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/bharatsingh430/Deformable-ConvNets</a></li>
  </ul> 
  <h1 id="tools" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Tools</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">BeaverDam: Video annotation tool for deep learning training labels</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="https://github.com/antingshen/BeaverDam" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/antingshen/BeaverDam</a></p> 
  <h1 id="blogs" class="clickable-header top-level-header" style="margin:0px 0px 1em; padding:0px; font-size:22px; color:rgb(77,77,77); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; background-color:rgb(227,222,216)"> Blogs</h1> 
  <span class="icon-arrow-up back-to-top" style=""></span>
  <span style="font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"></span> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Convolutional Neural Networks for Object Detection</span></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <a target="_blank" href="http://rnd.azoft.com/convolutional-neural-networks-object-detection/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://rnd.azoft.com/convolutional-neural-networks-object-detection/</a></p> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Introducing automatic object detection to visual search (Pinterest)</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">keywords: Faster R-CNN</li>
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://engineering.pinterest.com/blog/introducing-automatic-object-detection-visual-search" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://engineering.pinterest.com/blog/introducing-automatic-object-detection-visual-search</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">demo:&nbsp;<a target="_blank" href="https://engineering.pinterest.com/sites/engineering/files/Visual%20Search%20V1%20-%20Video.mp4" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://engineering.pinterest.com/sites/engineering/files/Visual%20Search%20V1%20-%20Video.mp4</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">review:&nbsp;<a target="_blank" href="https://news.developer.nvidia.com/pinterest-introduces-the-future-of-visual-search/?mkt_tok=eyJpIjoiTnpaa01UWXpPRE0xTURFMiIsInQiOiJJRjcybjkwTmtmallORUhLOFFFODBDclFqUlB3SWlRVXJXb1MrQ013TDRIMGxLQWlBczFIeWg0TFRUdnN2UHY2ZWFiXC9QQVwvQzBHM3B0UzBZblpOSmUyU1FcLzNPWXI4cml2VERwTTJsOFwvOEk9In0%3D" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://news.developer.nvidia.com/pinterest-introduces-the-future-of-visual-search/?mkt_tok=eyJpIjoiTnpaa01UWXpPRE0xTURFMiIsInQiOiJJRjcybjkwTmtmallORUhLOFFFODBDclFqUlB3SWlRVXJXb1MrQ013TDRIMGxLQWlBczFIeWg0TFRUdnN2UHY2ZWFiXC9QQVwvQzBHM3B0UzBZblpOSmUyU1FcLzNPWXI4cml2VERwTTJsOFwvOEk9In0%3D</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Deep Learning for Object Detection with DIGITS</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Analyzing The Papers Behind Facebook’s Computer Vision Approach</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">keywords: DeepMask, SharpMask, MultiPathNet</li>
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook' rel=" nofollow"s-computer-vision-approach " style="margin:0px; padding:0px; color:rgb(145,115,107)">https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook’s-Computer-Vision-Approach/</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Easily Create High Quality Object Detectors with Deep Learning</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">intro: dlib v19.2</li>
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="http://blog.dlib.net/2016/10/easily-create-high-quality-object.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">http://blog.dlib.net/2016/10/easily-create-high-quality-object.html</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">How to Train a Deep-Learned Object Detection Model in the Microsoft Cognitive Toolkit</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://blogs.technet.microsoft.com/machinelearning/2016/10/25/how-to-train-a-deep-learned-object-detection-model-in-cntk/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://blogs.technet.microsoft.com/machinelearning/2016/10/25/how-to-train-a-deep-learned-object-detection-model-in-cntk/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Object Detection in Satellite Imagery, a Low Overhead Approach</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">part 1:&nbsp;<a target="_blank" href="https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7#.2csh4iwx9" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7#.2csh4iwx9</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">part 2:&nbsp;<a target="_blank" href="https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-ii-893f40122f92#.f9b7dgf64" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-ii-893f40122f92#.f9b7dgf64</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">You Only Look Twice — Multi-Scale Object Detection in Satellite Imagery With Convolutional Neural Networks</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">part 1:&nbsp;<a target="_blank" href="https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571#.fmmi2o3of" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571#.fmmi2o3of</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">part 2:&nbsp;<a target="_blank" href="https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588#.nwzarsz1t" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588#.nwzarsz1t</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Faster R-CNN Pedestrian and Car Detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://bigsnarf.wordpress.com/2016/11/07/faster-r-cnn-pedestrian-and-car-detection/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://bigsnarf.wordpress.com/2016/11/07/faster-r-cnn-pedestrian-and-car-detection/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">ipn:&nbsp;<a target="_blank" href="https://gist.github.com/bigsnarfdude/2f7b2144065f6056892a98495644d3e0#file-demo_faster_rcnn_notebook-ipynb" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://gist.github.com/bigsnarfdude/2f7b2144065f6056892a98495644d3e0#file-demo_faster_rcnn_notebook-ipynb</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/bigsnarfdude/Faster-RCNN_TF" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/bigsnarfdude/Faster-RCNN_TF</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Small U-Net for vehicle detection</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://medium.com/@vivek.yadav/small-u-net-for-vehicle-detection-9eec216f9fd6#.md4u80kad" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://medium.com/@vivek.yadav/small-u-net-for-vehicle-detection-9eec216f9fd6#.md4u80kad</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Region of interest pooling explained</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://deepsense.io/region-of-interest-pooling-explained/" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://deepsense.io/region-of-interest-pooling-explained/</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/deepsense-io/roi-pooling" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/deepsense-io/roi-pooling</a></li>
  </ul> 
  <p style="margin-top:1em; margin-bottom:1em; padding-top:0px; padding-bottom:0px; line-height:1.6; color:rgb(86,86,86); font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> <span style="margin:0px; padding:0px">Supercharge your Computer Vision models with the TensorFlow Object Detection API</span></p> 
  <ul style="margin:0px 0px 1em; padding:0px 0px 0px 20px; font-family:Georgia,&quot;Hiragino Sans GB&quot;,宋体; font-size:14px; background-color:rgb(227,222,216)"> 
   <li style="margin:0px; padding:0px; line-height:24px">blog:&nbsp;<a target="_blank" href="https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html</a></li>
   <li style="margin:0px; padding:0px; line-height:24px">github:&nbsp;<a target="_blank" href="https://github.com/tensorflow/models/tree/master/object_detection" rel="nofollow" style="margin:0px; padding:0px; color:rgb(145,115,107)">https://github.com/tensorflow/models/tree/master/object_detection</a></li>
  </ul> 
  <br> 
  <p><br> </p> 
  <p><br> </p> 
 </div> 
</div> 
<div class="hide-article-box text-center"> 
 <a class="btn btn-red-hollow" id="btn-readmore" data-track-view="{&quot;mod&quot;:&quot;popu_376&quot;,&quot;con&quot;:&quot;,https://blog.csdn.net/leoking01/article/details/78531902,&quot;}" data-track-click="{&quot;mod&quot;:&quot;popu_376&quot;,&quot;con&quot;:&quot;,https://blog.csdn.net/leoking01/article/details/78531902,&quot;}">阅读更多</a> 
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?ee64533f3c6a7a284cd39a37ee732c8b";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
