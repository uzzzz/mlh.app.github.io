<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>源码:Spark SQL 分区特性第一弹 | 有组织在！</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="源码:Spark SQL 分区特性第一弹" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="版权声明：本文为博主原创文章，未经博主允许不得转载。 https://blog.csdn.net/rlnLo2pNEfx9c/article/details/84901179 头条号上说过近期分享Spark SQL系列文章，前面在头条号上分享了Dataset API的基本操作和复杂操作，不知道下面大家有没有自己测试一下。 今天主要是分享Spark SQL Dataset数据源的分区特性，而且是第一弹的数据格式是partquet。 常见RDD分区 Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。 举几个浪尖在星球里分享比较多的例子，比如： Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。 Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。 普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。 这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。 准备数据 首先是由Seq数据集合生成一个Dataset val sales = spark.createDataFrame(Seq( &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2016, 110), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2017, 10), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 100), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 50), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 80), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 100), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 130), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 160), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2017, 200), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2017, 100), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2016, 150), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2015, 50), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2015, 30), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2015, 10), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2014, 200), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2014, 170), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 50), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 70), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 110), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 150), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 180), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2016, 30), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2015, 200), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2014, 20) &nbsp; &nbsp;)).toDF(&quot;city&quot;, &quot;year&quot;, &quot;amount&quot;) 将Dataset存处为partquet格式的hive表，分两种情况： 用city和year字段分区 sales.write.partitionBy(&quot;city&quot;,&quot;year&quot;).mode(SaveMode.Overwrite).saveAsTable(&quot;ParquetTestCityAndYear&quot;) 用city字段分区 sales.write.partitionBy(&quot;city&quot;).mode(SaveMode.Overwrite).saveAsTable(&quot;ParquetTestCity&quot;) 读取数据采用的是 val res = spark.read.parquet(&quot;/user/hive/warehouse/parquettestcity&quot;) 直接展示，结果发现结果会随着spark.default.parallelism变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。 1. spark.default.parallelism =40 Dataset的分区数是由参数： println(&quot;partition size = &quot;+res.rdd.partitions.length) 目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。 这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。 2. spark.default.parallelism =4 Dataset的分区数是由参数： println(&quot;partition size = &quot;+res.rdd.partitions.length) 目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。 那么数据源生成的Dataset的分区数到底是如何决定的呢？ 我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。 private def createNonBucketedReadRDD( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;readFile: (PartitionedFile) =&gt; Iterator[InternalRow], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;selectedPartitions: Seq[PartitionDirectory], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;fsRelation: HadoopFsRelation): RDD[InternalRow] = { &nbsp; &nbsp;/* &nbsp; &nbsp; &nbsp;selectedPartitions 的大小代表目录数目 &nbsp; &nbsp; */ &nbsp; &nbsp;println(&quot;selectedPartitions.size : &quot;+ selectedPartitions.size) &nbsp; &nbsp;val defaultMaxSplitBytes = &nbsp; &nbsp; &nbsp;fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes &nbsp; &nbsp;val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes &nbsp; &nbsp;// spark.default.parallelism &nbsp; &nbsp;val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism &nbsp; &nbsp;// 计算文件总大小，单位字节数 &nbsp; &nbsp;val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum &nbsp; &nbsp;//计算平均每个并行度读取数据大小 &nbsp; &nbsp;val bytesPerCore = totalBytes / defaultParallelism &nbsp; &nbsp;// 首先spark.sql.files.openCostInBytes 该参数配置的值和bytesPerCore 取最大值 &nbsp; &nbsp;// 然后，比较spark.sql.files.maxPartitionBytes 取小者 &nbsp; &nbsp;val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore)) &nbsp; &nbsp;logInfo(s&quot;Planning scan with bin packing, max size: $maxSplitBytes bytes, &quot; + &nbsp; &nbsp; &nbsp;s&quot;open cost is considered as scanning $openCostInBytes bytes.&quot;) &nbsp; &nbsp;// 这对目录遍历 &nbsp; &nbsp;val splitFiles = selectedPartitions.flatMap { partition =&gt; &nbsp; &nbsp; &nbsp;partition.files.flatMap { file =&gt; &nbsp; &nbsp; &nbsp; &nbsp;val blockLocations = getBlockLocations(file) &nbsp; &nbsp; &nbsp; &nbsp;//判断文件类型是否支持分割，以parquet为例，是支持分割的 &nbsp; &nbsp; &nbsp; &nbsp;if (fsRelation.fileFormat.isSplitable( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;fsRelation.sparkSession, fsRelation.options, file.getPath)) {// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;eg. 0 until 2不包括 2。相当于// &nbsp; &nbsp; &nbsp; &nbsp;println(0 until(10) by 3) 输出 Range(0, 3, 6, 9) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(0L until file.getLen by maxSplitBytes).map { offset =&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// 计算文件剩余的量 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val remaining = file.getLen - offset// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val size = if (remaining &gt; maxSplitBytes) maxSplitBytes else remaining// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;位置信息 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val hosts = getBlockHosts(blockLocations, offset, size) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;PartitionedFile( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partition.values, file.getPath.toUri.toString, offset, size, hosts) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp; &nbsp;} else {// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;不可分割的话，那即是一个文件一个分区 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val hosts = getBlockHosts(blockLocations, 0, file.getLen) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Seq(PartitionedFile( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts)) &nbsp; &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp;}.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse) &nbsp; &nbsp;val partitions = new ArrayBuffer[FilePartition] &nbsp; &nbsp;val currentFiles = new ArrayBuffer[PartitionedFile] &nbsp; &nbsp;var currentSize = 0L &nbsp; &nbsp;/** Close the current partition and move to the next. */ &nbsp; &nbsp;def closePartition(): Unit = { &nbsp; &nbsp; &nbsp;if (currentFiles.nonEmpty) { &nbsp; &nbsp; &nbsp; &nbsp;val newPartition = &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;FilePartition( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partitions.size, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;currentFiles.toArray.toSeq) // Copy to a new Array. &nbsp; &nbsp; &nbsp; &nbsp;partitions += newPartition &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp;currentFiles.clear() &nbsp; &nbsp; &nbsp;currentSize = 0 &nbsp; &nbsp;} &nbsp; &nbsp;// Assign files to partitions using &quot;Next Fit Decreasing&quot; &nbsp; &nbsp;splitFiles.foreach { file =&gt; &nbsp; &nbsp; &nbsp;if (currentSize + file.length &gt; maxSplitBytes) { &nbsp; &nbsp; &nbsp; &nbsp;closePartition() &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp;// Add the given file to the current partition. &nbsp; &nbsp; &nbsp;currentSize += file.length + openCostInBytes &nbsp; &nbsp; &nbsp;currentFiles += file &nbsp; &nbsp;} &nbsp; &nbsp;closePartition() &nbsp; &nbsp;println(&quot;FileScanRDD partitions size : &quot;+partitions.size) &nbsp; &nbsp;new FileScanRDD(fsRelation.sparkSession, readFile, partitions) &nbsp;} 这次视频上传到了今日头条了，可以点击阅读原文观看。 知识星球球友可以直接获取视频及所有spark SQL所有相关的代码。 欢迎加入知识星球，学习像本文一样直接调试源码，更加深入掌握Spark SQL～" />
<meta property="og:description" content="版权声明：本文为博主原创文章，未经博主允许不得转载。 https://blog.csdn.net/rlnLo2pNEfx9c/article/details/84901179 头条号上说过近期分享Spark SQL系列文章，前面在头条号上分享了Dataset API的基本操作和复杂操作，不知道下面大家有没有自己测试一下。 今天主要是分享Spark SQL Dataset数据源的分区特性，而且是第一弹的数据格式是partquet。 常见RDD分区 Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。 举几个浪尖在星球里分享比较多的例子，比如： Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。 Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。 普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。 这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。 准备数据 首先是由Seq数据集合生成一个Dataset val sales = spark.createDataFrame(Seq( &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2016, 110), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2017, 10), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 100), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 50), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 80), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 100), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 130), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 160), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2017, 200), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2017, 100), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2016, 150), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2015, 50), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2015, 30), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2015, 10), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2014, 200), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2014, 170), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 50), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 70), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 110), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 150), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 180), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2016, 30), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2015, 200), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2014, 20) &nbsp; &nbsp;)).toDF(&quot;city&quot;, &quot;year&quot;, &quot;amount&quot;) 将Dataset存处为partquet格式的hive表，分两种情况： 用city和year字段分区 sales.write.partitionBy(&quot;city&quot;,&quot;year&quot;).mode(SaveMode.Overwrite).saveAsTable(&quot;ParquetTestCityAndYear&quot;) 用city字段分区 sales.write.partitionBy(&quot;city&quot;).mode(SaveMode.Overwrite).saveAsTable(&quot;ParquetTestCity&quot;) 读取数据采用的是 val res = spark.read.parquet(&quot;/user/hive/warehouse/parquettestcity&quot;) 直接展示，结果发现结果会随着spark.default.parallelism变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。 1. spark.default.parallelism =40 Dataset的分区数是由参数： println(&quot;partition size = &quot;+res.rdd.partitions.length) 目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。 这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。 2. spark.default.parallelism =4 Dataset的分区数是由参数： println(&quot;partition size = &quot;+res.rdd.partitions.length) 目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。 那么数据源生成的Dataset的分区数到底是如何决定的呢？ 我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。 private def createNonBucketedReadRDD( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;readFile: (PartitionedFile) =&gt; Iterator[InternalRow], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;selectedPartitions: Seq[PartitionDirectory], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;fsRelation: HadoopFsRelation): RDD[InternalRow] = { &nbsp; &nbsp;/* &nbsp; &nbsp; &nbsp;selectedPartitions 的大小代表目录数目 &nbsp; &nbsp; */ &nbsp; &nbsp;println(&quot;selectedPartitions.size : &quot;+ selectedPartitions.size) &nbsp; &nbsp;val defaultMaxSplitBytes = &nbsp; &nbsp; &nbsp;fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes &nbsp; &nbsp;val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes &nbsp; &nbsp;// spark.default.parallelism &nbsp; &nbsp;val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism &nbsp; &nbsp;// 计算文件总大小，单位字节数 &nbsp; &nbsp;val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum &nbsp; &nbsp;//计算平均每个并行度读取数据大小 &nbsp; &nbsp;val bytesPerCore = totalBytes / defaultParallelism &nbsp; &nbsp;// 首先spark.sql.files.openCostInBytes 该参数配置的值和bytesPerCore 取最大值 &nbsp; &nbsp;// 然后，比较spark.sql.files.maxPartitionBytes 取小者 &nbsp; &nbsp;val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore)) &nbsp; &nbsp;logInfo(s&quot;Planning scan with bin packing, max size: $maxSplitBytes bytes, &quot; + &nbsp; &nbsp; &nbsp;s&quot;open cost is considered as scanning $openCostInBytes bytes.&quot;) &nbsp; &nbsp;// 这对目录遍历 &nbsp; &nbsp;val splitFiles = selectedPartitions.flatMap { partition =&gt; &nbsp; &nbsp; &nbsp;partition.files.flatMap { file =&gt; &nbsp; &nbsp; &nbsp; &nbsp;val blockLocations = getBlockLocations(file) &nbsp; &nbsp; &nbsp; &nbsp;//判断文件类型是否支持分割，以parquet为例，是支持分割的 &nbsp; &nbsp; &nbsp; &nbsp;if (fsRelation.fileFormat.isSplitable( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;fsRelation.sparkSession, fsRelation.options, file.getPath)) {// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;eg. 0 until 2不包括 2。相当于// &nbsp; &nbsp; &nbsp; &nbsp;println(0 until(10) by 3) 输出 Range(0, 3, 6, 9) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(0L until file.getLen by maxSplitBytes).map { offset =&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// 计算文件剩余的量 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val remaining = file.getLen - offset// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val size = if (remaining &gt; maxSplitBytes) maxSplitBytes else remaining// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;位置信息 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val hosts = getBlockHosts(blockLocations, offset, size) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;PartitionedFile( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partition.values, file.getPath.toUri.toString, offset, size, hosts) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp; &nbsp;} else {// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;不可分割的话，那即是一个文件一个分区 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val hosts = getBlockHosts(blockLocations, 0, file.getLen) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Seq(PartitionedFile( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts)) &nbsp; &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp;}.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse) &nbsp; &nbsp;val partitions = new ArrayBuffer[FilePartition] &nbsp; &nbsp;val currentFiles = new ArrayBuffer[PartitionedFile] &nbsp; &nbsp;var currentSize = 0L &nbsp; &nbsp;/** Close the current partition and move to the next. */ &nbsp; &nbsp;def closePartition(): Unit = { &nbsp; &nbsp; &nbsp;if (currentFiles.nonEmpty) { &nbsp; &nbsp; &nbsp; &nbsp;val newPartition = &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;FilePartition( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partitions.size, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;currentFiles.toArray.toSeq) // Copy to a new Array. &nbsp; &nbsp; &nbsp; &nbsp;partitions += newPartition &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp;currentFiles.clear() &nbsp; &nbsp; &nbsp;currentSize = 0 &nbsp; &nbsp;} &nbsp; &nbsp;// Assign files to partitions using &quot;Next Fit Decreasing&quot; &nbsp; &nbsp;splitFiles.foreach { file =&gt; &nbsp; &nbsp; &nbsp;if (currentSize + file.length &gt; maxSplitBytes) { &nbsp; &nbsp; &nbsp; &nbsp;closePartition() &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp;// Add the given file to the current partition. &nbsp; &nbsp; &nbsp;currentSize += file.length + openCostInBytes &nbsp; &nbsp; &nbsp;currentFiles += file &nbsp; &nbsp;} &nbsp; &nbsp;closePartition() &nbsp; &nbsp;println(&quot;FileScanRDD partitions size : &quot;+partitions.size) &nbsp; &nbsp;new FileScanRDD(fsRelation.sparkSession, readFile, partitions) &nbsp;} 这次视频上传到了今日头条了，可以点击阅读原文观看。 知识星球球友可以直接获取视频及所有spark SQL所有相关的代码。 欢迎加入知识星球，学习像本文一样直接调试源码，更加深入掌握Spark SQL～" />
<link rel="canonical" href="https://mlh.app/2018/12/09/5cde183f3a08701243dca85d6107b9f5.html" />
<meta property="og:url" content="https://mlh.app/2018/12/09/5cde183f3a08701243dca85d6107b9f5.html" />
<meta property="og:site_name" content="有组织在！" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-12-09T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"版权声明：本文为博主原创文章，未经博主允许不得转载。 https://blog.csdn.net/rlnLo2pNEfx9c/article/details/84901179 头条号上说过近期分享Spark SQL系列文章，前面在头条号上分享了Dataset API的基本操作和复杂操作，不知道下面大家有没有自己测试一下。 今天主要是分享Spark SQL Dataset数据源的分区特性，而且是第一弹的数据格式是partquet。 常见RDD分区 Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。 举几个浪尖在星球里分享比较多的例子，比如： Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。 Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。 普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。 这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。 准备数据 首先是由Seq数据集合生成一个Dataset val sales = spark.createDataFrame(Seq( &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2016, 110), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2017, 10), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 100), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 50), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 80), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 100), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 130), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2015, 160), &nbsp; &nbsp; &nbsp;(&quot;Warsaw&quot;, 2017, 200), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2017, 100), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2016, 150), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2015, 50), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2015, 30), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2015, 10), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2014, 200), &nbsp; &nbsp; &nbsp;(&quot;Beijing&quot;, 2014, 170), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 50), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 70), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 110), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 150), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2017, 180), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2016, 30), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2015, 200), &nbsp; &nbsp; &nbsp;(&quot;Boston&quot;, 2014, 20) &nbsp; &nbsp;)).toDF(&quot;city&quot;, &quot;year&quot;, &quot;amount&quot;) 将Dataset存处为partquet格式的hive表，分两种情况： 用city和year字段分区 sales.write.partitionBy(&quot;city&quot;,&quot;year&quot;).mode(SaveMode.Overwrite).saveAsTable(&quot;ParquetTestCityAndYear&quot;) 用city字段分区 sales.write.partitionBy(&quot;city&quot;).mode(SaveMode.Overwrite).saveAsTable(&quot;ParquetTestCity&quot;) 读取数据采用的是 val res = spark.read.parquet(&quot;/user/hive/warehouse/parquettestcity&quot;) 直接展示，结果发现结果会随着spark.default.parallelism变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。 1. spark.default.parallelism =40 Dataset的分区数是由参数： println(&quot;partition size = &quot;+res.rdd.partitions.length) 目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。 这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。 2. spark.default.parallelism =4 Dataset的分区数是由参数： println(&quot;partition size = &quot;+res.rdd.partitions.length) 目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。 那么数据源生成的Dataset的分区数到底是如何决定的呢？ 我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。 private def createNonBucketedReadRDD( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;readFile: (PartitionedFile) =&gt; Iterator[InternalRow], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;selectedPartitions: Seq[PartitionDirectory], &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;fsRelation: HadoopFsRelation): RDD[InternalRow] = { &nbsp; &nbsp;/* &nbsp; &nbsp; &nbsp;selectedPartitions 的大小代表目录数目 &nbsp; &nbsp; */ &nbsp; &nbsp;println(&quot;selectedPartitions.size : &quot;+ selectedPartitions.size) &nbsp; &nbsp;val defaultMaxSplitBytes = &nbsp; &nbsp; &nbsp;fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes &nbsp; &nbsp;val openCostInBytes = fsRelation.sparkSession.sessionState.conf.filesOpenCostInBytes &nbsp; &nbsp;// spark.default.parallelism &nbsp; &nbsp;val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism &nbsp; &nbsp;// 计算文件总大小，单位字节数 &nbsp; &nbsp;val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum &nbsp; &nbsp;//计算平均每个并行度读取数据大小 &nbsp; &nbsp;val bytesPerCore = totalBytes / defaultParallelism &nbsp; &nbsp;// 首先spark.sql.files.openCostInBytes 该参数配置的值和bytesPerCore 取最大值 &nbsp; &nbsp;// 然后，比较spark.sql.files.maxPartitionBytes 取小者 &nbsp; &nbsp;val maxSplitBytes = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore)) &nbsp; &nbsp;logInfo(s&quot;Planning scan with bin packing, max size: $maxSplitBytes bytes, &quot; + &nbsp; &nbsp; &nbsp;s&quot;open cost is considered as scanning $openCostInBytes bytes.&quot;) &nbsp; &nbsp;// 这对目录遍历 &nbsp; &nbsp;val splitFiles = selectedPartitions.flatMap { partition =&gt; &nbsp; &nbsp; &nbsp;partition.files.flatMap { file =&gt; &nbsp; &nbsp; &nbsp; &nbsp;val blockLocations = getBlockLocations(file) &nbsp; &nbsp; &nbsp; &nbsp;//判断文件类型是否支持分割，以parquet为例，是支持分割的 &nbsp; &nbsp; &nbsp; &nbsp;if (fsRelation.fileFormat.isSplitable( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;fsRelation.sparkSession, fsRelation.options, file.getPath)) {// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;eg. 0 until 2不包括 2。相当于// &nbsp; &nbsp; &nbsp; &nbsp;println(0 until(10) by 3) 输出 Range(0, 3, 6, 9) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(0L until file.getLen by maxSplitBytes).map { offset =&gt; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// 计算文件剩余的量 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val remaining = file.getLen - offset// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val size = if (remaining &gt; maxSplitBytes) maxSplitBytes else remaining// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;位置信息 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val hosts = getBlockHosts(blockLocations, offset, size) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;PartitionedFile( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partition.values, file.getPath.toUri.toString, offset, size, hosts) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp; &nbsp;} else {// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;不可分割的话，那即是一个文件一个分区 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val hosts = getBlockHosts(blockLocations, 0, file.getLen) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Seq(PartitionedFile( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partition.values, file.getPath.toUri.toString, 0, file.getLen, hosts)) &nbsp; &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp;}.toArray.sortBy(_.length)(implicitly[Ordering[Long]].reverse) &nbsp; &nbsp;val partitions = new ArrayBuffer[FilePartition] &nbsp; &nbsp;val currentFiles = new ArrayBuffer[PartitionedFile] &nbsp; &nbsp;var currentSize = 0L &nbsp; &nbsp;/** Close the current partition and move to the next. */ &nbsp; &nbsp;def closePartition(): Unit = { &nbsp; &nbsp; &nbsp;if (currentFiles.nonEmpty) { &nbsp; &nbsp; &nbsp; &nbsp;val newPartition = &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;FilePartition( &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partitions.size, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;currentFiles.toArray.toSeq) // Copy to a new Array. &nbsp; &nbsp; &nbsp; &nbsp;partitions += newPartition &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp;currentFiles.clear() &nbsp; &nbsp; &nbsp;currentSize = 0 &nbsp; &nbsp;} &nbsp; &nbsp;// Assign files to partitions using &quot;Next Fit Decreasing&quot; &nbsp; &nbsp;splitFiles.foreach { file =&gt; &nbsp; &nbsp; &nbsp;if (currentSize + file.length &gt; maxSplitBytes) { &nbsp; &nbsp; &nbsp; &nbsp;closePartition() &nbsp; &nbsp; &nbsp;} &nbsp; &nbsp; &nbsp;// Add the given file to the current partition. &nbsp; &nbsp; &nbsp;currentSize += file.length + openCostInBytes &nbsp; &nbsp; &nbsp;currentFiles += file &nbsp; &nbsp;} &nbsp; &nbsp;closePartition() &nbsp; &nbsp;println(&quot;FileScanRDD partitions size : &quot;+partitions.size) &nbsp; &nbsp;new FileScanRDD(fsRelation.sparkSession, readFile, partitions) &nbsp;} 这次视频上传到了今日头条了，可以点击阅读原文观看。 知识星球球友可以直接获取视频及所有spark SQL所有相关的代码。 欢迎加入知识星球，学习像本文一样直接调试源码，更加深入掌握Spark SQL～","@type":"BlogPosting","url":"https://mlh.app/2018/12/09/5cde183f3a08701243dca85d6107b9f5.html","headline":"源码:Spark SQL 分区特性第一弹","dateModified":"2018-12-09T00:00:00+08:00","datePublished":"2018-12-09T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlh.app/2018/12/09/5cde183f3a08701243dca85d6107b9f5.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script src="/assets/js/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123344652-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-123344652-3');
    </script>
    
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-8889449066804352",
        enable_page_level_ads: true
      });
    </script>
    
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
    
    <style>
      @media screen and (max-width:760px){
        .sm-hidden{display:none; }
      }
    </style>

  </head>
  <body>
    
        <amp-auto-ads type="adsense"
              data-ad-client="ca-pub-8889449066804352">
        </amp-auto-ads>
    
    <div class="wrapper">
      <header  class="without-description" >
        <h1>源码:Spark SQL 分区特性第一弹</h1>
        
        
        <ul>
            <li><a href="https://uzshare.com/" style="line-height: unset;" target="_blank"><strong>柚子社区</strong></a></li>
        </ul>
        
        
        
      </header>
      <section>

<div style="margin:0 0 8px 0;">
<style>
table.gsc-input {
    margin: 0;
}
.cse .gsc-control-cse, .gsc-control-cse {
    padding: 0;
    width: auto;
}
.gsc-search-box td {
    border-bottom: none;
}
</style>
<script>
  (function() {
    var cx = '004431708863642777669:qan2_6ugotw';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</div>
	

        <div id="article_content" class="article_content clearfix csdn-tracking-statistics" data-pid="blog" data-mod="popu_307" data-dsm="post"> 
 <div class="article-copyright">
   版权声明：本文为博主原创文章，未经博主允许不得转载。 https://blog.csdn.net/rlnLo2pNEfx9c/article/details/84901179 
 </div> 
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d7e2a68c7c.css"> 
 <div class="htmledit_views" id="content_views"> 
  <div class="rich_media_content" id="js_content"> 
   <p>头条号上说过近期分享Spark SQL系列文章，前面在头条号上分享了Dataset API的基本操作和复杂操作，不知道下面大家有没有自己测试一下。<br></p>
   <p>今天主要是分享Spark SQL Dataset数据源的分区特性，而且是第一弹的数据格式是partquet。</p>
   <p><strong>常见RDD分区</strong></p>
   <p>Spark Core 中的RDD的分区特性大家估计都很了解，这里说的分区特性是指从数据源读取数据的第一个RDD或者Dataset的分区，而后续再介绍转换过程中分区的变化。</p>
   <p>举几个浪尖在星球里分享比较多的例子，比如：</p>
   <ol class="list-paddingleft-2" style="list-style-type:decimal;">
    <li><p>Spark Streaming 与kafka 结合 DirectDstream 生成的微批RDD（kafkardd）分区数和kafka分区数一样。</p></li>
    <li><p>Spark Streaming 与kafka结合 基于receiver的方式，生成的微批RDD（blockRDD），分区数就是block数。</p></li>
    <li><p>普通的文件RDD，那么分可分割和不可分割，通常不可分割的分区数就是文件数。可分割需要计算而且是有条件的，在星球里分享过了。</p></li>
   </ol>
   <p>这些都很简单，那么今天咱们要谈的是Spark DataSet的分区数的决定因素。</p>
   <p><strong>准备数据</strong></p>
   <p>首先是由Seq数据集合生成一个Dataset</p>
   <pre style="background:none;"></pre>
   <p style="font-size:.85em;background:rgb(40,44,52);color:rgb(171,178,191);"><span class="hljs-default-attribute" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:21px;font-weight:400;font-style:normal;">val</span> sales = spark.createDataFrame(Seq(<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Warsaw"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2016</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">110</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Warsaw"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2017</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">10</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Warsaw"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2015</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">100</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Warsaw"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2015</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">50</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Warsaw"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2015</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">80</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Warsaw"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2015</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">100</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Warsaw"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2015</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">130</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Warsaw"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2015</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">160</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Warsaw"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2017</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">200</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:65px;font-weight:400;font-style:normal;">"Beijing"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">2017</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">100</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:65px;font-weight:400;font-style:normal;">"Beijing"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">2016</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">150</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:65px;font-weight:400;font-style:normal;">"Beijing"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">2015</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:15px;font-weight:400;font-style:normal;">50</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:65px;font-weight:400;font-style:normal;">"Beijing"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">2015</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:15px;font-weight:400;font-style:normal;">30</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:65px;font-weight:400;font-style:normal;">"Beijing"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">2015</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:15px;font-weight:400;font-style:normal;">10</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:65px;font-weight:400;font-style:normal;">"Beijing"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">2014</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">200</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:65px;font-weight:400;font-style:normal;">"Beijing"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">2014</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">170</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Boston"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2017</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">50</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Boston"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2017</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">70</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Boston"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2017</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">110</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Boston"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2017</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">150</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Boston"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2017</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">180</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Boston"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2016</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">30</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Boston"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2015</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">200</span>),<br> &nbsp; &nbsp; &nbsp;(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"Boston"</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">2014</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">20</span>)<br> &nbsp; &nbsp;)).toDF(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:44px;font-weight:400;font-style:normal;">"city"</span>, <span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:44px;font-weight:400;font-style:normal;">"year"</span>, <span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:58px;font-weight:400;font-style:normal;">"amount"</span>)</p>
   <p>将Dataset存处为partquet格式的hive表，分两种情况：</p>
   <p>用city和year字段分区</p>
   <pre style="background:none;"></pre>
   <p style="font-size:.85em;background:rgb(40,44,52);color:rgb(171,178,191);"><span class="hljs-default-selector-tag" style="color:rgb(224,108,117);background:rgba(0,0,0,0);width:36px;font-weight:400;font-style:normal;">sales</span><span class="hljs-default-selector-class" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:43px;font-weight:400;font-style:normal;">.write</span><span class="hljs-default-selector-class" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:87px;font-weight:400;font-style:normal;">.partitionBy</span>(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:44px;font-weight:400;font-style:normal;">"city"</span>,<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:43px;font-weight:400;font-style:normal;">"year"</span>)<span class="hljs-default-selector-class" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:36px;font-weight:400;font-style:normal;">.mode</span>(SaveMode.Overwrite)<span class="hljs-default-selector-class" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:87px;font-weight:400;font-style:normal;">.saveAsTable</span>(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:173px;font-weight:400;font-style:normal;">"ParquetTestCityAndYear"</span>)</p>
   <p>用city字段分区</p>
   <pre style="background:none;"></pre>
   <p style="font-size:.85em;background:rgb(40,44,52);color:rgb(171,178,191);"><span class="hljs-default-selector-tag" style="color:rgb(224,108,117);background:rgba(0,0,0,0);width:36px;font-weight:400;font-style:normal;">sales</span><span class="hljs-default-selector-class" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:43px;font-weight:400;font-style:normal;">.write</span><span class="hljs-default-selector-class" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:87px;font-weight:400;font-style:normal;">.partitionBy</span>(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:44px;font-weight:400;font-style:normal;">"city"</span>)<span class="hljs-default-selector-class" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:36px;font-weight:400;font-style:normal;">.mode</span>(SaveMode.Overwrite)<span class="hljs-default-selector-class" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:87px;font-weight:400;font-style:normal;">.saveAsTable</span>(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:123px;font-weight:400;font-style:normal;">"ParquetTestCity"</span>)</p>
   <p>读取数据采用的是<br></p>
   <pre style="background:none;"></pre>
   <p style="font-size:.85em;background:rgb(40,44,52);color:rgb(171,178,191);">val <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:21px;font-weight:400;font-style:normal;">res</span> = spark.<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">read</span>.parquet(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:275px;font-weight:400;font-style:normal;">"/user/hive/warehouse/parquettestcity"</span>)</p>
   <p>直接展示，结果发现结果会随着<span style="background-color:rgb(255,255,255);color:rgb(68,68,68);font-family:Menlo, 'Lucida Console', monospace;font-size:12px;">spark.default.parallelism</span><span style="background-color:rgb(255,255,255);color:rgb(68,68,68);font-family:Menlo, 'Lucida Console', monospace;font-size:16px;">变化而变化。文章里只读取city字段分区的数据，特点就是只有单个分区字段。</span></p>
   <p><strong>1. spark.default.parallelism =40</strong></p>
   <p>Dataset的分区数是由参数：</p>
   <pre style="background-image:none;"><code class="hljs-default" style="margin-left:.15em;font-size:.85em;background:rgb(40,44,52);color:rgb(171,178,191);">println(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:137px;">"partition size = "</span>+<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:22px;">res</span>.rdd.partitions.length)</code></pre>
   <p>目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。</p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/adI0ApTVBFUibDu2gaX89M600Ip3MDrVYQdrjhFibsnK47og5cdZ77AXMDFVGAaEZLibeicsR1hw5icr1wC26kM7lRA/640" alt="640"></p>
   <p style="text-align:left;">这个分区数目正好是文件数，那么假如不了解细节的话，肯定会认为分区数就是由文件数决定的，其实不然。<br></p>
   <p><strong>2. spark.default.parallelism =4</strong></p>
   <p>Dataset的分区数是由参数：</p>
   <pre style="background:none;"><code style="font-size:.85em;background:rgb(40,44,52);color:rgb(171,178,191);" class="hljs-default">println(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:137px;font-weight:400;font-style:normal;">"partition size = "</span>+<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">res</span>.rdd.partitions.length)</code></pre>
   <p style="text-align:left;">目录数和生成的FileScanRDD的分区数分别数下面截图的第一行和第二行。<br></p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/adI0ApTVBFUibDu2gaX89M600Ip3MDrVYZKkgN5JLzbDDNNMDeKw4dmCGjFCdHvBPia5am813qyQbV0a7p8UgEuQ/640" alt="640"></p>
   <p style="text-align:left;">那么数据源生成的Dataset的分区数到底是如何决定的呢？</p>
   <p style="text-align:left;">我们这种情况，我只能告诉你是由下面的函数在生成FileScanRDD的时候计算得到的，具体计算细节可以仔细阅读该函数。该函数是类FileSourceScanExec的方法。</p>
   <pre style="background:none;"></pre>
   <p style="font-size:.85em;background:rgb(40,44,52);color:rgb(171,178,191);">private def createNonBucketedReadRDD(<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;readFile: (PartitionedFile) =&gt; Iterator[InternalRow],<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;selectedPartition<span class="hljs-default-variable" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">s:</span> Seq[PartitionDirectory],<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;fsRelation: HadoopFsRelation): RDD[InternalRow] = {<br> &nbsp; &nbsp;/*<br> &nbsp; &nbsp; &nbsp;selectedPartitions 的大小代表目录数目<br> &nbsp; &nbsp; */<br> &nbsp; &nbsp;println(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:203px;font-weight:400;font-style:normal;">"selectedPartitions.size : "</span>+ selectedPartitions.size)<br> &nbsp; &nbsp;val defaultMaxSplitBytes =<br> &nbsp; &nbsp; &nbsp;fsRelation.sparkSession.sessionState.<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">conf</span>.filesMaxPartitionBytes<br> &nbsp; &nbsp;val openCostInBytes = fsRelation.sparkSession.sessionState.<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">conf</span>.filesOpenCostInBytes<br><br> &nbsp; &nbsp;// spark.default.parallelism<br> &nbsp; &nbsp;val defaultParallelism = fsRelation.sparkSession.sparkContext.defaultParallelism<br><br> &nbsp; &nbsp;// 计算文件总大小，单位字节数<br> &nbsp; &nbsp;val totalBytes = selectedPartitions.flatMap(_.<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:36px;font-weight:400;font-style:normal;">files</span>.<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">map</span>(_.getLen + openCostInBytes)).sum<br><br> &nbsp; &nbsp;//计算平均每个并行度读取数据大小<br> &nbsp; &nbsp;val bytesPerCore = totalBytes / defaultParallelism<br><br> &nbsp; &nbsp;// 首先spark.sql.<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:36px;font-weight:400;font-style:normal;">files</span>.openCostInBytes 该参数配置的值和bytesPerCore 取最大值<br> &nbsp; &nbsp;// 然后，比较spark.sql.<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:36px;font-weight:400;font-style:normal;">files</span>.maxPartitionBytes 取小者<br> &nbsp; &nbsp;val maxSplitBytes = Math.<span class="hljs-default-built_in" style="color:rgb(230,192,123);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">min</span>(defaultMaxSplitBytes, Math.<span class="hljs-default-built_in" style="color:rgb(230,192,123);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">max</span>(openCostInBytes, bytesPerCore))<br> &nbsp; &nbsp;logInfo(s<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:477px;font-weight:400;font-style:normal;">"Planning scan with bin packing, max size: $maxSplitBytes bytes, "</span> +<br> &nbsp; &nbsp; &nbsp;s<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:441px;font-weight:400;font-style:normal;">"open cost is considered as scanning $openCostInBytes bytes."</span>)<br><br> &nbsp; &nbsp;// 这对目录遍历<br> &nbsp; &nbsp;val splitFiles = selectedPartitions.flatMap { partition =&gt;<br> &nbsp; &nbsp; &nbsp;partition.<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:37px;font-weight:400;font-style:normal;">files</span>.flatMap { <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span> =&gt;<br> &nbsp; &nbsp; &nbsp; &nbsp;val blockLocations = getBlockLocations(<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span>)<br><br> &nbsp; &nbsp; &nbsp; &nbsp;//判断文件类型是否支持分割，以parquet为例，是支持分割的<br> &nbsp; &nbsp; &nbsp; &nbsp;<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">if</span> (fsRelation.fileFormat.isSplitable(<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;fsRelation.sparkSession, fsRelation.<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:51px;font-weight:400;font-style:normal;">options</span>, <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span>.getPath)) {<br><br>// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;eg. <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:8px;font-weight:400;font-style:normal;">0</span> until <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:7px;font-weight:400;font-style:normal;">2</span>不包括 <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:7px;font-weight:400;font-style:normal;">2</span>。相当于<br>// &nbsp; &nbsp; &nbsp; &nbsp;println(<span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:7px;font-weight:400;font-style:normal;">0</span> until(<span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">10</span>) by <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:7px;font-weight:400;font-style:normal;">3</span>) 输出 Range(<span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:8px;font-weight:400;font-style:normal;">0</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:7px;font-weight:400;font-style:normal;">3</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:7px;font-weight:400;font-style:normal;">6</span>, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:8px;font-weight:400;font-style:normal;">9</span>)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;(<span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:7px;font-weight:400;font-style:normal;">0</span>L until <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span>.getLen by maxSplitBytes).<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">map</span> { offset =&gt;<br><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;// 计算文件剩余的量<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val remaining = <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span>.getLen - offset<br><br>// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;假如剩余量不足 maxSplitBytes 那么就剩余的作为一个分区<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val size = <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">if</span> (remaining &gt; maxSplitBytes) maxSplitBytes <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">else</span> remaining<br><br>// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;位置信息<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val hosts = getBlockHosts(blockLocations, offset, size)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;PartitionedFile(<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partition.<span class="hljs-default-built_in" style="color:rgb(230,192,123);background:rgba(0,0,0,0);width:44px;font-weight:400;font-style:normal;">values</span>, <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span>.getPath.toUri.toString, offset, size, hosts)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}<br> &nbsp; &nbsp; &nbsp; &nbsp;} <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">else</span> {<br>// &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;不可分割的话，那即是一个文件一个分区<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;val hosts = getBlockHosts(blockLocations, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:8px;font-weight:400;font-style:normal;">0</span>, <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span>.getLen)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Seq(PartitionedFile(<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partition.<span class="hljs-default-built_in" style="color:rgb(230,192,123);background:rgba(0,0,0,0);width:43px;font-weight:400;font-style:normal;">values</span>, <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">file</span>.getPath.toUri.toString, <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:7px;font-weight:400;font-style:normal;">0</span>, <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span>.getLen, hosts))<br> &nbsp; &nbsp; &nbsp; &nbsp;}<br> &nbsp; &nbsp; &nbsp;}<br> &nbsp; &nbsp;}.toArray.sortBy(_.length)(implicitly[Ordering[Long]].<span class="hljs-default-built_in" style="color:rgb(230,192,123);background:rgba(0,0,0,0);width:50px;font-weight:400;font-style:normal;">reverse</span>)<br><br> &nbsp; &nbsp;val partitions = <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:21px;font-weight:400;font-style:normal;">new</span> ArrayBuffer[FilePartition]<br> &nbsp; &nbsp;val currentFiles = <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">new</span> ArrayBuffer[PartitionedFile]<br> &nbsp; &nbsp;var currentSize = <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:7px;font-weight:400;font-style:normal;">0</span>L<br><br> &nbsp; &nbsp;/** Close the current partition <span class="hljs-default-built_in" style="color:rgb(230,192,123);background:rgba(0,0,0,0);width:22px;font-weight:400;font-style:normal;">and</span> <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">move</span> <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">to</span> the <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">next</span>. */<br> &nbsp; &nbsp;def closePartition(): Unit = {<br> &nbsp; &nbsp; &nbsp;<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:15px;font-weight:400;font-style:normal;">if</span> (currentFiles.nonEmpty) {<br> &nbsp; &nbsp; &nbsp; &nbsp;val newPartition =<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;FilePartition(<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;partitions.size,<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;currentFiles.toArray.toSeq) // Copy <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:14px;font-weight:400;font-style:normal;">to</span> <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:7px;font-weight:400;font-style:normal;">a</span> <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:21px;font-weight:400;font-style:normal;">new</span> Array.<br> &nbsp; &nbsp; &nbsp; &nbsp;partitions += newPartition<br> &nbsp; &nbsp; &nbsp;}<br> &nbsp; &nbsp; &nbsp;currentFiles.clear()<br> &nbsp; &nbsp; &nbsp;currentSize = <span class="hljs-default-number" style="color:rgb(209,154,102);background:rgba(0,0,0,0);width:8px;font-weight:400;font-style:normal;">0</span><br> &nbsp; &nbsp;}<br><br> &nbsp; &nbsp;// Assign <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:36px;font-weight:400;font-style:normal;">files</span> <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:15px;font-weight:400;font-style:normal;">to</span> partitions using <span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:151px;font-weight:400;font-style:normal;">"Next Fit Decreasing"</span><br> &nbsp; &nbsp;splitFiles.foreach { <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span> =&gt;<br> &nbsp; &nbsp; &nbsp;<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:15px;font-weight:400;font-style:normal;">if</span> (currentSize + <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span>.length &gt; maxSplitBytes) {<br> &nbsp; &nbsp; &nbsp; &nbsp;closePartition()<br> &nbsp; &nbsp; &nbsp;}<br> &nbsp; &nbsp; &nbsp;// Add the given <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span> <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:15px;font-weight:400;font-style:normal;">to</span> the current partition.<br> &nbsp; &nbsp; &nbsp;currentSize += <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:28px;font-weight:400;font-style:normal;">file</span>.length + openCostInBytes<br> &nbsp; &nbsp; &nbsp;currentFiles += <span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:29px;font-weight:400;font-style:normal;">file</span><br> &nbsp; &nbsp;}<br> &nbsp; &nbsp;closePartition()<br><br> &nbsp; &nbsp;println(<span class="hljs-default-string" style="color:rgb(152,195,121);background:rgba(0,0,0,0);width:232px;font-weight:400;font-style:normal;">"FileScanRDD partitions size : "</span>+partitions.size)<br> &nbsp; &nbsp;<span class="hljs-default-keyword" style="color:rgb(198,120,221);background:rgba(0,0,0,0);width:21px;font-weight:400;font-style:normal;">new</span> FileScanRDD(fsRelation.sparkSession, readFile, partitions)<br> &nbsp;}</p>
   <p><br></p>
   <p>这次视频上传到了今日头条了，可以点击<strong>阅读原文</strong>观看。</p>
   <p><br></p>
   <p>知识星球球友可以直接获取视频及所有spark SQL所有相关的代码。</p>
   <p><br></p>
   <p>欢迎加入知识星球，学习像本文一样直接调试源码，更加深入掌握Spark SQL～</p>
   <p style="text-align:center;"><img src="https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/adI0ApTVBFUibDu2gaX89M600Ip3MDrVY0ibqsWg1gHuTdGCX1QfbyyCPlf31gLQna9OvNyRsBaByXsZrZeqibtVQ/640" alt="640"></p> 
  </div> 
 </div> 
</div> 
<div class="hide-article-box text-center">  
</div>

	<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
	<!-- 自定义广告 -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-8889449066804352"
	     data-ad-slot="1494696990"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
		(adsbygoogle = window.adsbygoogle || []).push({});
	</script>


        <br />
        <a href="https://uzshare.com/">更多精彩内容</a>
      </section>
      
      <header style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
        <ul style="display: block;">
          <li><a href="/" style="line-height: 40px;padding-top:0px;">回首页</a></li>
        </ul>
      </header>
      <header class="sm-hidden" style="right: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/imgqcode.png" style="width:160px;" />
      </header>
      
      <header class="sm-hidden" style="left: 0;position: fixed;bottom: 60px;z-index: 100;background: none;border-bottom:none;">
          <img src="https://uzzz.org.cn/hou_imgqcode.png" style="width:160px;">
      </header>
    </div>
    
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->

    <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?0d1dbe5a3e5863242418b768d1601633";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
    </script>

  </body>
</html>
